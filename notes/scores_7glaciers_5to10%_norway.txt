train_glaciers = [
    'Tunsbergdalsbreen',
    'Austre Memurubreen',
    'Svartisheibreen',
    'Bondhusbrea',
    'Harbardsbreen',
    'Moesevassbrea',
    'Graasubreen'
]

'length train set'5802
'length test set'51163

Unique POINT_IDs: 605


==================NOR Flexible 7 Glaciers 5-10% fine tuning=======================
=================Nothing fozen just fine tuning==============
---
lr=0.0001,
max_epochs=10,
{'score': -1.527253633473294,
 'mse': 1.5272536512513448,
 'rmse': 1.2358210433761616,
 'mae': 0.9027927257212641,
 'pearson': 0.9038018047719645}

lr=0.0001,
max_epochs=20,
{'score': -1.42968364707421,
 'mse': 1.4296836413364888,
 'rmse': 1.195693790791141,
 'mae': 0.875304165837915,
 'pearson': 0.9059157622647052}

lr=0.0001,
max_epochs=30,
{'score': -1.383091092094759,
 'mse': 1.3830911122689886,
 'rmse': 1.1760489412728488,
 'mae': 0.8666876004969712,
 'pearson': 0.9035236460868111}

lr=0.0001,
max_epochs=50,
{'score': -1.3368878342208765,
 'mse': 1.3368878328725518,
 'rmse': 1.1562386574027663,
 'mae': 0.8626704500409826,
 'pearson': 0.90070695185345}

lr=0.0001,
max_epochs=200, stopped at 97
{'score': -1.3037573080952247,
 'mse': 1.3037573195105832,
 'rmse': 1.1418219298605992,            BEST
 'mae': 0.8576647954654492,
 'pearson': 0.898982464740716}

----

lr=0.00005,
max_epochs=10,
{'score': -1.7468020937820417,
 'mse': 1.7468020966338087,
 'rmse': 1.3216664089829206,
 'mae': 0.9593760394390992,
 'pearson': 0.8954512154311189}

lr=0.00005,
max_epochs=50,
{'score': -1.3811703047008261,
 'mse': 1.38117029922141,
 'rmse': 1.1752320193142332,
 'mae': 0.8650145589141215,
 'pearson': 0.9062976024585865}

lr=0.00005,
max_epochs=200, stopped at 147
{'score': -1.3373839594176455,
 'mse': 1.3373839641468293,
 'rmse': 1.1564531828599156,            BEST
 'mae': 0.862894169446329,
 'pearson': 0.8989977954364337}

==================Last 2 Layers unfrozen=======================

lr=0.0001,
max_epochs=10,
{'score': -1.8805614315891164,
 'mse': 1.880561435165434,
 'rmse': 1.3713356391363254,
 'mae': 0.9970809195270547,
 'pearson': 0.8808182542456888}

lr=0.0001,
max_epochs=50,
{'score': -1.6895992541254874,
 'mse': 1.6895992735521626,
 'rmse': 1.299845865305638,
 'mae': 0.9439578751795334,
 'pearson': 0.8863253915610869}

lr=0.0001,
max_epochs=200, stopped at 97
{'score': -1.6514917256079789,
 'mse': 1.6514917241640876,
 'rmse': 1.285103779530699,
 'mae': 0.932623656897208,
 'pearson': 0.8867601441569892}

----
lr=0.0005,
max_epochs=200, stopped at 97
{'score': -1.5283975406840187,
 'mse': 1.5283975570226755,
 'rmse': 1.2362837688098456,
 'mae': 0.8953012819834033,
 'pearson': 0.8973785964046956}

lr=0.0005,
max_epochs=50,
{'score': -1.6349790163181315,
 'mse': 1.6349790063171188,
 'rmse': 1.2786629760484656,
 'mae': 0.9206045252904472,
 'pearson': 0.8941198804795577}

lr=0.0005,
max_epochs=20,
{'score': -1.605153057920862,
 'mse': 1.6051530508665628,
 'rmse': 1.266946348850875,
 'mae': 0.9235158377881549,
 'pearson': 0.8897471241527005}

lr=0.0005,
max_epochs=10,
{'score': -1.712498666998695,
 'mse': 1.7124986506411501,
 'rmse': 1.3086247172666234,
 'mae': 0.9473006078622779,
 'pearson': 0.8832114798646199}

 -----
==================Last 3 Layers unfrozen=======================

lr=0.0005,
max_epochs=10,
{'score': -1.512374066991115,
'mse': 1.5123740793663452,
'rmse': 1.2297861925417544,
'mae': 0.8947046285408281,
'pearson': 0.8979384107834876}

lr=0.0005,
max_epochs=20,
{'score': -1.4156461615546947,
 'mse': 1.4156461507241065,
 'rmse': 1.189809291745575,
 'mae': 0.8767418960212935,
 'pearson': 0.9007728394650398}

lr=0.0005,
max_epochs=50,
{'score': -1.370003989170182,
 'mse': 1.3700039830357424,
 'rmse': 1.1704716925392695,
 'mae': 0.8691429490724547,
 'pearson': 0.8975314753721871}

lr=0.0005,
max_epochs=200, stopped at 82
{'score': -1.3429972605798264,
 'mse': 1.342997268516202,
 'rmse': 1.1588775899620296,
 'mae': 0.864980032816353,
 'pearson': 0.8975553143582725}


=================Progressive unfreezing====================

layer_groups = [
        (['model.16.weight', 'model.16.bias'], 50, 0.001),
        (['model.12.weight', 'model.12.bias', 'model.13.weight', 'model.13.bias'], 50, 0.0005),
        (['model.8.weight', 'model.8.bias', 'model.9.weight', 'model.9.bias'], 50, 0.0001)
    ]
{'score': -1.472524080275597,
 'mse': 1.4725240801341217,
 'rmse': 1.2134760319570064,
 'mae': 0.8870180036795717,
 'pearson': 0.8963181377267898}


layer_groups = [
        (['model.16.weight', 'model.16.bias'], 200, 0.001),     stopped at 97
        (['model.12.weight', 'model.12.bias', 'model.13.weight', 'model.13.bias'], 200, 0.0005),    stopped at 185
        (['model.8.weight', 'model.8.bias', 'model.9.weight', 'model.9.bias'], 200, 0.0001)     stopped at 281
    ]
{'score': -1.4525790659466185,
 'mse': 1.4525790686672455,
 'rmse': 1.2052298820835987,
 'mae': 0.8829701068068543,
 'pearson': 0.8976445170539966}


layer_groups = [
        (['model.16.weight', 'model.16.bias'], 200, 0.001),     stopped at 97
        (['model.12.weight', 'model.12.bias', 'model.13.weight', 'model.13.bias'], 200, 0.001), stopped at 231
        (['model.8.weight', 'model.8.bias', 'model.9.weight', 'model.9.bias'], 200, 0.001) stopped at 265
    ]
{'score': -1.3301062596748736,
 'mse': 1.330106276249921,
 'rmse': 1.1533023351445713,          BEST
 'mae': 0.8679021041499562,
 'pearson': 0.8970204460273291}

=================All linear frozen all batchnorm unfrozen====================

lr=0.0005,
max_epochs=200, stopped at 128
{'score': -1.4449785732502851,
 'mse': 1.4449785595513711,
 'rmse': 1.2020726099330985,
 'mae': 0.8782542392617813,
 'pearson': 0.9057569831782969}

lr=0.0005,
max_epochs=50,
{'score': -1.5763194285742543,
 'mse': 1.5763194315060836,
 'rmse': 1.2555156038481097,
 'mae': 0.9149778645505584,
 'pearson': 0.9009344017143569}

-------

lr=0.001,
max_epochs=50,
{'score': -1.437980441127122,
 'mse': 1.4379804329745625,
 'rmse': 1.1991582184910223,
 'mae': 0.8758612891445718,
 'pearson': 0.906115841576278}


lr=0.001,
max_epochs=200, stopped at 146
{'score': -1.3509444461367566,
 'mse': 1.3509444345187707,
 'rmse': 1.1623013527131294,
 'mae': 0.8550548395680365,
 'pearson': 0.9064326650239283}

lr=0.005,
max_epochs=50,
{'score': -1.25519708239983,
 'mse': 1.2551970881069108,
 'rmse': 1.1203557863941753,
 'mae': 0.8379602462784348,
 'pearson': 0.9056230485057498}

lr=0.005,
max_epochs=200, stopped at 97
{'score': -1.234604827821224,
 'mse': 1.2346048312433169,
 'rmse': 1.1111277294907713,
 'mae': 0.8343956157644902,
 'pearson': 0.9051089048998351}

lr=0.01,
max_epochs=200, stopped at 82
{'score': -1.164180284217144,
 'mse': 1.1641802904324152,
 'rmse': 1.0789718673035062,
 'mae': 0.8180751121579273,
 'pearson': 0.9064611452487259}

lr=0.05,
max_epochs=200, stopped at 70
{'score': -1.123088931619536,
 'mse': 1.1230889267503719,
 'rmse': 1.0597589002930676,        BEST overall
 'mae': 0.8076789356777223,
 'pearson': 0.9087810225211016}

lr=0.1,
max_epochs=200, stopped at 30
{'score': -1.1572722484084483,
 'mse': 1.157272253779093,
 'rmse': 1.0757658917158013,
 'mae': 0.8141343918437901,
 'pearson': 0.9082211771844354}

==========Unfreeze Batchnorm and then progressively unfreeze linear==========
layer_groups = [
        (
        [
            'model.1.weight', 'model.1.bias',
            'model.5.weight', 'model.5.bias',
            'model.9.weight', 'model.9.bias',
            'model.13.weight', 'model.13.bias'
        ],200,  0.1                                 stopped at 70
    ),
        (['model.16.weight', 'model.16.bias'], 200, 0.001),
        (['model.12.weight', 'model.12.bias'], 200, 0.001),
        (['model.8.weight', 'model.8.bias'], 200, 0.001)
    ]
    REMAINING LINEAR LAYER DIDNT IMPROVE AT ALL so same result as only batchnowm unfreeze
{'score': -1.123088931619536,
 'mse': 1.1230889267503719,
 'rmse': 1.0597589002930676,
 'mae': 0.8076789356777223,
 'pearson': 0.9087810225211016}


==========all batchnorm frozen all linear unfrozen==========

lr=0.00005,
max_epochs=200, stopped at I dont know
{'score': -1.2733095376480403,
 'mse': 1.2733095417606677,
 'rmse': 1.1284101832935876,
 'mae': 0.858273540455739,
 'pearson': 0.8949221972945635}

lr=0.0001,
max_epochs=200, stopped at 97
{'score': -1.309232543281908,
 'mse': 1.309232551087399,
 'rmse': 1.1442170034951409,
 'mae': 0.8604729907352874,
 'pearson': 0.8978189107346654}

lr=0.00001,
max_epochs=200, stopped at 128
{'score': -1.546167364423653,
 'mse': 1.5461673705263124,
 'rmse': 1.2434497860896163,
 'mae': 0.9074846631025981,
 'pearson': 0.9039427310084822}

=================Progressive unfreezing, but only linear layers====================
layer_groups = [

        (['model.16.weight', 'model.16.bias'], 200, 0.001), stopped at 97
        (['model.12.weight', 'model.12.bias'], 200, 0.001),  stopped at 231
        (['model.8.weight', 'model.8.bias'], 200, 0.001) stopped at 265
    ]
{'score': -1.3238933095469985,
 'mse': 1.323893293644999,
 'rmse': 1.1506056203778074,
 'mae': 0.8655722645065762,
 'pearson': 0.8968842915842811}