{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from cmcrameri import cm\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "from scripts.utils import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.glamos import *\n",
    "from scripts.dataset import *\n",
    "from scripts.plotting import *\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "use_mbm_style()\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                          'CH_wgms_dataset_all.csv')\n",
    "\n",
    "# Capitalize glacier names:\n",
    "glacierCap = {}\n",
    "for gl in data_glamos['GLACIER'].unique():\n",
    "    if isinstance(gl, str):  # Ensure the glacier name is a string\n",
    "        if gl.lower() == 'claridenu':\n",
    "            glacierCap[gl] = 'Clariden_U'\n",
    "        elif gl.lower() == 'claridenl':\n",
    "            glacierCap[gl] = 'Clariden_L'\n",
    "        else:\n",
    "            glacierCap[gl] = gl.capitalize()\n",
    "    else:\n",
    "        print(f\"Warning: Non-string glacier name encountered: {gl}\")\n",
    "\n",
    "# Cut to glaciers with pcsr:\n",
    "glacier_list = [\n",
    "    'adler', 'albigna', 'aletsch', 'allalin', 'basodino', 'clariden',\n",
    "    'corbassiere', 'corvatsch', 'findelen', 'forno', 'gietro', 'gorner',\n",
    "    'gries', 'hohlaub', 'joeri', 'limmern', 'morteratsch', 'murtel', 'oberaar',\n",
    "    'otemma', 'pizol', 'plattalva', 'rhone', 'sanktanna', 'schwarzbach',\n",
    "    'schwarzberg', 'sexrouge', 'silvretta', 'tortin', 'tsanfleuron'\n",
    "]\n",
    "\n",
    "data_glamos = data_glamos[data_glamos['GLACIER'].isin(glacier_list)]\n",
    "\n",
    "# Print number of total, annual and winter observations:\n",
    "print(\"Total observations:\", len(data_glamos))\n",
    "data_annual = data_glamos[data_glamos['PERIOD'] == 'annual']\n",
    "print(\"Annual observations:\", len(data_annual))\n",
    "data_winter = data_glamos[data_glamos['PERIOD'] == 'winter']\n",
    "print(\"Winter observations:\", len(data_winter))\n",
    "\n",
    "# Filter to test glaciers:\n",
    "data_glamos_test = data_glamos[data_glamos['GLACIER'].isin(TEST_GLACIERS)]\n",
    "data_glamos_test = data_glamos_test[data_glamos_test.YEAR < 2025]\n",
    "print(\"Total test observations:\", len(data_glamos_test))\n",
    "data_annual = data_glamos_test[data_glamos_test['PERIOD'] == 'annual']\n",
    "print(\"Annual test observations:\", len(data_annual))\n",
    "data_winter = data_glamos_test[data_glamos_test['PERIOD'] == 'winter']\n",
    "print(\"Winter test observations:\", len(data_winter))\n",
    "print('Percentage of test data:', len(data_glamos_test)/len(data_glamos)*100)\n",
    "print('Percentage of annual test data:', len(data_annual)/len(data_glamos[data_glamos['PERIOD'] == 'annual'])*100)\n",
    "print('Percentage of winter test data:', len(data_winter)/len(data_glamos[data_glamos['PERIOD'] == 'winter'])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load RGI glacier IDs ===\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids)\n",
    "rgi_df.columns = rgi_df.columns.str.strip()\n",
    "rgi_df = rgi_df.sort_values(by='short_name').set_index('short_name')\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "# get number of measurements per glacier:\n",
    "glacier_info = data_glamos.groupby('GLACIER').size().sort_values(\n",
    "    ascending=False).reset_index()\n",
    "glacier_info.rename(columns={0: 'Nb. measurements'}, inplace=True)\n",
    "glacier_info.set_index('GLACIER', inplace=True)\n",
    "\n",
    "glacier_loc = data_glamos.groupby('GLACIER')[['POINT_LAT', 'POINT_LON']].mean()\n",
    "\n",
    "glacier_info = glacier_loc.merge(glacier_info, on='GLACIER')\n",
    "\n",
    "glacier_period = data_glamos.groupby(['GLACIER', 'PERIOD'\n",
    "                                      ]).size().unstack().fillna(0).astype(int)\n",
    "\n",
    "glacier_info = glacier_info.merge(glacier_period, on='GLACIER')\n",
    "\n",
    "glacier_info['Train/Test glacier'] = glacier_info.apply(\n",
    "    lambda x: 'Test' if x.name in TEST_GLACIERS else 'Train', axis=1)\n",
    "glacier_info.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro & methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geoplots (Fig 1):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original raster\n",
    "tif_name = \"landesforstinventar-vegetationshoehenmodell_relief_sentinel_2024_2056.tif\"\n",
    "tif_path = os.path.join(cfg.dataPath, 'GLAMOS/RGI/', tif_name)\n",
    "\n",
    "# Desired output resolution (in degrees)\n",
    "# Approx. 100 m in degrees: ~0.0009 deg\n",
    "target_res = 0.0009\n",
    "output_crs = \"EPSG:4326\"  # WGS84\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    # Calculate transform and shape with coarser resolution\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs,\n",
    "        output_crs,\n",
    "        src.width,\n",
    "        src.height,\n",
    "        *src.bounds,\n",
    "        resolution=target_res)\n",
    "\n",
    "    # Set up destination array and metadata\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': output_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    # Prepare empty destination array\n",
    "    destination = np.empty((height, width), dtype=src.dtypes[0])\n",
    "\n",
    "    # Reproject with coarsening\n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=destination,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=output_crs,\n",
    "        resampling=Resampling.\n",
    "        average  # average to reduce noise when downsampling\n",
    "    )\n",
    "\n",
    "    extent = [\n",
    "        transform[2], transform[2] + transform[0] * width,\n",
    "        transform[5] + transform[4] * height, transform[5]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Preprocessing ----\n",
    "# Square-root scaling of number of measurements\n",
    "glacier_info['sqrt_size'] = np.sqrt(glacier_info['Nb. measurements'])\n",
    "\n",
    "# Cache dataset-wide min and max\n",
    "sqrt_min = glacier_info['sqrt_size'].min()\n",
    "sqrt_max = glacier_info['sqrt_size'].max()\n",
    "\n",
    "# Define the desired marker size range in points^2\n",
    "sizes = (100, 1500)  # min and max scatter size\n",
    "\n",
    "\n",
    "# Function to scale individual values consistently\n",
    "def scaled_size(val, min_out=sizes[0], max_out=sizes[1]):\n",
    "    sqrt_val = np.sqrt(val)\n",
    "    if sqrt_max == sqrt_min:\n",
    "        return (min_out + max_out) / 2\n",
    "    return min_out + (max_out - min_out) * ((sqrt_val - sqrt_min) /\n",
    "                                            (sqrt_max - sqrt_min))\n",
    "\n",
    "\n",
    "# Apply scaling to full dataset for the actual plot\n",
    "glacier_info['scaled_size'] = glacier_info['Nb. measurements'].apply(\n",
    "    scaled_size)\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "#latN, latS = 48, 45.8\n",
    "latN, latS = 47.1, 45.8\n",
    "lonW, lonE = 5.8, 10.5\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "# ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "# Add the image to the cartopy map\n",
    "\n",
    "masked_destination = np.ma.masked_where(destination == 0, destination)\n",
    "cmap = plt.cm.gray\n",
    "cmap.set_bad(color='white')  # Set masked (bad) values to white\n",
    "ax2.imshow(\n",
    "    masked_destination,\n",
    "    origin='upper',\n",
    "    extent=extent,\n",
    "    transform=ccrs.PlateCarree(),  # Assuming raster is in WGS84\n",
    "    cmap=cmap,  # or any other colormap\n",
    "    alpha=0.4,  # transparency\n",
    "    zorder=0)\n",
    "\n",
    "# Glacier outlines\n",
    "glacier_outline_sgi.plot(ax=ax2, transform=projPC, color='black', alpha=0.7)\n",
    "\n",
    "# ---- 3. Scatterplot ----\n",
    "# custom_palette = {'Train': '#35978f', 'Test': '#8c510a'}\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "custom_palette = {'Train': color_dark_blue, 'Test': '#b2182b'}\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=glacier_info,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    size='scaled_size',\n",
    "    hue='Train/Test glacier',\n",
    "    sizes=sizes,\n",
    "    alpha=0.6,\n",
    "    palette=custom_palette,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False\n",
    "\n",
    "# ---- 5. Custom Combined Legend ----\n",
    "\n",
    "# Hue legend handles\n",
    "handles, labels = g.get_legend_handles_labels()\n",
    "expected_labels = list(custom_palette.keys())\n",
    "hue_entries = [(h, l) for h, l in zip(handles, labels) if l in expected_labels]\n",
    "\n",
    "# Size legend values and handles\n",
    "size_values = [30, 100, 1000, 6000]\n",
    "size_handles = [\n",
    "    Line2D(\n",
    "        [],\n",
    "        [],\n",
    "        marker='o',\n",
    "        linestyle='None',\n",
    "        markersize=np.sqrt(scaled_size(val)),  # matplotlib uses radius\n",
    "        markerfacecolor='gray',\n",
    "        alpha=0.6,\n",
    "        label=f'{val}') for val in size_values\n",
    "]\n",
    "\n",
    "# Separator label\n",
    "separator_handle = Patch(facecolor='none',\n",
    "                         edgecolor='none',\n",
    "                         label='Nb. measurements')\n",
    "\n",
    "# Combine all legend entries\n",
    "# combined_handles = [h for h, _ in hue_entries] + [separator_handle] + size_handles\n",
    "# combined_labels = [l for _, l in hue_entries] + ['Nb. measurements'] + [str(v) for v in size_values]\n",
    "\n",
    "# same but without separator\n",
    "combined_handles = [h for h, _ in hue_entries] + size_handles\n",
    "combined_labels = [l for _, l in hue_entries] + [str(v) for v in size_values]\n",
    "\n",
    "# Final legend\n",
    "ax2.legend(combined_handles,\n",
    "           combined_labels,\n",
    "           title='Number of measurements',\n",
    "           loc='lower right',\n",
    "           frameon=True,\n",
    "           fontsize=18,\n",
    "           title_fontsize=18,\n",
    "           borderpad=1.2,\n",
    "           labelspacing=1.2,\n",
    "           ncol=3)\n",
    "# ax2.set_title('Glacier measurement locations', fontsize = 25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('figures/paper/fig1_ch_map.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANT COLORS FOR PLOTS\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_winter = colors[0]\n",
    "color_annual = \"#c51b7d\"\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "# Number of measurements per year:\n",
    "data_glamos.groupby(['YEAR', 'PERIOD']).count()['POINT_ID'].unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(20, 5),\n",
    "    color=[color_annual, color_winter],\n",
    "    ax=ax)\n",
    "# plt.title('Number of measurements per year for all glaciers', fontsize = 25)\n",
    "# get legend\n",
    "plt.legend(title='Period', fontsize=18, title_fontsize=20, ncol=2)\n",
    "# save figure\n",
    "fig.savefig('figures/paper/fig1_num_year.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_period = data_glamos.groupby(['YEAR',\n",
    "                                   'PERIOD']).count()['POINT_ID'].unstack()\n",
    "meas_period.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils.build_head_tail_pads_from_monthly_df(\n",
    "    data_monthly)\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of test data:', len(data_test))\n",
    "\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "# Number of annual versus winter measurements:\n",
    "print('Train:')\n",
    "print('Number of winter and annual samples:', len(data_train))\n",
    "print('Number of annual samples:',\n",
    "      len(data_train[data_train.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_train[data_train.PERIOD == 'winter']))\n",
    "\n",
    "# Same for test\n",
    "data_test_annual = data_test[data_test.PERIOD == 'annual']\n",
    "data_test_winter = data_test[data_test.PERIOD == 'winter']\n",
    "\n",
    "print('Test:')\n",
    "print('Number of winter and annual samples:', len(data_test))\n",
    "print('Number of annual samples:', len(data_test_annual))\n",
    "print('Number of winter samples:', len(data_test_winter))\n",
    "\n",
    "print('Total:')\n",
    "print('Number of monthly rows:', len(dataloader_gl.data))\n",
    "print('Number of annual rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'annual']))\n",
    "print('Number of winter rows:',\n",
    "      len(dataloader_gl.data[dataloader_gl.data.PERIOD == 'winter']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap annual (Fig 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_glwd_csv(path):\n",
    "    rows = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            # skip first line entirely\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            parts = [p.strip() for p in line.strip().split(\",\")]\n",
    "\n",
    "            # first 13 fixed columns\n",
    "            fixed = parts[:13]\n",
    "\n",
    "            # remaining text fields (could be 0,1,2,n)\n",
    "            tail = parts[13:]\n",
    "            tail = tail + [\"\"] * (3 - len(tail))  # pad\n",
    "            tail = tail[:3]  # trim extra, just in case\n",
    "\n",
    "            rows.append(fixed + tail)\n",
    "\n",
    "    cols = [\n",
    "        \"glacier\",\n",
    "        \"glacier_id\",\n",
    "        \"date_start\",\n",
    "        \"date_end_winter\",\n",
    "        \"date_end\",\n",
    "        \"Bw\",\n",
    "        \"Bs\",\n",
    "        \"Ba\",\n",
    "        \"ELA\",\n",
    "        \"AAR\",\n",
    "        \"area\",\n",
    "        \"h_min\",\n",
    "        \"h_max\",\n",
    "        \"source_observer\",\n",
    "        \"observer\",\n",
    "        \"source\",\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "\n",
    "path = os.path.join(cfg.dataPath, 'GLAMOS/glacier-wide',\n",
    "                    'massbalance_observation_2025_r2025.csv')\n",
    "glwd_csv = parse_glwd_csv(path)\n",
    "glwd_csv.head()\n",
    "\n",
    "sgi_id = rgi_df.loc['morteratsch']['sgi-id']\n",
    "print(glwd_csv[glwd_csv['glacier_id'] == sgi_id])\n",
    "\n",
    "sgi_id = rgi_df.loc['gorner']['sgi-id']\n",
    "print(glwd_csv[glwd_csv['glacier_id'] == sgi_id])\n",
    "\n",
    "all_glwd = []  # list of dataframes to concat\n",
    "for gl in data_glamos.GLACIER.unique():\n",
    "    # if path does not exist, skip\n",
    "    if not os.path.exists(\n",
    "            os.path.join(cfg.dataPath, 'GLAMOS/glacier-wide/csv/obs',\n",
    "                         f'{gl}_obs.csv')):\n",
    "        print(f\"Warning: Glacier CSV for {gl} not found. Skipping.\")\n",
    "        continue\n",
    "    # read individual glacier CSV\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(cfg.dataPath, 'GLAMOS/glacier-wide/csv/obs',\n",
    "                     f'{gl}_obs.csv'))\n",
    "\n",
    "    # keep only years >= 1951\n",
    "    df = df[df.YEAR >= 1951].copy()\n",
    "\n",
    "    # compute MB in m w.e.\n",
    "    df[\"MB\"] = df[\"Annual.Balance\"] / 1000\n",
    "\n",
    "    # add a GLACIER column holding the name\n",
    "    df[\"GLACIER\"] = gl\n",
    "\n",
    "    # keep only what we need\n",
    "    df = df[[\"YEAR\", \"GLACIER\", \"MB\"]]\n",
    "\n",
    "    all_glwd.append(df)\n",
    "\n",
    "# combine into single dataframe\n",
    "glwd_all = pd.concat(all_glwd, ignore_index=True)\n",
    "\n",
    "data_glamos_compl = pd.merge(data_glamos,\n",
    "                             glwd_all,\n",
    "                             on=['GLACIER', 'YEAR'],\n",
    "                             how='inner')\n",
    "fig = plot_heatmap(TEST_GLACIERS,\n",
    "                  data_glamos_compl,\n",
    "                  glacierCap,\n",
    "                  period='annual',\n",
    "                  var_to_plot='MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PMB (Fig 2a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_heatmap(TEST_GLACIERS,\n",
    "                  data_glamos,\n",
    "                  glacierCap,\n",
    "                  period='annual',\n",
    "                  cbar_label=\"Mean PMB [m w.e. $a^{-1}$]\")\n",
    "\n",
    "# save figure\n",
    "fig.savefig('figures/paper/fig_heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a copy to avoid chained-assignment warnings\n",
    "data_annual = data_glamos.loc[data_glamos.PERIOD == 'annual'].copy()\n",
    "\n",
    "# Parse FROM/TO into proper datetimes\n",
    "data_annual['FROM_date'] = pd.to_datetime(data_annual['FROM_DATE'].astype(str),\n",
    "                                          format='%Y%m%d',\n",
    "                                          errors='coerce')\n",
    "\n",
    "# Work on a copy to avoid chained-assignment warnings\n",
    "data_winter = data_glamos.loc[data_glamos.PERIOD == 'winter'].copy()\n",
    "\n",
    "# Parse FROM/TO into proper datetimes\n",
    "data_winter['TO_date'] = pd.to_datetime(data_winter['TO_DATE'].astype(str),\n",
    "                                        format='%Y%m%d',\n",
    "                                        errors='coerce')\n",
    "\n",
    "\n",
    "# Helper: compute mean date and std (in days) for a date Series, using a dummy year (2000)\n",
    "def mean_date_and_std(date_series: pd.Series, circular: bool = False):\n",
    "    # Drop NaT\n",
    "    s = date_series.dropna()\n",
    "    if s.empty:\n",
    "        return pd.NaT, np.nan\n",
    "\n",
    "    # Map to a dummy, fixed year so we can compute day-of-year\n",
    "    dummy = pd.to_datetime({\n",
    "        'year': 2000,\n",
    "        'month': s.dt.month,\n",
    "        'day': s.dt.day\n",
    "    },\n",
    "                           errors='coerce').dropna()\n",
    "\n",
    "    doy = dummy.dt.dayofyear.astype(float)\n",
    "\n",
    "    if not circular:\n",
    "        mean_doy = doy.mean()\n",
    "        std_doy = doy.std()\n",
    "    else:\n",
    "        # Circular mean/std over the year (useful if dates wrap around New Year)\n",
    "        theta = 2 * np.pi * (doy - 1) / 365.0\n",
    "        C = np.mean(np.cos(theta))\n",
    "        S = np.mean(np.sin(theta))\n",
    "        mean_ang = np.arctan2(S, C)\n",
    "        if mean_ang < 0:\n",
    "            mean_ang += 2 * np.pi\n",
    "        mean_doy = (mean_ang / (2 * np.pi)) * 365.0 + 1\n",
    "        R = np.sqrt(C**2 + S**2)\n",
    "        # Convert circular std (radians) to days\n",
    "        std_ang = np.sqrt(-2 * np.log(max(R, 1e-12)))\n",
    "        std_doy = std_ang * 365.0 / (2 * np.pi)\n",
    "\n",
    "    mean_date = pd.Timestamp('2000-01-01') + pd.to_timedelta(mean_doy - 1,\n",
    "                                                             unit='D')\n",
    "    return mean_date, std_doy\n",
    "\n",
    "# Compute stats for FROM and TO\n",
    "mean_from_annual, std_from_annual = mean_date_and_std(data_annual['FROM_date'],\n",
    "                                                      circular=False)\n",
    "mean_from_winter, std_from_winter = mean_date_and_std(data_winter['TO_date'],\n",
    "                                                      circular=False)\n",
    "\n",
    "print(\n",
    "    f\"ANNUAL FROM_DATE -> mean: {mean_from_annual.strftime('%m-%d')} | std: {std_from_annual:.2f} days\"\n",
    ")\n",
    "print(\n",
    "    f\"WINTER TO_DATE   -> mean: {mean_from_winter.strftime('%m-%d')} | std: {std_from_winter:.2f} days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elevations (Fig 2b):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_elev = data_glamos.POINT_ELEVATION.min()\n",
    "max_elev = data_glamos.POINT_ELEVATION.max()\n",
    "\n",
    "rows_min = data_glamos[data_glamos.POINT_ELEVATION == min_elev]\n",
    "rows_max = data_glamos[data_glamos.POINT_ELEVATION == max_elev]\n",
    "\n",
    "print('Min elevation measurement:', min_elev, 'on glacier',\n",
    "      rows_min.GLACIER.values[0])\n",
    "print('Max elevation measurement:', max_elev, 'on glacier',\n",
    "      rows_max.GLACIER.values[0])\n",
    "\n",
    "# Mean, min and max PMB:\n",
    "# For annual only\n",
    "data_glamos_a = data_glamos[data_glamos.PERIOD == 'annual']\n",
    "mean_pmb = data_glamos_a.POINT_BALANCE.mean()\n",
    "min_pmb = data_glamos_a.POINT_BALANCE.min()\n",
    "max_pmb = data_glamos_a.POINT_BALANCE.max()\n",
    "\n",
    "rows_min = data_glamos_a[data_glamos_a.POINT_BALANCE == min_pmb]\n",
    "rows_max = data_glamos_a[data_glamos_a.POINT_BALANCE == max_pmb]\n",
    "\n",
    "print('Annual:')\n",
    "print('Mean PMB (m w.e.): {:.2f}'.format(mean_pmb))\n",
    "print('Min PMB (m w.e.): {:.2f}'.format(min_pmb), 'on glacier',\n",
    "      rows_min.GLACIER.values[0], 'in', rows_min.YEAR.values[0])\n",
    "print('Max PMB (m w.e.): {:.2f}'.format(max_pmb), 'on glacier',\n",
    "      rows_max.GLACIER.values[0], 'in', rows_max.YEAR.values[0])\n",
    "\n",
    "data_glamos_w = data_glamos[data_glamos.PERIOD == 'winter']\n",
    "mean_pmb = data_glamos_w.POINT_BALANCE.mean()\n",
    "min_pmb = data_glamos_w.POINT_BALANCE.min()\n",
    "max_pmb = data_glamos_w.POINT_BALANCE.max()\n",
    "\n",
    "rows_min = data_glamos_w[data_glamos_w.POINT_BALANCE == min_pmb]\n",
    "rows_max = data_glamos_w[data_glamos_w.POINT_BALANCE == max_pmb]\n",
    "\n",
    "print('Winter:')\n",
    "print('Mean PMB (m w.e.): {:.2f}'.format(mean_pmb))\n",
    "print('Min PMB (m w.e.): {:.2f}'.format(min_pmb), 'on glacier',\n",
    "      rows_min.GLACIER.values[0], 'in', rows_min.YEAR.values[0])\n",
    "print('Max PMB (m w.e.): {:.2f}'.format(max_pmb), 'on glacier',\n",
    "      rows_max.GLACIER.values[0], 'in', rows_max.YEAR.values[0])\n",
    "\n",
    "# get elevation of glaciers:\n",
    "# gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "#     ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = data_glamos.groupby(['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "# Plot elevation:\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "sns.lineplot(gl_per_el.sort_values(ascending=True),\n",
    "             ax=ax,\n",
    "             color='gray',\n",
    "             marker='v')\n",
    "ax.set_xticklabels('', rotation=90)\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winter MB before and after 2000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Identify training glaciers\n",
    "train_glaciers = data_glamos.GLACIER.unique()\n",
    "train_glaciers = [g for g in train_glaciers if g not in TEST_GLACIERS]\n",
    "\n",
    "# Split dataset\n",
    "train = data_glamos[data_glamos.GLACIER.isin(train_glaciers) & (data_glamos.PERIOD == 'winter')]\n",
    "test  = data_glamos[data_glamos.GLACIER.isin(TEST_GLACIERS)      & (data_glamos.PERIOD == 'winter')]\n",
    "\n",
    "# Before / after 2003 splits\n",
    "train_bef = train[train.YEAR < 2003]\n",
    "train_aft = train[train.YEAR >= 2003]\n",
    "\n",
    "test_bef = test[test.YEAR < 2003]\n",
    "test_aft = test[test.YEAR >= 2003]\n",
    "\n",
    "# Extract POINT_BALANCE\n",
    "train_bef_x = train_bef['POINT_BALANCE'].dropna()\n",
    "train_aft_x = train_aft['POINT_BALANCE'].dropna()\n",
    "\n",
    "test_bef_x = test_bef['POINT_BALANCE'].dropna()\n",
    "test_aft_x = test_aft['POINT_BALANCE'].dropna()\n",
    "\n",
    "# KDEs\n",
    "train_kde_bef = gaussian_kde(train_bef_x)\n",
    "train_kde_aft = gaussian_kde(train_aft_x)\n",
    "\n",
    "test_kde_bef = gaussian_kde(test_bef_x)\n",
    "test_kde_aft = gaussian_kde(test_aft_x)\n",
    "\n",
    "# Combined grid\n",
    "xmin = min(train_bef_x.min(), train_aft_x.min(),\n",
    "           test_bef_x.min(), test_aft_x.min())\n",
    "\n",
    "xmax = max(train_bef_x.max(), train_aft_x.max(),\n",
    "           test_bef_x.max(), test_aft_x.max())\n",
    "\n",
    "xgrid = np.linspace(xmin, xmax, 500)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# TRAIN PANEL\n",
    "axs[0].plot(xgrid, train_kde_bef(xgrid), label='Train < 2003', linewidth=2)\n",
    "axs[0].plot(xgrid, train_kde_aft(xgrid), label='Train ≥ 2003', linewidth=2)\n",
    "axs[0].set_title(\"Training Glaciers\")\n",
    "axs[0].set_xlabel(\"POINT_BALANCE\")\n",
    "axs[0].set_ylabel(\"Density\")\n",
    "axs[0].grid(alpha=0.3)\n",
    "axs[0].legend()\n",
    "\n",
    "# TEST PANEL\n",
    "axs[1].plot(xgrid, test_kde_bef(xgrid), label='Test < 2003', linewidth=2)\n",
    "axs[1].plot(xgrid, test_kde_aft(xgrid), label='Test ≥ 2003', linewidth=2)\n",
    "axs[1].set_title(\"Test Glaciers\")\n",
    "axs[1].set_xlabel(\"POINT_BALANCE\")\n",
    "axs[1].grid(alpha=0.3)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
