{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "    \"millan_v\", \"svf\"\n",
    "]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = True\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf_IS.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking on glaciers:\n",
    "\n",
    "Model is trained on all glaciers --> \"Within sample\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "#                                             test_split_on='GLACIER',\n",
    "#                                             test_splits=TEST_GLACIERS,\n",
    "#                                             random_state=cfg.seed)\n",
    "\n",
    "# print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "#                                       test_set['splits_vals']))\n",
    "# test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "# print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "# print('Size of test set:', len(test_set['df_X']))\n",
    "# print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "#                                        train_set['splits_vals']))\n",
    "# print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "    'pcsr'\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', \"svf\"\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# Look at the padding for one example\n",
    "key = ('adler', 2009, 11, 'winter')\n",
    "\n",
    "# find the index of this key\n",
    "try:\n",
    "    idx = ds_train.keys.index(key)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Key {key} not found in dataset.\")\n",
    "\n",
    "# fetch the corresponding sequence\n",
    "sequence = ds_train[idx]\n",
    "sequence['mv'], sequence['mw'], sequence['ma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_two_heads_param_search_progress_no_oggm_IS_2025-10-22.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='test_rmse_a')\n",
    "df = pd.read_csv(log_path)\n",
    "df[\"avg_test_loss\"] = (df[\"test_rmse_a\"] + df[\"test_rmse_w\"]) / 2\n",
    "df.sort_values(by=\"avg_test_loss\", inplace=True)\n",
    "print(best_params)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_params = {\n",
    "#     'Fm': 8,\n",
    "#     'Fs': 6,\n",
    "#     'hidden_size': 128,\n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': False,\n",
    "#     'dropout': 0.1,\n",
    "#     'static_layers': 2,\n",
    "#     'static_hidden': [128, 64],\n",
    "#     'static_dropout': 0.1,\n",
    "#     'lr': 0.001,\n",
    "#     'weight_decay': 0.0,\n",
    "#     'loss_name': 'neutral',\n",
    "#     'two_heads': True,\n",
    "#     'head_dropout': 0.0,\n",
    "#     'loss_spec': None\n",
    "# }\n",
    "\n",
    "custom_params = best_params\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_no_oggm_IS.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "    \n",
    "model_filename = f\"models/lstm_model_2025-10-23_no_oggm_IS.pt\"\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_area = get_gl_area(cfg)\n",
    "gl_area[\"clariden\"] = gl_area[\"claridenL\"]\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "train_glaciers = {\n",
    "    'adler', 'albigna', 'aletsch', 'allalin', 'basodino', 'clariden',\n",
    "    'corbassiere', 'corvatsch', 'findelen', 'forno', 'gietro', 'gorner',\n",
    "    'gries', 'hohlaub', 'joeri', 'limmern', 'morteratsch', 'murtel', 'oberaar',\n",
    "    'otemma', 'pizol', 'plattalva', 'rhone', 'sanktanna', 'schwarzberg',\n",
    "    'sexrouge', 'silvretta', 'tortin', 'tsanfleuron'\n",
    "}\n",
    "test_gl_per_el = gl_per_el[list(train_glaciers)].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(7, 3, figsize=(25, 30), sharex=False)\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                       axs=axs,\n",
    "                                       color_annual=color_dark_blue,\n",
    "                                       color_winter=color_pink,\n",
    "                                       custom_order=test_gl_per_el,\n",
    "                                       add_text=True,\n",
    "                                       ax_xlim=None,\n",
    "                                       gl_area = gl_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.parallel_mb import MBJobConfig, run_glacier_mb\n",
    "\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/testing_LSTM/glamos_dems_LSTM_no_oggm_IS')\n",
    "job = MBJobConfig(\n",
    "    cfg=cfg,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    fields_not_features=cfg.fieldsNotFeatures,\n",
    "    model_filename=model_filename,\n",
    "    custom_params=custom_params,\n",
    "    ds_train=ds_train,\n",
    "    train_idx=train_idx,\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    data_path=cfg.dataPath,\n",
    "    path_glacier_grid_glamos=path_glacier_grid_glamos,\n",
    "    path_xr_grids=os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM',\n",
    "                               'xr_masked_grids'),\n",
    "    path_save_glw=path_save_glw,\n",
    "    seed=cfg.seed,\n",
    "    max_workers=None,  # or an int\n",
    "    cpu_only=True,\n",
    "    ONLY_GEODETIC=False)\n",
    "\n",
    "# 3) Run\n",
    "summary = run_glacier_mb(job, glacier_list, periods_per_glacier)\n",
    "print(\"SUMMARY:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                                  df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "fig = plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel PFI for MBM LSTM (CPU, Linux)\n",
    "import os, sys, random, numpy as np, pandas as pd, torch, xarray as xr\n",
    "from typing import Dict, List, Callable, Any, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ------------------------------- determinism helpers -------------------------------\n",
    "\n",
    "\n",
    "def _set_cpu_env_threads():\n",
    "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")  # force CPU\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"1\")\n",
    "    try:\n",
    "        torch.set_num_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _set_seeds(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # deterministic CPU path\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ------------------------------- globals for worker -------------------------------\n",
    "# Filled by initializer once per process\n",
    "_PFI_G = {\n",
    "    \"model\": None,\n",
    "    \"device\": None,\n",
    "    \"ds_train_copy\": None,\n",
    "    \"MONTHLY_COLS\": None,\n",
    "    \"STATIC_COLS\": None,\n",
    "    \"months_head_pad\": None,\n",
    "    \"months_tail_pad\": None,\n",
    "    \"target_col\": None,\n",
    "    \"baseline\": None,\n",
    "    \"df_eval\": None,\n",
    "    \"seed\": None,\n",
    "    \"batch_size\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _pfi_worker_init(cfg, custom_params: Dict[str, Any], model_filename: str,\n",
    "                     ds_train, train_idx, MONTHLY_COLS: List[str],\n",
    "                     STATIC_COLS: List[str], months_head_pad: int,\n",
    "                     months_tail_pad: int, df_eval: pd.DataFrame,\n",
    "                     target_col: str, seed: int, batch_size: int):\n",
    "    \"\"\"Initializer: quiet stdout, set threads, build scalers, load model, compute baseline once.\"\"\"\n",
    "    # local import avoids pickling a module from parent\n",
    "    import massbalancemachine as mbm\n",
    "\n",
    "    # silence worker prints\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "\n",
    "    _set_cpu_env_threads()\n",
    "    _set_seeds(seed)\n",
    "\n",
    "    # Fit scalers on TRAIN only\n",
    "    ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        ds_train)\n",
    "    ds_train_copy.fit_scalers(train_idx)\n",
    "\n",
    "    # Load model on CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "                                                       custom_params,\n",
    "                                                       device,\n",
    "                                                       verbose=False)\n",
    "    state = torch.load(model_filename, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    # Build eval ds/loader with targets\n",
    "    ds_eval = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_eval,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True,\n",
    "        show_progress=False)\n",
    "    dl_eval = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_eval, ds_train_copy, seed=seed, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        df_pred_base = model.predict_with_keys(device, dl_eval, ds_eval)\n",
    "\n",
    "    # Try to pick targets directly from pred df, otherwise from df_eval by ID merge\n",
    "    if _PFI_G[\"target_col\"] is None:\n",
    "        pass  # set below\n",
    "    y_true = None\n",
    "    if target_col in df_pred_base.columns:\n",
    "        y_true = df_pred_base[target_col].to_numpy()\n",
    "    else:\n",
    "        merged = df_pred_base.merge(df_eval[[\"ID\", target_col\n",
    "                                             ]].drop_duplicates(\"ID\"),\n",
    "                                    on=\"ID\",\n",
    "                                    how=\"left\")\n",
    "        if merged[target_col].isna().any():\n",
    "            raise ValueError(\n",
    "                \"Missing targets after merge; ensure df_eval contains per-ID targets.\"\n",
    "            )\n",
    "        y_true = merged[target_col].to_numpy()\n",
    "\n",
    "    y_pred = df_pred_base[\"pred\"].to_numpy()\n",
    "    baseline = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "    # store globals\n",
    "    _PFI_G.update(\n",
    "        dict(model=model,\n",
    "             device=device,\n",
    "             ds_train_copy=ds_train_copy,\n",
    "             MONTHLY_COLS=MONTHLY_COLS,\n",
    "             STATIC_COLS=STATIC_COLS,\n",
    "             months_head_pad=months_head_pad,\n",
    "             months_tail_pad=months_tail_pad,\n",
    "             target_col=target_col,\n",
    "             baseline=baseline,\n",
    "             df_eval=df_eval,\n",
    "             seed=seed,\n",
    "             batch_size=batch_size))\n",
    "\n",
    "\n",
    "def _permute_within_groups(values: np.ndarray, groups: np.ndarray,\n",
    "                           rng: np.random.Generator) -> np.ndarray:\n",
    "    out = np.empty_like(values)\n",
    "    # group by group label; shuffle within each block\n",
    "    # to preserve seasonal distribution\n",
    "    u, inv = np.unique(groups, return_inverse=True)\n",
    "    for gi, g in enumerate(u):\n",
    "        idx = np.where(inv == gi)[0]\n",
    "        shuf = idx.copy()\n",
    "        rng.shuffle(shuf)\n",
    "        out[idx] = values[shuf]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _pfi_worker_task(task: Tuple[str, str, int]) -> Tuple[str, str, float]:\n",
    "    \"\"\"\n",
    "    Task = (feature, type, repeat_seed_offset) -> returns (feature, type, delta_rmse)\n",
    "    \"\"\"\n",
    "    # local import\n",
    "    import massbalancemachine as mbm\n",
    "\n",
    "    feat, ftype, seed_offset = task\n",
    "    rng = np.random.default_rng(int(_PFI_G[\"seed\"]) + int(seed_offset))\n",
    "\n",
    "    df = _PFI_G[\"df_eval\"].copy()\n",
    "    if ftype == \"monthly\":\n",
    "        if \"MONTHS\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"MONTHS column required for monthly feature permutation.\")\n",
    "        df[feat] = _permute_within_groups(df[feat].to_numpy(),\n",
    "                                          df[\"MONTHS\"].to_numpy(), rng)\n",
    "    elif ftype == \"static\":\n",
    "        idx = np.arange(len(df))\n",
    "        rng.shuffle(idx)\n",
    "        df[feat] = df[feat].to_numpy()[idx]\n",
    "    else:\n",
    "        raise ValueError(\"ftype must be 'monthly' or 'static'.\")\n",
    "\n",
    "    # Rebuild ds/loader for permuted df\n",
    "    ds_p = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df,\n",
    "        _PFI_G[\"MONTHLY_COLS\"],\n",
    "        _PFI_G[\"STATIC_COLS\"],\n",
    "        months_tail_pad=_PFI_G[\"months_tail_pad\"],\n",
    "        months_head_pad=_PFI_G[\"months_head_pad\"],\n",
    "        expect_target=True,\n",
    "        show_progress=False)\n",
    "    dl_p = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_p,\n",
    "        _PFI_G[\"ds_train_copy\"],\n",
    "        seed=_PFI_G[\"seed\"],\n",
    "        batch_size=_PFI_G[\"batch_size\"])\n",
    "    with torch.no_grad():\n",
    "        df_pred = _PFI_G[\"model\"].predict_with_keys(_PFI_G[\"device\"], dl_p,\n",
    "                                                    ds_p)\n",
    "\n",
    "    # Targets\n",
    "    tcol = _PFI_G[\"target_col\"]\n",
    "    if tcol in df_pred.columns:\n",
    "        y_true = df_pred[tcol].to_numpy()\n",
    "    else:\n",
    "        merged = df_pred.merge(df[[\"ID\", tcol]].drop_duplicates(\"ID\"),\n",
    "                               on=\"ID\",\n",
    "                               how=\"left\")\n",
    "        if merged[tcol].isna().any():\n",
    "            raise ValueError(\n",
    "                \"Missing targets after merge; ensure df_eval contains per-ID targets.\"\n",
    "            )\n",
    "        y_true = merged[tcol].to_numpy()\n",
    "    y_pred = df_pred[\"pred\"].to_numpy()\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    delta = rmse - float(_PFI_G[\"baseline\"])\n",
    "    return feat, ftype, delta\n",
    "\n",
    "\n",
    "# ------------------------------- user-facing function -------------------------------\n",
    "\n",
    "\n",
    "def permutation_feature_importance_mbm_parallel(\n",
    "    cfg,\n",
    "    custom_params: Dict[str, Any],\n",
    "    model_filename: str,\n",
    "    df_eval: pd.DataFrame,\n",
    "    MONTHLY_COLS: List[str],\n",
    "    STATIC_COLS: List[str],\n",
    "    ds_train,\n",
    "    train_idx,\n",
    "    target_col: str,\n",
    "    months_head_pad: int,\n",
    "    months_tail_pad: int,\n",
    "    seed: int = 42,\n",
    "    n_repeats: int = 5,\n",
    "    batch_size: int = 256,\n",
    "    max_workers: int = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parallel Permutation Feature Importance (CPU).\n",
    "    Returns DataFrame: ['feature','type','mean_delta','std_delta','baseline','metric_name'].\n",
    "    \"\"\"\n",
    "\n",
    "    # Build list of all tasks (feature x repeat)\n",
    "    feats = [(f, \"monthly\") for f in MONTHLY_COLS] + [(f, \"static\")\n",
    "                                                      for f in STATIC_COLS]\n",
    "    tasks = []\n",
    "    for feat, ftype in feats:\n",
    "        for r in range(n_repeats):\n",
    "            tasks.append((feat, ftype, r))\n",
    "\n",
    "    # Use Linux fork so df_eval stays shared copy-on-write\n",
    "    ctx = mp.get_context(\"fork\")\n",
    "    if max_workers is None:\n",
    "        max_workers = min(max(1, (os.cpu_count() or 2) - 1), 32)\n",
    "\n",
    "    # Run\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(\n",
    "            max_workers=max_workers,\n",
    "            mp_context=ctx,\n",
    "            initializer=_pfi_worker_init,\n",
    "            initargs=(\n",
    "                cfg,\n",
    "                custom_params,\n",
    "                model_filename,\n",
    "                ds_train,\n",
    "                train_idx,\n",
    "                MONTHLY_COLS,\n",
    "                STATIC_COLS,\n",
    "                months_head_pad,\n",
    "                months_tail_pad,\n",
    "                df_eval,\n",
    "                target_col,\n",
    "                seed,\n",
    "                batch_size,\n",
    "            ),\n",
    "    ) as ex:\n",
    "        futs = [ex.submit(_pfi_worker_task, t) for t in tasks]\n",
    "        for fut in tqdm(as_completed(futs),\n",
    "                        total=len(futs),\n",
    "                        desc=f\"PFI (workers={max_workers})\"):\n",
    "            results.append(fut.result())\n",
    "\n",
    "    # Aggregate\n",
    "    rows = []\n",
    "    baseline = _PFI_G.get(\n",
    "        \"baseline\")  # won't be set in parent; recompute baseline here:\n",
    "    # baseline computed inside workers, but not shared; recompute baseline serially in parent:\n",
    "    # Minimal recompute on parent with single pass:\n",
    "\n",
    "    import massbalancemachine as mbm\n",
    "    _set_cpu_env_threads()\n",
    "    _set_seeds(seed)\n",
    "    ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        ds_train)\n",
    "    ds_train_copy.fit_scalers(train_idx)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "                                                       custom_params,\n",
    "                                                       device,\n",
    "                                                       verbose=False)\n",
    "    state = torch.load(model_filename, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    ds_eval_parent = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_eval,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True,\n",
    "        show_progress=False)\n",
    "    dl_eval_parent = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_eval_parent, ds_train_copy, seed=seed, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        df_pred_base = model.predict_with_keys(device, dl_eval_parent,\n",
    "                                               ds_eval_parent)\n",
    "    if target_col in df_pred_base.columns:\n",
    "        y_true = df_pred_base[target_col].to_numpy()\n",
    "    else:\n",
    "        merged = df_pred_base.merge(df_eval[[\"ID\", target_col\n",
    "                                             ]].drop_duplicates(\"ID\"),\n",
    "                                    on=\"ID\",\n",
    "                                    how=\"left\")\n",
    "        if merged[target_col].isna().any():\n",
    "            raise ValueError(\n",
    "                \"Missing targets after merge; ensure df_eval has per-ID targets.\"\n",
    "            )\n",
    "        y_true = merged[target_col].to_numpy()\n",
    "    y_pred = df_pred_base[\"pred\"].to_numpy()\n",
    "    baseline = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "\n",
    "    # Build table\n",
    "    import collections\n",
    "    bucket: Dict[Tuple[str, str], List[float]] = collections.defaultdict(list)\n",
    "    for feat, ftype, delta in results:\n",
    "        bucket[(feat, ftype)].append(float(delta))\n",
    "\n",
    "    for (feat, ftype), deltas in bucket.items():\n",
    "        mu = float(np.mean(deltas))\n",
    "        sd = float(np.std(deltas, ddof=1)) if len(deltas) > 1 else 0.0\n",
    "        rows.append({\n",
    "            \"feature\": feat,\n",
    "            \"type\": ftype,\n",
    "            \"mean_delta\": mu,\n",
    "            \"std_delta\": sd\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\n",
    "        \"mean_delta\", ascending=False).reset_index(drop=True)\n",
    "    out[\"baseline\"] = baseline\n",
    "    out[\"metric_name\"] = \"rmse\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi_parallel = permutation_feature_importance_mbm_parallel(\n",
    "    cfg=cfg,\n",
    "    custom_params=custom_params,\n",
    "    model_filename=model_filename,\n",
    "    df_eval=df_train,  # your eval dataframe WITH TARGETS aligned to predictions\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    ds_train=ds_train,\n",
    "    train_idx=train_idx,\n",
    "    target_col=\"POINT_BALANCE\",  # <-- set your target column name\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    seed=cfg.seed,\n",
    "    n_repeats=5,\n",
    "    batch_size=256,\n",
    "    max_workers=None,  # auto: n_cpus-1 (cap 32)\n",
    ")\n",
    "\n",
    "# Optional: quick plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, max(3, 0.35 * len(pfi_parallel))))\n",
    "plt.barh(pfi_parallel[\"feature\"],\n",
    "         pfi_parallel[\"mean_delta\"],\n",
    "         xerr=pfi_parallel[\"std_delta\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\n",
    "    f\"Permutation Feature Importance (Δ{pfi_parallel['metric_name'].iloc[0]}; baseline={pfi_parallel['baseline'].iloc[0]:.3f})\"\n",
    ")\n",
    "plt.xlabel(\n",
    "    f\"Increase in {pfi_parallel['metric_name'].iloc[0]} (higher = more important)\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
