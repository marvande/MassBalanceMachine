{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & utilities ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import traceback\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Add repo root for MBM imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../\"))\n",
    "\n",
    "# --- Data science stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "\n",
    "# --- Machine learning / DL ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# --- Cartography / plotting ---\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Custom MBM modules ---\n",
    "import massbalancemachine as mbm\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Warnings & autoreload (notebook) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Configuration ---\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "# --- CUDA / device ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "df_missing = export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# gdf_proj.set_index('rgi_id', inplace=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# total glacier area\n",
    "total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# explode the list of missing vars into rows (one var per row)\n",
    "df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# 1) COUNT: number of glaciers missing each variable\n",
    "counts_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "        ascending=False))\n",
    "\n",
    "# 2) TOTAL % AREA with ANY missing var\n",
    "total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "print(f\"Total glacier area with ANY missing variable: \"\n",
    "      f\"{total_missing_area_km2:,.2f} kmÂ² \"\n",
    "      f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# Optional: also show % area per variable (kept from your earlier logic)\n",
    "area_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "        ascending=False))\n",
    "perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "print(\"\\n% of total glacier area missing per variable:\")\n",
    "for var, pct in perc_missing_per_var.items():\n",
    "    print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# ---- barplot: number of glaciers missing each variable ----\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "plt.xlabel(\"Missing variable\")\n",
    "plt.ylabel(\"Number of glaciers\")\n",
    "plt.title(\"Count of glaciers missing each variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked xarray grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_glacier(path_RGIs, rgi_gl, path_out_tiff):\n",
    "    \"\"\"\n",
    "    Create masked glacier dataset from OGGM .zarr file,\n",
    "    and save the masked DEM ('masked_elev') as a GeoTIFF in LV95 (EPSG:2056).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load dataset ---\n",
    "    ds = xr.open_zarr(os.path.join(path_RGIs, f\"{rgi_gl}.zarr\"))\n",
    "\n",
    "    # --- Check for glacier mask ---\n",
    "    if \"glacier_mask\" not in ds:\n",
    "        raise ValueError(\n",
    "            f\"'glacier_mask' variable not found in dataset {rgi_gl}\")\n",
    "\n",
    "    # --- Build mask (NaN outside glacier) ---\n",
    "    glacier_mask = np.where(ds[\"glacier_mask\"].values == 0, np.nan,\n",
    "                            ds[\"glacier_mask\"].values)\n",
    "\n",
    "    # --- Apply mask to core variables ---\n",
    "    ds = ds.assign(masked_slope=glacier_mask * ds[\"slope\"])\n",
    "    ds = ds.assign(masked_elev=glacier_mask * ds[\"topo\"])\n",
    "    ds = ds.assign(masked_aspect=glacier_mask * ds[\"aspect\"])\n",
    "    ds = ds.assign(masked_dis=glacier_mask * ds[\"dis_from_border\"])\n",
    "\n",
    "    # --- Optional fields ---\n",
    "    if \"hugonnet_dhdt\" in ds:\n",
    "        ds = ds.assign(masked_hug=glacier_mask * ds[\"hugonnet_dhdt\"])\n",
    "    if \"consensus_ice_thickness\" in ds:\n",
    "        ds = ds.assign(masked_cit=glacier_mask * ds[\"consensus_ice_thickness\"])\n",
    "    if \"millan_v\" in ds:\n",
    "        ds = ds.assign(masked_miv=glacier_mask * ds[\"millan_v\"])\n",
    "\n",
    "    # --- Get indices where glacier_mask == 1 ---\n",
    "    glacier_indices = np.where(ds[\"glacier_mask\"].values == 1)\n",
    "\n",
    "    # --- Extract masked elevation ---\n",
    "    dem = ds[\"masked_elev\"]\n",
    "\n",
    "    # --- Attach CRS and write GeoTIFF ---\n",
    "    dem = dem.rio.write_crs(\"EPSG:2056\", inplace=True)\n",
    "\n",
    "    # Prepare output folder\n",
    "    os.makedirs(path_out_tiff, exist_ok=True)\n",
    "    out_tif = os.path.join(path_out_tiff, f\"{rgi_gl}.tif\")\n",
    "\n",
    "    dem.rio.to_raster(\n",
    "        out_tif,\n",
    "        dtype=\"float32\",\n",
    "        compress=\"LZW\",\n",
    "        BIGTIFF=\"IF_SAFER\",\n",
    "        tiled=True,\n",
    "        predictor=3,  # better compression for float rasters\n",
    "    )\n",
    "\n",
    "    #print(f\"Saved masked DEM to: {out_tif}\")\n",
    "\n",
    "    return ds, glacier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\",\n",
    "                             \"xr_masked_grids/\")\n",
    "path_xr_svf = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\",\n",
    "                           \"svf_nc_latlon/\")\n",
    "path_RGIs = cfg.dataPath + path_OGGM + \"xr_grids/\"\n",
    "path_geotiff = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\",\n",
    "                            \"geotiff_meters_lv95/\")\n",
    "\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11.6\")\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # 1) Build masked OGGM grid in LV95 (x/y)\n",
    "            ds, glacier_indices = create_masked_glacier(\n",
    "                path_RGIs, rgi_gl, path_geotiff)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2) (Optional) coarsen in projected space\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "        if 20 < dx_m < 50:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=50)\n",
    "\n",
    "        # 3) Reproject masked grid to WGS84 lat/lon\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\").rename({\n",
    "            \"x\": \"lon\",\n",
    "            \"y\": \"lat\"\n",
    "        })\n",
    "\n",
    "        # 4) Load corresponding SVF (already in lat/lon) and merge\n",
    "        svf_path = os.path.join(path_xr_svf, f\"{rgi_gl}_svf_latlon.nc\")\n",
    "        if not os.path.exists(svf_path):\n",
    "            print(f\"SVF not found for {rgi_gl}: {svf_path}\")\n",
    "            # still save ds_latlon without SVF\n",
    "            ds_latlon.to_zarr(os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\"))\n",
    "            continue\n",
    "\n",
    "        ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "        # Make sure coords are named lon/lat\n",
    "        if \"x\" in ds_svf.dims or \"y\" in ds_svf.dims:\n",
    "            ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "        if \"longitude\" in ds_svf.dims or \"latitude\" in ds_svf.dims:\n",
    "            ds_svf = ds_svf.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "\n",
    "        # Sort ascending for interpolation stability\n",
    "        if ds_latlon.lon[0] > ds_latlon.lon[-1]:\n",
    "            ds_latlon = ds_latlon.sortby(\"lon\")\n",
    "        if ds_latlon.lat[0] > ds_latlon.lat[-1]:\n",
    "            ds_latlon = ds_latlon.sortby(\"lat\")\n",
    "        if ds_svf.lon[0] > ds_svf.lon[-1]: ds_svf = ds_svf.sortby(\"lon\")\n",
    "        if ds_svf.lat[0] > ds_svf.lat[-1]: ds_svf = ds_svf.sortby(\"lat\")\n",
    "\n",
    "        svf_vars = [\n",
    "            v for v in [\"svf\", \"asvf\", \"opns\"] if v in ds_svf.data_vars\n",
    "        ]\n",
    "\n",
    "        # If grids match, merge; else interpolate SVF to ds_latlon grid\n",
    "        if (np.array_equal(ds_latlon.lon.values, ds_svf.lon.values)\n",
    "                and np.array_equal(ds_latlon.lat.values, ds_svf.lat.values)):\n",
    "            ds_latlon = xr.merge([ds_latlon, ds_svf[svf_vars]])\n",
    "        else:\n",
    "            svf_on_grid = ds_svf[svf_vars].interp(lon=ds_latlon.lon,\n",
    "                                                  lat=ds_latlon.lat,\n",
    "                                                  method=\"linear\")\n",
    "            for v in svf_vars:\n",
    "                svf_on_grid[v] = svf_on_grid[v].astype(\"float32\")\n",
    "            ds_latlon = ds_latlon.assign(\n",
    "                **{v: svf_on_grid[v]\n",
    "                   for v in svf_vars})\n",
    "\n",
    "        # Add masked versions using glacier_mask already in ds_latlon\n",
    "        if \"glacier_mask\" in ds_latlon:\n",
    "            gmask = xr.where(ds_latlon[\"glacier_mask\"] == 1, 1.0, np.nan)\n",
    "            for v in svf_vars:\n",
    "                ds_latlon[f\"masked_{v}\"] = gmask * ds_latlon[v]\n",
    "\n",
    "        # 5) Save final lat/lon grid (with SVF) to Zarr\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path)\n",
    "\n",
    "rhone_rgi = \"RGI60-11.01238\"\n",
    "rhone_path = os.path.join(path_geotiff, f\"{rhone_rgi}.tif\")\n",
    "\n",
    "# Open DEM\n",
    "rhone = rioxarray.open_rasterio(rhone_path).squeeze()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "rhone.plot(cmap=\"terrain\")\n",
    "plt.title(f\"DEM of Glacier {rhone_rgi}\", fontsize=13)\n",
    "plt.xlabel(\"Easting [m]\")\n",
    "plt.ylabel(\"Northing [m]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open xarray grid\n",
    "xr.open_zarr(path_xr_grids + f'{rhone_rgi}.zarr').svf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Climate columns\n",
    "# vois_climate = [\n",
    "#     't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "# ]\n",
    "# # Topographical columns\n",
    "# vois_topographical = [\n",
    "#     \"aspect\",\n",
    "#     \"slope\",\n",
    "#     \"hugonnet_dhdt\",\n",
    "#     \"consensus_ice_thickness\",\n",
    "#     \"millan_v\",\n",
    "#     \"topo\",\n",
    "#     \"svf\"\n",
    "# ]\n",
    "\n",
    "# RUN = True\n",
    "# path_rgi_alps = os.path.join(cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "# #emptyfolder(path_rgi_alps)\n",
    "\n",
    "# # ---- helpers ----\n",
    "# def expected_fname(rgi_gl: str, year: int) -> str:\n",
    "#     # Expected: RGI60-11.00001_grid_1999.parquet\n",
    "#     return f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "\n",
    "# def years_present_for_glacier(folder_path: str, rgi_gl: str) -> set:\n",
    "#     \"\"\"Return the set of 4-digit years found for this glacier in its output folder.\"\"\"\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         return set()\n",
    "#     rx = re.compile(rf\"^{re.escape(rgi_gl)}_grid_(\\d{{4}})\\.parquet$\")\n",
    "#     years_found = set()\n",
    "#     for f in os.listdir(folder_path):\n",
    "#         m = rx.match(f)\n",
    "#         if m:\n",
    "#             years_found.add(int(m.group(1)))\n",
    "#     return years_found\n",
    "\n",
    "# def glacier_is_complete(rgi_gl: str, years: range) -> bool:\n",
    "#     folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "#     found = years_present_for_glacier(folder_path, rgi_gl)\n",
    "#     return set(years).issubset(found)\n",
    "\n",
    "# # ---- main ----\n",
    "# if RUN:\n",
    "#     # inclusive 1999..2024\n",
    "#     years = range(1999, 2025)\n",
    "\n",
    "#     os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "\n",
    "#     valid_rgis = [\n",
    "#         f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "#         if f.endswith('.zarr')\n",
    "#     ]\n",
    "\n",
    "#     # Glaciers that are already complete (all yearly files exist)\n",
    "#     complete_rgis = [r for r in valid_rgis if glacier_is_complete(r, years)]\n",
    "#     # Glaciers that still need work\n",
    "#     rest_rgis = list(set(valid_rgis) - set(complete_rgis))\n",
    "\n",
    "#     print(f\"Glaciers already complete: {len(complete_rgis)}\")\n",
    "#     print(f\"Number of glaciers to process: {len(rest_rgis)}\")\n",
    "\n",
    "#     for gdir in tqdm(gdirs, desc=\"Processing glaciers\"):\n",
    "#         rgi_gl = gdir.rgi_id\n",
    "\n",
    "#         if rgi_gl not in valid_rgis:\n",
    "#             print(f\"Skipping {rgi_gl}: not found in valid RGI glaciers\")\n",
    "#             continue\n",
    "\n",
    "#         # Skip if already fully complete\n",
    "#         if glacier_is_complete(rgi_gl, years):\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "\n",
    "#             # Open Zarr\n",
    "#             try:\n",
    "#                 ds = xr.open_zarr(file_path, consolidated=True)\n",
    "#             except Exception:\n",
    "#                 ds = xr.open_zarr(file_path)\n",
    "\n",
    "#             # Build grid for all years once\n",
    "#             try:\n",
    "#                 df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed creating glacier grid for {rgi_gl}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             df_grid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#             # Add GLWD_ID and GLACIER columns\n",
    "#             df_grid['GLWD_ID'] = [\n",
    "#                 mbm.data_processing.utils.get_hash(f\"{r}_{y}\")\n",
    "#                 for r, y in zip(df_grid['RGIId'].astype(str),\n",
    "#                                 df_grid['YEAR'].astype(str))\n",
    "#             ]\n",
    "#             df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "#             df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "#             # Output folder\n",
    "#             folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "#             os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#             # Determine missing years for this glacier (idempotent)\n",
    "#             existing_years = years_present_for_glacier(folder_path, rgi_gl)\n",
    "#             missing_years = [y for y in years if y not in existing_years]\n",
    "\n",
    "#             if not missing_years:\n",
    "#                 # Another process may have finished meanwhile\n",
    "#                 continue\n",
    "\n",
    "#             for year in missing_years:\n",
    "#                 try:\n",
    "#                     df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "#                     if df_grid_y.empty:\n",
    "#                         # No data for that year; keep going\n",
    "#                         continue\n",
    "\n",
    "#                     # Build dataset & add climate features\n",
    "#                     try:\n",
    "#                         dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "#                             cfg=cfg,\n",
    "#                             data=df_grid_y,\n",
    "#                             region_name='CH',\n",
    "#                             region_id=11,\n",
    "#                             data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv)\n",
    "#                         )\n",
    "\n",
    "#                         era5_climate_data = os.path.join(\n",
    "#                             cfg.dataPath, path_ERA5_raw, 'era5_monthly_averaged_data_Alps.nc'\n",
    "#                         )\n",
    "#                         geopotential_data = os.path.join(\n",
    "#                             cfg.dataPath, path_ERA5_raw, 'era5_geopotential_pressure_Alps.nc'\n",
    "#                         )\n",
    "\n",
    "#                         dataset_grid_yearly.get_climate_features(\n",
    "#                             climate_data=era5_climate_data,\n",
    "#                             geopotential_data=geopotential_data,\n",
    "#                             change_units=True,\n",
    "#                             smoothing_vois={'vois_climate': vois_climate,\n",
    "#                                             'vois_other': ['ALTITUDE_CLIMATE']}\n",
    "#                         )\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Failed adding climate features for {rgi_gl} (year {year}): {e}\")\n",
    "#                         continue\n",
    "\n",
    "#                     vois_topographical_sub = [voi for voi in vois_topographical\n",
    "#                                               if voi in df_grid_y.columns]\n",
    "\n",
    "#                     dataset_grid_yearly.convert_to_monthly(\n",
    "#                         meta_data_columns=cfg.metaData,\n",
    "#                         vois_climate=vois_climate,\n",
    "#                         vois_topographical=vois_topographical_sub\n",
    "#                     )\n",
    "\n",
    "#                     save_path = os.path.join(folder_path, expected_fname(rgi_gl, year))\n",
    "#                     # If a stale/partial file exists, overwrite it\n",
    "#                     dataset_grid_yearly.data.to_parquet(\n",
    "#                         save_path, engine=\"pyarrow\", compression=\"snappy\"\n",
    "#                     )\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed processing {rgi_gl} for year {year}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with glacier {rgi_gl}: {e}\")\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"svf\"\n",
    "]\n",
    "\n",
    "RUN = True\n",
    "path_rgi_alps = os.path.join(cfg.dataPath,\n",
    "                             'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "# #emptyfolder(path_rgi_alps)\n",
    "\n",
    "# Avoid BLAS/OpenMP oversubscription inside each worker\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ------------ config ------------\n",
    "years = range(2000, 2024)  # inclusive\n",
    "#max_workers = max(1, min(os.cpu_count() or 4, 8))  # be gentle with I/O\n",
    "#max_workers = min(os.cpu_count(), 16)   # or 20 if SSD & RAM are strong\n",
    "max_workers = min(os.cpu_count(), 12)  # or 20 if SSD & RAM are strong\n",
    "\n",
    "# ------------ helpers (unchanged) ------------\n",
    "def expected_fname(rgi_gl: str, year: int) -> str:\n",
    "    return f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "\n",
    "\n",
    "def years_present_for_glacier(folder_path: str, rgi_gl: str) -> set:\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return set()\n",
    "    rx = re.compile(rf\"^{re.escape(rgi_gl)}_grid_(\\d{{4}})\\.parquet$\")\n",
    "    years_found = set()\n",
    "    for f in os.listdir(folder_path):\n",
    "        m = rx.match(f)\n",
    "        if m:\n",
    "            years_found.add(int(m.group(1)))\n",
    "    return years_found\n",
    "\n",
    "\n",
    "def glacier_is_complete(rgi_gl: str, years: range) -> bool:\n",
    "    folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "    found = years_present_for_glacier(folder_path, rgi_gl)\n",
    "    return set(years).issubset(found)\n",
    "\n",
    "\n",
    "# ------------ per-glacier worker ------------\n",
    "def process_one_glacier(rgi_gl: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (rgi_gl, 'ok') or (rgi_gl, 'skip:<reason>'/'error:<message>')\n",
    "    Runs in a separate process: DO NOT capture big globals except paths & configs safely.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input files\n",
    "        file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return (rgi_gl, f\"skip:missing_zarr {file_path}\")\n",
    "\n",
    "        # Skip if already fully complete\n",
    "        folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        existing_years = years_present_for_glacier(folder_path, rgi_gl)\n",
    "        missing_years = [y for y in years if y not in existing_years]\n",
    "        if not missing_years:\n",
    "            return (rgi_gl, \"skip:complete\")\n",
    "\n",
    "        # Open Zarr *inside* the worker\n",
    "        try:\n",
    "            ds = xr.open_zarr(file_path, consolidated=True)\n",
    "        except Exception:\n",
    "            ds = xr.open_zarr(file_path)\n",
    "\n",
    "        # Build grid (once) for all years\n",
    "        try:\n",
    "            df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "        except Exception as e:\n",
    "            return (rgi_gl, f\"error:create_grid {e}\")\n",
    "\n",
    "        df_grid = df_grid.reset_index(drop=True)\n",
    "\n",
    "        # GLWD_ID & GLACIER\n",
    "        df_grid['GLWD_ID'] = [\n",
    "            mbm.data_processing.utils.get_hash(f\"{r}_{y}\") for r, y in zip(\n",
    "                df_grid['RGIId'].astype(str), df_grid['YEAR'].astype(str))\n",
    "        ]\n",
    "        df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "        df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "        # Process only the missing years\n",
    "        for year in missing_years:\n",
    "            try:\n",
    "                df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "                if df_grid_y.empty:\n",
    "                    continue\n",
    "\n",
    "                # Build dataset & add climate features\n",
    "                dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                    cfg=cfg,\n",
    "                    data=df_grid_y,\n",
    "                    region_name='CH',\n",
    "                    region_id=11,\n",
    "                    data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv))\n",
    "\n",
    "                era5_climate_data = os.path.join(\n",
    "                    cfg.dataPath, path_ERA5_raw,\n",
    "                    'era5_monthly_averaged_data_Alps.nc')\n",
    "                geopotential_data = os.path.join(\n",
    "                    cfg.dataPath, path_ERA5_raw,\n",
    "                    'era5_geopotential_pressure_Alps.nc')\n",
    "\n",
    "                dataset_grid_yearly.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True,\n",
    "                    smoothing_vois={\n",
    "                        'vois_climate': vois_climate,\n",
    "                        'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                    })\n",
    "\n",
    "                vois_topographical_sub = [\n",
    "                    v for v in vois_topographical if v in df_grid_y.columns\n",
    "                ]\n",
    "\n",
    "                dataset_grid_yearly.convert_to_monthly(\n",
    "                    meta_data_columns=cfg.metaData,\n",
    "                    vois_climate=vois_climate,\n",
    "                    vois_topographical=vois_topographical_sub)\n",
    "\n",
    "                save_path = os.path.join(folder_path,\n",
    "                                         expected_fname(rgi_gl, year))\n",
    "                dataset_grid_yearly.data.to_parquet(save_path,\n",
    "                                                    engine=\"pyarrow\",\n",
    "                                                    compression=\"snappy\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # keep going with other years but record error\n",
    "                return (rgi_gl, f\"error:year_{year} {e}\")\n",
    "\n",
    "        return (rgi_gl, \"ok\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (rgi_gl, f\"error:{e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "# ------------ main parallel driver ------------\n",
    "if RUN:\n",
    "    os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "    #emptyfolder(path_rgi_alps)\n",
    "\n",
    "    valid_rgis = [\n",
    "        f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "        if f.endswith('.zarr')\n",
    "    ]\n",
    "\n",
    "    # Filter to those not fully complete\n",
    "    targets = [r for r in valid_rgis if not glacier_is_complete(r, years)]\n",
    "    print(\n",
    "        f\"Total valid glaciers: {len(valid_rgis)} | Remaining to process: {len(targets)}\"\n",
    "    )\n",
    "\n",
    "    results = {\"ok\": [], \"skip\": [], \"error\": []}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(process_one_glacier, rgi): rgi for rgi in targets}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Glaciers\"):\n",
    "            rgi = futs[fut]\n",
    "            try:\n",
    "                rid, status = fut.result()\n",
    "            except Exception as e:\n",
    "                rid, status = rgi, f\"error:{e}\"\n",
    "            if status.startswith(\"ok\"):\n",
    "                results[\"ok\"].append(rgi)\n",
    "            elif status.startswith(\"skip\"):\n",
    "                results[\"skip\"].append((rgi, status))\n",
    "            else:\n",
    "                results[\"error\"].append((rgi, status))\n",
    "\n",
    "    print(\n",
    "        f\"\\nFinished. ok={len(results['ok'])}, skip={len(results['skip'])}, error={len(results['error'])}\"\n",
    "    )\n",
    "    if results[\"error\"]:\n",
    "        for rgi, msg in results[\"error\"][:10]:\n",
    "            print(\"  \", rgi, \"â\", msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.00001':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2004\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "year = 2000\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
