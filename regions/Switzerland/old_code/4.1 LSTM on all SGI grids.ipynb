{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.utils import *\n",
    "from scripts.glamos import *\n",
    "from scripts.models import *\n",
    "from scripts.geo_data import *\n",
    "from scripts.dataset import *\n",
    "from scripts.geodetic import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DEMs to geotiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_list = [\n",
    "    re.split('_',\n",
    "             re.split('.grid', f)[0])[1]\n",
    "    for f in os.listdir(os.path.join(cfg.dataPath, path_SGI_topo, 'aspect'))\n",
    "]\n",
    "\n",
    "path_out_tiff = os.path.join(cfg.dataPath,\n",
    "                             \"GLAMOS/topo/SGI2020/DEMs_geotiff_lv95/\")\n",
    "os.makedirs(path_out_tiff, exist_ok=True)\n",
    "\n",
    "# read shapefile once per worker process\n",
    "shp_path = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                        \"inventory_sgi2016_r2020/SGI_2016_glaciers.shp\")\n",
    "glacier_outline_sgi = gpd.read_file(shp_path)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    for sgi_id in tqdm(sgi_list):\n",
    "        path_DEM = os.path.join(cfg.dataPath, path_SGI_topo, \"dem_HR\")\n",
    "        dem_gl = [f for f in os.listdir(path_DEM) if sgi_id in f][0]\n",
    "        metadata_dem, grid_data_dem = load_grid_file(join(path_DEM, dem_gl))\n",
    "        dem = convert_to_xarray_geodata(grid_data_dem, metadata_dem)\n",
    "        gdf_mask_gl = glacier_outline_sgi[glacier_outline_sgi[\"sgi-id\"] ==\n",
    "                                          sgi_id]\n",
    "\n",
    "        mask, masked_dem = extract_topo_over_outline(dem,\n",
    "                                                     gdf_mask_gl,\n",
    "                                                     target_crs=2056)\n",
    "\n",
    "        # --- Attach CRS and write GeoTIFF ---\n",
    "        masked_dem = masked_dem.rio.write_crs(\"EPSG:2056\", inplace=True)\n",
    "\n",
    "        # Prepare output folder\n",
    "        os.makedirs(path_out_tiff, exist_ok=True)\n",
    "        out_tif = os.path.join(path_out_tiff, f\"{sgi_id}.tif\")\n",
    "        masked_dem.rio.to_raster(\n",
    "            out_tif,\n",
    "            dtype=\"float32\",\n",
    "            compress=\"LZW\",\n",
    "            BIGTIFF=\"IF_SAFER\",\n",
    "            tiled=True,\n",
    "            predictor=3,  # better compression for float rasters\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked xarrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgi_list = [\n",
    "    re.split('_',\n",
    "             re.split('.grid', f)[0])[1]\n",
    "    for f in os.listdir(os.path.join(cfg.dataPath, path_SGI_topo, 'aspect'))\n",
    "]\n",
    "\n",
    "# unique SGI IDs\n",
    "sgi_list = list(set(sgi_list))\n",
    "print('Number of unique SGI IDs:', len(sgi_list))\n",
    "\n",
    "glaciers_glamos_dems = os.listdir(\n",
    "    os.path.join(cfg.dataPath, path_GLAMOS_topo, 'lv95'))\n",
    "\n",
    "path_xr_svf = os.path.join(cfg.dataPath, \"GLAMOS/topo/SGI2020/svf_nc_latlon/\")\n",
    "os.makedirs(path_xr_svf, exist_ok=True)\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    # Create SGI topographical masks\n",
    "    # Note: This function will take a while to run\n",
    "    # It creates a mask for each glacier in the SGI list\n",
    "    # and saves them in the specified directory.\n",
    "    path_save = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                             'xr_masked_grids_sgi/')\n",
    "    emptyfolder(path_save)\n",
    "    create_sgi_topo_masks_parallel(cfg,\n",
    "                                   path_xr_svf,\n",
    "                                   sgi_list,\n",
    "                                   type='sgi_id',\n",
    "                                   path_save=path_save)\n",
    "path = os.path.join(cfg.dataPath, path_SGI_topo, 'xr_masked_grids_sgi/')\n",
    "xr.open_dataset(path + 'A10g-02.zarr').masked_aspect.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(path + 'A10g-02.zarr').svf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                          'CH_wgms_dataset_all.csv')\n",
    "gl_area = get_gl_area(cfg)\n",
    "areas_train_set = [\n",
    "    gl_area[gl] for gl in data_glamos['GLACIER'].unique()\n",
    "    if gl in gl_area.keys()\n",
    "]\n",
    "\n",
    "# histogram\n",
    "plt.hist(areas_train_set, bins=50)\n",
    "plt.xlabel('Area (km2)')\n",
    "plt.title('Histogram of glacier areas with stakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "shapefile_path = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                              'inventory_sgi2016_r2020',\n",
    "                              'SGI_2016_glaciers.shp')\n",
    "gdf_shapefiles = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Histogram of area:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.histplot(gdf_shapefiles.area / (10**6),\n",
    "             color='blue',\n",
    "             kde=True,\n",
    "             bins=50,\n",
    "             ax=axs[0])\n",
    "\n",
    "# boxplot\n",
    "sns.boxplot(x=gdf_shapefiles.area / (10**6), color='blue', ax=axs[1])\n",
    "\n",
    "# set x label to km2\n",
    "axs[0].set_xlabel('Area (km2)')\n",
    "axs[1].set_xlabel('Area (km2)')\n",
    "\n",
    "plt.suptitle('Histogram and Boxplot of all glaciers in SGI 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create monthly grids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016 - 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QUIET parallel run: tqdm in notebook, detailed logs to file ---\n",
    "\n",
    "import os, io, sys, logging, warnings, multiprocessing as mp\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"svf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- per-process caches for heavy I/O ----------\n",
    "_GDF_SGI = None\n",
    "_GDF_RGI = None\n",
    "\n",
    "\n",
    "def _load_outlines_once(cfg):\n",
    "    \"\"\"Load GeoDataFrames once per worker; cache in globals.\"\"\"\n",
    "    global _GDF_SGI, _GDF_RGI\n",
    "    if _GDF_SGI is None:\n",
    "        import geopandas as gpd\n",
    "        shp_sgi = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                               'inventory_sgi2016_r2020',\n",
    "                               'SGI_2016_glaciers_copy.shp')\n",
    "        _GDF_SGI = gpd.read_file(shp_sgi)\n",
    "    if _GDF_RGI is None:\n",
    "        import geopandas as gpd\n",
    "        _GDF_RGI = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "    return _GDF_SGI, _GDF_RGI\n",
    "\n",
    "\n",
    "# ---------- worker init: cap threads + silence worker stdout/stderr ----------\n",
    "def _worker_init_quiet():\n",
    "    # silence prints in workers\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "    # cap BLAS threads to avoid oversubscription\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"1\")\n",
    "    try:\n",
    "        import torch\n",
    "        torch.set_num_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ---------- one task (sgi_id, year) ----------\n",
    "def _process_glacier_year(sgi_id, year, cfg, year_out_dir):\n",
    "    \"\"\"\n",
    "    Returns: (status, sgi_id, year, message)\n",
    "      status in {\"success\",\"skipped\",\"error\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        import pandas as pd\n",
    "\n",
    "        # load outlines once per worker\n",
    "        glacier_outline_sgi, glacier_outline_rgi = _load_outlines_once(cfg)\n",
    "\n",
    "        # --- load coarsened SGI topo (zarr) ---\n",
    "        path_zarr_root = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                                      'xr_masked_grids_sgi/')\n",
    "        zarr_path = os.path.join(path_zarr_root, f\"{sgi_id}.zarr\")\n",
    "        try:\n",
    "            ds_coarsened = xr.open_dataset(zarr_path)\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"load zarr error: {e}\")\n",
    "\n",
    "        # --- build grid dataframe ---\n",
    "        try:\n",
    "            rgi_id = None\n",
    "            df_grid = create_glacier_grid_SGI(sgi_id, year, rgi_id,\n",
    "                                              ds_coarsened)\n",
    "            df_grid = df_grid.reset_index(drop=True)\n",
    "            dataset_grid = mbm.data_processing.Dataset(cfg=cfg,\n",
    "                                                       data=df_grid,\n",
    "                                                       region_name='CH',\n",
    "                                                       region_id=11,\n",
    "                                                       data_path=cfg.dataPath +\n",
    "                                                       path_PMB_GLAMOS_csv)\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"create grid error: {e}\")\n",
    "\n",
    "        # --- add climate features ---\n",
    "        try:\n",
    "            era5_climate_data = os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                                             'era5_monthly_averaged_data.nc')\n",
    "            geopotential_data = os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                                             'era5_geopotential_pressure.nc')\n",
    "            dataset_grid.get_climate_features(\n",
    "                climate_data=era5_climate_data,\n",
    "                geopotential_data=geopotential_data,\n",
    "                change_units=True,\n",
    "                smoothing_vois={\n",
    "                    'vois_climate': vois_climate,\n",
    "                    'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                })\n",
    "            if dataset_grid.data.empty:\n",
    "                return (\"error\", sgi_id, year, \"no climate rows\")\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"climate features error: {e}\")\n",
    "\n",
    "        # --- intersect with RGI + add OGGM features ---\n",
    "        try:\n",
    "            df_y_gl = dataset_grid.data\n",
    "            df_y_gl = df_y_gl.rename(columns={'RGIId': 'RGIId_old'})\n",
    "            df_y_gl = mbm.data_processing.utils.get_rgi(\n",
    "                data=df_y_gl, glacier_outlines=glacier_outline_rgi)\n",
    "            df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "            if df_y_gl.empty:\n",
    "                return (\"skipped\", sgi_id, year, \"no RGI intersection\")\n",
    "\n",
    "            voi = [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"]\n",
    "            df_y_gl = add_OGGM_features(df_y_gl, voi, cfg.dataPath + path_OGGM)\n",
    "            df_y_gl['GLWD_ID'] = df_y_gl.apply(\n",
    "                lambda x: mbm.data_processing.utils.get_hash(\n",
    "                    f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "                axis=1).astype(str)\n",
    "\n",
    "            dataset_grid = mbm.data_processing.Dataset(cfg=cfg,\n",
    "                                                       data=df_y_gl,\n",
    "                                                       region_name='CH',\n",
    "                                                       region_id=11,\n",
    "                                                       data_path=cfg.dataPath +\n",
    "                                                       path_PMB_GLAMOS_csv)\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"OGGM features error: {e}\")\n",
    "\n",
    "        # --- to monthly ---\n",
    "        try:\n",
    "            dataset_grid.convert_to_monthly(\n",
    "                meta_data_columns=cfg.metaData,\n",
    "                vois_climate=vois_climate,\n",
    "                vois_topographical=vois_topographical)\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"convert monthly error: {e}\")\n",
    "\n",
    "        # --- final save ---\n",
    "        try:\n",
    "            df_oggm = dataset_grid.data.copy()\n",
    "            df_oggm = df_oggm.rename(columns={\n",
    "                'aspect': 'aspect_sgi',\n",
    "                'slope': 'slope_sgi'\n",
    "            })\n",
    "            df_oggm['POINT_ELEVATION'] = df_oggm['topo']\n",
    "            save_path = os.path.join(year_out_dir,\n",
    "                                     f\"{sgi_id}_grid_{year}.parquet\")\n",
    "            df_oggm.to_parquet(save_path,\n",
    "                               engine=\"pyarrow\",\n",
    "                               compression=\"snappy\")\n",
    "        except Exception as e:\n",
    "            return (\"error\", sgi_id, year, f\"save error: {e}\")\n",
    "\n",
    "        return (\"success\", sgi_id, year, \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"error\", sgi_id, year, f\"unexpected: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- logging (single text log, no console spam) ----------\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_PATH = f\"logs/process_log_SGI_full.log\"\n",
    "logging.basicConfig(\n",
    "    filename=LOG_PATH,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"sgi\")\n",
    "\n",
    "# ---------- run (shows only tqdm) ----------\n",
    "years = range(2016, 2023)\n",
    "RUN = False\n",
    "\n",
    "# optional: wipe the root yearly output dir ONCE, before loop\n",
    "root_out = os.path.join(cfg.dataPath,\n",
    "                        'GLAMOS/topo/gridded_topo_inputs/SGI2020_all')\n",
    "\n",
    "\n",
    "# helper to silence main-process stdout/stderr within the run\n",
    "class _Devnull(io.StringIO):\n",
    "\n",
    "    def write(self, *args, **kwargs):\n",
    "        return 0\n",
    "\n",
    "\n",
    "if RUN:\n",
    "    emptyfolder(root_out)\n",
    "    ctx = mp.get_context(\"fork\")  # Linux\n",
    "    max_workers = min(max(1, (os.cpu_count() or 2) - 1), 32)\n",
    "\n",
    "    with redirect_stdout(_Devnull()):  # keep stderr open for tqdm\n",
    "        for year in years:\n",
    "            year_out_dir = os.path.join(root_out, str(year))\n",
    "            if not os.path.exists(year_out_dir):\n",
    "                os.makedirs(year_out_dir, exist_ok=True)\n",
    "                log.info(f\"Created directory {year_out_dir}\")\n",
    "            else:\n",
    "                emptyfolder(year_out_dir)\n",
    "                log.info(f\"Emptied directory {year_out_dir}\")\n",
    "\n",
    "            ok = skip = err = 0\n",
    "            with ProcessPoolExecutor(\n",
    "                    max_workers=max_workers,\n",
    "                    initializer=_worker_init_quiet,\n",
    "                    mp_context=ctx,\n",
    "            ) as ex:\n",
    "                futures = [\n",
    "                    ex.submit(_process_glacier_year, sgi_id, year, cfg,\n",
    "                              year_out_dir) for sgi_id in sgi_list\n",
    "                ]\n",
    "\n",
    "                for fut in tqdm(as_completed(futures),\n",
    "                                total=len(futures),\n",
    "                                desc=f\"Year {year} ({max_workers} workers)\"):\n",
    "                    try:\n",
    "                        status, sgi_id, y, message = fut.result()\n",
    "                    except Exception as e:\n",
    "                        status, sgi_id, y, message = \"error\", \"unknown\", year, str(\n",
    "                            e)\n",
    "\n",
    "                    if status == \"success\":\n",
    "                        ok += 1\n",
    "                        #log.info(f\"SUCCESS {sgi_id} {y}\")\n",
    "                    elif status == \"skipped\":\n",
    "                        skip += 1\n",
    "                        #log.warning(f\"SKIP    {sgi_id} {y}: {message}\")\n",
    "                    else:\n",
    "                        err += 1\n",
    "                        #log.error(f\"ERROR   {sgi_id} {y}: {message}\")\n",
    "\n",
    "            log.info(\n",
    "                f\"SUMMARY {year}: ok={ok} skip={skip} err={err} total={len(futures)}\"\n",
    "            )\n",
    "\n",
    "print(f\"Run complete. See log: {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "path_save_monthly = os.path.join(root_out, f'{year}')\n",
    "\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "\n",
    "# Plot all OGGM variables\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_save_monthly, f\"{sgi_id}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'svf', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect', 'slope', 'hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v',\n",
    "    'svf'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_all_SGI.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': len(MONTHLY_COLS),\n",
    "    'Fs': len(STATIC_COLS),\n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.2,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': True,\n",
    "    'head_dropout': 0.0\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_SGI2020.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "# Evaluate on test\n",
    "# model_filename = 'models/lstm_model_2025-10-13_SGI2020.pt'\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on SGI 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- quiet main logging (optional) -----------------\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_PATH = f\"logs/predict_glaciers_{datetime.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(filename=LOG_PATH,\n",
    "                    level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "log = logging.getLogger(\"predict\")\n",
    "\n",
    "# ----------------- constants & paths -----------------\n",
    "REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "all_columns = MONTHLY_COLS + STATIC_COLS + cfg.fieldsNotFeatures\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/swisswide')\n",
    "os.makedirs(path_save_glw, exist_ok=True)\n",
    "emptyfolder(path_save_glw)\n",
    "path_xr_grids = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                             'xr_masked_grids_sgi/')\n",
    "\n",
    "path_gridded_inputs = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/SGI2020_all')\n",
    "\n",
    "\n",
    "# ----------------- worker init (quiet + CPU threads cap) -----------------\n",
    "def _worker_init_quiet():\n",
    "    # keep stderr for tqdm in main; silence worker prints\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")  # CPU only\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"1\")\n",
    "    try:\n",
    "        torch.set_num_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ----------------- per-process model cache -----------------\n",
    "_MODEL = None\n",
    "\n",
    "\n",
    "def _get_model_cpu(cfg, params_used, model_filename):\n",
    "    \"\"\"Build+load the model once per worker (cached).\"\"\"\n",
    "    global _MODEL\n",
    "    if _MODEL is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "                                                           params_used,\n",
    "                                                           device,\n",
    "                                                           verbose=False)\n",
    "        state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "        _MODEL = model\n",
    "    return _MODEL\n",
    "\n",
    "\n",
    "# ----------------- one glacier-year task -----------------\n",
    "def _process_glacier_year(args):\n",
    "    glacier_name, year = args  # everything else taken from globals via fork\n",
    "    try:\n",
    "        # Seed for reproducibility if you wish\n",
    "        seed_all(cfg.seed)\n",
    "\n",
    "        glacier_path = os.path.join(path_gridded_inputs, year)\n",
    "        if not os.path.exists(glacier_path):\n",
    "            return (\"skip\", glacier_name, year, \"glacier folder missing\")\n",
    "\n",
    "        file_name = f\"{glacier_name}_grid_{year}.parquet\"\n",
    "        parquet_path = os.path.join(glacier_path, file_name)\n",
    "        if not os.path.exists(parquet_path):\n",
    "            return (\"skip\", glacier_name, year, \"parquet missing\")\n",
    "\n",
    "        df_grid_monthly = pd.read_parquet(parquet_path).copy()\n",
    "        df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "        # rename columns\n",
    "        df_grid_monthly.rename(columns={\n",
    "            'aspect_sgi': 'aspect',\n",
    "            'slope_sgi': 'slope'\n",
    "        },\n",
    "                               inplace=True)\n",
    "\n",
    "        # Keep required + feature columns; preserve order\n",
    "        needed = set(all_columns) | set(REQUIRED)\n",
    "        keep = [c for c in df_grid_monthly.columns if c in needed]\n",
    "        df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "        # Build winter subset (Sep–Apr)\n",
    "        winter_months = [\n",
    "            'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr'\n",
    "        ]\n",
    "        df_grid_monthly_w = (\n",
    "            df_grid_monthly[df_grid_monthly['MONTHS'].str.lower().isin(\n",
    "                winter_months)].copy().dropna(subset=['ID', 'MONTHS']))\n",
    "        df_grid_monthly_w['PERIOD'] = 'winter'\n",
    "\n",
    "        # Minimal NaN cleanup for annual\n",
    "        df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "\n",
    "        # Fit scalers on TRAIN only (clone/train ds are global via fork)\n",
    "        ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train)\n",
    "        ds_train_copy.fit_scalers(train_idx)\n",
    "\n",
    "        # Annual dataset/loader\n",
    "        ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_grid_monthly_a,\n",
    "            MONTHLY_COLS,\n",
    "            STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad,\n",
    "            months_head_pad=months_head_pad,\n",
    "            expect_target=False,\n",
    "            show_progress=False)\n",
    "        test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_gl_a, ds_train_copy, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "        # Model (cached per worker)\n",
    "        model = _get_model_cpu(cfg, custom_params, model_filename)\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        # Predict annual\n",
    "        df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "\n",
    "        # Aggregate annual\n",
    "        data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "        meta_cols = [\n",
    "            c for c in ['YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID']\n",
    "            if c in df_grid_monthly_a.columns\n",
    "        ]\n",
    "        grouped_ids_a = (\n",
    "            df_grid_monthly_a.groupby('ID')[meta_cols].first().merge(\n",
    "                data_a, left_index=True, right_index=True, how='left'))\n",
    "        months_per_id_a = df_grid_monthly_a.groupby('ID')['MONTHS'].unique()\n",
    "        grouped_ids_a = grouped_ids_a.merge(months_per_id_a,\n",
    "                                            left_index=True,\n",
    "                                            right_index=True)\n",
    "        grouped_ids_a.reset_index(inplace=True)\n",
    "        grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "        pred_y_annual = grouped_ids_a.copy()\n",
    "        pred_y_annual['PERIOD'] = 'annual'\n",
    "        pred_y_annual = pred_y_annual.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        # Load per-year DEM grid and save annual\n",
    "        path_glacier_dem = os.path.join(path_xr_grids, f\"{glacier_name}.zarr\")\n",
    "        if not os.path.exists(path_glacier_dem):\n",
    "            return (\"skip\", glacier_name, year, \"DEM zarr missing\")\n",
    "        ds = xr.open_zarr(path_glacier_dem)\n",
    "\n",
    "        geoData = mbm.geodata.GeoData(df_grid_monthly_a,\n",
    "                                      months_head_pad=months_head_pad,\n",
    "                                      months_tail_pad=months_tail_pad)\n",
    "\n",
    "        # save to yearly subfolders\n",
    "        path_save_y = os.path.join(path_save_glw, str(year))\n",
    "        os.makedirs(path_save_y, exist_ok=True)\n",
    "        geoData._save_prediction(ds, pred_y_annual, glacier_name, year,\n",
    "                                 path_save_y, \"annual\")\n",
    "\n",
    "        # # Winter branch\n",
    "        # if len(df_grid_monthly_w) == 0:\n",
    "        #     return (\"ok\", glacier_name, year, \"no winter months\")\n",
    "\n",
    "        # ds_gl_w = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        #     df_grid_monthly_w,\n",
    "        #     MONTHLY_COLS,\n",
    "        #     STATIC_COLS,\n",
    "        #     months_tail_pad=months_tail_pad,\n",
    "        #     months_head_pad=months_head_pad,\n",
    "        #     expect_target=False,\n",
    "        #     show_progress=False)\n",
    "        # test_gl_dl_w = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        #     ds_gl_w, ds_train_copy, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "        # df_preds_w = model.predict_with_keys(device, test_gl_dl_w, ds_gl_w)\n",
    "\n",
    "        # data_w = df_preds_w[['ID', 'pred']].set_index('ID')\n",
    "        # grouped_ids_w = (\n",
    "        #     df_grid_monthly_w.groupby('ID')[meta_cols].first().merge(\n",
    "        #         data_w, left_index=True, right_index=True, how='left'))\n",
    "        # months_per_id_w = df_grid_monthly_w.groupby('ID')['MONTHS'].unique()\n",
    "        # grouped_ids_w = grouped_ids_w.merge(months_per_id_w,\n",
    "        #                                     left_index=True,\n",
    "        #                                     right_index=True)\n",
    "        # grouped_ids_w.reset_index(inplace=True)\n",
    "        # grouped_ids_w.sort_values(by='ID', inplace=True)\n",
    "\n",
    "        # pred_y_winter = grouped_ids_w.copy()\n",
    "        # pred_y_winter['PERIOD'] = 'winter'\n",
    "        # pred_y_winter = pred_y_winter.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        # geoData_w = mbm.geodata.GeoData(df_grid_monthly_w,\n",
    "        #                                 months_head_pad=months_head_pad,\n",
    "        #                                 months_tail_pad=months_tail_pad)\n",
    "        # geoData_w._save_prediction(ds, pred_y_winter, glacier_name, year,\n",
    "        #                            path_save_glw, \"winter\")\n",
    "\n",
    "        return (\"ok\", glacier_name, year, \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"err\", glacier_name, year, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- build tasks -----------------\n",
    "root = Path(path_gridded_inputs)\n",
    "\n",
    "# years = ['2016', ..., '2022'] – digits-only dirs, sorted numerically\n",
    "years = sorted(\n",
    "    [p.name for p in root.iterdir() if p.is_dir() and p.name.isdigit()],\n",
    "    key=int)\n",
    "\n",
    "# Regex: <SGI>_grid_<YEAR>.parquet\n",
    "pat = re.compile(r'^(?P<sgi_id>.+?)_grid_(?P<year>\\d{4})\\.parquet$')\n",
    "\n",
    "# Build SGI list from a reference year (2016). Fall back to first available year if 2016 missing.\n",
    "ref_year = '2016' if (root /\n",
    "                      '2016').is_dir() else (years[0] if years else None)\n",
    "if ref_year is None:\n",
    "    raise RuntimeError(f\"No year folders found in {root}\")\n",
    "\n",
    "sgi_list = []\n",
    "for f in (root / ref_year).glob(\"*.parquet\"):\n",
    "    m = pat.match(f.name)\n",
    "    if m:\n",
    "        sgi_list.append(m['sgi_id'])\n",
    "\n",
    "# Unique & sorted (optional)\n",
    "sgi_list = sorted(set(sgi_list))\n",
    "\n",
    "# Build tasks as (sgi_id, year) — worker will skip if parquet missing in that year\n",
    "tasks = []\n",
    "for y in years:\n",
    "    year_dir = root / y\n",
    "    for sgi_id in sgi_list:\n",
    "        if (year_dir / f\"{sgi_id}_grid_{y}.parquet\").exists():\n",
    "            tasks.append((sgi_id, y))\n",
    "\n",
    "\n",
    "# ----------------- run in parallel (quiet stdout, keep tqdm) -----------------\n",
    "class _Devnull(io.StringIO):\n",
    "\n",
    "    def write(self, *args, **kwargs):\n",
    "        return 0\n",
    "\n",
    "\n",
    "ctx = mp.get_context(\"fork\")  # Linux\n",
    "max_workers = min(max(1, (os.cpu_count() or 2) - 1), 32)\n",
    "\n",
    "with redirect_stdout(_Devnull()):  # keep stderr so tqdm is visible\n",
    "    ok = skip = err = 0\n",
    "    with ProcessPoolExecutor(max_workers=max_workers,\n",
    "                             initializer=_worker_init_quiet,\n",
    "                             mp_context=ctx) as ex:\n",
    "        futures = [ex.submit(_process_glacier_year, t) for t in tasks]\n",
    "        for fut in tqdm(as_completed(futures),\n",
    "                        total=len(futures),\n",
    "                        desc=f\"Predicting ({max_workers} workers)\"):\n",
    "            status, g, y, msg = fut.result()\n",
    "            if status == \"ok\":\n",
    "                ok += 1\n",
    "                if msg:  # no winter months, non-fatal\n",
    "                    log.info(f\"OK {g} {y}: {msg}\")\n",
    "            elif status == \"skip\":\n",
    "                skip += 1\n",
    "                log.warning(f\"SKIP {g} {y}: {msg}\")\n",
    "            else:\n",
    "                err += 1\n",
    "                log.error(f\"ERR {g} {y}: {msg}\")\n",
    "\n",
    "log.info(f\"SUMMARY: ok={ok} skip={skip} err={err} total={len(tasks)}\")\n",
    "print(f\"Done. Logs → {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an example\n",
    "sgi_id = 'B36-26'  # Aletsch\n",
    "year = 2016\n",
    "path_save_glw = f'{cfg.dataPath}/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "path = os.path.join(path_save_glw, f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\")\n",
    "\n",
    "xr.open_dataset(path).pred_masked.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at 2016:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean predicted MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "path_save_glw = f'{cfg.dataPath}/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "sgi_id_list = os.listdir(path_save_glw)\n",
    "\n",
    "\n",
    "def get_mean_mb_year(year):\n",
    "    path_save_glw = f'{cfg.dataPath}/GLAMOS/distributed_MB_grids/MBM/swisswide/{year}/'\n",
    "\n",
    "    # Calculate mean predicted mb for each glacier\n",
    "    rows = []\n",
    "    for sgi_id in tqdm(sgi_id_list):\n",
    "        path_file = os.path.join(path_save_glw,\n",
    "                                 f\"{sgi_id}/{sgi_id}_{year}_annual.zarr\")\n",
    "        if os.path.exists(path_file):\n",
    "            gridd_mb = xr.open_dataset(path_file)\n",
    "            mean_value = gridd_mb.pred_masked.mean().values.item()\n",
    "            rows.append({'sgi_id': sgi_id, 'mean_mb': mean_value})\n",
    "        else:\n",
    "            print(f\"File not found: {path_file}\")\n",
    "            continue\n",
    "    mean_mb = pd.DataFrame(rows)\n",
    "    return mean_mb\n",
    "\n",
    "\n",
    "mean_mb_2016 = get_mean_mb_year(2016)\n",
    "mean_mb_2017 = get_mean_mb_year(2017)\n",
    "mean_mb_2018 = get_mean_mb_year(2018)\n",
    "mean_mb_2019 = get_mean_mb_year(2019)\n",
    "mean_mb_2020 = get_mean_mb_year(2020)\n",
    "mean_mb_2021 = get_mean_mb_year(2021)\n",
    "mean_mb_2022 = get_mean_mb_year(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean mb from MBM and GLAMOS:\n",
    "# open reference GLAMOS\n",
    "df_reference = pd.read_csv(\n",
    "    f'{cfg.dataPath}/GLAMOS/massbalance_swisswide_2024_r2024_clean.csv'\n",
    ").iloc[1:]\n",
    "\n",
    "ref_MB_glamos = []\n",
    "\n",
    "for year in range(2016, 2023):\n",
    "    ref_CH_y = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                            & (df_reference.year == str(year))]\n",
    "    ref_MB_glamos.append(ref_CH_y['massbalance evolution'].values[0])\n",
    "\n",
    "# Prepare the data\n",
    "years = list(range(2016, 2023))\n",
    "mbm_mb = [\n",
    "    mean_mb_2016.mean_mb.mean(),\n",
    "    mean_mb_2017.mean_mb.mean(),\n",
    "    mean_mb_2018.mean_mb.mean(),\n",
    "    mean_mb_2019.mean_mb.mean(),\n",
    "    mean_mb_2020.mean_mb.mean(),\n",
    "    mean_mb_2021.mean_mb.mean(),\n",
    "    mean_mb_2022.mean_mb.mean()\n",
    "]\n",
    "\n",
    "# Build DataFrame correctly\n",
    "df = pd.DataFrame({'MBM MB': mbm_mb, 'GLAMOS MB': ref_MB_glamos}, index=years)\n",
    "\n",
    "# give same type to columns\n",
    "df['MBM MB'] = df['MBM MB'].astype(float)\n",
    "df['GLAMOS MB'] = df['GLAMOS MB'].astype(float)\n",
    "\n",
    "# Now plotting works\n",
    "df.plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean Mass Balance (m w.e.)')\n",
    "plt.title('Comparison of Mean Mass Balance: MBM vs GLAMOS')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get volumes and areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id(id_str):\n",
    "    return id_str.replace('/', '-')\n",
    "\n",
    "\n",
    "# Paths\n",
    "path_volumes = cfg.dataPath + '/GLAMOS/volumes/'\n",
    "path_areas = cfg.dataPath + '/GLAMOS/topo/SGI2020/inventory_sgi2016_r2020'\n",
    "\n",
    "# Load the shapefile of volumes\n",
    "volgdf = gpd.read_file(os.path.join(path_volumes, 'Summary.shp'))\n",
    "volgdf['sgi-id'] = volgdf['pk_sgi'].apply(convert_id)\n",
    "volgdf['V_2016'] = volgdf['V_2016'] * 10**9  # convert to m³\n",
    "\n",
    "# Load the shapefile of areas from SGI 2016\n",
    "areagdf = gpd.read_file(os.path.join(path_areas, 'SGI_2016_glaciers.shp'))\n",
    "areagdf['area_2016'] = areagdf['area_km2'] * 10**6  # convert to m²\n",
    "\n",
    "# Initialize glacier_info with volumes and areas\n",
    "glacier_info = volgdf[['sgi-id',\n",
    "                       'V_2016']].merge(areagdf[['sgi-id', 'area_2016']],\n",
    "                                        on='sgi-id',\n",
    "                                        how='inner')\n",
    "\n",
    "# List of years you want to process\n",
    "years = range(2016, 2023)  # includes 2022\n",
    "\n",
    "# Now loop over the years and merge mean mass balance year by year\n",
    "for year in years:\n",
    "    mean_mb_df = globals().get(f\"mean_mb_{year}\", None)\n",
    "    if mean_mb_df is not None:\n",
    "        mean_mb_df = mean_mb_df.copy()\n",
    "        mean_mb_df['sgi-id'] = mean_mb_df['sgi_id'].apply(convert_id)\n",
    "        glacier_info = glacier_info.merge(\n",
    "            mean_mb_df[['sgi-id', 'mean_mb'\n",
    "                        ]].rename(columns={'mean_mb': f'mean_mb_{year}'}),\n",
    "            on='sgi-id',\n",
    "            how=\n",
    "            'left'  # use 'left' to avoid dropping glaciers if some years are missing\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Warning: mean_mb_{year} not found in globals.\")\n",
    "\n",
    "glacier_info.dropna(inplace=True)  # Drop rows with NaN values\n",
    "glacier_info.set_index('sgi-id', inplace=True)\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total vol change 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_ice = 916.7  # or 917 kg/m³\n",
    "density_water = 1000  # kg/m³\n",
    "\n",
    "# Calculate volume changes\n",
    "glacier_info['vol_change_2016'] = (glacier_info['area_2016'] *\n",
    "                                   glacier_info['mean_mb_2016']) * (\n",
    "                                       density_water / density_ice)\n",
    "\n",
    "vol_change_2016 = glacier_info['vol_change_2016'].sum(\n",
    ") / 10**9  # convert to km3\n",
    "volume_2016 = glacier_info['V_2016'].sum() / 10**9  # convert to km3\n",
    "area_2016 = glacier_info['area_2016'].sum() / 10**6  # convert to km2\n",
    "volume_change_2016_perc = vol_change_2016 / volume_2016 * 100\n",
    "mb_2016 = glacier_info['mean_mb_2016'].mean()\n",
    "\n",
    "ref_CH_2016 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                           & (df_reference.year == '2016')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2016['volume change'].values[0],\n",
    "      '%')  # in %\n",
    "print('Volume change from MBM:', np.round(volume_change_2016_perc, 2),\n",
    "      '%')  # in %\n",
    "\n",
    "print('Mean mass balance from GLAMOS:',\n",
    "      ref_CH_2016['massbalance evolution'].values[0], 'm w.e.')\n",
    "print('Mean mass balance from MBM:', np.round(mb_2016, 2), 'm w.e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume area scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_area_scaling(\n",
    "        glacier_info,\n",
    "        t1,\n",
    "        beta=1.36,\n",
    "        density_ice=916.7,  # kg/m³\n",
    "        density_water=1000  # kg/m³\n",
    "):\n",
    "    \"\"\"\n",
    "    Update glacier_info by applying volume-area scaling from year t1 to t1+1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate c if not already done\n",
    "    if 'c' not in glacier_info.columns:\n",
    "        glacier_info['c'] = glacier_info[f'V_{t1}'] / (\n",
    "            glacier_info[f'area_{t1}']**beta)\n",
    "\n",
    "    # Get starting volume and area\n",
    "    V_t1 = glacier_info[f'V_{t1}']\n",
    "    A_t1 = glacier_info[f'area_{t1}']\n",
    "\n",
    "    # Get mass balance in m w.e. for the following year (mean_mb at t1+1)\n",
    "    mb = glacier_info[f'mean_mb_{t1}']\n",
    "\n",
    "    # Calculate volume change [m³ of ice]\n",
    "    vol_change = mb * A_t1 * (density_water / density_ice)\n",
    "\n",
    "    # Update volume, ensuring non-negative\n",
    "    V_t2 = (V_t1 + vol_change).clip(lower=0)\n",
    "\n",
    "    # Update area using volume-area scaling, ensuring non-negative\n",
    "    A_t2 = (V_t2 / glacier_info['c'])**(1 / beta)\n",
    "    A_t2 = A_t2.clip(lower=0)\n",
    "\n",
    "    # Save results back to glacier_info\n",
    "    glacier_info[f'V_{t1+1}'] = V_t2\n",
    "    glacier_info[f'area_{t1+1}'] = A_t2\n",
    "\n",
    "\n",
    "end_year = 2023\n",
    "for year in range(2016, end_year):\n",
    "    volume_area_scaling(glacier_info, t1=year, beta=1.36)\n",
    "\n",
    "glacier_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = glacier_info.sort_values(by='area_2016', ascending=False).head(20)\n",
    "df_sub['area_2016'] = df_sub['area_2016'] / 10**6  # convert to km2\n",
    "df_sub['area_2023'] = df_sub['area_2023'] / 10**6  # convert to km2\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "index = np.arange(len(df_sub))\n",
    "bar_width = 0.35\n",
    "\n",
    "# Bars for 2016 and 2023 areas\n",
    "ax.bar(index, df_sub['area_2016'], bar_width, label='Area 2016')\n",
    "ax.bar(index + bar_width, df_sub['area_2023'], bar_width, label='Area 2023')\n",
    "\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Glacier Index')\n",
    "ax.set_ylabel('Area (km²)')\n",
    "ax.set_title('Glacier Area Comparison: 2016 vs 2023')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(df_sub.index)\n",
    "\n",
    "# rotate x labels\n",
    "plt.xticks(rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Layout optimization\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ax = axs.flatten()\n",
    "for i, year in enumerate(range(2017, 2023)):\n",
    "    area_perc_loss_y = (\n",
    "        glacier_info['area_2016'] -\n",
    "        glacier_info[f'area_{year}']) / glacier_info['area_2016']\n",
    "\n",
    "    sns.boxplot(area_perc_loss_y, ax=ax[i], color='blue')\n",
    "    ax[i].set_title(f'Loss from 2016 to {year}')\n",
    "    ax[i].set_ylabel('Area loss (%)')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate new volume changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total vol change 2022: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_info['vol_change_2022'] = (glacier_info['area_2022'] *\n",
    "                                   glacier_info['mean_mb_2022']) * (\n",
    "                                       density_water / density_ice)\n",
    "\n",
    "vol_change_2022 = glacier_info['vol_change_2022'].sum(\n",
    ") / 10**9  # convert to km3\n",
    "\n",
    "volume_2022 = glacier_info['V_2022'].sum() / 10**9  # convert to km3\n",
    "mb_2022 = glacier_info['mean_mb_2022'].mean()\n",
    "\n",
    "volume_change_2022_perc = vol_change_2022 / volume_2022 * 100\n",
    "\n",
    "ref_CH_2022 = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                           & (df_reference.year == '2022')]\n",
    "\n",
    "print('Volume change from GLAMOS:', ref_CH_2022['volume change'].values[0],\n",
    "      '%')  # in %\n",
    "print('Volume change from MBM:', np.round(volume_change_2022_perc, 2),\n",
    "      '%')  # in %\n",
    "\n",
    "print('Mean mass balance from GLAMOS:',\n",
    "      ref_CH_2022['massbalance evolution'].values[0], 'm w.e.')\n",
    "print('Mean mass balance from MBM:', np.round(mb_2022, 2), 'm w.e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot vol & area change all years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_change_y_perc, ref_V_glamos = [], []\n",
    "for year in range(2016, 2023):\n",
    "    glacier_info[f'vol_change_{year}'] = (glacier_info[f'area_{year}'] *\n",
    "                                          glacier_info[f'mean_mb_{year}']) * (\n",
    "                                              density_water / density_ice)\n",
    "\n",
    "    vol_change_y = glacier_info[f'vol_change_{year}'].sum(\n",
    "    ) / 10**9  # convert to km3\n",
    "\n",
    "    volume_y = glacier_info[f'V_{year}'].sum() / 10**9  # convert to km3\n",
    "    mb_y = glacier_info[f'mean_mb_{year}'].mean()\n",
    "\n",
    "    volume_change_y_perc.append(vol_change_y / volume_y * 100)\n",
    "\n",
    "    ref_CH_y = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                            & (df_reference.year == str(year))]\n",
    "\n",
    "    ref_V_glamos.append(ref_CH_y['volume change'].values[0])\n",
    "\n",
    "# Build DataFrame correctly\n",
    "df = pd.DataFrame({\n",
    "    'MBM V': volume_change_y_perc,\n",
    "    'GLAMOS V': ref_V_glamos\n",
    "},\n",
    "                  index=years)\n",
    "\n",
    "# give same type to columns\n",
    "df['MBM V'] = df['MBM V'].astype(float)\n",
    "df['GLAMOS V'] = df['GLAMOS V'].astype(float)\n",
    "\n",
    "# Now plotting works\n",
    "df.plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('% volume change')\n",
    "plt.title('Volume change: MBM vs GLAMOS')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_change_y_perc, ref_V_glamos = [], []\n",
    "for year in range(2016, 2023):\n",
    "    glacier_info[f'vol_change_{year}'] = (glacier_info[f'area_{year}'] *\n",
    "                                          glacier_info[f'mean_mb_{year}']) * (\n",
    "                                              density_water / density_ice)\n",
    "\n",
    "    vol_change_y = glacier_info[f'vol_change_{year}'].sum(\n",
    "    ) / 10**9  # convert to km3\n",
    "\n",
    "    volume_y = glacier_info[f'V_{year}'].sum() / 10**9  # convert to km3\n",
    "    mb_y = glacier_info[f'mean_mb_{year}'].mean()\n",
    "\n",
    "    volume_change_y_perc.append(vol_change_y / volume_y * 100)\n",
    "\n",
    "    ref_CH_y = df_reference[(df_reference.catchment == 'Switzerland')\n",
    "                            & (df_reference.year == str(year))]\n",
    "\n",
    "    ref_V_glamos.append(ref_CH_y['volume change'].values[0])\n",
    "\n",
    "# Build DataFrame correctly\n",
    "df = pd.DataFrame({\n",
    "    'MBM V': volume_change_y_perc,\n",
    "    'GLAMOS V': ref_V_glamos\n",
    "},\n",
    "                  index=years)\n",
    "\n",
    "# give same type to columns\n",
    "df['MBM V'] = df['MBM V'].astype(float)\n",
    "df['GLAMOS V'] = df['GLAMOS V'].astype(float)\n",
    "\n",
    "# Now plotting works\n",
    "df['MBM V'].plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('% volume change')\n",
    "plt.title('Volume change by Mass Balance Machine over the Swiss Alps',\n",
    "          fontsize=16)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend([], [], frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
