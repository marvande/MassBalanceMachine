{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "    \"millan_v\", \"svf\"\n",
    "]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf_IS.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_gs_no_oggm_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "\n",
    "existing_glaciers = set(data_monthly_Aug_.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train_Aug_))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train_Aug_ = data_train_Aug_\n",
    "data_train_Aug_['y'] = data_train_Aug_['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'ELEVATION_DIFFERENCE',\n",
    "    'pcsr'\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', \"svf\"]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_all(cfg.seed)\n",
    "\n",
    "# df_train = data_train.copy()\n",
    "# df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# # --- build train dataset from dataframe ---\n",
    "# ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "#     df_train,\n",
    "#     MONTHLY_COLS,\n",
    "#     STATIC_COLS,\n",
    "#     months_tail_pad=months_tail_pad,\n",
    "#     months_head_pad=months_head_pad,\n",
    "#     expect_target=True,\n",
    "#     normalize_target=True)\n",
    "\n",
    "# train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "#     len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "ds_train_combd = build_combined_LSTM_dataset(\n",
    "    df_loss=data_train,\n",
    "    df_full=data_train_Aug_,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=months_head_pad_Aug_,\n",
    "    months_tail_pad=months_tail_pad_Aug_,\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "train_idx_combd, val_idx_combd = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_combd), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_path = 'logs/lstm_one_head_param_search_progress_no_oggm_IS_2025-11-04.csv'\n",
    "# best_params = get_best_params_for_lstm(log_path, select_by='test_rmse_a')\n",
    "# custom_params = best_params\n",
    "# custom_params['two_heads'] = False\n",
    "\n",
    "# # --- build model, resolve loss, train, reload best ---\n",
    "# current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# model_filename = f\"models/lstm_model_2025-12-01_no_oggm_IS_norm_y.pt\"\n",
    "\n",
    "# # --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "# ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#     ds_train)\n",
    "\n",
    "# train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "#     train_idx=train_idx,\n",
    "#     val_idx=val_idx,\n",
    "#     batch_size_train=64,\n",
    "#     batch_size_val=128,\n",
    "#     seed=cfg.seed,\n",
    "#     fit_and_transform=\n",
    "#     True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "#     shuffle_train=True,\n",
    "#     use_weighted_sampler=True  # use weighted sampler for training\n",
    "# )\n",
    "\n",
    "# # --- build model, resolve loss, train, reload best ---\n",
    "# model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "# loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "# ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#     ds_train)\n",
    "\n",
    "# test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#     ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# # Evaluate on test\n",
    "# state = torch.load(model_filename, map_location=device)\n",
    "# model.load_state_dict(state)\n",
    "# test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "#     device, test_dl, ds_test_copy)\n",
    "# test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "#     'RMSE_winter']\n",
    "\n",
    "# print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "#     test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_one_head_param_search_progress_no_oggm_IS_2025-11-25.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='avg_test_loss')\n",
    "custom_params = best_params\n",
    "custom_params['two_heads'] = False\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_2025-11-28_no_oggm_IS_norm_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_combd_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_combd)\n",
    "\n",
    "train_dl_combd, val_dl_combd = ds_train_combd_copy.make_loaders(\n",
    "    train_idx=train_idx_combd,\n",
    "    val_idx=val_idx_combd,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_combd = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "ds_test_combd_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_combd)\n",
    "\n",
    "test_dl_combd = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_combd_copy, ds_train_combd_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_combd.load_state_dict(state)\n",
    "test_metrics_combd, test_df_preds_combd = model_combd.evaluate_with_preds(\n",
    "    device, test_dl_combd, ds_test_combd_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics_combd[\n",
    "    'RMSE_annual'], test_metrics_combd['RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds_combd,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds_combd,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-8, 6),\n",
    "    ax_ylim=(-8, 6),\n",
    "    color_annual=color_annual,\n",
    "    color_winter=color_winter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity to past information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation Test (how many months matter?):\n",
    "During inference, we only reveal the first K months to the model and set the rest to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_with_truncation(model, device, dl, ds, k):\n",
    "    # keep first k months, mask rest 0\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    all_keys = ds.keys\n",
    "    i = 0\n",
    "\n",
    "    for batch in dl:\n",
    "        bs = batch[\"x_m\"].shape[0]\n",
    "        keys = all_keys[i:i + bs]\n",
    "        i += bs\n",
    "\n",
    "        batch = model.to_device(device, batch)\n",
    "        x_m = batch[\"x_m\"].clone()\n",
    "\n",
    "        # mask months >= k\n",
    "        x_m[:, k:, :] = 0.0\n",
    "\n",
    "        _, y_w, y_a = model(x_m, batch[\"x_s\"], batch[\"mv\"], batch[\"mw\"],\n",
    "                            batch[\"ma\"])\n",
    "\n",
    "        y_true = batch[\"y\"] * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_w = y_w * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_a = y_a * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "\n",
    "        for j in range(bs):\n",
    "            g, yr, mid, per = keys[j]\n",
    "            target = float(y_true[j].cpu())\n",
    "            pred = float((y_w if per == \"winter\" else y_a)[j].cpu())\n",
    "            rows.append({\"pred\": pred, \"target\": target, \"PERIOD\": per})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    rmse_w = np.sqrt(\n",
    "        np.mean((df[df.PERIOD == \"winter\"].pred -\n",
    "                 df[df.PERIOD == \"winter\"].target)**2))\n",
    "    rmse_a = np.sqrt(\n",
    "        np.mean((df[df.PERIOD == \"annual\"].pred -\n",
    "                 df[df.PERIOD == \"annual\"].target)**2))\n",
    "    return rmse_w, rmse_a\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def evaluate_with_truncation_from_end(model, device, dl, ds, k):\n",
    "    \"\"\"\n",
    "    k = number of visible months counting backward from the end.\n",
    "    Now ensures we ONLY evaluate on samples that actually have\n",
    "    at least one valid/used month in the truncated visible region.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    all_keys = ds.keys\n",
    "    i = 0\n",
    "    T = 15\n",
    "\n",
    "    for batch in dl:\n",
    "        bs = batch[\"x_m\"].shape[0]\n",
    "        keys = all_keys[i:i + bs]\n",
    "        i += bs\n",
    "\n",
    "        batch = model.to_device(device, batch)\n",
    "\n",
    "        x_m = batch[\"x_m\"].clone()\n",
    "        mv = batch[\"mv\"]  # valid-month mask  (1 where month is used)\n",
    "        mw = batch[\"mw\"]  # winter mask\n",
    "        ma = batch[\"ma\"]  # annual mask\n",
    "\n",
    "        # determine region kept from end\n",
    "        idx_start = T - k\n",
    "\n",
    "        # mask the early months in x_m\n",
    "        x_m[:, :idx_start, :] = 0.0\n",
    "\n",
    "        # forward pass\n",
    "        _, y_w, y_a = model(x_m, batch[\"x_s\"], mv, mw, ma)\n",
    "\n",
    "        # denormalize\n",
    "        y_true = batch[\"y\"] * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_w = y_w * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "        y_a = y_a * ds.y_std.to(device) + ds.y_mean.to(device)\n",
    "\n",
    "        # select only samples that actually use ANY of the visible months\n",
    "        # i.e., mv[j, idx_start:T] contains at least one 1\n",
    "        valid = (mv[:, idx_start:T].sum(dim=1) > 0)\n",
    "\n",
    "        for j in range(bs):\n",
    "            if not valid[j]:    # skip if sample does not use any visible months\n",
    "                continue\n",
    "            g, yr, mid, per = keys[j]\n",
    "            target = float(y_true[j].cpu())\n",
    "            pred = float((y_w if per == \"winter\" else y_a)[j].cpu())\n",
    "            rows.append({\"pred\": pred, \"target\": target, \"PERIOD\": per})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # compute RMSE only on samples that survived filtering\n",
    "    def rmse(period):\n",
    "        sub = df[df.PERIOD == period]\n",
    "        return np.sqrt(np.mean((sub.pred - sub.target) ** 2)) if len(sub) > 0 else np.nan\n",
    "\n",
    "    rmse_w = rmse(\"winter\")\n",
    "    rmse_a = rmse(\"annual\")\n",
    "    return rmse_w, rmse_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "#  Forward truncation (first K months visible)\n",
    "# -----------------------------\n",
    "Ks = list(range(1, 16))  # 1..15 months included\n",
    "month_idx_last_winter = 10  # cap winter curve at 9 (1-based)\n",
    "\n",
    "rmse_annual = []\n",
    "rmse_winter = []\n",
    "\n",
    "for k in Ks:\n",
    "    rmse_w, rmse_a = evaluate_with_truncation(model_combd, device,\n",
    "                                              test_dl_combd,\n",
    "                                              ds_test_combd_copy, k)\n",
    "    rmse_annual.append(rmse_a)\n",
    "    rmse_winter.append(rmse_w)\n",
    "    print(\n",
    "        f\"K(forward)={k:02d} → RMSE annual={rmse_a:.3f}, winter={rmse_w:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "#  Backward truncation (last K months visible)\n",
    "# -----------------------------\n",
    "rmse_annual_end = []\n",
    "rmse_winter_end = []\n",
    "\n",
    "for k in Ks:\n",
    "    rmse_w, rmse_a = evaluate_with_truncation_from_end(model_combd, device,\n",
    "                                                       test_dl_combd,\n",
    "                                                       ds_test_combd_copy, k)\n",
    "    rmse_winter_end.append(rmse_w)\n",
    "    rmse_annual_end.append(rmse_a)\n",
    "    print(\n",
    "        f\"K(backward)={k:02d} → RMSE annual={rmse_a:.3f}, winter={rmse_w:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "#  Cap both winter curves at 9 months max\n",
    "# -----------------------------\n",
    "Ks_winter = Ks[:month_idx_last_winter]\n",
    "rmse_winter_fwd = rmse_winter[:len(Ks_winter)]\n",
    "\n",
    "# -----------------------------\n",
    "#  X-labels (month names)\n",
    "# -----------------------------\n",
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "    \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "x_labels = [month_order[k - 1] for k in Ks]\n",
    "\n",
    "# -----------------------------\n",
    "#  Reverse backward curves so \"more info → right\"\n",
    "# -----------------------------\n",
    "rmse_annual_bwd_plot = rmse_annual_end[::-1]\n",
    "rmse_winter_end = rmse_winter_end[::-1]\n",
    "rmse_winter_bwd_plot = rmse_winter_end[:len(Ks_winter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#  Plotting\n",
    "# -----------------------------\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "ax.plot(Ks,\n",
    "        rmse_annual,\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        color=\"tab:purple\",\n",
    "        label=\"Annual MB (forward)\")\n",
    "ax.plot(Ks,\n",
    "        rmse_annual_bwd_plot,\n",
    "        marker=\"s\",\n",
    "        linewidth=2,\n",
    "        color=\"tab:pink\",\n",
    "        label=\"Annual MB (backward)\")\n",
    "\n",
    "ax.plot(Ks_winter,\n",
    "        rmse_winter_fwd,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        color=\"tab:cyan\",\n",
    "        label=\"Winter MB (forward)\")\n",
    "ax.plot(Ks_winter,\n",
    "        rmse_winter_bwd_plot,\n",
    "        marker=\"s\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        color=\"tab:blue\",\n",
    "        label=\"Winter MB (backward)\")\n",
    "\n",
    "ax.set_xticks(Ks)\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "ax.set_xlabel(\"Months visible to the LSTM\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_title(\"LSTM truncation sensitivity — forward vs backward temporal information\")\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# --- LEGEND BELOW FIGURE ---\n",
    "legend = ax.legend(\n",
    "    ncol=2,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.40),   # shift legend downward\n",
    "    frameon=True,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "ds_train_combd = build_combined_LSTM_dataset(\n",
    "    df_loss=data_train,\n",
    "    df_full=data_train_Aug_,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=months_head_pad_Aug_,\n",
    "    months_tail_pad=months_tail_pad_Aug_,\n",
    "    normalize_target=False,\n",
    "    expect_target=True)\n",
    "train_idx_combd, val_idx_combd = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_combd), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "log_path = 'logs/lstm_one_head_param_search_progress_no_oggm_IS_2025-11-25.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='avg_test_loss')\n",
    "\n",
    "custom_params = best_params\n",
    "custom_params['two_heads'] = False\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_2025-11-28_no_oggm_IS_original_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_combd_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_combd)\n",
    "\n",
    "train_dl_combd, val_dl_combd = ds_train_combd_copy.make_loaders(\n",
    "    train_idx=train_idx_combd,\n",
    "    val_idx=val_idx_combd,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_combd = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "ds_test_combd_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_combd)\n",
    "\n",
    "test_dl_combd = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_combd_copy, ds_train_combd_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_combd.load_state_dict(state)\n",
    "test_metrics_combd, test_df_preds_combd = model_combd.evaluate_with_preds(\n",
    "    device, test_dl_combd, ds_test_combd_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics_combd[\n",
    "    'RMSE_annual'], test_metrics_combd['RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "    \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "month_to_idx = {m: i for i, m in enumerate(month_order)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sensitivity_all_features_to_target_month(\n",
    "    model,\n",
    "    device,\n",
    "    dl,\n",
    "    ds,\n",
    "    monthly_cols,\n",
    "    target_month=\"jul\",\n",
    "    boost_factor=3.0,\n",
    "    month_order=None,\n",
    "):\n",
    "    if month_order is None:\n",
    "        month_order = [\n",
    "            \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\",\n",
    "            \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "        ]\n",
    "\n",
    "    month_to_idx = {m: i for i, m in enumerate(month_order)}\n",
    "    idx_target = month_to_idx[target_month]  # loop only until this month\n",
    "\n",
    "    model.eval()\n",
    "    y_std = ds.y_std.to(device)\n",
    "\n",
    "    # result[feat][month_idx] = Δ physical MB\n",
    "    results = {feat: [] for feat in monthly_cols}\n",
    "\n",
    "    # total number of iterations = n_features × n_months_before_target\n",
    "    total_iters = len(monthly_cols) * idx_target\n",
    "    pbar = tqdm(total=total_iters, desc=\"ΔMB sensitivity\", leave=True)\n",
    "\n",
    "    for feat_i, feat in enumerate(monthly_cols):\n",
    "        for m_idx in range(\n",
    "                idx_target\n",
    "        ):  # only months preceding July (inclusive if you choose)\n",
    "            deltas = []\n",
    "            all_keys = ds.keys\n",
    "            i0 = 0\n",
    "\n",
    "            for batch in dl:\n",
    "                bs = batch[\"x_m\"].shape[0]\n",
    "                keys = all_keys[i0:i0 + bs]\n",
    "                i0 += bs\n",
    "\n",
    "                batch = model.to_device(device, batch)\n",
    "                x_m, x_s = batch[\"x_m\"], batch[\"x_s\"]\n",
    "                mv, mw, ma = batch[\"mv\"], batch[\"mw\"], batch[\"ma\"]\n",
    "\n",
    "                # baseline\n",
    "                y_month_base, _, _ = model(x_m, x_s, mv, mw, ma)\n",
    "\n",
    "                # perturb\n",
    "                x_m_pert = x_m.clone()\n",
    "                x_m_pert[:, m_idx, feat_i] *= boost_factor\n",
    "                y_month_pert, _, _ = model(x_m_pert, x_s, mv, mw, ma)\n",
    "\n",
    "                delta = (y_month_pert[:, idx_target] -\n",
    "                         y_month_base[:, idx_target]) * y_std\n",
    "\n",
    "                for j in range(bs):\n",
    "                    g, yr, mid, per = keys[j]\n",
    "                    if per == \"annual\":  # keep annual MB only\n",
    "                        deltas.append(float(delta[j].cpu()))\n",
    "\n",
    "            results[feat].append(np.mean(deltas))\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    df = pd.DataFrame(results, index=month_order[:idx_target]).T\n",
    "    df.columns.name = \"month\"\n",
    "    df.index.name = \"feature\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = ds_test_combd_copy.keys\n",
    "i0 = 0\n",
    "\n",
    "y_month_base_all = []\n",
    "\n",
    "for batch in test_dl_combd:\n",
    "    bs = batch[\"x_m\"].shape[0]\n",
    "    keys = all_keys[i0:i0 + bs]\n",
    "    i0 += bs\n",
    "\n",
    "    batch = model_combd.to_device(device, batch)\n",
    "    x_m, x_s = batch[\"x_m\"], batch[\"x_s\"]\n",
    "    mv, mw, ma = batch[\"mv\"], batch[\"mw\"], batch[\"ma\"]\n",
    "\n",
    "    # forward pass\n",
    "    y_month_base, _, _ = model_combd(x_m, x_s, mv, mw, ma)\n",
    "    y_month_base_all.append(y_month_base.cpu())   # move to CPU to free GPU memory\n",
    "\n",
    "# Concatenate\n",
    "y_month_base_all = torch.cat(y_month_base_all, dim=0)   # (N_samples, n_months)\n",
    "\n",
    "y_min = float(y_month_base_all.min())\n",
    "y_max = float(y_month_base_all.max())\n",
    "\n",
    "print(f\"Min monthly MB prediction: {y_min:.3f}\")\n",
    "print(f\"Max monthly MB prediction: {y_max:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_columns = ['t2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'pcsr']\n",
    "\n",
    "df_delta_MB = sensitivity_all_features_to_target_month(\n",
    "    model=model_combd,\n",
    "    device=device,\n",
    "    dl=test_dl_combd,\n",
    "    ds=ds_test_combd_copy,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    target_month=\"aug\",\n",
    "    boost_factor=3.0,\n",
    "    month_order=month_order,  # your 15-month hydrological order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# --- Copy sensitivity matrix and map features to long names ---\n",
    "piv = df_delta_MB.copy()\n",
    "piv[\"feature_long\"] = piv.index.map(lambda x: vois_climate_long_name.get(x, x))\n",
    "piv = piv.set_index(\"feature_long\")\n",
    "\n",
    "# --- Reorder columns according to hydrological month order (up to the target month) ---\n",
    "piv = piv[[m for m in month_order if m in piv.columns]]\n",
    "\n",
    "# --- Order features by average |ΔMB| (ascending = strongest at top visually) ---\n",
    "feat_order = piv.abs().mean(axis=1).sort_values(ascending=True).index\n",
    "\n",
    "# --- Smooth individual curves ---\n",
    "piv_smooth = pd.DataFrame(\n",
    "    np.vstack([gaussian_filter1d(piv.loc[f], sigma=1) for f in feat_order]),\n",
    "    index=feat_order,\n",
    "    columns=piv.columns,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 11))\n",
    "palette = sns.color_palette(\"magma\", n_colors=len(feat_order))\n",
    "month_idx = np.arange(len(piv_smooth.columns))\n",
    "\n",
    "offset_step = np.nanmax(abs(piv_smooth.values)) * 0.85\n",
    "offset = 0\n",
    "\n",
    "for feat, color in zip(feat_order, palette):\n",
    "    y = piv_smooth.loc[feat].values\n",
    "\n",
    "    # ridge curve\n",
    "    ax.plot(month_idx, y + offset, color=color, lw=2)\n",
    "    ax.fill_between(month_idx, offset, y + offset, color=color, alpha=0.4)\n",
    "\n",
    "    # feature label on left\n",
    "    ax.text(-0.6, offset, feat, va='center', ha='right', fontsize=13)\n",
    "\n",
    "    # ---- annotate max (positive ΔMB) ----\n",
    "    max_i = np.argmax(y)\n",
    "    ax.text(\n",
    "        max_i,\n",
    "        y[max_i] + offset + 0.03 * offset_step,\n",
    "        f\"+{piv.iloc[piv.index.get_loc(feat), max_i]:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.8, edgecolor=\"none\", pad=1.2),\n",
    "    )\n",
    "\n",
    "    # ---- annotate min (negative ΔMB) ----\n",
    "    min_i = np.argmin(y)\n",
    "    ax.text(\n",
    "        min_i,\n",
    "        y[min_i] + offset - 0.04 * offset_step,\n",
    "        f\"{piv.iloc[piv.index.get_loc(feat), min_i]:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.8, edgecolor=\"none\", pad=1.2),\n",
    "    )\n",
    "\n",
    "    offset += offset_step\n",
    "\n",
    "# formatting\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks(month_idx)\n",
    "ax.set_xticklabels([m.strip(\"_\").capitalize() for m in piv_smooth.columns],\n",
    "                   rotation=45,\n",
    "                   ha=\"right\")\n",
    "ax.set_xlabel(\"Month boosted (×3)\")\n",
    "ax.set_title(\n",
    "    \"ΔMB Sensitivity — Effect of Monthly Climate Perturbations on July MB\",\n",
    "    pad=20)\n",
    "\n",
    "ax.set_facecolor(\"white\")\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "for spine in [\"top\", \"right\", \"left\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_color(\"black\")\n",
    "ax.tick_params(colors=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
