{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & utilities ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import traceback\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Add repo root for MBM imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../\"))\n",
    "\n",
    "# --- Data science stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "\n",
    "# --- Machine learning / DL ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# --- Cartography / plotting ---\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Custom MBM modules ---\n",
    "import massbalancemachine as mbm\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Warnings & autoreload (notebook) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Configuration ---\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "# --- CUDA / device ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_all(cfg.seed)\n",
    "# free_up_cuda()  # in case no memory\n",
    "\n",
    "# # Plot styles:\n",
    "# path_style_sheet = 'scripts/example.mplstyle'\n",
    "# plt.style.use(path_style_sheet)\n",
    "\n",
    "# # Climate columns\n",
    "# vois_climate = [\n",
    "#     't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "# ]\n",
    "# # Topographical columns\n",
    "# vois_topographical = [\n",
    "#     \"aspect\",\n",
    "#     \"slope\",\n",
    "#     \"hugonnet_dhdt\",\n",
    "#     \"consensus_ice_thickness\",\n",
    "#     \"millan_v\",\n",
    "#     \"topo\",\n",
    "# ]\n",
    "\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    # base_url=\n",
    "    # \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "df_missing = export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# gdf_proj.set_index('rgi_id', inplace=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# total glacier area\n",
    "total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# explode the list of missing vars into rows (one var per row)\n",
    "df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# 1) COUNT: number of glaciers missing each variable\n",
    "counts_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "        ascending=False))\n",
    "\n",
    "# 2) TOTAL % AREA with ANY missing var\n",
    "total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "print(f\"Total glacier area with ANY missing variable: \"\n",
    "      f\"{total_missing_area_km2:,.2f} kmÂ² \"\n",
    "      f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# Optional: also show % area per variable (kept from your earlier logic)\n",
    "area_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "        ascending=False))\n",
    "perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "print(\"\\n% of total glacier area missing per variable:\")\n",
    "for var, pct in perc_missing_per_var.items():\n",
    "    print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# ---- barplot: number of glaciers missing each variable ----\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "plt.xlabel(\"Missing variable\")\n",
    "plt.ylabel(\"Number of glaciers\")\n",
    "plt.title(\"Count of glaciers missing each variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgi_alps = os.path.join(cfg.dataPath,\n",
    "                             'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "year = 2022\n",
    "rgi_ids = os.listdir(path_rgi_alps)\n",
    "pos_gl, rgis = [], []\n",
    "for rgi_gl in tqdm(rgi_ids):\n",
    "    # if path exists:\n",
    "    if os.path.exists(\n",
    "            os.path.join(path_rgi_alps, rgi_gl,\n",
    "                         f\"{rgi_gl}_grid_{year}.parquet\")):\n",
    "        df = pd.read_parquet(\n",
    "            os.path.join(path_rgi_alps, rgi_gl,\n",
    "                         f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "        pos_gl.append((df.POINT_LAT.mean(), df.POINT_LON.mean()))\n",
    "        rgis.append(rgi_gl)\n",
    "    else:\n",
    "        continue\n",
    "df_pos_all = pd.DataFrame(pos_gl, columns=['lat', 'lon'])\n",
    "df_pos_all['rgi_id'] = rgis\n",
    "\n",
    "print('Number of glaciers in RGI region 11.6:', len(df_pos_all))\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "latN, latS = 48, 44\n",
    "lonW, lonE = 4, 14\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=df_pos_all,\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    alpha=0.6,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "glacier_outline_rgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"SVF\"\n",
    "]\n",
    "\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_CA.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'SVF']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_simple, val_idx_simple = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_simple), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': len(MONTHLY_COLS),\n",
    "    'Fs': len(STATIC_COLS),\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_simple_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_simple.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_simple) ---\n",
    "ds_train_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_simple)\n",
    "\n",
    "ds_test_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_simple)\n",
    "\n",
    "train_dl, val_dl = ds_train_simple_copy.make_loaders(\n",
    "    train_idx=train_idx_simple,\n",
    "    val_idx=val_idx_simple,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_simple and transforms it) ---\n",
    "test_dl_simple = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_simple_copy, ds_train_simple_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "model_filename = f\"models/lstm_model_2025-10-07_CA_simple.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_simple, ds_test_simple_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "vars = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "\n",
    "all_combos = [('hugonnet_dhdt', ), ('consensus_ice_thickness', ),\n",
    "              ('millan_v', ), ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "              ('hugonnet_dhdt', 'millan_v'),\n",
    "              ('consensus_ice_thickness', 'millan_v')]\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    for combo in all_combos:\n",
    "        print(combo)\n",
    "\n",
    "        STATIC_COLS = ['aspect', 'slope', 'SVF', *combo]\n",
    "        feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "        seed_all(cfg.seed)\n",
    "\n",
    "        # prepare train/test data\n",
    "        df_train = data_train.copy()\n",
    "        df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "        df_test = data_test.copy()\n",
    "        df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "        # datasets\n",
    "        ds_train_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_train,\n",
    "            MONTHLY_COLS,\n",
    "            STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad,\n",
    "            months_head_pad=months_head_pad,\n",
    "            expect_target=True)\n",
    "        ds_test_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_test,\n",
    "            MONTHLY_COLS,\n",
    "            STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad,\n",
    "            months_head_pad=months_head_pad,\n",
    "            expect_target=True)\n",
    "\n",
    "        # split train/val\n",
    "        train_idx_full, val_idx_full = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "            len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "        # params\n",
    "        custom_params = {\n",
    "            'Fm': len(MONTHLY_COLS),\n",
    "            'Fs': len(STATIC_COLS),\n",
    "            'hidden_size': 128,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.0,\n",
    "            'static_layers': 0,\n",
    "            'static_hidden': None,\n",
    "            'static_dropout': None,\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 0.0,\n",
    "            'loss_name': 'neutral',\n",
    "            'loss_spec': None,\n",
    "            'two_heads': True,\n",
    "            'head_dropout': 0.0\n",
    "        }\n",
    "        params_full_model = custom_params.copy()\n",
    "\n",
    "        # --- model filename ---\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        combo_str = \"_\".join(combo)\n",
    "        model_filename = f\"models/lstm_model_{current_date}_CA_{combo_str}.pt\"\n",
    "\n",
    "        # loaders\n",
    "        ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train_full)\n",
    "        ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_test_full)\n",
    "\n",
    "        train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "            train_idx=train_idx_full,\n",
    "            val_idx=val_idx_full,\n",
    "            batch_size_train=64,\n",
    "            batch_size_val=128,\n",
    "            seed=cfg.seed,\n",
    "            fit_and_transform=True,\n",
    "            shuffle_train=True,\n",
    "            use_weighted_sampler=True)\n",
    "\n",
    "        test_dl_full = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_test_full_copy,\n",
    "            ds_train_full_copy,\n",
    "            batch_size=128,\n",
    "            seed=cfg.seed)\n",
    "\n",
    "        # model\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(\n",
    "            cfg, custom_params, device)\n",
    "        loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=custom_params['lr'],\n",
    "            weight_decay=custom_params['weight_decay'],\n",
    "            clip_val=1,\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            log_every=5,\n",
    "            verbose=True,\n",
    "            save_best_path=model_filename,\n",
    "            loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model (with OGGM variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "STATIC_COLS = [\n",
    "    'aspect', 'slope', 'SVF', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_full, val_idx_full = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': len(MONTHLY_COLS),\n",
    "    'Fs': len(STATIC_COLS),\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_full_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_full.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_full) ---\n",
    "ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_full)\n",
    "\n",
    "ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_full)\n",
    "\n",
    "train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "    train_idx=train_idx_full,\n",
    "    val_idx=val_idx_full,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_full and transforms it) ---\n",
    "test_dl_full = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_full_copy, ds_train_full_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "model_filename = f\"models/lstm_model_2025-10-07_CA_full.pt\"\n",
    "\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_full, ds_test_full_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate to all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# fixed order to avoid accidental shuffles\n",
    "VARS_ORDER = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "all_combos = [\n",
    "    ('hugonnet_dhdt', ),\n",
    "    ('consensus_ice_thickness', ),\n",
    "    ('millan_v', ),\n",
    "    ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "    ('hugonnet_dhdt', 'millan_v'),\n",
    "    ('consensus_ice_thickness', 'millan_v'),\n",
    "]\n",
    "\n",
    "\n",
    "def combo_key(tup):\n",
    "    ordered = [v for v in VARS_ORDER if v in tup]\n",
    "    return \"__\".join(ordered)  # e.g. \"hugonnet_dhdt__millan_v\"\n",
    "\n",
    "\n",
    "def make_params(Fm, STATIC_COLS):\n",
    "    # Your template, with Fs derived from STATIC_COLS length\n",
    "    return {\n",
    "        'Fm': Fm,  # number of monthly features\n",
    "        'Fs': len(STATIC_COLS),  # number of static features\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 1,\n",
    "        'bidirectional': False,\n",
    "        'dropout': 0.0,\n",
    "        'static_layers': 0,\n",
    "        'static_hidden': None,\n",
    "        'static_dropout': None,\n",
    "        'lr': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'loss_name': 'neutral',\n",
    "        'loss_spec': None,\n",
    "        'two_heads': True,\n",
    "        'head_dropout': 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "Fm = len(MONTHLY_COLS)  # safer than hardcoding 8\n",
    "\n",
    "# Build params for each requested combo\n",
    "params_by_key = {}\n",
    "for combo in all_combos:\n",
    "    STATIC_COLS = ['aspect', 'slope', 'SVF', *combo]\n",
    "    key = combo_key(combo)  # e.g. 'hugonnet_dhdt__millan_v'\n",
    "    params_by_key[key] = make_params(Fm, STATIC_COLS)\n",
    "\n",
    "# Unpack to the variable names you mentioned\n",
    "params_hugonnet_only = copy.deepcopy(params_by_key['hugonnet_dhdt'])\n",
    "params_consensus_only = copy.deepcopy(params_by_key['consensus_ice_thickness'])\n",
    "params_millan_only = copy.deepcopy(params_by_key['millan_v'])\n",
    "params_hugonnet_consensus = copy.deepcopy(\n",
    "    params_by_key['hugonnet_dhdt__consensus_ice_thickness'])\n",
    "params_hugonnet_millan = copy.deepcopy(\n",
    "    params_by_key['hugonnet_dhdt__millan_v'])\n",
    "params_consensus_millan = copy.deepcopy(\n",
    "    params_by_key['consensus_ice_thickness__millan_v'])\n",
    "\n",
    "# (optional) One dict for your loader mapping\n",
    "PARAMS_BY_COMBO = {\n",
    "    \"hugonnet_dhdt\": params_hugonnet_only,\n",
    "    \"consensus_ice_thickness\": params_consensus_only,\n",
    "    \"millan_v\": params_millan_only,\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\": params_hugonnet_consensus,\n",
    "    \"hugonnet_dhdt__millan_v\": params_hugonnet_millan,\n",
    "    \"consensus_ice_thickness__millan_v\": params_consensus_millan,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ---- config / constants ----\n",
    "VARS_ORDER = ['hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "              'millan_v']  # fixed order\n",
    "# Your existing MONTHLY_COLS\n",
    "# MONTHLY_COLS = ['t2m','tp','slhf','sshf','ssrd','fal','str','ELEVATION_DIFFERENCE']\n",
    "\n",
    "\n",
    "def combo_key_from_tuple(tup):\n",
    "    if len(tup) == 0:\n",
    "        return \"simple\"\n",
    "    ordered = [v for v in VARS_ORDER if v in tup]\n",
    "    if ordered == VARS_ORDER:\n",
    "        return \"full\"\n",
    "    return \"__\".join(ordered)\n",
    "\n",
    "\n",
    "# The partial combos you trained\n",
    "partial_combos = [\n",
    "    ('hugonnet_dhdt', ),\n",
    "    ('consensus_ice_thickness', ),\n",
    "    ('millan_v', ),\n",
    "    ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "    ('hugonnet_dhdt', 'millan_v'),\n",
    "    ('consensus_ice_thickness', 'millan_v'),\n",
    "]\n",
    "\n",
    "# Include simple and full for completeness\n",
    "ALL_COMBOS = [()] + partial_combos + [tuple(VARS_ORDER)]\n",
    "\n",
    "# ---- normalize input dataframes once ----\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()  # optional, if you also want test ds per combo\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# ---- builders ----\n",
    "DS_TRAIN_BY_COMBO = {}\n",
    "SPLIT_IDXS_BY_COMBO = {}  # (train_idx, val_idx)\n",
    "STATIC_COLS_BY_COMBO = {}\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "for combo in ALL_COMBOS:\n",
    "    key = combo_key_from_tuple(combo)\n",
    "\n",
    "    # Build static columns in stable order\n",
    "    if key == \"simple\":\n",
    "        STATIC_COLS = ['aspect', 'slope', 'SVF']\n",
    "    elif key == \"full\":\n",
    "        STATIC_COLS = ['aspect', 'slope', 'SVF', *VARS_ORDER]\n",
    "    else:\n",
    "        # enforce VARS_ORDER inside the combo\n",
    "        ordered = [v for v in VARS_ORDER if v in combo]\n",
    "        STATIC_COLS = ['aspect', 'slope', 'SVF', *ordered]\n",
    "\n",
    "    STATIC_COLS_BY_COMBO[key] = STATIC_COLS\n",
    "\n",
    "    # --- build train dataset from dataframe ---\n",
    "    ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_train,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True)\n",
    "    DS_TRAIN_BY_COMBO[key] = ds_train\n",
    "\n",
    "    # keep per-combo split (use your preferred val_ratio/seed)\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "    SPLIT_IDXS_BY_COMBO[key] = (train_idx, val_idx)\n",
    "\n",
    "# ---- (optional) create named variables for convenience, matching your earlier mapping ----\n",
    "ds_train_simple = DS_TRAIN_BY_COMBO['simple']\n",
    "ds_train_full = DS_TRAIN_BY_COMBO['full']\n",
    "\n",
    "ds_train_hugonnet_only = DS_TRAIN_BY_COMBO['hugonnet_dhdt']\n",
    "ds_train_consensus_only = DS_TRAIN_BY_COMBO['consensus_ice_thickness']\n",
    "ds_train_millan_only = DS_TRAIN_BY_COMBO['millan_v']\n",
    "ds_train_hugonnet_consensus = DS_TRAIN_BY_COMBO[\n",
    "    'hugonnet_dhdt__consensus_ice_thickness']\n",
    "ds_train_hugonnet_millan = DS_TRAIN_BY_COMBO['hugonnet_dhdt__millan_v']\n",
    "ds_train_consensus_millan = DS_TRAIN_BY_COMBO[\n",
    "    'consensus_ice_thickness__millan_v']\n",
    "\n",
    "# and their splits if you want named access\n",
    "train_idx_simple, val_idx_simple = SPLIT_IDXS_BY_COMBO['simple']\n",
    "train_idx_full, val_idx_full = SPLIT_IDXS_BY_COMBO['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- setup (top of file) ----------\n",
    "VARS_ORDER = ['hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "              'millan_v']  # keep a fixed order\n",
    "\n",
    "\n",
    "def combo_key_from_tuple(tup):\n",
    "    \"\"\"('hugonnet_dhdt','millan_v') -> 'hugonnet_dhdt__millan_v' ; () -> 'simple' ; all 3 -> 'full'\"\"\"\n",
    "    if not tup:\n",
    "        return \"simple\"\n",
    "    ordered = [v for v in VARS_ORDER if v in tup]\n",
    "    if ordered == VARS_ORDER:\n",
    "        return \"full\"\n",
    "    return \"__\".join(ordered)\n",
    "\n",
    "\n",
    "# List *exactly* the combos you trained (keys must match your saved filenames/params)\n",
    "TRAINED_COMBOS = [\n",
    "    (),  # simple (no OGGM)\n",
    "    ('hugonnet_dhdt', ),\n",
    "    ('consensus_ice_thickness', ),\n",
    "    ('millan_v', ),\n",
    "    ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "    ('hugonnet_dhdt', 'millan_v'),\n",
    "    ('consensus_ice_thickness', 'millan_v'),\n",
    "    tuple(VARS_ORDER),  # full (all OGGM)\n",
    "]\n",
    "\n",
    "# Build mapping keys\n",
    "COMBO_KEYS = [combo_key_from_tuple(c) for c in TRAINED_COMBOS]\n",
    "\n",
    "# ---- checkpoints for each combo (update paths to match your training) ----\n",
    "# Example filename convention used when training: \"lstm_model_YYYY-MM-DD_CA_<combo>.pt\"\n",
    "# where <combo> joined by underscores; simple/full named accordingly.\n",
    "MODEL_PATHS = {\n",
    "    \"simple\": \"models/lstm_model_2025-10-07_CA_simple.pt\",\n",
    "    \"hugonnet_dhdt\": \"models/lstm_model_2025-10-07_CA_hugonnet_dhdt.pt\",\n",
    "    \"consensus_ice_thickness\":\n",
    "    \"models/lstm_model_2025-10-07_CA_consensus_ice_thickness.pt\",\n",
    "    \"millan_v\": \"models/lstm_model_2025-10-07_CA_millan_v.pt\",\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\":\n",
    "    \"models/lstm_model_2025-10-07_CA_hugonnet_dhdt_consensus_ice_thickness.pt\",\n",
    "    \"hugonnet_dhdt__millan_v\":\n",
    "    \"models/lstm_model_2025-10-07_CA_hugonnet_dhdt_millan_v.pt\",\n",
    "    \"consensus_ice_thickness__millan_v\":\n",
    "    \"models/lstm_model_2025-10-07_CA_consensus_ice_thickness_millan_v.pt\",\n",
    "    \"full\": \"models/lstm_model_2025-10-07_CA_full.pt\",\n",
    "}\n",
    "\n",
    "# ---- params per combo (you already have simple/full; add the partials) ----\n",
    "# Each dict must match what you used to train that checkpoint (Fs must reflect STATIC_COLS length!)\n",
    "PARAMS_BY_COMBO = {\n",
    "    \"simple\": params_simple_model,  # you already have this\n",
    "    \"full\": params_full_model,  # you already have this\n",
    "    # add your partials (example: copy from your training configs)\n",
    "    \"hugonnet_dhdt\": params_hugonnet_only,\n",
    "    \"consensus_ice_thickness\": params_consensus_only,\n",
    "    \"millan_v\": params_millan_only,\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\": params_hugonnet_consensus,\n",
    "    \"hugonnet_dhdt__millan_v\": params_hugonnet_millan,\n",
    "    \"consensus_ice_thickness__millan_v\": params_consensus_millan,\n",
    "}\n",
    "\n",
    "# ---- training datasets used only to provide the *matching scalers* for each combo ----\n",
    "# Build them once (or reuse objects you already created after training).\n",
    "# They must be produced with the *same* MONTHLY_COLS and STATIC_COLS used to train that combo.\n",
    "TRAIN_DS_BY_COMBO = {\n",
    "    \"simple\": ds_train_simple,  # you already had this for simple\n",
    "    \"full\": ds_train_full,  # you already had this for full\n",
    "    \"hugonnet_dhdt\": ds_train_hugonnet_only,\n",
    "    \"consensus_ice_thickness\": ds_train_consensus_only,\n",
    "    \"millan_v\": ds_train_millan_only,\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\": ds_train_hugonnet_consensus,\n",
    "    \"hugonnet_dhdt__millan_v\": ds_train_hugonnet_millan,\n",
    "    \"consensus_ice_thickness__millan_v\": ds_train_consensus_millan,\n",
    "}\n",
    "\n",
    "# Cache for loaded models\n",
    "_MODEL_CACHE = {}\n",
    "\n",
    "\n",
    "def get_model_by_combo(combo_key: str, cfg, device):\n",
    "    \"\"\"Generalized model loader with cache.\"\"\"\n",
    "    if combo_key in _MODEL_CACHE:\n",
    "        return _MODEL_CACHE[combo_key]\n",
    "\n",
    "    if combo_key not in MODEL_PATHS:\n",
    "        raise ValueError(f\"No model path for combo '{combo_key}'\")\n",
    "    if combo_key not in PARAMS_BY_COMBO:\n",
    "        raise ValueError(f\"No params for combo '{combo_key}'\")\n",
    "\n",
    "    ckpt_path = MODEL_PATHS[combo_key]\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found for '{combo_key}': {ckpt_path}\")\n",
    "\n",
    "    params = PARAMS_BY_COMBO[combo_key]\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "                                                       params,\n",
    "                                                       device,\n",
    "                                                       verbose=False)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    _MODEL_CACHE[combo_key] = model\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect_available_combo_key(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Return the most specific trained combo that matches columns present in df.\n",
    "    Priority: full > any 2-var combo > any 1-var combo > simple.\n",
    "    \"\"\"\n",
    "    present = [c for c in VARS_ORDER if c in df.columns]\n",
    "    present_set = set(present)\n",
    "\n",
    "    # try exact matches by decreasing size\n",
    "    candidates = sorted(TRAINED_COMBOS, key=lambda t: len(t), reverse=True)\n",
    "    for tup in candidates:\n",
    "        if set(tup).issubset(present_set):\n",
    "            return combo_key_from_tuple(tup)\n",
    "\n",
    "    return \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple\n",
    "STATIC_COLS_simple = ['aspect', 'slope', 'SVF']\n",
    "params_simple_model = make_params(Fm, STATIC_COLS_simple)\n",
    "PARAMS_BY_COMBO['simple'] = params_simple_model\n",
    "\n",
    "# full\n",
    "STATIC_COLS_full = ['aspect', 'slope', 'SVF', *VARS_ORDER]\n",
    "params_full_model = make_params(Fm, STATIC_COLS_full)\n",
    "PARAMS_BY_COMBO['full'] = params_full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required by the dataset builder regardless of your feature list\n",
    "REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "# Define paths\n",
    "# path_save_glw = os.path.join(\n",
    "#     cfg.dataPath, 'GLAMOS/distributed_MB_grids/MBM/central_europe/')\n",
    "rgi_id_list = os.listdir(path_rgi_alps)\n",
    "\n",
    "# Safe rename helper\n",
    "def safe_rename(df, mapping):\n",
    "    present = {k: v for k, v in mapping.items() if k in df.columns}\n",
    "    return df.rename(columns=present) if present else df\n",
    "\n",
    "# Robust year regex: RGI60-11.01238_grid_2003.parquet\n",
    "YEAR_RE = re.compile(r\"_grid_(\\d{4})\\.parquet$\")\n",
    "\n",
    "# --- main run loop (replaces your has_oggm/simple branch) ---\n",
    "RUN = True\n",
    "if RUN:\n",
    "    # emptyfolder(path_save_glw)\n",
    "\n",
    "    output_file = os.path.join(f\"logs/glacier_mean_MB_{current_date}.csv\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "\n",
    "    index_counter = 0\n",
    "\n",
    "    for rgi_gl in tqdm(rgi_id_list):\n",
    "        seed_all(cfg.seed)\n",
    "        glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "        if not os.path.exists(glacier_path):\n",
    "            print(f\"Folder not found for {rgi_gl}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # collect available years\n",
    "        years = []\n",
    "        for fname in os.listdir(glacier_path):\n",
    "            if not (fname.endswith(\".parquet\") and rgi_gl in fname):\n",
    "                continue\n",
    "            m = YEAR_RE.search(fname)\n",
    "            if m:\n",
    "                years.append(int(m.group(1)))\n",
    "        years = sorted(set(years))\n",
    "        if not years:\n",
    "            print(f\"No parquet years for {rgi_gl}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # clear model cache per glacier (optional)\n",
    "        _MODEL_CACHE.clear()\n",
    "\n",
    "        for year in years:\n",
    "            file_name = f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "            file_path = os.path.join(glacier_path, file_name)\n",
    "\n",
    "            try:\n",
    "                df_grid_monthly = pd.read_parquet(file_path)\n",
    "                df_grid_monthly.rename(columns={'svf':'SVF'}, inplace=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[{rgi_gl} {year}] read error: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # harmonize column names if needed\n",
    "            # df_grid_monthly = safe_rename(df_grid_monthly, {\n",
    "            #     'aspect': 'aspect_sgi',\n",
    "            #     'slope': 'slope_sgi',\n",
    "            # })\n",
    "\n",
    "            # --- choose the best available trained combo for this file ---\n",
    "            combo_key = detect_available_combo_key(\n",
    "                df_grid_monthly\n",
    "            )  # e.g. 'simple', 'hugonnet_dhdt', 'hugonnet_dhdt__millan_v', 'full'\n",
    "\n",
    "            # build STATIC_COLS in stable order matching training schema\n",
    "            if combo_key == \"simple\":\n",
    "                oggm_cols = []\n",
    "            elif combo_key == \"full\":\n",
    "                oggm_cols = VARS_ORDER[:]  # ['hugonnet_dhdt','consensus_ice_thickness','millan_v']\n",
    "            else:\n",
    "                raw = combo_key.split(\"__\")\n",
    "                oggm_cols = [v for v in VARS_ORDER if v in raw]\n",
    "\n",
    "            STATIC_COLS = ['aspect', 'slope', 'SVF'] + oggm_cols\n",
    "            feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "            all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "            # retain only required + feature columns\n",
    "            keep = [\n",
    "                c for c in (set(all_columns) | set(REQUIRED))\n",
    "                if c in df_grid_monthly.columns\n",
    "            ]\n",
    "            df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "            # required checks\n",
    "            missing_req = [\n",
    "                c for c in REQUIRED if c not in df_grid_monthly.columns\n",
    "            ]\n",
    "            if missing_req:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] missing required cols: {missing_req}, skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # minimal NaN cleanup\n",
    "            df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "            if df_grid_monthly_a.empty:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] empty after NaN cleanup, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # --- build inference dataset with the combo's schema ---\n",
    "            ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "                df_grid_monthly_a,\n",
    "                MONTHLY_COLS,\n",
    "                STATIC_COLS,\n",
    "                months_tail_pad=months_tail_pad,\n",
    "                months_head_pad=months_head_pad,\n",
    "                expect_target=False,\n",
    "                show_progress=False,\n",
    "            )\n",
    "\n",
    "            # bring the *matching* training scalers for this combo\n",
    "            if combo_key not in DS_TRAIN_BY_COMBO:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] no training dataset registered for combo '{combo_key}', skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            ds_train_for_combo = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "                DS_TRAIN_BY_COMBO[combo_key])\n",
    "\n",
    "            if combo_key not in SPLIT_IDXS_BY_COMBO:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] no split indices registered for combo '{combo_key}', skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            train_idx_for_combo, _ = SPLIT_IDXS_BY_COMBO[combo_key]\n",
    "            ds_train_for_combo.fit_scalers(train_idx_for_combo)\n",
    "\n",
    "            test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "                ds_gl_a, ds_train_for_combo, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "            # --- load + predict with the appropriate model for this combo ---\n",
    "            try:\n",
    "                model = get_model_by_combo(combo_key, cfg, device)\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] model load error for '{combo_key}': {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_preds_a = model.predict_with_keys(device, test_gl_dl_a,\n",
    "                                                     ds_gl_a)\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] prediction error for '{combo_key}': {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # --- aggregate to annual glacier mean and write ---\n",
    "            data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "            meta_cols = [\n",
    "                c for c in ['YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID']\n",
    "                if c in df_grid_monthly_a.columns\n",
    "            ]\n",
    "\n",
    "            grouped_ids_a = (\n",
    "                df_grid_monthly_a.groupby('ID')[meta_cols].first().merge(\n",
    "                    data_a, left_index=True, right_index=True, how='left'))\n",
    "            months_per_id_a = df_grid_monthly_a.groupby(\n",
    "                'ID')['MONTHS'].unique()\n",
    "            grouped_ids_a = grouped_ids_a.merge(months_per_id_a,\n",
    "                                                left_index=True,\n",
    "                                                right_index=True)\n",
    "\n",
    "            grouped_ids_a.reset_index(inplace=True)\n",
    "            grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "            pred_y_annual = grouped_ids_a.copy()\n",
    "            pred_y_annual['PERIOD'] = 'annual'\n",
    "            pred_y_annual = pred_y_annual.drop(columns=['YEAR'],\n",
    "                                               errors='ignore')\n",
    "\n",
    "            mean_MB = pred_y_annual['pred'].mean()\n",
    "\n",
    "            with open(output_file, 'a') as f:\n",
    "                f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "\n",
    "            index_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, traceback, sys\n",
    "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ----------------- config -----------------\n",
    "# YEAR_RE = re.compile(r\"_grid_(\\d{4})\\.parquet$\")\n",
    "# REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "\n",
    "# # Avoid BLAS/OpenMP oversubscription inside each worker\n",
    "# os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "# os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# MAX_WORKERS = min(os.cpu_count() or 4, 12)\n",
    "# current_date = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# # ----------------- helpers -----------------\n",
    "# def list_years(glacier_path: str, rgi_gl: str):\n",
    "#     years = []\n",
    "#     for fname in os.listdir(glacier_path):\n",
    "#         if fname.endswith(\".parquet\") and rgi_gl in fname:\n",
    "#             m = YEAR_RE.search(fname)\n",
    "#             if m:\n",
    "#                 years.append(int(m.group(1)))\n",
    "#     return sorted(set(years))\n",
    "\n",
    "# def process_one_glacier(rgi_gl: str):\n",
    "#     \"\"\"\n",
    "#     Returns dict:\n",
    "#       {\n",
    "#         'rgi': rgi_gl,\n",
    "#         'rows': [(rgi_gl, year, mean_MB), ...],\n",
    "#         'log':  [\"message\", ...],     # warnings/information\n",
    "#         'err':  \"traceback or None\"\n",
    "#       }\n",
    "#     \"\"\"\n",
    "#     out = {'rgi': rgi_gl, 'rows': [], 'log': [], 'err': None}\n",
    "#     try:\n",
    "#         seed_all(cfg.seed)\n",
    "\n",
    "#         glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "#         if not os.path.isdir(glacier_path):\n",
    "#             out['log'].append(f\"[{rgi_gl}] folder not found: {glacier_path}\")\n",
    "#             return out\n",
    "\n",
    "#         years = list_years(glacier_path, rgi_gl)\n",
    "#         if not years:\n",
    "#             out['log'].append(f\"[{rgi_gl}] no parquet years in {glacier_path}\")\n",
    "#             return out\n",
    "\n",
    "#         _MODEL_CACHE.clear()\n",
    "#         local_model_cache = {}\n",
    "\n",
    "#         for year in years:\n",
    "#             file_path = os.path.join(glacier_path, f\"{rgi_gl}_grid_{year}.parquet\")\n",
    "#             try:\n",
    "#                 df = pd.read_parquet(file_path)\n",
    "#             except Exception as e:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] read error: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             # normalize SVF column name\n",
    "#             if 'svf' in df.columns and 'SVF' not in df.columns:\n",
    "#                 df.rename(columns={'svf': 'SVF'}, inplace=True)\n",
    "\n",
    "#             # choose combo\n",
    "#             try:\n",
    "#                 combo_key = detect_available_combo_key(df)\n",
    "#             except Exception as e:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] detect combo failed: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             if combo_key == \"simple\":\n",
    "#                 oggm_cols = []\n",
    "#             elif combo_key == \"full\":\n",
    "#                 oggm_cols = VARS_ORDER[:]   # defined in your config\n",
    "#             else:\n",
    "#                 raw = combo_key.split(\"__\")\n",
    "#                 oggm_cols = [v for v in VARS_ORDER if v in raw]\n",
    "\n",
    "#             STATIC_COLS = ['aspect', 'slope', 'SVF'] + oggm_cols\n",
    "#             feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "#             all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "#             keep = [c for c in (set(all_columns) | set(REQUIRED)) if c in df.columns]\n",
    "#             df = df[keep]\n",
    "\n",
    "#             missing_req = [c for c in REQUIRED if c not in df.columns]\n",
    "#             if missing_req:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] missing required: {missing_req}\")\n",
    "#                 continue\n",
    "\n",
    "#             df = df.dropna(subset=['ID', 'MONTHS'])\n",
    "#             if df.empty:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] empty after NaN cleanup\")\n",
    "#                 continue\n",
    "\n",
    "#             # dataset\n",
    "#             try:\n",
    "#                 ds = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "#                     df,\n",
    "#                     MONTHLY_COLS,\n",
    "#                     STATIC_COLS,\n",
    "#                     months_tail_pad=months_tail_pad,\n",
    "#                     months_head_pad=months_head_pad,\n",
    "#                     expect_target=False,\n",
    "#                     show_progress=False,\n",
    "#                 )\n",
    "#             except Exception as e:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] build dataset failed: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             # scalers\n",
    "#             if combo_key not in DS_TRAIN_BY_COMBO or combo_key not in SPLIT_IDXS_BY_COMBO:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] missing train/scaler for combo {combo_key}\")\n",
    "#                 continue\n",
    "\n",
    "#             ds_train = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#                 DS_TRAIN_BY_COMBO[combo_key]\n",
    "#             )\n",
    "#             train_idx, _ = SPLIT_IDXS_BY_COMBO[combo_key]\n",
    "#             ds_train.fit_scalers(train_idx)\n",
    "\n",
    "#             test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#                 ds, ds_train, seed=cfg.seed, batch_size=128\n",
    "#             )\n",
    "\n",
    "#             # model\n",
    "#             try:\n",
    "#                 if combo_key not in local_model_cache:\n",
    "#                     local_model_cache[combo_key] = get_model_by_combo(combo_key, cfg, device)\n",
    "#                 model = local_model_cache[combo_key]\n",
    "#             except Exception as e:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] model load error: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             # predict\n",
    "#             try:\n",
    "#                 df_preds = model.predict_with_keys(device, test_dl, ds)\n",
    "#             except Exception as e:\n",
    "#                 out['log'].append(f\"[{rgi_gl} {year}] predict error: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             mean_MB = float(df_preds['pred'].mean())\n",
    "#             out['rows'].append((rgi_gl, year, mean_MB))\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     except Exception as e:\n",
    "#         out['err'] = f\"{e}\\n{traceback.format_exc()}\"\n",
    "#         return out\n",
    "\n",
    "# # ----------------- driver with progress + incremental write -----------------\n",
    "# output_file = os.path.join(\"logs\", f\"glacier_mean_MB_{current_date}.csv\")\n",
    "# os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# # list glacier folders to run\n",
    "# if not os.path.isdir(path_rgi_alps):\n",
    "#     print(f\"â path_rgi_alps not found: {path_rgi_alps}\")\n",
    "# else:\n",
    "#     rgi_id_list = [d for d in os.listdir(path_rgi_alps)\n",
    "#                    if os.path.isdir(os.path.join(path_rgi_alps, d))]\n",
    "#     print(f\"Will process {len(rgi_id_list)} glacier folders under {path_rgi_alps}\")\n",
    "#     if rgi_id_list[:5]:\n",
    "#         print(\"First few:\", rgi_id_list[:5])\n",
    "\n",
    "# # write header\n",
    "# with open(output_file, 'w') as f:\n",
    "#     f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "# index_counter = 0\n",
    "\n",
    "# # run\n",
    "# with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "#     futs = {ex.submit(process_one_glacier, rgi): rgi for rgi in rgi_id_list}\n",
    "#     with tqdm(total=len(futs), desc=\"Glaciers\", ncols=100) as pbar:\n",
    "#         for fut in as_completed(futs):\n",
    "#             rgi = futs[fut]\n",
    "#             try:\n",
    "#                 result = fut.result()\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[{rgi}] worker crashed: {e}\")\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "\n",
    "#             # log diagnostics\n",
    "#             for msg in result.get('log', []):\n",
    "#                 print(msg)\n",
    "#             if result.get('err'):\n",
    "#                 print(f\"[{rgi}] ERROR:\\n{result['err']}\")\n",
    "\n",
    "#             rows = result.get('rows', [])\n",
    "#             if rows:\n",
    "#                 with open(output_file, 'a') as f:\n",
    "#                     for rgi_gl, year, mean_MB in rows:\n",
    "#                         f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "#                         index_counter += 1\n",
    "#                 # ensure it hits disk even if kernel dies\n",
    "#                 try:\n",
    "#                     os.fsync(open(output_file, 'a').fileno())\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "\n",
    "#             pbar.set_postfix_str(f\"{rgi}: {len(rows)} rows\")\n",
    "#             pbar.update(1)\n",
    "\n",
    "# print(f\"â Done. Wrote {index_counter} lines to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_rgi = cfg.dataPath+'GLAMOS/RGI/RGI2000-v7.0-G-11_central_europe/RGI2000-v7.0-G-11_central_europe.shp'\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "\n",
    "# check CRS\n",
    "print(gdf.crs)\n",
    "\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.set_index('RGIId', inplace=True, drop=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_df = pd.read_csv(\"logs/glacier_mean_MB_2025-10-08.csv\").drop(['Index'], axis=1)\n",
    "\n",
    "output_df['area_gl'] = output_df['RGIId'].map(\n",
    "    lambda x: gdf_proj.loc[x, 'area_km2'])\n",
    "\n",
    "# yearly_mean_mb_CA = output_df.groupby('Year',\n",
    "#                                       as_index=False).agg({'Mean_MB': 'mean'})\n",
    "# yearly_cum_mb_CA = output_df.groupby('Year',\n",
    "#                                      as_index=False).agg({'Mean_MB': 'sum'})\n",
    "# yearly_cum_mb_CA['Cum_MB'] = yearly_cum_mb_CA['Mean_MB'].cumsum()\n",
    "# yearly_cum_mb_CA['Mean_MB'] = yearly_mean_mb_CA['Mean_MB']\n",
    "# # yearly_cum_mb_CA['Mean_MB'] = yearly_cum_mb_CA['Mean_MB'] / total_area\n",
    "# yearly_cum_mb_CA.head()\n",
    "\n",
    "df = output_df.copy()\n",
    "\n",
    "# annual change per glacier in Gt\n",
    "df[\"annual_change_gt\"] = (df[\"Mean_MB\"] * df[\"area_gl\"]) / 1e9\n",
    "\n",
    "# total annual change in Gt (sum across glaciers)\n",
    "annual_gt = df.groupby(\"Year\")[\"annual_change_gt\"].sum().reset_index(\n",
    "    name=\"Annual_MB_Gt\")\n",
    "\n",
    "# cumulative MB in Gt\n",
    "annual_gt[\"Cumulative_MB_Gt\"] = annual_gt[\"Annual_MB_Gt\"].cumsum()\n",
    "\n",
    "# compute weighted mean MB per year\n",
    "yearly_weighted = (output_df.groupby(\"Year\").apply(lambda g: (g[\"Mean_MB\"] * g[\n",
    "    \"area_gl\"]).sum() / g[\"area_gl\"].sum()).reset_index(name=\"Weighted_MB\"))\n",
    "\n",
    "print(yearly_weighted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glambie_df = pd.read_csv('glambie_values.csv')\n",
    "date_columns = [\n",
    "    'central_europe_dates', 'central_europe_start_dates',\n",
    "    'central_europe_end_dates'\n",
    "]\n",
    "\n",
    "glambie_df[date_columns] = glambie_df[date_columns].apply(\n",
    "    lambda x: x.round() - 1)\n",
    "glambie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plotting ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "\n",
    "# --------------------\n",
    "# Left: LSTM results\n",
    "# --------------------\n",
    "ax1 = axs[0]\n",
    "years = yearly_weighted['Year']\n",
    "\n",
    "# barplot: annual weighted MB (m w.e.)\n",
    "ax1.bar(years,\n",
    "        yearly_weighted['Weighted_MB'],\n",
    "        color=\"skyblue\",\n",
    "        label=\"Area-weighted annual MB\")\n",
    "ax1.set_ylabel(\"Annual MB (m w.e.)\", color=\"skyblue\")\n",
    "\n",
    "# lineplot: cumulative MB in Gt (secondary axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(annual_gt['Year'],\n",
    "         annual_gt['Cumulative_MB_Gt'],\n",
    "         color=\"red\",\n",
    "         marker=\"o\",\n",
    "         label=\"Cumulative MB\")\n",
    "ax2.set_ylabel(\"Cumulative MB (Gt)\", color=\"red\")\n",
    "\n",
    "ax1.set_title(\"Central Alps annual MB (LSTM)\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Right: GLAMBIE results\n",
    "# --------------------\n",
    "ax3 = axs[1]\n",
    "\n",
    "# annual MB (bars)\n",
    "ax3.bar(glambie_df['central_europe_end_dates'],\n",
    "        glambie_df['central_europe_annual_change_mwe'],\n",
    "        color=\"lightgreen\",\n",
    "        label=\"Annual MB (GLAMBIE)\")\n",
    "ax3.set_ylabel(\"Annual MB (m w.e.)\", color=\"lightgreen\")\n",
    "\n",
    "# cumulative MB (line, secondary axis)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(glambie_df['central_europe_dates'],\n",
    "         glambie_df['central_europe_cumulative_change_gt'],\n",
    "         color=\"darkgreen\",\n",
    "         marker=\"s\",\n",
    "         label=\"Cumulative MB (GLAMBIE)\")\n",
    "ax4.set_ylabel(\"Cumulative MB (Gt)\", color=\"darkgreen\")\n",
    "\n",
    "ax3.set_title(\"Central Europe MB (GLAMBIE)\")\n",
    "ax3.legend(loc=\"upper left\")\n",
    "ax4.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Formatting\n",
    "# --------------------\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both datasets use the same x-axis type\n",
    "years_lstm = yearly_weighted['Year']\n",
    "\n",
    "years_glambie = glambie_df['central_europe_end_dates']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# bar width\n",
    "width = 0.4\n",
    "\n",
    "# LSTM bars (slightly shifted left)\n",
    "ax.bar(years_lstm - 0.2,\n",
    "       yearly_weighted['Weighted_MB'],\n",
    "       width=width,\n",
    "       color=\"skyblue\",\n",
    "       label=\"LSTM Annual MB\")\n",
    "\n",
    "# GLAMBIE bars (slightly shifted right)\n",
    "ax.bar(years_glambie + 0.2,\n",
    "       glambie_df['central_europe_annual_change_mwe'],\n",
    "       width=width,\n",
    "       color=\"lightgreen\",\n",
    "       label=\"GLAMBIE Annual MB\")\n",
    "\n",
    "# formatting\n",
    "ax.set_ylabel(\"Annual MB (m w.e.)\")\n",
    "ax.set_title(\"Annual Mass Balance: LSTM vs GLAMBIE\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
