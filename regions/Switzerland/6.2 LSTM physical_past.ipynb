{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Remove 2025\n",
    "data_monthly = data_monthly[data_monthly['YEAR']\n",
    "                            < 2025]  # Used elsewhere for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Remove 2025\n",
    "data_monthly_Aug_ = data_monthly_Aug_[data_monthly_Aug_['YEAR']\n",
    "                                      < 2025]  # Used elsewhere for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_LSTM_IS = os.path.join(cfg.dataPath, \"GLAMOS\",\n",
    "                                        \"distributed_MB_grids\",\n",
    "                                        \"MBM/paper/LSTM_IS_ORIGINAL_Y_PAST\")\n",
    "\n",
    "# PATH_PREDICTIONS_LSTM_IS = os.path.join(cfg.dataPath, \"GLAMOS\",\n",
    "#                                         \"distributed_MB_grids\",\n",
    "#                                         \"MBM/paper/LSTM_IS_NORM_Y_PAST\")\n",
    "\n",
    "PATH_PREDICTIONS_NN = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                   'distributed_MB_grids', 'MBM/paper/NN')\n",
    "\n",
    "PATH_PREDICTIONS_XGB = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                    'distributed_MB_grids', 'MBM/paper/XGB')\n",
    "\n",
    "hydro_months = [\n",
    "    'oct',\n",
    "    'nov',\n",
    "    'dec',\n",
    "    'jan',\n",
    "    'feb',\n",
    "    'mar',\n",
    "    'apr',\n",
    "    'may',\n",
    "    'jun',\n",
    "    'jul',\n",
    "    'aug',\n",
    "    'sep',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "#     glacier_name=\"rhone\",\n",
    "#     year=2008,\n",
    "#     path_pred_lstm=PATH_PREDICTIONS_NN,\n",
    "#     apply_smoothing_fn=apply_gaussian_filter,\n",
    "# )\n",
    "\n",
    "# fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "#     glacier_name=\"rhone\",\n",
    "#     year=2008,\n",
    "#     path_pred_lstm=PATH_PREDICTIONS_LSTM_IS,\n",
    "#     apply_smoothing_fn=apply_gaussian_filter,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(\n",
    "    cfg.dataPath,\n",
    "    \"GLAMOS/distributed_MB_grids/MBM/paper/processed_dfs\",\n",
    ")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "REBUILD_CACHE = False  # set True only when inputs change\n",
    "\n",
    "paths = {\n",
    "    \"LSTM\": os.path.join(CACHE_DIR, \"df_months_LSTM.parquet\"),\n",
    "    \"NN\": os.path.join(CACHE_DIR, \"df_months_NN.parquet\"),\n",
    "    \"XGB\": os.path.join(CACHE_DIR, \"df_months_XGB.parquet\"),\n",
    "    \"GW\": os.path.join(CACHE_DIR, \"df_GLAMOS_w.parquet\"),\n",
    "    \"GA\": os.path.join(CACHE_DIR, \"df_GLAMOS_a.parquet\"),\n",
    "}\n",
    "\n",
    "if REBUILD_CACHE or not all(os.path.exists(p) for p in paths.values()):\n",
    "\n",
    "    print(\"Building monthly prediction DataFrames...\")\n",
    "\n",
    "    df_months_LSTM = load_glwd_lstm_predictions(PATH_PREDICTIONS_LSTM_IS,\n",
    "                                                hydro_months)\n",
    "    df_months_NN = load_glwd_nn_predictions(PATH_PREDICTIONS_NN, hydro_months)\n",
    "    df_months_XGB = load_glwd_nn_predictions(PATH_PREDICTIONS_XGB,\n",
    "                                             hydro_months)\n",
    "\n",
    "    PATH_GLAMOS = os.path.join(cfg.dataPath, path_distributed_MB_glamos,\n",
    "                               \"GLAMOS\")\n",
    "    glaciers = os.listdir(PATH_GLAMOS)\n",
    "\n",
    "    glacier_years = (df_months_LSTM.groupby(\"glacier\")[\"year\"].unique().apply(\n",
    "        sorted).to_dict())\n",
    "\n",
    "    df_GLAMOS_w, df_GLAMOS_a = load_all_glamos(cfg, glacier_years, PATH_GLAMOS)\n",
    "\n",
    "    # Save cache\n",
    "    df_months_LSTM.to_parquet(paths[\"LSTM\"])\n",
    "    df_months_NN.to_parquet(paths[\"NN\"])\n",
    "    df_months_XGB.to_parquet(paths[\"XGB\"])\n",
    "    df_GLAMOS_w.to_parquet(paths[\"GW\"])\n",
    "    df_GLAMOS_a.to_parquet(paths[\"GA\"])\n",
    "\n",
    "    print(f\"Cached DataFrames to {CACHE_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading cached monthly prediction DataFrames...\")\n",
    "\n",
    "    df_months_LSTM = pd.read_parquet(paths[\"LSTM\"])\n",
    "    df_months_NN = pd.read_parquet(paths[\"NN\"])\n",
    "    df_months_XGB = pd.read_parquet(paths[\"XGB\"])\n",
    "    df_GLAMOS_w = pd.read_parquet(paths[\"GW\"])\n",
    "    df_GLAMOS_a = pd.read_parquet(paths[\"GA\"])\n",
    "\n",
    "    print(f\"Loaded DataFrames from {CACHE_DIR}\")\n",
    "    \n",
    "\n",
    "# Cut to years above 2000\n",
    "# df_months_LSTM = df_months_LSTM[df_months_LSTM[\"year\"] >= 2000]\n",
    "# df_months_NN = df_months_NN[df_months_NN[\"year\"] >= 2000]\n",
    "# df_months_XGB = df_months_XGB[df_months_XGB[\"year\"] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_months_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glacier-wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Glacier-wide annual mean MB per year ---\n",
    "glwd_months_NN = df_months_NN.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_XGB = df_months_XGB.groupby(['glacier',\n",
    "                                         'year']).mean().reset_index()\n",
    "glwd_months_LSTM = df_months_LSTM.groupby(['glacier',\n",
    "                                           'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_w = df_GLAMOS_w.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_a = df_GLAMOS_a.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "\n",
    "# --- 2. Compute the intersection of valid glacier–year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_months_NN['glacier'], glwd_months_NN['year']))\n",
    "    & set(zip(glwd_months_XGB['glacier'], glwd_months_XGB['year']))\n",
    "    & set(zip(glwd_months_LSTM['glacier'], glwd_months_LSTM['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_w['glacier'], glwd_months_GLAMOS_w['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_a['glacier'], glwd_months_GLAMOS_a['year'])))\n",
    "\n",
    "\n",
    "# --- 3. Helper function for filtering by glacier–year pairs ---\n",
    "def filter_to_valid(df):\n",
    "    return df[df[['glacier', 'year'\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 4. Apply filtering to all datasets ---\n",
    "glwd_months_NN_filtered = filter_to_valid(glwd_months_NN)\n",
    "glwd_months_XGB_filtered = filter_to_valid(glwd_months_XGB)\n",
    "glwd_months_LSTM_filtered = filter_to_valid(glwd_months_LSTM)\n",
    "glwd_months_GLAMOS_filtered_w = filter_to_valid(glwd_months_GLAMOS_w)\n",
    "glwd_months_GLAMOS_filtered_a = filter_to_valid(glwd_months_GLAMOS_a)\n",
    "\n",
    "print(\n",
    "    len(glwd_months_GLAMOS_filtered_w),\n",
    "    len(glwd_months_GLAMOS_filtered_a),\n",
    "    len(glwd_months_NN_filtered),\n",
    "    len(glwd_months_XGB_filtered),\n",
    "    len(glwd_months_LSTM_filtered),\n",
    ")\n",
    "\n",
    "# --- 5. Prepare for plotting ---\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_months_LSTM_filtered,\n",
    "    glwd_months_NN_filtered,\n",
    "    glwd_months_XGB_filtered,\n",
    "    glwd_months_GLAMOS_filtered_w,\n",
    "    glwd_months_GLAMOS_filtered_a,\n",
    ")\n",
    "\n",
    "df_months_nn_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Compute min & max across models being plotted ----\n",
    "min_ = df_months_nn_long[['mb_nn', 'mb_lstm', 'mb_xgb']].min().min()\n",
    "max_ = df_months_nn_long[['mb_nn', 'mb_lstm', 'mb_xgb']].max().max()\n",
    "\n",
    "# ---- Plot ----\n",
    "fig = plot_monthly_joyplot(\n",
    "    df_months_nn_long,\n",
    "    x_range=(np.floor(min_), np.ceil(max_)),\n",
    "    color_lstm=color_annual,  # or rename to your liking\n",
    "    color_nn=color_winter,\n",
    "    color_xgb=\"darkgreen\",\n",
    "    color_glamos=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = df_months_nn_long.min()[[\n",
    "    'mb_nn', 'mb_glamos'\n",
    "]].min(), df_months_nn_long.max()[['mb_nn', 'mb_glamos']].max()\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)),\n",
    "                                  model_name='MBM')\n",
    "fig.savefig('figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_glwd.png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation bands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = 200\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# Copy datasets\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_XGB_ = df_months_XGB.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# Assign elevation bands\n",
    "for df_ in [\n",
    "        df_months_NN_,\n",
    "        df_months_XGB_,\n",
    "        df_months_LSTM_,\n",
    "        df_GLAMOS_a_,\n",
    "        df_GLAMOS_w_,\n",
    "]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "def extract_highest_band(df, bin_width):\n",
    "    max_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"max\")\n",
    "    highest_band = df[df[\"elevation\"] >= (max_elev - bin_width)]\n",
    "    return (highest_band.groupby([\"glacier\", \"year\"\n",
    "                                  ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "glwd_high_NN = extract_highest_band(df_months_NN_, bin)\n",
    "glwd_high_XGB = extract_highest_band(df_months_XGB_, bin)\n",
    "glwd_high_LSTM = extract_highest_band(df_months_LSTM_, bin)\n",
    "glwd_high_GLAMOS_a = extract_highest_band(df_GLAMOS_a_, bin)\n",
    "glwd_high_GLAMOS_w = extract_highest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_high_NN[\"glacier\"], glwd_high_NN[\"year\"]))\n",
    "    & set(zip(glwd_high_XGB[\"glacier\"], glwd_high_XGB[\"year\"]))\n",
    "    & set(zip(glwd_high_LSTM[\"glacier\"], glwd_high_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_w[\"glacier\"], glwd_high_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_a[\"glacier\"], glwd_high_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return (df[df[[\"glacier\", \"year\"\n",
    "                   ]].apply(tuple,\n",
    "                            axis=1).isin(valid_pairs)].reset_index(drop=True))\n",
    "\n",
    "\n",
    "glwd_high_NN_filt = filter_to_valid(glwd_high_NN)\n",
    "glwd_high_XGB_filt = filter_to_valid(glwd_high_XGB)\n",
    "glwd_high_LSTM_filt = filter_to_valid(glwd_high_LSTM)\n",
    "glwd_high_GLAMOS_a_filt = filter_to_valid(glwd_high_GLAMOS_a)\n",
    "glwd_high_GLAMOS_w_filt = filter_to_valid(glwd_high_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_high_GLAMOS_w_filt),\n",
    "    len(glwd_high_GLAMOS_a_filt),\n",
    "    len(glwd_high_NN_filt),\n",
    "    len(glwd_high_XGB_filt),\n",
    "    len(glwd_high_LSTM_filt),\n",
    ")\n",
    "\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_high_LSTM_filt,\n",
    "    glwd_high_NN_filt,\n",
    "    glwd_high_XGB_filt,\n",
    "    glwd_high_GLAMOS_w_filt,\n",
    "    glwd_high_GLAMOS_a_filt,\n",
    ")\n",
    "\n",
    "min_, max_ = (\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\", \"mb_glamos\"]].min().min(),\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\", \"mb_glamos\"]].max().max(),\n",
    ")\n",
    "\n",
    "fig = plot_monthly_joyplot_single(\n",
    "    df_months_nn_long,\n",
    "    variable=\"mb_lstm\",\n",
    "    color_model=color_annual,\n",
    "    x_range=(np.floor(min_), np.ceil(max_)),\n",
    "    model_name=\"MBM\",\n",
    ")\n",
    "\n",
    "fig.savefig(\n",
    "    \"figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_high_elv.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Lowest-elevation band\n",
    "# =========================\n",
    "\n",
    "bin = 200\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# --- Copy to avoid modifying originals ---\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_XGB_ = df_months_XGB.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# --- Assign elevation bands ---\n",
    "for df_ in [\n",
    "        df_months_NN_,\n",
    "        df_months_XGB_,\n",
    "        df_months_LSTM_,\n",
    "        df_GLAMOS_a_,\n",
    "        df_GLAMOS_w_,\n",
    "]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# --- Helper: extract lowest-elevation band per glacier ---\n",
    "def extract_lowest_band(df, bin_width):\n",
    "    min_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"min\")\n",
    "    lowest_band = df[df[\"elevation\"] <= (min_elev + bin_width)]\n",
    "    return (lowest_band.groupby([\"glacier\", \"year\"\n",
    "                                 ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "# --- Compute lowest-elevation bands ---\n",
    "glwd_low_NN = extract_lowest_band(df_months_NN_, bin)\n",
    "glwd_low_XGB = extract_lowest_band(df_months_XGB_, bin)\n",
    "glwd_low_LSTM = extract_lowest_band(df_months_LSTM_, bin)\n",
    "glwd_low_GLAMOS_a = extract_lowest_band(df_GLAMOS_a_, bin)\n",
    "glwd_low_GLAMOS_w = extract_lowest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "# --- Define common glacier–year pairs ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_low_NN[\"glacier\"], glwd_low_NN[\"year\"]))\n",
    "    & set(zip(glwd_low_XGB[\"glacier\"], glwd_low_XGB[\"year\"]))\n",
    "    & set(zip(glwd_low_LSTM[\"glacier\"], glwd_low_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_w[\"glacier\"], glwd_low_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_a[\"glacier\"], glwd_low_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return (df[df[[\"glacier\", \"year\"\n",
    "                   ]].apply(tuple,\n",
    "                            axis=1).isin(valid_pairs)].reset_index(drop=True))\n",
    "\n",
    "\n",
    "# --- Apply consistent filtering ---\n",
    "glwd_low_NN_filt = filter_to_valid(glwd_low_NN)\n",
    "glwd_low_XGB_filt = filter_to_valid(glwd_low_XGB)\n",
    "glwd_low_LSTM_filt = filter_to_valid(glwd_low_LSTM)\n",
    "glwd_low_GLAMOS_a_filt = filter_to_valid(glwd_low_GLAMOS_a)\n",
    "glwd_low_GLAMOS_w_filt = filter_to_valid(glwd_low_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_low_GLAMOS_w_filt),\n",
    "    len(glwd_low_GLAMOS_a_filt),\n",
    "    len(glwd_low_NN_filt),\n",
    "    len(glwd_low_XGB_filt),\n",
    "    len(glwd_low_LSTM_filt),\n",
    ")\n",
    "\n",
    "# --- Prepare long-format dataframe for plotting ---\n",
    "df_months_nn_long_low = prepare_monthly_long_df(\n",
    "    glwd_low_LSTM_filt,\n",
    "    glwd_low_NN_filt,\n",
    "    glwd_low_XGB_filt,\n",
    "    glwd_low_GLAMOS_w_filt,\n",
    "    glwd_low_GLAMOS_a_filt,\n",
    ")\n",
    "\n",
    "# --- Determine x-axis limits ---\n",
    "min_, max_ = (\n",
    "    df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\",\n",
    "                           \"mb_glamos\"]].min().min(),\n",
    "    df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\",\n",
    "                           \"mb_glamos\"]].max().max(),\n",
    ")\n",
    "\n",
    "# Optional manual clamp for ablation-dominated lowest band\n",
    "min_ = -10\n",
    "\n",
    "# --- Plot ---\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long_low,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)),\n",
    "                                  model_name=\"MBM\",\n",
    "                                  y_offset=0.15)\n",
    "\n",
    "# --- Save figure ---\n",
    "fig.savefig(\n",
    "    \"figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_low_elv.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train = data_monthly[data_monthly.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly_train[data_monthly_train.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train['y'] = data_train['POINT_BALANCE']\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train_Aug_.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train_Aug_ = data_monthly_train_Aug_[data_monthly_train_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train_Aug_))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train_Aug_['y'] = data_train_Aug_['POINT_BALANCE']\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "ds_train = build_combined_LSTM_dataset(df_loss=data_train,\n",
    "                                       df_full=data_train_Aug_,\n",
    "                                       monthly_cols=MONTHLY_COLS,\n",
    "                                       static_cols=STATIC_COLS,\n",
    "                                       months_head_pad=months_head_pad_Aug_,\n",
    "                                       months_tail_pad=months_tail_pad_Aug_,\n",
    "                                       normalize_target=True,\n",
    "                                       expect_target=True)\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, PARAMS_LSTM_IS_past,\n",
    "                                                   device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(PARAMS_LSTM_IS_past)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "state = torch.load(LSTM_IS_ORIGIN_Y_PAST, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_rmse_pfi(model, device, dl, ds):\n",
    "    \"\"\"\n",
    "    Compute RMSE_winter and RMSE_annual exactly like evaluate_with_preds:\n",
    "    - uses ds.keys to route samples to winter vs annual\n",
    "    - denormalizes with ds.y_std / ds.y_mean\n",
    "    Assumes dl iterates ds in order (shuffle=False).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    y_true_w, y_pred_w = [], []\n",
    "    y_true_a, y_pred_a = [], []\n",
    "\n",
    "    all_keys = ds.keys\n",
    "    i = 0\n",
    "\n",
    "    y_std = ds.y_std.to(device)\n",
    "    y_mean = ds.y_mean.to(device)\n",
    "\n",
    "    for batch in dl:\n",
    "        bs = batch[\"x_m\"].shape[0]\n",
    "        batch_keys = all_keys[i:i + bs]\n",
    "        i += bs\n",
    "\n",
    "        x_m = batch[\"x_m\"].to(device)\n",
    "        x_s = batch[\"x_s\"].to(device)\n",
    "        mv = batch[\"mv\"].to(device)\n",
    "        mw = batch[\"mw\"].to(device)\n",
    "        ma = batch[\"ma\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        _, y_w, y_a = model(x_m, x_s, mv, mw, ma)\n",
    "\n",
    "        # denormalize (matches evaluate_with_preds)\n",
    "        y_true = y * y_std + y_mean\n",
    "        y_w = y_w * y_std + y_mean\n",
    "        y_a = y_a * y_std + y_mean\n",
    "\n",
    "        for j in range(bs):\n",
    "            *_, per = batch_keys[j]\n",
    "            if per == \"winter\":\n",
    "                y_true_w.append(float(y_true[j].cpu()))\n",
    "                y_pred_w.append(float(y_w[j].cpu()))\n",
    "            else:  # annual\n",
    "                y_true_a.append(float(y_true[j].cpu()))\n",
    "                y_pred_a.append(float(y_a[j].cpu()))\n",
    "\n",
    "    def rmse(t, p):\n",
    "        t = np.asarray(t, dtype=float)\n",
    "        p = np.asarray(p, dtype=float)\n",
    "        if len(t) == 0:\n",
    "            return float(\"nan\")\n",
    "        return float(np.sqrt(np.mean((p - t)**2)))\n",
    "\n",
    "    return rmse(y_true_w, y_pred_w), rmse(y_true_a, y_pred_a)\n",
    "\n",
    "\n",
    "def permutation_importance_LSTM_MB_full(\n",
    "    model,\n",
    "    device,\n",
    "    ds_test,\n",
    "    monthly_cols,\n",
    "    static_cols,\n",
    "    n_repeats=5,\n",
    "    seed=0,\n",
    "    batch_size=128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregated PFI (period-aware, denormalized):\n",
    "      - Monthly features: permute feature k across samples for ALL months ([:, :, k])\n",
    "      - Static features:  permute feature j across samples ([:, j])\n",
    "\n",
    "    Outputs both absolute ΔRMSE and relative ΔRMSE (Δ / baseline),\n",
    "    plus a *sample-weighted* global relative metric:\n",
    "        global_rel = (Nw * dw_rel + Na * da_rel) / (Nw + Na)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    print(\"\\n▶️ Running aggregated permutation feature importance (PFI)...\")\n",
    "\n",
    "    base_dl = torch.utils.data.DataLoader(ds_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "    base_w, base_a = eval_rmse_pfi(model, device, base_dl, ds_test)\n",
    "\n",
    "    # counts for sample-weighted global\n",
    "    Nw = int(ds_test.iw.sum())\n",
    "    Na = int(ds_test.ia.sum())\n",
    "\n",
    "    print(\n",
    "        f\"[Baseline RMSE] winter={base_w:.3f} | annual={base_a:.3f} (Nw={Nw}, Na={Na})\"\n",
    "    )\n",
    "\n",
    "    Xm0 = ds_test.Xm.clone()\n",
    "    Xs0 = ds_test.Xs.clone()\n",
    "\n",
    "    rows = []\n",
    "    total_steps = (len(monthly_cols) + len(static_cols)) * n_repeats\n",
    "    pbar = tqdm(total=total_steps, desc=\"Aggregated permutation importance\")\n",
    "\n",
    "    def _record(fname, ftype, dw, da):\n",
    "        dw = np.asarray(dw, dtype=float)\n",
    "        da = np.asarray(da, dtype=float)\n",
    "\n",
    "        # relative deltas\n",
    "        dw_rel = dw / base_w\n",
    "        da_rel = da / base_a\n",
    "\n",
    "        # sample-weighted global (relative)\n",
    "        global_rel = (Nw * dw_rel + Na * da_rel) / max(Nw + Na, 1)\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                feature=fname,\n",
    "                type=ftype,\n",
    "                baseline_winter=float(base_w),\n",
    "                baseline_annual=float(base_a),\n",
    "                mean_delta_winter=float(dw.mean()),\n",
    "                std_delta_winter=float(dw.std(ddof=0)),\n",
    "                mean_delta_annual=float(da.mean()),\n",
    "                std_delta_annual=float(da.std(ddof=0)),\n",
    "                mean_delta_winter_rel=float(dw_rel.mean()),\n",
    "                std_delta_winter_rel=float(dw_rel.std(ddof=0)),\n",
    "                mean_delta_annual_rel=float(da_rel.mean()),\n",
    "                std_delta_annual_rel=float(da_rel.std(ddof=0)),\n",
    "                mean_delta_global_rel=float(global_rel.mean()),\n",
    "                std_delta_global_rel=float(global_rel.std(ddof=0)),\n",
    "            ))\n",
    "\n",
    "    # ---------- Monthly features ----------\n",
    "    for k, fname in enumerate(monthly_cols):\n",
    "        dw, da = [], []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            perm = rng.permutation(len(ds_test))\n",
    "            ds_test.Xm[:, :, k] = Xm0[perm, :, k]\n",
    "\n",
    "            dl = torch.utils.data.DataLoader(ds_test,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "            w, a = eval_rmse_pfi(model, device, dl, ds_test)\n",
    "\n",
    "            dw.append(w - base_w)\n",
    "            da.append(a - base_a)\n",
    "            pbar.update(1)\n",
    "\n",
    "        _record(fname, \"monthly\", dw, da)\n",
    "        ds_test.Xm[:] = Xm0  # restore\n",
    "\n",
    "    # ---------- Static features ----------\n",
    "    for j, fname in enumerate(static_cols):\n",
    "        dw, da = [], []\n",
    "\n",
    "        for _ in range(n_repeats):\n",
    "            perm = rng.permutation(len(ds_test))\n",
    "            ds_test.Xs[:, j] = Xs0[perm, j]\n",
    "\n",
    "            dl = torch.utils.data.DataLoader(ds_test,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "            w, a = eval_rmse_pfi(model, device, dl, ds_test)\n",
    "\n",
    "            dw.append(w - base_w)\n",
    "            da.append(a - base_a)\n",
    "            pbar.update(1)\n",
    "\n",
    "        _record(fname, \"static\", dw, da)\n",
    "        ds_test.Xs[:] = Xs0  # restore\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # Useful default sorting: by global_rel, then annual_rel, then winter_rel\n",
    "    out = out.sort_values(\n",
    "        [\n",
    "            \"mean_delta_global_rel\", \"mean_delta_annual_rel\",\n",
    "            \"mean_delta_winter_rel\"\n",
    "        ],\n",
    "        ascending=False,\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pfi_annual(df):\n",
    "    d = df.sort_values(\"mean_delta_annual\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, max(3, 0.35 * len(d))))\n",
    "    plt.barh(d[\"feature\"], d[\"mean_delta_annual\"], xerr=d[\"std_delta_annual\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\n",
    "        f\"Permutation Importance – Annual (baseline RMSE={d.baseline_annual.iloc[0]:.3f})\"\n",
    "    )\n",
    "    plt.xlabel(\"Increase in RMSE_annual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pfi_winter(df):\n",
    "    d = df.sort_values(\"mean_delta_winter\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, max(3, 0.35 * len(d))))\n",
    "    plt.barh(d[\"feature\"], d[\"mean_delta_winter\"], xerr=d[\"std_delta_winter\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\n",
    "        f\"Permutation Importance – Winter (baseline RMSE={d.baseline_winter.iloc[0]:.3f})\"\n",
    "    )\n",
    "    plt.xlabel(\"Increase in RMSE_winter\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pfi_combined(df):\n",
    "    d = df.sort_values(\"mean_delta_combined\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(8, max(3, 0.35 * len(d))))\n",
    "    plt.barh(d[\"feature\"],\n",
    "             d[\"mean_delta_combined\"],\n",
    "             xerr=d[\"std_delta_combined\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Permutation Importance – Combined (winter + annual)\")\n",
    "    plt.xlabel(\"Weighted increase in RMSE\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pfi_full = permutation_importance_LSTM_MB_full(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    ds_test=ds_test_copy,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    n_repeats=8,\n",
    "    seed=cfg.seed,\n",
    "    w_winter=1.0,\n",
    "    w_annual=1.0,\n",
    "    batch_size=128,\n",
    ")\n",
    "plot_pfi_annual(pfi_full)\n",
    "plot_pfi_winter(pfi_full)\n",
    "plot_pfi_combined(pfi_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train = data_monthly[data_monthly.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly_train[data_monthly_train.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train['y'] = data_train['POINT_BALANCE']\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train_Aug_.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train_Aug_ = data_monthly_train_Aug_[data_monthly_train_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train_Aug_))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train_Aug_['y'] = data_train_Aug_['POINT_BALANCE']\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "ds_train = build_combined_LSTM_dataset(df_loss=data_train,\n",
    "                                       df_full=data_train_Aug_,\n",
    "                                       monthly_cols=MONTHLY_COLS,\n",
    "                                       static_cols=STATIC_COLS,\n",
    "                                       months_head_pad=months_head_pad_Aug_,\n",
    "                                       months_tail_pad=months_tail_pad_Aug_,\n",
    "                                       normalize_target=False,\n",
    "                                       expect_target=True)\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, PARAMS_LSTM_IS_past,\n",
    "                                                   device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(PARAMS_LSTM_IS_past)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "state = torch.load(LSTM_IS_ORIGIN_Y_PAST, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_rmse_pfi(model, device, dl, ds):\n",
    "    model.eval()\n",
    "    y_true_w, y_pred_w = [], []\n",
    "    y_true_a, y_pred_a = [], []\n",
    "\n",
    "    all_keys = ds.keys\n",
    "    i = 0\n",
    "    y_std = ds.y_std.to(device)\n",
    "    y_mean = ds.y_mean.to(device)\n",
    "\n",
    "    for batch in dl:\n",
    "        bs = batch[\"x_m\"].shape[0]\n",
    "        batch_keys = all_keys[i:i + bs]\n",
    "        i += bs\n",
    "\n",
    "        x_m = batch[\"x_m\"].to(device)\n",
    "        x_s = batch[\"x_s\"].to(device)\n",
    "        mv = batch[\"mv\"].to(device)\n",
    "        mw = batch[\"mw\"].to(device)\n",
    "        ma = batch[\"ma\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        _, y_w, y_a = model(x_m, x_s, mv, mw, ma)\n",
    "\n",
    "        y_true = y * y_std + y_mean\n",
    "        y_w = y_w * y_std + y_mean\n",
    "        y_a = y_a * y_std + y_mean\n",
    "\n",
    "        for j in range(bs):\n",
    "            *_, per = batch_keys[j]\n",
    "            if per == \"winter\":\n",
    "                y_true_w.append(float(y_true[j].cpu()))\n",
    "                y_pred_w.append(float(y_w[j].cpu()))\n",
    "            else:\n",
    "                y_true_a.append(float(y_true[j].cpu()))\n",
    "                y_pred_a.append(float(y_a[j].cpu()))\n",
    "\n",
    "    def rmse(t, p):\n",
    "        if len(t) == 0:\n",
    "            return float(\"nan\")\n",
    "        return float(np.sqrt(np.mean((np.asarray(p) - np.asarray(t))**2)))\n",
    "\n",
    "    return rmse(y_true_w, y_pred_w), rmse(y_true_a, y_pred_a)\n",
    "\n",
    "\n",
    "def _pfi_month_worker(task, model, device, ds_test, base_w, base_a, n_repeats,\n",
    "                      seed):\n",
    "    kind, k, t = task\n",
    "    rng = np.random.default_rng(seed + k * 1000 + t)\n",
    "\n",
    "    Xm0 = ds_test.Xm.clone()\n",
    "    Xs0 = ds_test.Xs.clone()\n",
    "\n",
    "    dw, da = [], []\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        perm = rng.permutation(len(ds_test))\n",
    "\n",
    "        if kind == \"monthly\":\n",
    "            ds_test.Xm[:, t, k] = Xm0[perm, t, k]\n",
    "        else:  # static\n",
    "            ds_test.Xs[:, k] = Xs0[perm, k]\n",
    "\n",
    "        dl = torch.utils.data.DataLoader(ds_test,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)\n",
    "        w, a = eval_rmse_pfi(model, device, dl, ds_test)\n",
    "\n",
    "        dw.append(w - base_w)\n",
    "        da.append(a - base_a)\n",
    "\n",
    "    return kind, k, t, np.mean(dw), np.std(dw), np.mean(da), np.std(da)\n",
    "\n",
    "\n",
    "def permutation_importance_LSTM_MB_monthly_parallel(\n",
    "    model,\n",
    "    device,\n",
    "    ds_test,\n",
    "    monthly_cols,\n",
    "    static_cols,\n",
    "    month_names,\n",
    "    n_repeats=5,\n",
    "    n_jobs=12,\n",
    "    seed=0,\n",
    "):\n",
    "\n",
    "    print(\"\\n▶️ Running monthly permutation feature importance (PFI)...\")\n",
    "\n",
    "    base_dl = torch.utils.data.DataLoader(ds_test,\n",
    "                                          batch_size=128,\n",
    "                                          shuffle=False)\n",
    "    base_w, base_a = eval_rmse_pfi(model, device, base_dl, ds_test)\n",
    "\n",
    "    print(f\"[Baseline RMSE] winter={base_w:.3f} | annual={base_a:.3f}\")\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Monthly\n",
    "    for k in range(len(monthly_cols)):\n",
    "        for t in range(len(month_names)):\n",
    "            if ds_test.mv[:, t].sum() > 0:\n",
    "                tasks.append((\"monthly\", k, t))\n",
    "\n",
    "    # Static per month\n",
    "    for j in range(len(static_cols)):\n",
    "        for t in range(len(month_names)):\n",
    "            if ds_test.mv[:, t].sum() > 0:\n",
    "                tasks.append((\"static\", j, t))\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(_pfi_month_worker)(\n",
    "        task,\n",
    "        model,\n",
    "        device,\n",
    "        mbm.data_processing.MBSequenceDataset._clone_for_permutation(ds_test),\n",
    "        base_w,\n",
    "        base_a,\n",
    "        n_repeats,\n",
    "        seed,\n",
    "    ) for task in tqdm(tasks, desc=\"Monthly permutation importance\"))\n",
    "\n",
    "    rows = []\n",
    "    for kind, k, t, mw, sw, ma, sa in results:\n",
    "\n",
    "        fname = monthly_cols[k] if kind == \"monthly\" else static_cols[k]\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                feature=fname,\n",
    "                month=month_names[t],\n",
    "                mean_delta_winter=mw,\n",
    "                std_delta_winter=sw,\n",
    "                mean_delta_annual=ma,\n",
    "                std_delta_annual=sa,\n",
    "                mean_delta_winter_rel=mw /\n",
    "                base_w if np.isfinite(mw) else np.nan,\n",
    "                mean_delta_annual_rel=ma /\n",
    "                base_a if np.isfinite(ma) else np.nan,\n",
    "                baseline_winter=base_w,\n",
    "                baseline_annual=base_a,\n",
    "            ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ----- FIX GLOBAL RELATIVE IMPORTANCE -----\n",
    "    Nw = int(ds_test.iw.sum())\n",
    "    Na = int(ds_test.ia.sum())\n",
    "\n",
    "    def compute_global(row):\n",
    "        parts, weights = [], []\n",
    "        if np.isfinite(row.mean_delta_winter_rel):\n",
    "            parts.append(row.mean_delta_winter_rel)\n",
    "            weights.append(Nw)\n",
    "        if np.isfinite(row.mean_delta_annual_rel):\n",
    "            parts.append(row.mean_delta_annual_rel)\n",
    "            weights.append(Na)\n",
    "        if len(parts) == 0:\n",
    "            return np.nan\n",
    "        return np.average(parts, weights=weights)\n",
    "\n",
    "    df[\"mean_delta_global_rel\"] = df.apply(compute_global, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def permutation_importance_LSTM_MB_monthly_parallel_absolute(\n",
    "    model,\n",
    "    device,\n",
    "    ds_test,\n",
    "    monthly_cols,\n",
    "    static_cols,\n",
    "    month_names,\n",
    "    n_repeats=5,\n",
    "    n_jobs=12,\n",
    "    seed=0,\n",
    "):\n",
    "\n",
    "    print(\"\\n▶️ Running monthly permutation feature importance (PFI – absolute ΔRMSE)...\")\n",
    "\n",
    "    base_dl = torch.utils.data.DataLoader(ds_test, batch_size=128, shuffle=False)\n",
    "    base_w, base_a = eval_rmse_pfi(model, device, base_dl, ds_test)\n",
    "\n",
    "    print(f\"[Baseline RMSE] winter={base_w:.3f} | annual={base_a:.3f}\")\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    # Monthly\n",
    "    for k in range(len(monthly_cols)):\n",
    "        for t in range(len(month_names)):\n",
    "            if ds_test.mv[:, t].sum() > 0:\n",
    "                tasks.append((\"monthly\", k, t))\n",
    "\n",
    "    # Static per month\n",
    "    for j in range(len(static_cols)):\n",
    "        for t in range(len(month_names)):\n",
    "            if ds_test.mv[:, t].sum() > 0:\n",
    "                tasks.append((\"static\", j, t))\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_pfi_month_worker)(\n",
    "            task,\n",
    "            model,\n",
    "            device,\n",
    "            mbm.data_processing.MBSequenceDataset._clone_for_permutation(ds_test),\n",
    "            base_w,\n",
    "            base_a,\n",
    "            n_repeats,\n",
    "            seed,\n",
    "        )\n",
    "        for task in tqdm(tasks, desc=\"Monthly permutation importance (absolute)\")\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for kind, k, t, mw, sw, ma, sa in results:\n",
    "        fname = monthly_cols[k] if kind == \"monthly\" else static_cols[k]\n",
    "\n",
    "        rows.append(dict(\n",
    "            feature=fname,\n",
    "            month=month_names[t],\n",
    "            mean_delta_winter=mw,\n",
    "            std_delta_winter=sw,\n",
    "            mean_delta_annual=ma,\n",
    "            std_delta_annual=sa,\n",
    "            baseline_winter=base_w,\n",
    "            baseline_annual=base_a,\n",
    "        ))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ----- absolute sample-weighted global ΔRMSE -----\n",
    "    Nw = int(ds_test.iw.sum())\n",
    "    Na = int(ds_test.ia.sum())\n",
    "\n",
    "    def compute_global_abs(row):\n",
    "        parts, weights = [], []\n",
    "        if np.isfinite(row.mean_delta_winter):\n",
    "            parts.append(row.mean_delta_winter)\n",
    "            weights.append(Nw)\n",
    "        if np.isfinite(row.mean_delta_annual):\n",
    "            parts.append(row.mean_delta_annual)\n",
    "            weights.append(Na)\n",
    "        if len(parts) == 0:\n",
    "            return np.nan\n",
    "        return np.average(parts, weights=weights)\n",
    "\n",
    "    df[\"mean_delta_global\"] = df.apply(compute_global_abs, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_names = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "    \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "\n",
    "RUN_PFI_MONTHLY = False\n",
    "if RUN_PFI_MONTHLY:\n",
    "\n",
    "    pfi_monthly = permutation_importance_LSTM_MB_monthly_parallel_absolute(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        ds_test=ds_test_copy,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        month_names=month_names,\n",
    "        n_repeats=8,\n",
    "        n_jobs=12,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "\n",
    "    # save to cache\n",
    "    pfi_monthly.to_csv('cache/pfi_LSTM_IS_monthly_absolute.csv', index=False)\n",
    "\n",
    "else:\n",
    "    pfi_monthly = pd.read_csv('cache/pfi_LSTM_IS_monthly_absolute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def plot_monthly_pfi_ridges(\n",
    "    pfi_monthly,\n",
    "    MONTHLY_COLS,\n",
    "    vois_climate_long_name,\n",
    "    months_tail_pad,\n",
    "    months_head_pad,\n",
    "    metric=\"global\",          # \"winter\", \"annual\", \"global\"\n",
    "    importance=\"relative\",    # \"relative\" or \"absolute\"  <<< NEW\n",
    "    drop_padded_months=True,\n",
    "    fname=None,\n",
    "    title=None,\n",
    "):\n",
    "\n",
    "    if importance == \"relative\":\n",
    "        if metric == \"winter\":\n",
    "            value_col = \"mean_delta_winter_rel\"\n",
    "            label = \"Relative ΔWinter RMSE\"\n",
    "        elif metric == \"annual\":\n",
    "            value_col = \"mean_delta_annual_rel\"\n",
    "            label = \"Relative ΔAnnual RMSE\"\n",
    "        else:\n",
    "            value_col = \"mean_delta_global_rel\"\n",
    "            label = \"Relative ΔGlobal RMSE\"\n",
    "        annot_fmt = \"ΔRMSE_rel={:.3f}\"\n",
    "    else:  # absolute\n",
    "        if metric == \"winter\":\n",
    "            value_col = \"mean_delta_winter\"\n",
    "            label = \"ΔWinter RMSE\"\n",
    "        elif metric == \"annual\":\n",
    "            value_col = \"mean_delta_annual\"\n",
    "            label = \"ΔAnnual RMSE\"\n",
    "        else:\n",
    "            value_col = \"mean_delta_global\"\n",
    "            label = \"ΔGlobal RMSE\"\n",
    "        annot_fmt = \"ΔRMSE={:.3f}\"\n",
    "\n",
    "    full_month_order = [\n",
    "        \"aug_\",\"sep_\",\"oct\",\"nov\",\"dec\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\n",
    "        \"jun\",\"jul\",\"aug\",\"sep\",\"oct_\"\n",
    "    ]\n",
    "\n",
    "    df = pfi_monthly.copy()\n",
    "    df = df[df.feature.isin(MONTHLY_COLS)]\n",
    "    df[\"feature_long\"] = df[\"feature\"].apply(lambda x: vois_climate_long_name.get(x, x))\n",
    "\n",
    "    if drop_padded_months:\n",
    "        padded = np.concatenate([months_tail_pad, months_head_pad])\n",
    "        df = df[~df.month.isin(padded)]\n",
    "        month_order = [m for m in full_month_order if m not in padded]\n",
    "    else:\n",
    "        month_order = [\n",
    "        \"sep_\",\"oct\",\"nov\",\"dec\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\n",
    "        \"jun\",\"jul\",\"aug\",\"sep\",\"oct_\"\n",
    "    ]\n",
    "\n",
    "    df = df.groupby([\"feature_long\", \"month\"], as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    all_idx = pd.MultiIndex.from_product(\n",
    "        [df.feature_long.unique(), month_order],\n",
    "        names=[\"feature_long\", \"month\"]\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df.set_index([\"feature_long\", \"month\"])\n",
    "          .reindex(all_idx)\n",
    "          .fillna(0.0)\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    piv = df.pivot(index=\"feature_long\", columns=\"month\", values=value_col)[month_order]\n",
    "\n",
    "    feat_order = piv.mean(axis=1).sort_values(ascending=True).index\n",
    "    piv = piv.loc[feat_order]\n",
    "\n",
    "    piv_smooth = pd.DataFrame(\n",
    "        np.vstack([gaussian_filter1d(piv.loc[f], sigma=1) for f in feat_order]),\n",
    "        index=feat_order,\n",
    "        columns=piv.columns,\n",
    "    )\n",
    "\n",
    "    if metric == \"winter\":\n",
    "        winter_months = [\"aug_\",\"sep_\",\"oct\",\"nov\",\"dec\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\"]\n",
    "        invalid = [m for m in piv_smooth.columns if m not in winter_months]\n",
    "        piv_smooth[invalid] = 0.0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    palette = sns.color_palette(\"magma\", n_colors=len(feat_order))\n",
    "    month_idx = np.arange(len(piv_smooth.columns))\n",
    "\n",
    "    offset_step = np.nanmax(piv_smooth.values) * 0.7\n",
    "    current_offset = 0.0\n",
    "    max_importance = piv.max(axis=1)\n",
    "\n",
    "    for feat, color in zip(feat_order, palette):\n",
    "        y = piv_smooth.loc[feat].values\n",
    "\n",
    "        ax.plot(month_idx, y + current_offset, color=color, lw=2)\n",
    "        ax.fill_between(month_idx, current_offset, y + current_offset, color=color, alpha=0.4)\n",
    "\n",
    "        ax.text(-0.6, current_offset, feat, va=\"center\", ha=\"right\", fontsize=13)\n",
    "\n",
    "        max_idx = np.argmax(y)\n",
    "        ax.text(\n",
    "            month_idx[max_idx],\n",
    "            y[max_idx] + current_offset + 0.05 * offset_step,\n",
    "            annot_fmt.format(max_importance[feat]),\n",
    "            ha=\"center\", va=\"bottom\", fontsize=11,\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"none\", pad=1.2)\n",
    "        )\n",
    "\n",
    "        current_offset += offset_step\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(0, len(month_idx) - 1)\n",
    "    ax.set_xticks(month_idx)\n",
    "    ax.set_xticklabels([m.strip(\"_\").capitalize() for m in piv_smooth.columns], rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_title(title or f\"Monthly Permutation Feature Importance – {label}\")\n",
    "\n",
    "    for spine in [\"top\", \"right\", \"left\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if fname:\n",
    "        fig.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = np.concatenate([months_tail_pad, months_head_pad])\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_plot = ['t2m',\n",
    " 'tp',\n",
    " 'slhf',\n",
    " 'sshf',\n",
    " 'ssrd',\n",
    " 'fal',\n",
    " 'str',\n",
    " 'pcsr',]\n",
    "plot_monthly_pfi_ridges(\n",
    "    pfi_monthly,\n",
    "    vars_to_plot,\n",
    "    vois_climate_long_name,\n",
    "    months_tail_pad,\n",
    "    months_head_pad,\n",
    "    metric=\"winter\",\n",
    "    drop_padded_months=True,\n",
    "    importance=\"absolute\",\n",
    "    fname=\"figures/paper/CH_LSTM_monthly_PFI_winter.png\",\n",
    "    title=\"Monthly Permutation Feature Importance – Winter\")\n",
    "\n",
    "plot_monthly_pfi_ridges(\n",
    "    pfi_monthly,\n",
    "    vars_to_plot,\n",
    "    vois_climate_long_name,\n",
    "    months_tail_pad,\n",
    "    months_head_pad,\n",
    "    metric=\"annual\",\n",
    "    drop_padded_months=True,\n",
    "    importance=\"absolute\",\n",
    "    fname=\"figures/paper/CH_LSTM_monthly_PFI_annual.png\",\n",
    "    title=\"Monthly Permutation Feature Importance – Annual\")\n",
    "\n",
    "plot_monthly_pfi_ridges(\n",
    "    pfi_monthly,\n",
    "    vars_to_plot,\n",
    "    vois_climate_long_name,\n",
    "    months_tail_pad,\n",
    "    months_head_pad,\n",
    "    metric=\"global\",\n",
    "    drop_padded_months=True,\n",
    "    importance=\"absolute\",\n",
    "    fname=\"figures/paper/fig9_CH_LSTM_monthly_PFI_global.png\",\n",
    "    title=\"Monthly Permutation Feature Importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # --- Month order ---\n",
    "# month_order = [\n",
    "#     \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "#     \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "# ]\n",
    "# # pfi_monthly = pfi_monthly[pfi_monthly.feature.isin(MONTHLY_COLS)]\n",
    "\n",
    "# # --- Map features to long names ---\n",
    "# pfi_monthly[\"feature_long\"] = pfi_monthly[\"feature\"].apply(\n",
    "#     lambda x: vois_climate_long_name.get(x, x))\n",
    "# pfi_monthly = pfi_monthly[~pfi_monthly.month.isin(\n",
    "#     np.concatenate([months_tail_pad, months_head_pad]))]\n",
    "\n",
    "# # --- Prepare pivot table for global ΔRMSE ---\n",
    "# piv_global = pfi_monthly.pivot(index=\"feature_long\",\n",
    "#                                columns=\"month\",\n",
    "#                                values=\"mean_delta_global\")\n",
    "\n",
    "# # --- Reorder columns (months) ---\n",
    "# piv_global = piv_global[[m for m in month_order if m in piv_global.columns]]\n",
    "\n",
    "# # --- Order features by average global importance (optional, makes it clean) ---\n",
    "# feat_order = (pfi_monthly.groupby(\"feature_long\")\n",
    "#               [\"mean_delta_global\"].mean().sort_values(ascending=False).index)\n",
    "# piv_global = piv_global.loc[feat_order]\n",
    "\n",
    "# # --- Plot single heatmap ---\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(piv_global,\n",
    "#             cmap=\"magma\",\n",
    "#             linewidths=0.3,\n",
    "#             cbar_kws={\"label\": \"ΔRMSE (global)\"})\n",
    "# plt.xlabel(\"Month\")\n",
    "# plt.ylabel(\"Feature\")\n",
    "# plt.title(\"Monthly Permutation Feature Importance – Global RMSE Δ\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
