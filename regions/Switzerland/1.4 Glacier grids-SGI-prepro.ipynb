{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from SGI or GLAMOS:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the SGI grid and use OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "import rasterio \n",
    "import rioxarray\n",
    "# scripts\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.geodata import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.config_CH import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "voi_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_glamos_dem = os.listdir(\n",
    "    os.path.join(cfg.dataPath, path_GLAMOS_topo, 'lv95/'))\n",
    "\n",
    "# Glacier outlines:\n",
    "glacier_outline_sgi = gpd.read_file(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'inventory_sgi2016_r2020',\n",
    "                 'SGI_2016_glaciers_copy.shp'))  # Load the shapefile\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "periods_per_glacier['silvretta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLAMOS grids:\n",
    "\n",
    "For the geodetic MB and gridded MB products computed by GLAMOS, they did not use the SGI grids (from 2015) but their own yearly DEMs. They're not available for all years, but we still compute monthly grids for these available glaciers and years, in order to make the comparison with geodetic MB fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "#     cfg,\n",
    "#     rgi_region=\"11\",\n",
    "#     rgi_version=\"62\",\n",
    "#     base_url=\n",
    "#     \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "#     log_level='WARNING',\n",
    "#     task_list=None,\n",
    "# )\n",
    "# df_missing = export_oggm_grids(cfg, gdirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of one glacier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'gietro'\n",
    "sgi_id, rgi_id, rgi_shp = get_rgi_sgi_ids(cfg, glacier_name)\n",
    "\n",
    "folder_path = os.path.join(cfg.dataPath, path_GLAMOS_topo, 'lv95',\n",
    "                           glacier_name)\n",
    "\n",
    "# Example file\n",
    "fileName = 'gl_2023_lv95.grid'\n",
    "metadata, grid_data = load_grid_file(folder_path + '/' + fileName)\n",
    "\n",
    "# Convert to xarray\n",
    "dem_y = convert_to_xarray_geodata(grid_data, metadata)\n",
    "\n",
    "# Transform the coordinates to WGS84\n",
    "dem_wgs84_y = transform_xarray_coords_lv95_to_wgs84(dem_y)\n",
    "\n",
    "# Create a mask where 'elevation' is not NaN (1 if not NaN, 0 if NaN)\n",
    "ds_gl = xr.Dataset({'dem': dem_wgs84_y})\n",
    "ds_gl[\"glacier_mask\"] = ds_gl[\"dem\"].notnull().astype(np.uint8)\n",
    "\n",
    "dx = abs(ds_gl.x[1] - ds_gl.x[0]).values\n",
    "dy = abs(ds_gl.y[1] - ds_gl.y[0]).values\n",
    "print(f\"Cell size of GLAMOS DEM: {dx} x {dy} meters\")\n",
    "\n",
    "#Â Extract SGI topo and aspect over GLAMOS DEM\n",
    "ds = xr_GLAMOS_masked_topo(cfg, sgi_id, ds_gl)\n",
    "\n",
    "# Coarson to 50 m resolution if needed\n",
    "ds = coarsenDS(ds)\n",
    "dx_m, dy_m = get_res_from_degrees(ds)\n",
    "print(f\"Coarsened ds resolution: {dx_m} x {dy_m} meters\")\n",
    "\n",
    "# Plot the masked data\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geotifs of DEMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    glaciers_glamos_dems = os.listdir(\n",
    "        os.path.join(cfg.dataPath, path_GLAMOS_topo, 'lv95'))\n",
    "\n",
    "    path_out_tiff = os.path.join(cfg.dataPath,\n",
    "                                 \"GLAMOS/topo/GLAMOS_DEM/DEMs_geotiff_lv95/\")\n",
    "    os.makedirs(path_out_tiff, exist_ok=True)\n",
    "    emptyfolder(path_out_tiff)\n",
    "\n",
    "    for glacier_name in tqdm(glaciers_glamos_dems, desc=\"Processing glaciers\"):\n",
    "\n",
    "        sgi_id, rgi_id, rgi_shp = get_rgi_sgi_ids(cfg, glacier_name)\n",
    "\n",
    "        folder_path = os.path.join(\n",
    "            cfg.dataPath, path_GLAMOS_topo, 'lv95',\n",
    "            'stanna' if glacier_name == 'sanktanna' else glacier_name)\n",
    "\n",
    "        # Regular expression to extract years from filenames\n",
    "        pattern = re.compile(r'gl_(\\d{4})_lv95\\.grid')\n",
    "\n",
    "        # Extract available years from filenames\n",
    "        years = sorted({\n",
    "            int(match.group(1))\n",
    "            for filename in os.listdir(folder_path)\n",
    "            if (match := pattern.match(filename))\n",
    "        })\n",
    "        for i, year in enumerate(years):\n",
    "            file_name = f'gl_{year}_lv95.grid'\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load grid file\n",
    "            metadata, grid_data = load_grid_file(file_path)\n",
    "\n",
    "            # Convert to xarray\n",
    "            masked_dem = convert_to_xarray_geodata(grid_data, metadata)\n",
    "\n",
    "            # --- Attach CRS and write GeoTIFF ---\n",
    "            masked_dem = masked_dem.rio.write_crs(\"EPSG:2056\", inplace=True)\n",
    "\n",
    "            # Prepare output folder\n",
    "            out_tif = os.path.join(path_out_tiff, f\"{glacier_name}_{year}.tif\")\n",
    "            masked_dem.rio.to_raster(\n",
    "                out_tif,\n",
    "                dtype=\"float32\",\n",
    "                compress=\"LZW\",\n",
    "                BIGTIFF=\"IF_SAFER\",\n",
    "                tiled=True,\n",
    "                predictor=3,  # better compression for float rasters\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly masked grids - xarrays:\n",
    "Save a .zarr xarray per glacier per year (not in monthly format) needed in the MBM later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- pull any non-picklable cfg bits NOW into plain strings/ints ----\n",
    "DATA_ROOT = cfg.dataPath  # assume this is just a string\n",
    "PATH_GLAMOS_TOPO = path_GLAMOS_topo  # e.g. \"GLAMOS/topo/...\"\n",
    "PATH_XR_SVF = os.path.join(DATA_ROOT, \"GLAMOS/topo/GLAMOS_DEM\",\n",
    "                           \"svf_nc_latlon\")\n",
    "PATH_XR_GRIDS = os.path.join(DATA_ROOT, PATH_GLAMOS_TOPO, \"xr_masked_grids\")\n",
    "\n",
    "\n",
    "def _open_and_merge_svf(ds_latlon: xr.Dataset, glacier_name: str,\n",
    "                        year: int) -> xr.Dataset:\n",
    "    \"\"\"Open SVF file, normalize coords, and merge/interp onto ds_latlon grid.\"\"\"\n",
    "    svf_path = os.path.join(PATH_XR_SVF,\n",
    "                            f\"{glacier_name}_{year}_svf_latlon.nc\")\n",
    "    if not os.path.exists(svf_path):\n",
    "        print(f\"SVF not found: {svf_path}\")\n",
    "        return ds_latlon\n",
    "\n",
    "    with xr.open_dataset(svf_path, decode_cf=True) as ds_svf_raw:\n",
    "        ds_svf = ds_svf_raw\n",
    "\n",
    "        # normalize coordinate names\n",
    "        ren = {}\n",
    "        if \"longitude\" in ds_svf.coords and \"lon\" not in ds_svf.coords:\n",
    "            ren[\"longitude\"] = \"lon\"\n",
    "        if \"latitude\" in ds_svf.coords and \"lat\" not in ds_svf.coords:\n",
    "            ren[\"latitude\"] = \"lat\"\n",
    "        if ren:\n",
    "            ds_svf = ds_svf.rename(ren)\n",
    "\n",
    "        if not ({\"lon\", \"lat\"} <= set(ds_svf.coords)):\n",
    "            print(f\"SVF lacks lon/lat: {svf_path}\")\n",
    "            return ds_latlon\n",
    "\n",
    "        # longitude range normalization (0â360 -> -180â180) if needed\n",
    "        if float(ds_svf.lon.max()) > 180 and float(ds_latlon.lon.min()) < 0:\n",
    "            ds_svf = ds_svf.assign_coords(lon=((ds_svf.lon + 180) % 360) - 180)\n",
    "\n",
    "        # sort ascending for interp stability\n",
    "        if ds_svf.lon[0] > ds_svf.lon[-1]:\n",
    "            ds_svf = ds_svf.sortby(\"lon\")\n",
    "        if ds_svf.lat[0] > ds_svf.lat[-1]:\n",
    "            ds_svf = ds_svf.sortby(\"lat\")\n",
    "        if ds_latlon.lon[0] > ds_latlon.lon[-1]:\n",
    "            ds_latlon = ds_latlon.sortby(\"lon\")\n",
    "        if ds_latlon.lat[0] > ds_latlon.lat[-1]:\n",
    "            ds_latlon = ds_latlon.sortby(\"lat\")\n",
    "\n",
    "        svf_vars = [\n",
    "            v for v in [\"svf\", \"asvf\", \"opns\"] if v in ds_svf.data_vars\n",
    "        ]\n",
    "        if not svf_vars:\n",
    "            print(f\"No SVF vars in {svf_path}\")\n",
    "            return ds_latlon\n",
    "\n",
    "        same_lon = np.array_equal(ds_latlon.lon.values, ds_svf.lon.values)\n",
    "        same_lat = np.array_equal(ds_latlon.lat.values, ds_svf.lat.values)\n",
    "\n",
    "        if same_lon and same_lat:\n",
    "            merged = xr.merge([ds_latlon, ds_svf[svf_vars]])\n",
    "        else:\n",
    "            svf_on_grid = ds_svf[svf_vars].interp(lon=ds_latlon.lon,\n",
    "                                                  lat=ds_latlon.lat,\n",
    "                                                  method=\"linear\")\n",
    "            for v in svf_vars:\n",
    "                svf_on_grid[v] = svf_on_grid[v].astype(\"float32\")\n",
    "            merged = ds_latlon.assign(**{v: svf_on_grid[v] for v in svf_vars})\n",
    "\n",
    "        # add masked versions\n",
    "        if \"glacier_mask\" in merged:\n",
    "            gmask = xr.where(merged[\"glacier_mask\"] == 1, 1.0,\n",
    "                             np.nan).astype(\"float32\")\n",
    "            for v in svf_vars:\n",
    "                merged[f\"masked_{v}\"] = (gmask * merged[v]).astype(\"float32\")\n",
    "        return merged\n",
    "\n",
    "\n",
    "def process_glacier_year(glacier_name: str,\n",
    "                         year: int,\n",
    "                         make_plot: bool = True) -> str:\n",
    "    \"\"\"Process one (glacier_name, year) pair. Returns the output zarr path or a warning string.\"\"\"\n",
    "    try:\n",
    "        # resolve SGI/RGI\n",
    "        sgi_id, rgi_id, rgi_shp = get_rgi_sgi_ids(cfg, glacier_name)\n",
    "        if not sgi_id or not rgi_shp:\n",
    "            return f\"SKIP {glacier_name} {year}: missing SGI or shapefile.\"\n",
    "\n",
    "        # folder with .grid files\n",
    "        folder_path = os.path.join(\n",
    "            DATA_ROOT, PATH_GLAMOS_TOPO, 'lv95',\n",
    "            'stanna' if glacier_name == 'sanktanna' else glacier_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            return f\"SKIP {glacier_name} {year}: folder missing.\"\n",
    "\n",
    "        if year < 1951:\n",
    "            return f\"SKIP {glacier_name} {year}: <1951.\"\n",
    "\n",
    "        file_name = f'gl_{year}_lv95.grid'\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"SKIP {glacier_name} {year}: grid file missing.\"\n",
    "\n",
    "        # load grid â xarray â lat/lon\n",
    "        metadata, grid_data = load_grid_file(file_path)\n",
    "        dem_y = convert_to_xarray_geodata(grid_data, metadata)\n",
    "        dem_wgs84_y = transform_xarray_coords_lv95_to_wgs84(dem_y)\n",
    "\n",
    "        ds_gl = xr.Dataset({'dem': dem_wgs84_y})\n",
    "        ds_gl[\"glacier_mask\"] = ds_gl[\"dem\"].notnull().astype(np.uint8)\n",
    "\n",
    "        # your topo enrichment (aspect/slope/etc) in WGS84\n",
    "        ds_latlon = xr_GLAMOS_masked_topo(cfg, sgi_id, ds_gl)\n",
    "\n",
    "        # # resolution and optional coarsening\n",
    "        # dx_m, dy_m = get_res_from_degrees(ds_latlon)\n",
    "        # if dx_m > 20:\n",
    "        #     ds_latlon = coarsenDS(ds_latlon, target_res_m=50)\n",
    "\n",
    "        # merge SVF\n",
    "        ds_latlon = _open_and_merge_svf(ds_latlon, glacier_name, year)\n",
    "\n",
    "        # save zarr\n",
    "        save_path = os.path.join(PATH_XR_GRIDS, f\"{glacier_name}_{year}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path, mode=\"w\", consolidated=True)\n",
    "        return f\"OK {glacier_name} {year} â {save_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR {glacier_name} {year}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    # ensure clean output folder if you want a fresh run\n",
    "    emptyfolder(PATH_XR_GRIDS)\n",
    "\n",
    "    # ---- Build task list (glacier, year) ----\n",
    "    glaciers_root = os.path.join(DATA_ROOT, PATH_GLAMOS_TOPO, 'lv95')\n",
    "    glacier_names = os.listdir(glaciers_root)\n",
    "\n",
    "    pattern = re.compile(r'gl_(\\d{4})_lv95\\.grid')\n",
    "\n",
    "    tasks = []\n",
    "    for glacier_name in glacier_names:\n",
    "        folder_path = os.path.join(\n",
    "            glaciers_root,\n",
    "            'stanna' if glacier_name == 'sanktanna' else glacier_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        years = sorted({\n",
    "            int(m.group(1))\n",
    "            for fn in os.listdir(folder_path) if (m := pattern.match(fn))\n",
    "        })\n",
    "        for year in years:\n",
    "            if year >= 1951:\n",
    "                tasks.append((glacier_name, year))\n",
    "\n",
    "    print(f\"Submitting {len(tasks)} tasks...\")\n",
    "\n",
    "    # ---- Run in parallel ----\n",
    "    max_workers = max(1, (os.cpu_count() or 4) - 1)\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [\n",
    "            ex.submit(process_glacier_year, g, y, True) for (g, y) in tasks\n",
    "        ]\n",
    "        for fut in as_completed(futs):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            print(res)\n",
    "\n",
    "    # (optional) summarize\n",
    "    n_ok = sum(r.startswith(\"OK \") for r in results)\n",
    "    n_err = sum(r.startswith(\"ERROR \") for r in results)\n",
    "    n_skip = sum(r.startswith(\"SKIP \") for r in results)\n",
    "    print(f\"Done. OK={n_ok}, SKIP={n_skip}, ERROR={n_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the masked data\n",
    "ds = xr.open_zarr(os.path.join(PATH_XR_GRIDS, 'aletsch_2016.zarr'))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.svf.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[4], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Skyview factor\")\n",
    "axs[4].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the masked data\n",
    "ds = xr.open_zarr(os.path.join(PATH_XR_GRIDS, 'gietro_2016.zarr'))\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ds.masked_aspect.plot(ax=axs[0, 0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[0, 1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[0, 2], cmap='terrain', add_colorbar=True)\n",
    "ds.svf.plot(ax=axs[1, 0], cmap='binary', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[1, 1], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0, 0].set_title(\"Aspect\")\n",
    "axs[0, 1].set_title(\"Slope\")\n",
    "axs[0, 2].set_title(\"DEM\")\n",
    "axs[1, 0].set_title(\"Skyview factor\")\n",
    "axs[1, 1].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly masked grids - dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------- pull only simple types out of cfg (picklable) -------------\n",
    "DATA_ROOT = cfg.dataPath\n",
    "PATH_XR_GRIDS = os.path.join(DATA_ROOT, path_GLAMOS_topo, 'xr_masked_grids')\n",
    "OUT_FOLDER_ROOT = os.path.join(DATA_ROOT,\n",
    "                               path_glacier_grid_glamos)  # parquet output root\n",
    "ERA5_MONTHLY = os.path.join(DATA_ROOT, path_ERA5_raw,\n",
    "                            'era5_monthly_averaged_data.nc')\n",
    "ERA5_GEOPOT = os.path.join(DATA_ROOT, path_ERA5_raw,\n",
    "                           'era5_geopotential_pressure.nc')\n",
    "PCSR_ZARR = os.path.join(DATA_ROOT, path_pcsr, 'zarr/')\n",
    "\n",
    "# If you have an RGI outlines shapefile path, use it here so workers load it once\n",
    "RGI_OUTLINES_PATH = cfg.dataPath + path_rgi_outlines  # <-- set this to your outlines file path if available\n",
    "\n",
    "VOIS_CLIMATE = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "VOIS_TOPO = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"svf\"\n",
    "]\n",
    "TOO_SMALL = set(['vorab', 'blauschnee', 'joeri'])\n",
    "ONLY_GEODETIC_YEARS = False\n",
    "\n",
    "# meta columns used later (copy from cfg.metaData)\n",
    "META_COLS = list(cfg.metaData)\n",
    "OGGM_PATH = os.path.join(DATA_ROOT, path_OGGM)\n",
    "\n",
    "# ============== worker initializer ==============\n",
    "\n",
    "_GLOBALS = {}\n",
    "\n",
    "\n",
    "def _init_worker(outlines_path):\n",
    "    \"\"\"Runs once per worker process. Load expensive, read-only resources here.\"\"\"\n",
    "    global _GLOBALS\n",
    "    _GLOBALS = {}\n",
    "    if outlines_path and os.path.exists(outlines_path):\n",
    "        import geopandas as gpd\n",
    "        _GLOBALS['RGI_OUTLINES'] = gpd.read_file(outlines_path)\n",
    "    else:\n",
    "        _GLOBALS['RGI_OUTLINES'] = None\n",
    "\n",
    "\n",
    "# ============== worker function ==============\n",
    "\n",
    "\n",
    "def _process_glacier_year(glacier_name: str, year: int, *, data_root: str,\n",
    "                          path_xr_grids: str, out_folder_root: str,\n",
    "                          vois_climate: list, vois_topo: list, meta_cols: list,\n",
    "                          era5_monthly_path: str, era5_geopot_path: str,\n",
    "                          pcsr_zarr_root: str, oggm_path: str) -> str:\n",
    "    \"\"\"Process one (glacier, year) -> writes a parquet; returns status string.\"\"\"\n",
    "    try:\n",
    "        if glacier_name in TOO_SMALL:\n",
    "            return f\"SKIP {glacier_name} {year}: too small\"\n",
    "\n",
    "        # find the Zarr file\n",
    "        file_name = f\"{glacier_name}_{year}.zarr\"\n",
    "        zarr_path = os.path.join(path_xr_grids, file_name)\n",
    "        if not os.path.exists(zarr_path):\n",
    "            return f\"SKIP {glacier_name} {year}: zarr not found\"\n",
    "\n",
    "        # open masked grid (WGS84 lat/lon)\n",
    "        ds = xr.open_zarr(zarr_path)\n",
    "\n",
    "        # resolve IDs\n",
    "        sgi_id, rgi_id, rgi_shp = get_rgi_sgi_ids(\n",
    "            cfg, glacier_name)  # uses global cfg; OK if itâs lightweight\n",
    "        if not sgi_id or not rgi_id or not rgi_shp:\n",
    "            return f\"SKIP {glacier_name} {year}: missing SGI/RGI\"\n",
    "\n",
    "        # create glacier grid (user function)\n",
    "        df_grid = create_glacier_grid_SGI(glacier_name, year, rgi_id, ds)\n",
    "        df_grid = df_grid.reset_index(drop=True)\n",
    "\n",
    "        # construct the Dataset wrapper\n",
    "        dataset_grid = mbm.data_processing.Dataset(\n",
    "            cfg=\n",
    "            cfg,  # if cfg is heavy/unpicklable, pass a lightweight surrogate instead\n",
    "            data=df_grid,\n",
    "            region_name='CH',\n",
    "            region_id=11,\n",
    "            data_path=os.path.join(data_root, path_PMB_GLAMOS_csv))\n",
    "\n",
    "        # climate\n",
    "        dataset_grid.get_climate_features(climate_data=era5_monthly_path,\n",
    "                                          geopotential_data=era5_geopot_path,\n",
    "                                          change_units=True,\n",
    "                                          smoothing_vois={\n",
    "                                              'vois_climate': vois_climate,\n",
    "                                              'vois_other':\n",
    "                                              ['ALTITUDE_CLIMATE']\n",
    "                                          })\n",
    "\n",
    "        # potential clear-sky radiation\n",
    "        dataset_grid.get_potential_rad(pcsr_zarr_root)\n",
    "\n",
    "        # get data and attach RGI ids via outlines (loaded once per worker)\n",
    "        df_y_gl = dataset_grid.data\n",
    "        df_y_gl.rename(columns={'RGIId': 'RGIId_old'}, inplace=True)\n",
    "\n",
    "        outlines = _GLOBALS.get('RGI_OUTLINES', None)\n",
    "        if outlines is None:\n",
    "            return f\"SKIP {glacier_name} {year}: outlines not loaded in worker\"\n",
    "        df_y_gl = mbm.data_processing.utils.get_rgi(data=df_y_gl,\n",
    "                                                    glacier_outlines=outlines)\n",
    "\n",
    "        # drop rows without RGI\n",
    "        df_y_gl = df_y_gl.dropna(subset=['RGIId'])\n",
    "\n",
    "        # OGGM features\n",
    "        df_y_gl = add_OGGM_features(\n",
    "            df_y_gl, [\"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\"],\n",
    "            oggm_path)\n",
    "\n",
    "        # GLWD_ID\n",
    "        df_y_gl['GLWD_ID'] = df_y_gl.apply(lambda x: mbm.data_processing.utils.\n",
    "                                           get_hash(f\"{x.GLACIER}_{x.YEAR}\"),\n",
    "                                           axis=1).astype(str)\n",
    "\n",
    "        # wrap again for monthly conversion\n",
    "        dataset_grid_oggm = mbm.data_processing.Dataset(\n",
    "            cfg=cfg,\n",
    "            data=df_y_gl,\n",
    "            region_name='CH',\n",
    "            region_id=11,\n",
    "            data_path=os.path.join(data_root, path_PMB_GLAMOS_csv))\n",
    "\n",
    "        dataset_grid_oggm.convert_to_monthly(meta_data_columns=meta_cols,\n",
    "                                             vois_climate=vois_climate +\n",
    "                                             ['pcsr'],\n",
    "                                             vois_topographical=vois_topo)\n",
    "\n",
    "        df_oggm = dataset_grid_oggm.data\n",
    "\n",
    "        expected_months = [\n",
    "            \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n",
    "            \"jul\", \"aug\", \"sep\"\n",
    "        ]\n",
    "\n",
    "        # Normalize months to lowercase (in case of mixed case)\n",
    "        months_present = df_oggm[\"MONTHS\"].astype(\n",
    "            str).str.lower().unique().tolist()\n",
    "        missing = [m for m in expected_months if m not in months_present]\n",
    "\n",
    "        if missing:\n",
    "            print(f\"â Missing months: {missing}\")\n",
    "\n",
    "        # sanity\n",
    "        if 'svf' not in df_oggm.columns:\n",
    "            return f\"ERROR {glacier_name} {year}: 'svf' missing after conversion\"\n",
    "        if 'pcsr' not in df_oggm.columns:\n",
    "            return f\"ERROR {glacier_name} {year}: 'pcsr' missing after conversion\"\n",
    "\n",
    "        # rename columns\n",
    "        df_oggm.rename(columns={\n",
    "            'aspect': 'aspect_sgi',\n",
    "            'slope': 'slope_sgi'\n",
    "        },\n",
    "                       inplace=True)\n",
    "        if 'POINT_ELEVATION' not in df_oggm.columns:\n",
    "            return f\"ERROR {glacier_name} {year}: 'POINT_ELEVATION' missing\"\n",
    "\n",
    "        # write parquet\n",
    "        out_folder = os.path.join(out_folder_root, glacier_name)\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        out_path = os.path.join(out_folder,\n",
    "                                f\"{glacier_name}_grid_{year}.parquet\")\n",
    "        df_oggm.to_parquet(out_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        return f\"OK {glacier_name} {year} -> {out_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR {glacier_name} {year}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = True\n",
    "if RUN:\n",
    "    # ============== build task list (glacier, year) ==============\n",
    "\n",
    "    # ensure output root exists and (optionally) clean it\n",
    "    os.makedirs(OUT_FOLDER_ROOT, exist_ok=True)\n",
    "    emptyfolder(OUT_FOLDER_ROOT)  # if you truly want a fresh run\n",
    "\n",
    "    # glaciers to consider\n",
    "    all_glaciers = [g for g in years_start_per_gl.keys() if g not in TOO_SMALL]\n",
    "\n",
    "    # map each glacier to missing years\n",
    "    tasks = []\n",
    "    pattern = re.compile(r'_(\\d{4})\\.zarr$')\n",
    "\n",
    "    for glacier_name in all_glaciers:\n",
    "        # zarr files present\n",
    "        zarr_files = [\n",
    "            f for f in os.listdir(PATH_XR_GRIDS)\n",
    "            if f.startswith(f\"{glacier_name}_\") and f.endswith(\".zarr\")\n",
    "        ]\n",
    "        if not zarr_files:\n",
    "            print(f\"No GLAMOS DEM for {glacier_name}, skipping.\")\n",
    "            continue\n",
    "        zarr_files.sort()\n",
    "\n",
    "        # parquet folder\n",
    "        out_folder = os.path.join(OUT_FOLDER_ROOT, glacier_name)\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "        # existing parquet years\n",
    "        existing = {\n",
    "            int(m.group(1))\n",
    "            for f in os.listdir(out_folder)\n",
    "            if (m := re.search(r'_grid_(\\d{4})\\.parquet$', f))\n",
    "        }\n",
    "\n",
    "        # geodetic period\n",
    "        if glacier_name not in years_start_per_gl or glacier_name not in years_end_per_gl:\n",
    "            print(f\"Skipping {glacier_name}: missing start/end years\")\n",
    "            continue\n",
    "        geodetic_start = years_start_per_gl[glacier_name][0]\n",
    "        geodetic_end = years_end_per_gl[glacier_name][-1]\n",
    "\n",
    "        # choose years\n",
    "        for f in zarr_files:\n",
    "            m = pattern.search(f)\n",
    "            if not m:\n",
    "                continue\n",
    "            year = int(m.group(1))\n",
    "            if year < 1951:\n",
    "                continue\n",
    "            if ONLY_GEODETIC_YEARS:\n",
    "                if (year in range(geodetic_start, geodetic_end +\n",
    "                                  1)) and (year not in existing):\n",
    "                    tasks.append((glacier_name, year))\n",
    "            else:\n",
    "                if year not in existing:\n",
    "                    tasks.append((glacier_name, year))\n",
    "\n",
    "    print(f\"Submitting {len(tasks)} tasksâ¦\")\n",
    "\n",
    "    # ============== run in parallel ==============\n",
    "\n",
    "    max_workers = max(1, (os.cpu_count() or 4) - 1)\n",
    "    results = []\n",
    "    n_ok = n_err = n_skip = 0\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers,\n",
    "                             initializer=_init_worker,\n",
    "                             initargs=(RGI_OUTLINES_PATH, )) as ex:\n",
    "        futs = [\n",
    "            ex.submit(\n",
    "                _process_glacier_year,\n",
    "                glacier_name,\n",
    "                year,\n",
    "                data_root=DATA_ROOT,\n",
    "                path_xr_grids=PATH_XR_GRIDS,\n",
    "                out_folder_root=OUT_FOLDER_ROOT,\n",
    "                vois_climate=VOIS_CLIMATE,\n",
    "                vois_topo=VOIS_TOPO,\n",
    "                meta_cols=META_COLS,\n",
    "                era5_monthly_path=ERA5_MONTHLY,\n",
    "                era5_geopot_path=ERA5_GEOPOT,\n",
    "                pcsr_zarr_root=PCSR_ZARR,\n",
    "                oggm_path=OGGM_PATH,\n",
    "            ) for glacier_name, year in tasks\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(futs), desc=\"Processing gl-years\",\n",
    "                  unit=\"task\") as pbar:\n",
    "            for fut in as_completed(futs):\n",
    "                res = fut.result()\n",
    "                results.append(res)\n",
    "                if res.startswith(\"OK \"):\n",
    "                    n_ok += 1\n",
    "                elif res.startswith(\"ERROR \"):\n",
    "                    n_err += 1\n",
    "                elif res.startswith(\"SKIP \"):\n",
    "                    n_skip += 1\n",
    "\n",
    "                pbar.set_postfix(ok=n_ok, skip=n_skip, err=n_err)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Done. OK={n_ok}, SKIP={n_skip}, ERROR={n_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "# Load GLAMOS masked grid\n",
    "glacier_name = 'aletsch'\n",
    "year = 2016\n",
    "\n",
    "month = 'sep'  # Example month, adjust as needed\n",
    "\n",
    "folder_path = os.path.join(cfg.dataPath, path_glacier_grid_glamos,\n",
    "                           glacier_name)\n",
    "# load the dataset\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(folder_path, f\"{glacier_name}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == month]\n",
    "\n",
    "stake_locs = df_stakes[df_stakes.GLACIER == glacier_name]\n",
    "\n",
    "# Variables of interest\n",
    "voi = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "]\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness', 'millan_v', 'aspect_sgi', 'slope_sgi', 'pcsr',\n",
    "    'svf'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])\n",
    "    axs[i].set_title(var)\n",
    "\n",
    "    # scatter stake location\n",
    "    sns.scatterplot(stake_locs,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    color='red',\n",
    "                    s=10,\n",
    "                    alpha=0.5,\n",
    "                    ax=axs[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "# Load GLAMOS masked grid\n",
    "glacier_name = 'gietro'\n",
    "year = 2016\n",
    "\n",
    "month = 'sep'  # Example month, adjust as needed\n",
    "\n",
    "folder_path = os.path.join(cfg.dataPath, path_glacier_grid_glamos,\n",
    "                           glacier_name)\n",
    "# load the dataset\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(folder_path, f\"{glacier_name}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == month]\n",
    "\n",
    "stake_locs = df_stakes[df_stakes.GLACIER == glacier_name]\n",
    "\n",
    "# Variables of interest\n",
    "voi = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "]\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness', 'millan_v', 'aspect_sgi', 'slope_sgi', 'pcsr',\n",
    "    'svf'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])\n",
    "    axs[i].set_title(var)\n",
    "\n",
    "    # scatter stake location\n",
    "    sns.scatterplot(stake_locs,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    color='red',\n",
    "                    s=10,\n",
    "                    alpha=0.5,\n",
    "                    ax=axs[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id = get_rgi_sgi_ids(cfg, 'gietro')[1]\n",
    "path_to_data = cfg.dataPath + path_OGGM + \"xr_grids/\"\n",
    "file_path = f\"{path_to_data}{rgi_id}.zarr\"\n",
    "ds_oggm = xr.open_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_oggm.millan_v.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_oggm.hugonnet_dhdt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    df,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    hue='millan_v',\n",
    "    s=5,\n",
    "    alpha=0.5,\n",
    "    palette='twilight_shifted',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    df,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    hue='svf',\n",
    "    s=5,\n",
    "    alpha=0.5,\n",
    "    palette='twilight_shifted',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
