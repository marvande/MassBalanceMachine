{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_annual = \"#c51b7d\"\n",
    "color_winter = colors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\"aspect_sgi\", \"slope_sgi\", \"svf\"]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_gs_no_oggm.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_gs_no_oggm_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "data_test_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    TEST_GLACIERS)]\n",
    "data_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "\n",
    "splits_Aug_, test_set_Aug_, train_set_Aug_ = get_CV_splits(\n",
    "    dataloader_gl_Aug_,\n",
    "    test_split_on='GLACIER',\n",
    "    test_splits=TEST_GLACIERS,\n",
    "    random_state=cfg.seed)\n",
    "\n",
    "# # Validation and train split:\n",
    "data_train_Aug_ = train_set_Aug_['df_X']\n",
    "data_train_Aug_['y'] = train_set_Aug_['y']\n",
    "data_test_Aug_ = test_set_Aug_['df_X']\n",
    "data_test_Aug_['y'] = test_set_Aug_['y']\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train_Aug_ = data_train_Aug_.copy()\n",
    "df_train_Aug_['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test_Aug_ = data_test_Aug_.copy()\n",
    "df_test_Aug_['PERIOD'] = df_test_Aug_['PERIOD'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_dataset(\n",
    "    df_loss,\n",
    "    df_full,\n",
    "    monthly_cols,\n",
    "    static_cols,\n",
    "    months_head_pad,\n",
    "    months_tail_pad,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "):\n",
    "    # Clean copies\n",
    "    df_loss = df_loss.copy()\n",
    "    df_full = df_full.copy()\n",
    "    df_loss[\"PERIOD\"] = df_loss[\"PERIOD\"].str.lower().str.strip()\n",
    "    df_full[\"PERIOD\"] = df_full[\"PERIOD\"].str.lower().str.strip()\n",
    "\n",
    "    # --------------------------------------\n",
    "    # STEP 1 — Remove POINT_BALANCE from df_full\n",
    "    # --------------------------------------\n",
    "    df_full_clean = df_full.drop(columns=[\"POINT_BALANCE\", \"y\"],\n",
    "                                 errors=\"ignore\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # STEP 2 — Keep only the POINT_BALANCE information from df_loss\n",
    "    # --------------------------------------\n",
    "    df_loss_reduced = df_loss[[\n",
    "        \"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\", \"MONTHS\", \"POINT_BALANCE\"\n",
    "    ]].copy()\n",
    "\n",
    "    # --------------------------------------\n",
    "    # STEP 3 — Merge\n",
    "    # padded months will have POINT_BALANCE = NaN\n",
    "    # --------------------------------------\n",
    "    df_combined = df_full_clean.merge(\n",
    "        df_loss_reduced,\n",
    "        on=[\"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\", \"MONTHS\"],\n",
    "        how=\"left\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # STEP 4 — Build dataset\n",
    "    # --------------------------------------\n",
    "    ds = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df=df_combined,\n",
    "        monthly_cols=monthly_cols,\n",
    "        static_cols=static_cols,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        expect_target=expect_target,\n",
    "        normalize_target=normalize_target,\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_full = build_combined_dataset(df_loss=data_train,\n",
    "                                       df_full=data_train_Aug_,\n",
    "                                       monthly_cols=MONTHLY_COLS,\n",
    "                                       static_cols=STATIC_COLS,\n",
    "                                       months_head_pad=months_head_pad_Aug_,\n",
    "                                       months_tail_pad=months_tail_pad_Aug_,\n",
    "                                       normalize_target=True,\n",
    "                                       expect_target=True)\n",
    "\n",
    "ds_test_full = build_combined_dataset(df_loss=data_test,\n",
    "                                      df_full=data_test_Aug_,\n",
    "                                      monthly_cols=MONTHLY_COLS,\n",
    "                                      static_cols=STATIC_COLS,\n",
    "                                      months_head_pad=months_head_pad_Aug_,\n",
    "                                      months_tail_pad=months_tail_pad_Aug_,\n",
    "                                      normalize_target=True,\n",
    "                                      expect_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_sample(ds, idx, month_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a dataset sample:\n",
    "    - Monthly climate inputs (Xm)\n",
    "    - Masks mv, mw, ma\n",
    "    - Show which months count toward the loss\n",
    "    \"\"\"\n",
    "    x_m = ds.Xm[idx].numpy()\n",
    "    mv = ds.mv[idx].numpy()\n",
    "    mw = ds.mw[idx].numpy()\n",
    "    ma = ds.ma[idx].numpy()\n",
    "    key = ds.keys[idx]\n",
    "\n",
    "    if month_labels is None:\n",
    "        # Infer from dataset order: (MONTHS => pos_map) → sorted by pos\n",
    "        month_labels = [f\"m{i}\" for i in range(x_m.shape[0])]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Month\": month_labels,\n",
    "        \"mv(valid)\": mv,\n",
    "        \"mw(winter)\": mw,\n",
    "        \"ma(annual)\": ma\n",
    "    })\n",
    "\n",
    "    print(\"=== Sample info ===\")\n",
    "    print(\"Key:\", key)\n",
    "    print(\"Target y:\", float(ds.y[idx].numpy()))\n",
    "    print()\n",
    "    print(df)\n",
    "\n",
    "    # Plot masks\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    ax.plot(mv, label=\"mv (valid inputs)\")\n",
    "    ax.plot(mw, label=\"mw (winter MB loss mask)\")\n",
    "    ax.plot(ma, label=\"ma (annual MB loss mask)\")\n",
    "    ax.set_title(\n",
    "        f\"Masks for sample {idx} (GLACIER={key[0]}, YEAR={key[1]}, PERIOD={key[3]})\"\n",
    "    )\n",
    "    ax.set_xticks(range(len(month_labels)))\n",
    "    ax.set_xticklabels(month_labels, rotation=45)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list, month_pos = mbm.data_processing.utils._rebuild_month_index(\n",
    "    months_head_pad_Aug_, months_tail_pad_Aug_)\n",
    "month_order = [m for m, _ in sorted(month_pos.items(), key=lambda x: x[1])]\n",
    "print(\"Month order used in sequences:\", month_order)\n",
    "\n",
    "inspect_sample(ds_train_full, 0, month_labels=month_order)\n",
    "inspect_sample(ds_train_full, 10, month_labels=month_order)\n",
    "inspect_sample(ds_train_full, 150, month_labels=month_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mv = 1   → the input exists (so the LSTM sees the climate features)\n",
    "# ma = 0   → the month is excluded from the annual loss\n",
    "# mw = 0   → (also excluded from winter loss)\n",
    "\n",
    "\n",
    "def inspect_padded_months(ds, n_samples=5, tol_zero=1e-6):\n",
    "    \"\"\"\n",
    "    Inspect climate inputs and masks for padded vs true months.\n",
    "    Shows: valid mask (mv), winter mask (mw), annual mask (ma),\n",
    "           mean of features, whether NaN, whether all-zero.\n",
    "    \"\"\"\n",
    "    Xm = ds.Xm.detach().cpu().numpy()  # (B, T, Fm)\n",
    "    mv = ds.mv.detach().cpu().numpy()  # (B, T)\n",
    "    mw = ds.mw.detach().cpu().numpy()  # (B, T)\n",
    "    ma = ds.ma.detach().cpu().numpy()  # (B, T)\n",
    "\n",
    "    B, T, F = Xm.shape\n",
    "    idxs = random.sample(range(B), min(n_samples, B))\n",
    "\n",
    "    print(f\"Inspecting {len(idxs)} random samples\")\n",
    "    for i in idxs:\n",
    "        print(\"\\n────────────────────────────────────\")\n",
    "        print(f\"Sample {i} — {ds.keys[i]}\")\n",
    "        print(\"MONTH | mv | mw | ma | mean(X) | has_NaN | all_zero\")\n",
    "        for t in range(T):\n",
    "            x = Xm[i, t, :]\n",
    "            mean_val = float(np.nanmean(x))\n",
    "            nan_mask = np.isnan(x).any()\n",
    "            zero_mask = np.all(np.abs(x) < tol_zero)\n",
    "            print(\n",
    "                f\"{t:2d} | {int(mv[i, t])}  | {int(mw[i, t])}  | {int(ma[i, t])}  | \"\n",
    "                f\"{mean_val:7.3f} |  {nan_mask}  |  {zero_mask}\")\n",
    "\n",
    "\n",
    "inspect_padded_months(ds_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_one_heads_param_search_progress_no_oggm_2025-11-06.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='valid_loss')\n",
    "print(best_params)\n",
    "df = pd.read_csv(log_path)\n",
    "# df = df[df['test_rmse_w'] < 0.4]\n",
    "df[\"avg_test_loss\"] = (df[\"test_rmse_a\"] + df[\"test_rmse_w\"]) / 2\n",
    "df.sort_values(by=\"valid_loss\", inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.2,\n",
    "    'head_dropout': 0.0,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'Fm': 9,\n",
    "    'Fs': 3,\n",
    "    'bidirectional': False,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': False\n",
    "}\n",
    "\n",
    "# custom_params = best_params\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_no_oggm_norm_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_full) ---\n",
    "seed_all(cfg.seed)\n",
    "ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_full)\n",
    "ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_full)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_full and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_full_copy, ds_train_full_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "seed_all(cfg.seed)\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_full_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_item(x):\n",
    "    return x.item() if x is not None else None\n",
    "\n",
    "\n",
    "print(\"Train dataset (after make_loaders):\")\n",
    "print(f\"  normalize_target = {ds_train_full_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_train_full_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_train_full_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_train_full_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_train_full_copy.y.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nTest dataset (after make_test_loader):\")\n",
    "print(f\"  normalize_target = {ds_test_full_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_test_full_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_test_full_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_test_full_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_test_full_copy.y.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = ds_train_full_copy.y.cpu().numpy()\n",
    "y_test = ds_test_full_copy.y.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(y_train, bins=30, alpha=0.6, label=\"Train\", density=True)\n",
    "plt.hist(y_test, bins=30, alpha=0.6, label=\"Test\", density=True)\n",
    "plt.axvline(y_train.mean(),\n",
    "            color='k',\n",
    "            linestyle='--',\n",
    "            lw=1,\n",
    "            label='mean (train)')\n",
    "plt.xlabel(\"Target (y)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\n",
    "    f\"Target distribution ({'normalized' if ds_train_full_copy.normalize_target else 'physical'} units)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-8, 6),\n",
    "    ax_ylim=(-8, 6),\n",
    "    color_annual=color_annual,\n",
    "    color_winter=color_winter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Areas (with clariden alias fix)\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area[\"clariden\"] = gl_area[\"claridenL\"]\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(32, 15), sharex=True)\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "subplot_labels = [\n",
    "    '(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)', '(i)'\n",
    "]\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                       axs=axs,\n",
    "                                       subplot_labels=subplot_labels,\n",
    "                                       color_annual=color_annual,\n",
    "                                       color_winter=color_winter,\n",
    "                                       custom_order=test_gl_per_el,\n",
    "                                       gl_area=gl_area)\n",
    "\n",
    "axs[3].set_ylabel(\"Modeled PMB [m w.e.]\", fontsize=20)\n",
    "\n",
    "fig.supxlabel('Observed PMB [m w.e.]', fontsize=20, y=0.06)\n",
    "# two distinct handles\n",
    "legend_scatter_annual = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_annual,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Annual')\n",
    "\n",
    "legend_scatter_winter = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_winter,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Winter')\n",
    "\n",
    "# if you already have other handles (e.g., bands/means), append these:\n",
    "# handles = existing_handles + [legend_scatter_annual, legend_scatter_winter]\n",
    "handles = [legend_scatter_annual, legend_scatter_winter]\n",
    "\n",
    "# You can let matplotlib use the labels from the handles; no need to pass `labels=...`\n",
    "fig.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 0.05),\n",
    "           ncol=4,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "denorm = ds_train_full_copy.normalize_target  # only denormalize if dataset used normalization\n",
    "print(\"Denormalize:\", denorm)\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.parallel_mb import MBJobConfig, run_glacier_mb\n",
    "\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/testing_LSTM/LSTM_no_oggm_OOS_norm_y_past')\n",
    "\n",
    "PATH_GLACIER_GRIDS = 'GLAMOS/topo/gridded_topo_inputs/GLAMOS_grid_Aug_/'\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    job = MBJobConfig(\n",
    "        cfg=cfg,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        fields_not_features=cfg.fieldsNotFeatures,\n",
    "        model_filename=model_filename,\n",
    "        custom_params=custom_params,\n",
    "        ds_train=ds_train_full,\n",
    "        train_idx=train_idx,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        data_path=cfg.dataPath,\n",
    "        path_glacier_grid_glamos=PATH_GLACIER_GRIDS,\n",
    "        path_xr_grids=os.path.join(cfg.dataPath, 'GLAMOS', 'topo',\n",
    "                                   'GLAMOS_DEM', 'xr_masked_grids'),\n",
    "        path_save_glw=path_save_glw,\n",
    "        seed=cfg.seed,\n",
    "        max_workers=16,  # or an int\n",
    "        cpu_only=True,\n",
    "        ONLY_GEODETIC=False,\n",
    "        denorm=ds_train_full_copy.normalize_target,\n",
    "        save_monthly=True)\n",
    "\n",
    "    # 3) Run\n",
    "    summary = run_glacier_mb(job, glacier_list, periods_per_glacier)\n",
    "    print(\"SUMMARY:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "    glacier_name=\"rhone\",\n",
    "    year=2008,\n",
    "    path_pred_lstm=path_save_glw,\n",
    "    apply_smoothing_fn=apply_gaussian_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'rhone'\n",
    "year = 2008\n",
    "# open xarray\n",
    "xr.open_dataset(\n",
    "    path_save_glw +\n",
    "    f'/{glacier_name}/{glacier_name}_{year}_annual.zarr').pred_masked.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                                  df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "fig = plot_mbm_vs_geodetic_by_area_bin(\n",
    "    df_all_nn,\n",
    "    bins=[0, 1, 5, 10, 100, np.inf],\n",
    "    labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "    max_bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
