{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.utils import *\n",
    "from scripts.glamos import *\n",
    "from scripts.models import *\n",
    "from scripts.geo_data import *\n",
    "from scripts.dataset import *\n",
    "from scripts.geodetic import *\n",
    "from scripts.physical import *\n",
    "from scripts.plotting import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    mbm.utils.free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GLAMOS stake data\n",
    "data_glamos = get_stakes_data(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train = data_monthly[data_monthly.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly_train[data_monthly_train.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train['y'] = data_train['POINT_BALANCE']\n",
    "\n",
    "data_test = data_monthly_train[(data_monthly_train.GLACIER == 'rhone')\n",
    "                               & (data_monthly_train.YEAR >= 2000)]\n",
    "data_test['y'] = data_test['POINT_BALANCE']\n",
    "\n",
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "# remove 2025\n",
    "data_monthly_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.YEAR < 2025]\n",
    "\n",
    "existing_glaciers = set(data_monthly_train_Aug_.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train_Aug_ = data_monthly_train_Aug_[data_monthly_train_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train_Aug_))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train_Aug_['y'] = data_train_Aug_['POINT_BALANCE']\n",
    "\n",
    "# Test (Rhone > 2000)\n",
    "data_test_Aug_ = data_monthly_train_Aug_[\n",
    "    (data_monthly_train_Aug_.GLACIER == 'rhone')\n",
    "    & (data_monthly_train_Aug_.YEAR >= 2000)]\n",
    "data_test_Aug_['y'] = data_test_Aug_['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_TRAIN_DS = \"cache/lstm_train_dataset_rhone.pt\"\n",
    "CACHE_TEST_DS = \"cache/lstm_test_dataset_rhone.pt\"\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# Load or build TRAIN dataset (all glaciers, all years)\n",
    "# ============================================================\n",
    "RUN_CACHE_TRAIN_DS = False\n",
    "if RUN_CACHE_TRAIN_DS:\n",
    "    print(\"Building TRAIN MBSequenceDataset...\")\n",
    "\n",
    "    ds_train = build_combined_LSTM_dataset(\n",
    "        df_loss=data_train,\n",
    "        df_full=data_train_Aug_,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad_Aug_,\n",
    "        months_tail_pad=months_tail_pad_Aug_,\n",
    "        normalize_target=True,\n",
    "        expect_target=True,\n",
    "    )\n",
    "\n",
    "    torch.save({\"dataset\": ds_train}, CACHE_TRAIN_DS)\n",
    "    print(\"Cached TRAIN dataset.\")\n",
    "else:\n",
    "    print(\"Loading cached TRAIN MBSequenceDataset...\")\n",
    "    ckpt = torch.load(CACHE_TRAIN_DS, map_location=\"cpu\")\n",
    "    ds_train = ckpt[\"dataset\"]\n",
    "\n",
    "# ============================================================\n",
    "# Load or build TEST dataset (Rhone glacier ≥ 2000)\n",
    "# ============================================================\n",
    "RUN_CACHE_TEST_DS = False\n",
    "if RUN_CACHE_TEST_DS:\n",
    "    print(\"Building TEST MBSequenceDataset...\")\n",
    "\n",
    "    ds_test = build_combined_LSTM_dataset(\n",
    "        df_loss=data_test,\n",
    "        df_full=data_test_Aug_,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad_Aug_,\n",
    "        months_tail_pad=months_tail_pad_Aug_,\n",
    "        normalize_target=True,\n",
    "        expect_target=True,\n",
    "    )\n",
    "\n",
    "    torch.save({\"dataset\": ds_test}, CACHE_TEST_DS)\n",
    "    print(\"Cached TEST dataset.\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading cached TEST MBSequenceDataset...\")\n",
    "    ckpt = torch.load(CACHE_TEST_DS, map_location=\"cpu\")\n",
    "    ds_test = ckpt[\"dataset\"]\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.0, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# custom_params = {\n",
    "#     'Fm': 9,\n",
    "#     'Fs': 3,\n",
    "#     'hidden_size': 64,\n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': False,\n",
    "#     'dropout': 0.1,\n",
    "#     'static_layers': 2,\n",
    "#     'static_hidden': 32,\n",
    "#     'static_dropout': 0.1,\n",
    "#     'lr': 0.0005,\n",
    "#     'weight_decay': 1e-05,\n",
    "#     'loss_name': 'neutral',\n",
    "#     'two_heads': False,\n",
    "#     'head_dropout': 0.0,\n",
    "#     'loss_spec': None\n",
    "# }\n",
    "\n",
    "custom_params = PARAMS_LSTM_IS_past\n",
    "\n",
    "################\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = LSTM_IS_NORM_Y_PAST\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "seed_all(cfg.seed)\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Load and evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-8, 6),\n",
    "    ax_ylim=(-8, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code builds a monthly climate input table for every grid cell of Rhone glacier, for all years 2007–2024, in the exact same column format as the stake dataset used to train the LSTM.\n",
    "So instead of having samples only at stakes, each (gridcell, year) now becomes one LSTM sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_glacier_grid_glamos = 'GLAMOS/topo/gridded_topo_inputs/GLAMOS_grid_Aug_/'\n",
    "glacier_name = 'rhone'\n",
    "fields_not_features = cfg.fieldsNotFeatures\n",
    "\n",
    "CACHE_GRID_DF = \"cache/rhone_grid_monthly_df.parquet\"\n",
    "CACHE_TRAIN_FULL = \"cache/train_full_pristine_ds_rhone.pt\"\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Cache Rhone glacier grid dataframe\n",
    "# ============================================================\n",
    "RUN_CACHE_GRID_DF = False\n",
    "\n",
    "if RUN_CACHE_GRID_DF or not os.path.exists(CACHE_GRID_DF):\n",
    "    if os.path.exists(CACHE_GRID_DF):\n",
    "        os.remove(CACHE_GRID_DF)\n",
    "\n",
    "    print(\"Reading Rhone glacier parquet files...\")\n",
    "\n",
    "    glacier_path = os.path.join(cfg.dataPath, path_glacier_grid_glamos,\n",
    "                                glacier_name)\n",
    "    dataframes = []\n",
    "    range_years = range(2007, 2025)  # small test range\n",
    "\n",
    "    for year in tqdm(range_years):\n",
    "        parquet_path = os.path.join(glacier_path,\n",
    "                                    f\"{glacier_name}_grid_{year}.parquet\")\n",
    "        if not os.path.exists(parquet_path):\n",
    "            raise FileNotFoundError(parquet_path)\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    df_grid_monthly = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Keep only required columns\n",
    "    REQUIRED = [\"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\", \"MONTHS\"]\n",
    "    all_columns = MONTHLY_COLS + STATIC_COLS + fields_not_features\n",
    "    needed = set(all_columns) | set(REQUIRED)\n",
    "    df_grid_monthly = df_grid_monthly[[\n",
    "        c for c in df_grid_monthly.columns if c in needed\n",
    "    ]]\n",
    "\n",
    "    # Fake target variable if missing\n",
    "    if \"POINT_BALANCE\" not in df_grid_monthly.columns:\n",
    "        df_grid_monthly[\"POINT_BALANCE\"] = 0.0\n",
    "\n",
    "    # Mask extrapolated months\n",
    "    extrapolate_months = [\"aug_\", \"sep_\"]\n",
    "    df_grid_monthly.loc[\n",
    "        df_grid_monthly[\"MONTHS\"].str.lower().isin(extrapolate_months),\n",
    "        \"POINT_BALANCE\",\n",
    "    ] = np.nan\n",
    "\n",
    "    df_grid_monthly.to_parquet(CACHE_GRID_DF)\n",
    "    print(\"Cached Rhone glacier grid dataframe.\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading cached Rhone glacier grid dataframe...\")\n",
    "    df_grid_monthly = pd.read_parquet(CACHE_GRID_DF)\n",
    "\n",
    "df_grid_monthly_a = df_grid_monthly.dropna(subset=[\"ID\", \"MONTHS\"])\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cache pristine TRAIN dataset for scalers\n",
    "# ============================================================\n",
    "RUN_CACHE_TRAIN_FULL = False\n",
    "\n",
    "if RUN_CACHE_TRAIN_FULL or not os.path.exists(CACHE_TRAIN_FULL):\n",
    "    if os.path.exists(CACHE_TRAIN_FULL):\n",
    "        os.remove(CACHE_TRAIN_FULL)\n",
    "\n",
    "    print(\"Building pristine TRAIN dataset for scalers...\")\n",
    "\n",
    "    ds_train_full = build_combined_LSTM_dataset(\n",
    "        df_loss=data_train,\n",
    "        df_full=data_train_Aug_,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad_Aug_,\n",
    "        months_tail_pad=months_tail_pad_Aug_,\n",
    "        normalize_target=True,\n",
    "        expect_target=True,\n",
    "    )\n",
    "\n",
    "    torch.save({\"dataset\": ds_train_full}, CACHE_TRAIN_FULL)\n",
    "    print(\"Cached pristine TRAIN dataset.\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading cached pristine TRAIN dataset...\")\n",
    "    ckpt = torch.load(CACHE_TRAIN_FULL, map_location=\"cpu\")\n",
    "    ds_train_full = ckpt[\"dataset\"]\n",
    "\n",
    "# ============================================================\n",
    "# 3) Fit scalers (fast)\n",
    "# ============================================================\n",
    "ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_full)\n",
    "ds_train_full_copy.fit_scalers(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build LSTM dataset for the full Rhone glacier grid (all years)\n",
    "# ============================================================\n",
    "# Each grid cell and year becomes one LSTM sequence.\n",
    "# This produces:\n",
    "#   x_m : (Ncells × Ny, Nmonths=16, Nfeatures=9)   monthly climate inputs\n",
    "#   x_s : (Ncells × Ny, 3)                         static topo features\n",
    "#\n",
    "CACHE_GL_DS = \"cache/rhone_grid_MBSequenceDataset_norm.pt\"\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "RUN_CACHE_GL_DS = False\n",
    "\n",
    "if RUN_CACHE_GL_DS:\n",
    "    print(\"Building Rhone glacier MBSequenceDataset from dataframe...\")\n",
    "\n",
    "    ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_grid_monthly_a,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True,  # dummy target required\n",
    "        show_progress=True,\n",
    "        normalize_target=True,  # we only predict & backprop\n",
    "    )\n",
    "\n",
    "    torch.save({\"dataset\": ds_gl_a}, CACHE_GL_DS)\n",
    "    print(\"Cached Rhone glacier MBSequenceDataset.\")\n",
    "else:\n",
    "    print(\"Loading cached Rhone glacier MBSequenceDataset...\")\n",
    "    ckpt = torch.load(CACHE_GL_DS, map_location=\"cpu\")\n",
    "    ds_gl_a = ckpt[\"dataset\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Normalize Rhone glacier grid with TRAINING statistics\n",
    "# ============================================================\n",
    "# The grid inputs are standardized using the SAME means/stds\n",
    "# that were fitted on the multi-glacier training set.\n",
    "# This guarantees physical consistency between training and grid inference.\n",
    "#\n",
    "test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_gl_a,\n",
    "    ds_train_full_copy,  # contains the fitted scalers\n",
    "    seed=cfg.seed,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Load trained regional LSTM glacier model\n",
    "# ============================================================\n",
    "model = get_lstm_model_cpu_cached(cfg, custom_params, model_filename)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# Prepare grid dataset for sensitivity analysis\n",
    "# ============================================================\n",
    "ds = ds_gl_a\n",
    "dl = test_gl_dl_a\n",
    "\n",
    "# Reference climate altitude of Rhone glacier (used to reconstruct\n",
    "# absolute elevation from ELEVATION_DIFFERENCE)\n",
    "alt = np.unique(data_train[data_train.YEAR >= 2000].ALTITUDE_CLIMATE)[1]\n",
    "print(f\"Reference climate altitude = {alt} m\")\n",
    "\n",
    "# Hydrological month axis including padding\n",
    "months_keys = (months_tail_pad + mbm.data_processing.utils.months_hydro_year +\n",
    "               months_head_pad)\n",
    "print(f\"{months_keys=}\")\n",
    "\n",
    "# Dataset geometry\n",
    "Nsamples = len(ds)  # number of gridcell × year sequences\n",
    "Nmonths = len(months_keys)  # = 16 monthly time steps\n",
    "Nfeatures = ds[0][\"x_m\"].shape[1]  # = 9 climate features per month\n",
    "\n",
    "print(f\"{Nsamples=}\")\n",
    "print(f\"{Nmonths=}\")\n",
    "print(f\"{Nfeatures=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Allocate storage:\n",
    "# sensitivity[m][b,t,f] = ∂ b_m / ∂ x_{t,f}\n",
    "# ============================================================\n",
    "sensitivity = {\n",
    "    m: torch.zeros(Nsamples, Nmonths, Nfeatures)\n",
    "    for m in months_keys\n",
    "}\n",
    "\n",
    "pred = torch.zeros(Nsamples, Nmonths)\n",
    "elevation = torch.zeros(Nsamples)\n",
    "\n",
    "model.eval()  # disable dropout for stable gradients\n",
    "\n",
    "all_keys = ds.keys\n",
    "i = 0\n",
    "\n",
    "pbar = tqdm(total=Nsamples, desc=\"Computing grid sensitivities\", unit=\"cells\")\n",
    "\n",
    "for batch in dl:\n",
    "    bs = batch[\"x_m\"].shape[0]\n",
    "\n",
    "    batch = model.to_device(device, batch)\n",
    "    x_m = batch[\"x_m\"].clone().requires_grad_(True)\n",
    "\n",
    "    # Forward pass once per batch\n",
    "    y_month, y_w, y_a = model(x_m, batch[\"x_s\"], batch[\"mv\"], batch[\"mw\"],\n",
    "                              batch[\"ma\"])\n",
    "    assert y_month.shape[1] == Nmonths\n",
    "\n",
    "    pred[i:i + bs] = y_month.detach()\n",
    "\n",
    "    # ========================================================\n",
    "    # Loop over output months\n",
    "    # ========================================================\n",
    "    for m_idx, m_name in enumerate(months_keys):\n",
    "\n",
    "        model.zero_grad()\n",
    "        if x_m.grad is not None:\n",
    "            x_m.grad.zero_()\n",
    "\n",
    "        one_hot = torch.nn.functional.one_hot(\n",
    "            torch.tensor([m_idx], device=device), Nmonths).float()\n",
    "\n",
    "        target = (y_month * one_hot).sum()\n",
    "        target.backward(retain_graph=True)\n",
    "\n",
    "        sensitivity[m_name][i:i + bs] = x_m.grad.detach()\n",
    "\n",
    "    # ========================================================\n",
    "    # Recover absolute elevation\n",
    "    # ========================================================\n",
    "    elevation[i:i +\n",
    "              bs] = ((batch[\"x_m\"] * ds_train_full_copy.month_std.to(device)) +\n",
    "                     ds_train_full_copy.month_mean.to(device)\n",
    "                     )[:, 0, MONTHLY_COLS.index(\"ELEVATION_DIFFERENCE\")] + alt\n",
    "\n",
    "    i += bs\n",
    "    pbar.update(bs)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Check which samples have zero total sensitivity\n",
    "# ============================================================\n",
    "for m in months_keys:\n",
    "    # Flatten (T,F) -> vector and compute L2 norm per sample\n",
    "    # This measures the total sensitivity magnitude of b_m to all inputs\n",
    "    norm_per_sample = sensitivity[m].reshape(sensitivity[m].shape[0],\n",
    "                                             -1).norm(dim=1)\n",
    "\n",
    "    # Print how many grid cells have exactly zero sensitivity\n",
    "    # (usually fully masked or invalid sequences)\n",
    "    print(m, sensitivity[m][norm_per_sample == 0.0].shape)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Build elevation bands across the glacier\n",
    "# ============================================================\n",
    "bands = np.linspace(elevation.min(), elevation.max(), 8)\n",
    "\n",
    "print(\"Bounds of the bands:\", bands)\n",
    "print(\"diff bands =\", np.diff(bands))  # thickness of each elevation band\n",
    "\n",
    "# ============================================================\n",
    "# 3) Group sensitivities by elevation band for each output month\n",
    "# ============================================================\n",
    "sens_bands = {m: [] for m in months_keys}\n",
    "\n",
    "for e, m in enumerate(months_keys):\n",
    "\n",
    "    # Loop over elevation intervals [lb, ub]\n",
    "    for i in range(bands.shape[0] - 1):\n",
    "\n",
    "        lb = bands[i]\n",
    "        ub = bands[i + 1]\n",
    "\n",
    "        # Boolean mask selecting grid cells in this elevation band\n",
    "        ind = (elevation <= ub) * (lb <= elevation)\n",
    "\n",
    "        # Print number of cells per band only once (for first month)\n",
    "        if e == 0:\n",
    "            print(ind.sum())\n",
    "\n",
    "        # Store sensitivities for this band and this output month\n",
    "        # Resulting shape: (Ncells_in_band, Nmonths, Nfeatures)\n",
    "        sens_bands[m].append(sensitivity[m][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For special variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_monthly_sensitivity_elevbands(\n",
    "    sens_bands=sens_bands,\n",
    "    plot_var=\"t2m\",\n",
    "    glacier_name=\"rhone\",\n",
    "    months_keys=months_keys,\n",
    "    vois_climate_long_name=vois_climate_long_name,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    bands=bands,\n",
    "    add_panel_labels=False,\n",
    "    drop_padded_months=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_monthly_sensitivity_elevbands(\n",
    "    sens_bands=sens_bands,\n",
    "    plot_var=\"tp\",\n",
    "    glacier_name=\"rhone\",\n",
    "    months_keys=months_keys,\n",
    "    vois_climate_long_name=vois_climate_long_name,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    bands=bands,\n",
    "    add_panel_labels=False,\n",
    "    drop_padded_months=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_var in ['slhf', 'sshf', 'ssrd', 'fal', 'str', 'pcsr']:\n",
    "    fig = plot_monthly_sensitivity_elevbands(\n",
    "        sens_bands=sens_bands,\n",
    "        plot_var=plot_var,\n",
    "        glacier_name=\"rhone\",\n",
    "        months_keys=months_keys,\n",
    "        vois_climate_long_name=vois_climate_long_name,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        bands=bands,\n",
    "        add_panel_labels=False,\n",
    "        drop_padded_months=True)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For unique months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_month = \"jul\"\n",
    "plot_vars = [\"tp\", \"t2m\", \"str\", \"slhf\", \"ssrd\", \"fal\", \"pcsr\", \"sshf\"]\n",
    "id_elev_bands = [0, 6]\n",
    "\n",
    "drop_padded_months = True\n",
    "keep_idx = month_keep_idx(months_keys, drop_padded_months)\n",
    "\n",
    "vals = []\n",
    "for plot_var in plot_vars:\n",
    "    f_idx = MONTHLY_COLS.index(plot_var)\n",
    "    for band in sens_bands[selected_month]:\n",
    "        mean = band[:, :, f_idx].mean(dim=0)[keep_idx]\n",
    "        std = band[:, :, f_idx].std(dim=0)[keep_idx]\n",
    "        vals.append((mean -\n",
    "                     std).min().item())\n",
    "        vals.append((mean + std).max().item())\n",
    "\n",
    "ylim = (1.1 * min(vals), 1.1 * max(vals))\n",
    "print(\"Global ylim:\", ylim)\n",
    "\n",
    "titles = [\n",
    "    \"Precip\",\n",
    "    \"Temp\",\n",
    "    \"Surf net therm radiation\",\n",
    "    \"Surface latent heat flux\",\n",
    "    \"Surface solar radiation downwards\",\n",
    "    \"Albedo\",\n",
    "    \"Potential clear sky rad.\",\n",
    "    \"Surface sensible heat flux\",\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 8), sharex=True)\n",
    "ax_list = axs.ravel()\n",
    "\n",
    "for ax, plot_var, title in zip(ax_list, plot_vars, titles):\n",
    "    plot_sensitivity_elev_band(\n",
    "        sens_bands=sens_bands[selected_month],\n",
    "        plot_var=plot_var,\n",
    "        text_var=title,\n",
    "        id_elev_bands=id_elev_bands,\n",
    "        month_labels=months_keys,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        bands=bands,\n",
    "        ax=ax,\n",
    "        ylim=ylim,\n",
    "        drop_padded_months=drop_padded_months,\n",
    "    )\n",
    "    ax.set_ylabel(\"Sens.\")\n",
    "\n",
    "# Turn off any unused axes (only matters if lists don't match grid size)\n",
    "for ax in ax_list[len(plot_vars):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Sensitivity for {selected_month.capitalize()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_month = \"feb\"\n",
    "plot_vars = [\"tp\", \"t2m\", \"str\", \"slhf\", \"ssrd\", \"fal\", \"pcsr\", \"sshf\"]\n",
    "id_elev_bands = [0, 6]\n",
    "\n",
    "vals = []\n",
    "for plot_var in plot_vars:\n",
    "    f_idx = MONTHLY_COLS.index(plot_var)\n",
    "    for band in sens_bands[selected_month]:\n",
    "        mean = band[:, :, f_idx].mean(dim=0)\n",
    "        std = band[:, :, f_idx].std(dim=0)\n",
    "        vals.append((mean - std).min().item())\n",
    "        vals.append((mean + std).max().item())\n",
    "\n",
    "ylim = (1.1 * min(vals), 1.1 * max(vals))\n",
    "print(\"Global ylim:\", ylim)\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 8), sharex=True)\n",
    "\n",
    "for ax, plot_var, title in zip(\n",
    "        axs.ravel(),\n",
    "        plot_vars,\n",
    "    [\n",
    "        \"Precip\",\n",
    "        \"Temp\",\n",
    "        \"Surf net therm radiation\",\n",
    "        \"Surface latent heat flux\",\n",
    "        \"Surface solar radiation downwards\",\n",
    "        \"Albedo\",\n",
    "        \"Potential clear sky rad.\",\n",
    "        \"Surface sensible heat flux\",\n",
    "    ],\n",
    "):\n",
    "    plot_sensitivity_elev_band(\n",
    "        sens_bands=sens_bands[selected_month],\n",
    "        plot_var=plot_var,\n",
    "        text_var=title,\n",
    "        id_elev_bands=id_elev_bands,\n",
    "        month_labels=months_keys,\n",
    "        ax=ax,\n",
    "        ylim=ylim,\n",
    "    )\n",
    "\n",
    "plt.suptitle(f\"Sensitivity for {selected_month.capitalize()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
