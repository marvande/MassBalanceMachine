{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for whole CA\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & utilities ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import traceback\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import copy\n",
    "\n",
    "# Add repo root for MBM imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../\"))\n",
    "\n",
    "# --- Data science stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "\n",
    "# --- Machine learning / DL ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# --- Cartography / plotting ---\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Custom MBM modules ---\n",
    "import massbalancemachine as mbm\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Warnings & autoreload (notebook) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Configuration ---\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "# --- CUDA / device ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "# gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "#     cfg,\n",
    "#     rgi_region=\"11\",\n",
    "#     rgi_version=\"62\",\n",
    "#     base_url=\n",
    "#     \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "#     log_level='WARNING',\n",
    "#     task_list=None,\n",
    "# )\n",
    "\n",
    "# # Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "# df_missing = export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "# path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# # load RGI shapefile\n",
    "# gdf = gpd.read_file(path_rgi)\n",
    "# # reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "# gdf_proj = gdf.to_crs(3035)\n",
    "# gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# # gdf_proj.set_index('rgi_id', inplace=True)\n",
    "# gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "# gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "# df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# # total glacier area\n",
    "# total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# # explode the list of missing vars into rows (one var per row)\n",
    "# df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# # 1) COUNT: number of glaciers missing each variable\n",
    "# counts_missing_per_var = (\n",
    "#     df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "#         ascending=False))\n",
    "\n",
    "# # 2) TOTAL % AREA with ANY missing var\n",
    "# total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "# total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "# print(f\"Total glacier area with ANY missing variable: \"\n",
    "#       f\"{total_missing_area_km2:,.2f} kmÂ² \"\n",
    "#       f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# # Optional: also show % area per variable (kept from your earlier logic)\n",
    "# area_missing_per_var = (\n",
    "#     df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "#         ascending=False))\n",
    "# perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "# print(\"\\n% of total glacier area missing per variable:\")\n",
    "# for var, pct in perc_missing_per_var.items():\n",
    "#     print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# # ---- barplot: number of glaciers missing each variable ----\n",
    "# plt.figure(figsize=(7, 4))\n",
    "# plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "# plt.xlabel(\"Missing variable\")\n",
    "# plt.ylabel(\"Number of glaciers\")\n",
    "# plt.title(\"Count of glaciers missing each variable\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stakes data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"svf\"\n",
    "]\n",
    "\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_CA.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate to all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- fixed order to avoid accidental shuffles ----\n",
    "VARS_ORDER = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "# one canonical list for the partial OGGM combos you trained\n",
    "PARTIAL_COMBOS = [\n",
    "    ('hugonnet_dhdt', ),\n",
    "    ('consensus_ice_thickness', ),\n",
    "    ('millan_v', ),\n",
    "    ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "    ('hugonnet_dhdt', 'millan_v'),\n",
    "    ('consensus_ice_thickness', 'millan_v'),\n",
    "]\n",
    "\n",
    "\n",
    "def combo_key(tup):\n",
    "    ordered = [v for v in VARS_ORDER if v in tup]\n",
    "    return \"__\".join(ordered)  # e.g. \"hugonnet_dhdt__millan_v\"\n",
    "\n",
    "\n",
    "def combo_key_from_tuple(tup):\n",
    "    \"\"\"('hugonnet_dhdt','millan_v')->'hugonnet_dhdt__millan_v'; ()->'simple'; all 3->'full'\"\"\"\n",
    "    if not tup:\n",
    "        return \"simple\"\n",
    "    ordered = [v for v in VARS_ORDER if v in tup]\n",
    "    if ordered == VARS_ORDER:\n",
    "        return \"full\"\n",
    "    return \"__\".join(ordered)\n",
    "\n",
    "\n",
    "def make_params(Fm, STATIC_COLS):\n",
    "    # Fs derived from STATIC_COLS length\n",
    "    return {\n",
    "        'Fm': Fm,  # number of monthly features\n",
    "        'Fs': len(STATIC_COLS),  # number of static features\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 1,\n",
    "        'bidirectional': False,\n",
    "        'dropout': 0.0,\n",
    "        'static_layers': 0,\n",
    "        'static_hidden': None,\n",
    "        'static_dropout': None,\n",
    "        'lr': 0.001,\n",
    "        'weight_decay': 0.0,\n",
    "        'loss_name': 'neutral',\n",
    "        'loss_spec': None,\n",
    "        'two_heads': True,\n",
    "        'head_dropout': 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- params for all combos (simple/partials/full) built once ----\n",
    "Fm = len(MONTHLY_COLS)\n",
    "\n",
    "STATIC_SIMPLE = ['aspect', 'slope', 'svf']  # no OGGM vars\n",
    "STATIC_FULL = ['aspect', 'slope', 'svf', *VARS_ORDER]  # all OGGM vars\n",
    "\n",
    "params_simple_model = make_params(Fm, STATIC_SIMPLE)\n",
    "params_full_model = make_params(Fm, STATIC_FULL)\n",
    "\n",
    "# partials\n",
    "params_by_key = {}\n",
    "for combo in PARTIAL_COMBOS:\n",
    "    ordered = [v for v in VARS_ORDER if v in combo]\n",
    "    static_cols = ['aspect', 'slope', 'svf', *ordered]\n",
    "    key = combo_key(combo)\n",
    "    params_by_key[key] = make_params(Fm, static_cols)\n",
    "\n",
    "params_hugonnet_only = copy.deepcopy(params_by_key['hugonnet_dhdt'])\n",
    "params_consensus_only = copy.deepcopy(params_by_key['consensus_ice_thickness'])\n",
    "params_millan_only = copy.deepcopy(params_by_key['millan_v'])\n",
    "params_hugonnet_consensus = copy.deepcopy(\n",
    "    params_by_key['hugonnet_dhdt__consensus_ice_thickness'])\n",
    "params_hugonnet_millan = copy.deepcopy(\n",
    "    params_by_key['hugonnet_dhdt__millan_v'])\n",
    "params_consensus_millan = copy.deepcopy(\n",
    "    params_by_key['consensus_ice_thickness__millan_v'])\n",
    "\n",
    "# single authoritative mapping (includes convenient aliases \"simple\"/\"full\")\n",
    "PARAMS_BY_COMBO = {\n",
    "    \"simple\": params_simple_model,\n",
    "    \"hugonnet_dhdt\": params_hugonnet_only,\n",
    "    \"consensus_ice_thickness\": params_consensus_only,\n",
    "    \"millan_v\": params_millan_only,\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\": params_hugonnet_consensus,\n",
    "    \"hugonnet_dhdt__millan_v\": params_hugonnet_millan,\n",
    "    \"consensus_ice_thickness__millan_v\": params_consensus_millan,\n",
    "    \"full\": params_full_model,\n",
    "}\n",
    "\n",
    "# ---- config / constants ----\n",
    "# Include simple and full for completeness\n",
    "ALL_COMBOS = [()] + PARTIAL_COMBOS + [tuple(VARS_ORDER)]\n",
    "\n",
    "# ---- normalize input dataframes once ----\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()  # optional, if you also want test ds per combo\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# ---- builders ----\n",
    "DS_TRAIN_BY_COMBO = {}\n",
    "SPLIT_IDXS_BY_COMBO = {}  # (train_idx, val_idx)\n",
    "STATIC_COLS_BY_COMBO = {}\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "for combo in ALL_COMBOS:\n",
    "    key = combo_key_from_tuple(combo)\n",
    "\n",
    "    # Build static columns in stable order\n",
    "    if key == \"simple\":\n",
    "        STATIC_COLS = STATIC_SIMPLE\n",
    "    elif key == \"full\":\n",
    "        STATIC_COLS = STATIC_FULL\n",
    "    else:\n",
    "        ordered = [v for v in VARS_ORDER if v in combo]\n",
    "        STATIC_COLS = ['aspect', 'slope', 'svf', *ordered]\n",
    "\n",
    "    STATIC_COLS_BY_COMBO[key] = STATIC_COLS\n",
    "\n",
    "    # --- build train dataset from dataframe ---\n",
    "    ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_train,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True,\n",
    "    )\n",
    "    DS_TRAIN_BY_COMBO[key] = ds_train\n",
    "\n",
    "    # keep per-combo split (use your preferred val_ratio/seed)\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "    SPLIT_IDXS_BY_COMBO[key] = (train_idx, val_idx)\n",
    "\n",
    "# convenience names\n",
    "ds_train_simple = DS_TRAIN_BY_COMBO['simple']\n",
    "ds_train_full = DS_TRAIN_BY_COMBO['full']\n",
    "ds_train_hugonnet_only = DS_TRAIN_BY_COMBO['hugonnet_dhdt']\n",
    "ds_train_consensus_only = DS_TRAIN_BY_COMBO['consensus_ice_thickness']\n",
    "ds_train_millan_only = DS_TRAIN_BY_COMBO['millan_v']\n",
    "ds_train_hugonnet_consensus = DS_TRAIN_BY_COMBO[\n",
    "    'hugonnet_dhdt__consensus_ice_thickness']\n",
    "ds_train_hugonnet_millan = DS_TRAIN_BY_COMBO['hugonnet_dhdt__millan_v']\n",
    "ds_train_consensus_millan = DS_TRAIN_BY_COMBO[\n",
    "    'consensus_ice_thickness__millan_v']\n",
    "\n",
    "train_idx_simple, val_idx_simple = SPLIT_IDXS_BY_COMBO['simple']\n",
    "train_idx_full, val_idx_full = SPLIT_IDXS_BY_COMBO['full']\n",
    "\n",
    "# List *exactly* the combos you trained (keys must match your saved filenames/params)\n",
    "TRAINED_COMBOS = [\n",
    "    (),  # simple (no OGGM)\n",
    "    *PARTIAL_COMBOS,\n",
    "    tuple(VARS_ORDER),  # full (all OGGM)\n",
    "]\n",
    "\n",
    "# Build mapping keys\n",
    "COMBO_KEYS = [combo_key_from_tuple(c) for c in TRAINED_COMBOS]\n",
    "\n",
    "# ---- checkpoints for each combo (update paths to match your training) ----\n",
    "MODEL_PATHS = {\n",
    "    \"simple\": \"models/lstm_model_2025-10-09_CA_simple.pt\",\n",
    "    \"hugonnet_dhdt\": \"models/lstm_model_2025-10-09_CA_hugonnet_dhdt.pt\",\n",
    "    \"consensus_ice_thickness\":\n",
    "    \"models/lstm_model_2025-10-09_CA_consensus_ice_thickness.pt\",\n",
    "    \"millan_v\": \"models/lstm_model_2025-10-09_CA_millan_v.pt\",\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\":\n",
    "    \"models/lstm_model_2025-10-09_CA_hugonnet_dhdt_consensus_ice_thickness.pt\",\n",
    "    \"hugonnet_dhdt__millan_v\":\n",
    "    \"models/lstm_model_2025-10-09_CA_hugonnet_dhdt_millan_v.pt\",\n",
    "    \"consensus_ice_thickness__millan_v\":\n",
    "    \"models/lstm_model_2025-10-09_CA_consensus_ice_thickness_millan_v.pt\",\n",
    "    \"full\": \"models/lstm_model_2025-10-09_CA_full.pt\",\n",
    "}\n",
    "\n",
    "# ---- params per combo (Fs must reflect STATIC_COLS length!) ----\n",
    "# Use the single authoritative PARAMS_BY_COMBO from above\n",
    "\n",
    "# ---- training datasets (for scalers) ----\n",
    "TRAIN_DS_BY_COMBO = {\n",
    "    \"simple\": ds_train_simple,\n",
    "    \"hugonnet_dhdt\": ds_train_hugonnet_only,\n",
    "    \"consensus_ice_thickness\": ds_train_consensus_only,\n",
    "    \"millan_v\": ds_train_millan_only,\n",
    "    \"hugonnet_dhdt__consensus_ice_thickness\": ds_train_hugonnet_consensus,\n",
    "    \"hugonnet_dhdt__millan_v\": ds_train_hugonnet_millan,\n",
    "    \"consensus_ice_thickness__millan_v\": ds_train_consensus_millan,\n",
    "    \"full\": ds_train_full,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for loaded models\n",
    "_MODEL_CACHE = {}\n",
    "\n",
    "def get_model_by_combo(combo_key_str: str, cfg, device):\n",
    "    \"\"\"Generalized model loader with cache.\"\"\"\n",
    "    if combo_key_str in _MODEL_CACHE:\n",
    "        return _MODEL_CACHE[combo_key_str]\n",
    "\n",
    "    if combo_key_str not in MODEL_PATHS:\n",
    "        raise ValueError(f\"No model path for combo '{combo_key_str}'\")\n",
    "    if combo_key_str not in PARAMS_BY_COMBO:\n",
    "        raise ValueError(f\"No params for combo '{combo_key_str}'\")\n",
    "\n",
    "    ckpt_path = MODEL_PATHS[combo_key_str]\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found for '{combo_key_str}': {ckpt_path}\")\n",
    "\n",
    "    params = PARAMS_BY_COMBO[combo_key_str]\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "                                                       params,\n",
    "                                                       device,\n",
    "                                                       verbose=False)\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "    _MODEL_CACHE[combo_key_str] = model\n",
    "    return model\n",
    "\n",
    "\n",
    "def detect_available_combo_key(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Return the most specific trained combo that matches columns present in df.\n",
    "    Priority: full > any 2-var combo > any 1-var combo > simple.\n",
    "    \"\"\"\n",
    "    present_set = set(c for c in VARS_ORDER if c in df.columns)\n",
    "    # try exact matches by decreasing size\n",
    "    candidates = sorted(TRAINED_COMBOS, key=lambda t: len(t), reverse=True)\n",
    "    for tup in candidates:\n",
    "        if set(tup).issubset(present_set):\n",
    "            return combo_key_from_tuple(tup)\n",
    "    return \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cache for loaded models\n",
    "# _MODEL_CACHE = {}\n",
    "\n",
    "# def get_model_by_combo(combo_key_str: str, cfg, device):\n",
    "#     \"\"\"Generalized model loader with cache.\"\"\"\n",
    "#     if combo_key_str in _MODEL_CACHE:\n",
    "#         return _MODEL_CACHE[combo_key_str]\n",
    "\n",
    "#     if combo_key_str not in MODEL_PATHS:\n",
    "#         raise ValueError(f\"No model path for combo '{combo_key_str}'\")\n",
    "#     if combo_key_str not in PARAMS_BY_COMBO:\n",
    "#         raise ValueError(f\"No params for combo '{combo_key_str}'\")\n",
    "\n",
    "#     ckpt_path = MODEL_PATHS[combo_key_str]\n",
    "#     if not os.path.exists(ckpt_path):\n",
    "#         raise FileNotFoundError(\n",
    "#             f\"Checkpoint not found for '{combo_key_str}': {ckpt_path}\")\n",
    "\n",
    "#     params = PARAMS_BY_COMBO[combo_key_str]\n",
    "#     model = mbm.models.LSTM_MB.build_model_from_params(cfg,\n",
    "#                                                        params,\n",
    "#                                                        device,\n",
    "#                                                        verbose=False)\n",
    "#     state = torch.load(ckpt_path, map_location=device)\n",
    "#     model.load_state_dict(state)\n",
    "#     model.eval()\n",
    "#     _MODEL_CACHE[combo_key_str] = model\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def detect_available_combo_key(df: pd.DataFrame) -> str:\n",
    "#     \"\"\"\n",
    "#     Return the most specific trained combo that matches columns present in df.\n",
    "#     Priority: full > any 2-var combo > any 1-var combo > simple.\n",
    "#     \"\"\"\n",
    "#     present_set = set(c for c in VARS_ORDER if c in df.columns)\n",
    "#     # try exact matches by decreasing size\n",
    "#     candidates = sorted(TRAINED_COMBOS, key=lambda t: len(t), reverse=True)\n",
    "#     for tup in candidates:\n",
    "#         if set(tup).issubset(present_set):\n",
    "#             return combo_key_from_tuple(tup)\n",
    "#     return \"simple\"\n",
    "\n",
    "# # Required by the dataset builder regardless of your feature list\n",
    "# REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "\n",
    "# current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# path_rgi_alps = os.path.join(cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "# rgi_id_list = os.listdir(path_rgi_alps)\n",
    "\n",
    "# # Safe rename helper (wire up if your files have alternate column names)\n",
    "# def safe_rename(df, mapping):\n",
    "#     present = {k: v for k, v in mapping.items() if k in df.columns}\n",
    "#     return df.rename(columns=present) if present else df\n",
    "\n",
    "# # Robust year regex: RGI60-11.01238_grid_2003.parquet\n",
    "# YEAR_RE = re.compile(r\"_grid_(\\d{4})\\.parquet$\")\n",
    "\n",
    "# # --- main run loop (replaces your has_oggm/simple branch) ---\n",
    "# RUN = True\n",
    "# if RUN:\n",
    "#     os.makedirs(\"logs\", exist_ok=True)  # ensure folder exists\n",
    "#     output_file = os.path.join(\"logs\", f\"glacier_mean_MB_{current_date}.csv\")\n",
    "#     with open(output_file, 'w') as f:\n",
    "#         f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "\n",
    "#     index_counter = 0\n",
    "\n",
    "#     for rgi_gl in tqdm(rgi_id_list):\n",
    "#         seed_all(cfg.seed)\n",
    "#         glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "#         if not os.path.exists(glacier_path):\n",
    "#             print(f\"Folder not found for {rgi_gl}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # collect available years\n",
    "#         years = []\n",
    "#         for fname in os.listdir(glacier_path):\n",
    "#             if not (fname.endswith(\".parquet\") and rgi_gl in fname):\n",
    "#                 continue\n",
    "#             m = YEAR_RE.search(fname)\n",
    "#             if m:\n",
    "#                 years.append(int(m.group(1)))\n",
    "#         years = sorted(set(years))\n",
    "#         if not years:\n",
    "#             print(f\"No parquet years for {rgi_gl}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         # clear model cache per glacier (optional)\n",
    "#         _MODEL_CACHE.clear()\n",
    "\n",
    "#         for year in years:\n",
    "#             file_name = f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "#             file_path = os.path.join(glacier_path, file_name)\n",
    "\n",
    "#             try:\n",
    "#                 df_grid_monthly = pd.read_parquet(file_path)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[{rgi_gl} {year}] read error: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "#             # Normalize required fields\n",
    "#             if 'PERIOD' in df_grid_monthly.columns:\n",
    "#                 df_grid_monthly['PERIOD'] = df_grid_monthly['PERIOD'].astype(str).str.strip().str.lower()\n",
    "\n",
    "#             # --- choose the best available trained combo for this file ---\n",
    "#             selected_combo_key = detect_available_combo_key(df_grid_monthly)\n",
    "#             # build STATIC_COLS in stable order matching training schema\n",
    "#             if selected_combo_key == \"simple\":\n",
    "#                 oggm_cols = []\n",
    "#             elif selected_combo_key == \"full\":\n",
    "#                 oggm_cols = VARS_ORDER[:]  # ['hugonnet_dhdt','consensus_ice_thickness','millan_v']\n",
    "#             else:\n",
    "#                 raw = selected_combo_key.split(\"__\")\n",
    "#                 oggm_cols = [v for v in VARS_ORDER if v in raw]\n",
    "\n",
    "#             STATIC_COLS = ['aspect', 'slope', 'svf'] + oggm_cols\n",
    "#             feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "#             all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "#             # retain only required + feature columns (preserve original order)\n",
    "#             need = set(all_columns) | set(REQUIRED)\n",
    "#             keep = [c for c in df_grid_monthly.columns if c in need]\n",
    "#             df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "#             # required checks\n",
    "#             missing_req = [c for c in REQUIRED if c not in df_grid_monthly.columns]\n",
    "#             if missing_req:\n",
    "#                 print(f\"[{rgi_gl} {year}] missing required cols: {missing_req}, skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#             # minimal NaN cleanup\n",
    "#             df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "#             if df_grid_monthly_a.empty:\n",
    "#                 print(f\"[{rgi_gl} {year}] empty after NaN cleanup, skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#             # --- build inference dataset with the combo's schema ---\n",
    "#             ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "#                 df_grid_monthly_a,\n",
    "#                 MONTHLY_COLS,\n",
    "#                 STATIC_COLS,\n",
    "#                 months_tail_pad=months_tail_pad,\n",
    "#                 months_head_pad=months_head_pad,\n",
    "#                 expect_target=False,\n",
    "#                 show_progress=False,\n",
    "#             )\n",
    "\n",
    "#             # bring the *matching* training scalers for this combo\n",
    "#             if selected_combo_key not in DS_TRAIN_BY_COMBO:\n",
    "#                 print(f\"[{rgi_gl} {year}] no training dataset registered for combo '{selected_combo_key}', skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#             ds_train_for_combo = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#                 DS_TRAIN_BY_COMBO[selected_combo_key]\n",
    "#             )\n",
    "\n",
    "#             if selected_combo_key not in SPLIT_IDXS_BY_COMBO:\n",
    "#                 print(f\"[{rgi_gl} {year}] no split indices registered for combo '{selected_combo_key}', skipping...\")\n",
    "#                 continue\n",
    "\n",
    "#             train_idx_for_combo, _ = SPLIT_IDXS_BY_COMBO[selected_combo_key]\n",
    "#             ds_train_for_combo.fit_scalers(train_idx_for_combo)\n",
    "\n",
    "#             test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#                 ds_gl_a, ds_train_for_combo, seed=cfg.seed, batch_size=128\n",
    "#             )\n",
    "\n",
    "#             # --- load + predict with the appropriate model for this combo ---\n",
    "#             try:\n",
    "#                 model = get_model_by_combo(selected_combo_key, cfg, device)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[{rgi_gl} {year}] model load error for '{selected_combo_key}': {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"[{rgi_gl} {year}] prediction error for '{selected_combo_key}': {e}\")\n",
    "#                 continue\n",
    "\n",
    "#             # --- aggregate to annual glacier mean and write ---\n",
    "#             data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "#             meta_cols = [c for c in ['YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID'] if c in df_grid_monthly_a.columns]\n",
    "\n",
    "#             grouped_ids_a = (\n",
    "#                 df_grid_monthly_a.groupby('ID')[meta_cols].first()\n",
    "#                 .merge(data_a, left_index=True, right_index=True, how='left')\n",
    "#             )\n",
    "#             months_per_id_a = df_grid_monthly_a.groupby('ID')['MONTHS'].unique()\n",
    "#             grouped_ids_a = grouped_ids_a.merge(months_per_id_a, left_index=True, right_index=True)\n",
    "\n",
    "#             grouped_ids_a.reset_index(inplace=True)\n",
    "#             grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "#             pred_y_annual = grouped_ids_a.copy()\n",
    "#             pred_y_annual['PERIOD'] = 'annual'\n",
    "#             pred_y_annual = pred_y_annual.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "#             mean_MB = pred_y_annual['pred'].mean()\n",
    "\n",
    "#             with open(output_file, 'a') as f:\n",
    "#                 f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "\n",
    "#             index_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "import os, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# -------- utilities for workers --------\n",
    "\n",
    "def _worker_init(seed: int):\n",
    "    # Avoid BLAS thread storms in multi-proc\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    torch.set_num_threads(1)\n",
    "    seed_all(seed)\n",
    "\n",
    "def _process_one_file(args):\n",
    "    \"\"\"\n",
    "    Args is a dict to keep signature pickle-friendly.\n",
    "    Returns: (status, payload) where status in {\"ok\",\"skip\",\"err\"}\n",
    "    payload for \"ok\": (rgi_gl, year, mean_MB)\n",
    "    \"\"\"\n",
    "    (rgi_gl, year, file_path, cfg, device_str) = (\n",
    "        args[\"rgi_gl\"], args[\"year\"], args[\"file_path\"], args[\"cfg\"], args[\"device_str\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df_grid_monthly = pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        return (\"err\", f\"[{rgi_gl} {year}] read error: {e}\")\n",
    "\n",
    "    df_grid_monthly.drop_duplicates(inplace=True)\n",
    "    if 'PERIOD' in df_grid_monthly.columns:\n",
    "        df_grid_monthly['PERIOD'] = (\n",
    "            df_grid_monthly['PERIOD'].astype(str).str.strip().str.lower()\n",
    "        )\n",
    "\n",
    "    # Decide combo\n",
    "    selected_combo_key = detect_available_combo_key(df_grid_monthly)\n",
    "    if selected_combo_key == \"simple\":\n",
    "        oggm_cols = []\n",
    "    elif selected_combo_key == \"full\":\n",
    "        oggm_cols = VARS_ORDER[:]\n",
    "    else:\n",
    "        raw = selected_combo_key.split(\"__\")\n",
    "        oggm_cols = [v for v in VARS_ORDER if v in raw]\n",
    "\n",
    "    STATIC_COLS = ['aspect', 'slope', 'svf'] + oggm_cols\n",
    "    feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "    all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "    REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "    need = set(all_columns) | set(REQUIRED)\n",
    "    keep = [c for c in df_grid_monthly.columns if c in need]\n",
    "    df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "    # Required checks\n",
    "    missing_req = [c for c in REQUIRED if c not in df_grid_monthly.columns]\n",
    "    if missing_req:\n",
    "        return (\"skip\", f\"[{rgi_gl} {year}] missing required cols: {missing_req}\")\n",
    "\n",
    "    df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "    if df_grid_monthly_a.empty:\n",
    "        return (\"skip\", f\"[{rgi_gl} {year}] empty after NaN cleanup\")\n",
    "\n",
    "    # Build inference dataset\n",
    "    ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_grid_monthly_a,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=False,\n",
    "        show_progress=False,\n",
    "    )\n",
    "\n",
    "    # Bring matching training scalers for this combo\n",
    "    if selected_combo_key not in DS_TRAIN_BY_COMBO:\n",
    "        return (\"skip\", f\"[{rgi_gl} {year}] no train DS for '{selected_combo_key}'\")\n",
    "\n",
    "    ds_train_for_combo = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        DS_TRAIN_BY_COMBO[selected_combo_key]\n",
    "    )\n",
    "\n",
    "    if selected_combo_key not in SPLIT_IDXS_BY_COMBO:\n",
    "        return (\"skip\", f\"[{rgi_gl} {year}] no split idxs for '{selected_combo_key}'\")\n",
    "\n",
    "    train_idx_for_combo, _ = SPLIT_IDXS_BY_COMBO[selected_combo_key]\n",
    "    ds_train_for_combo.fit_scalers(train_idx_for_combo)\n",
    "\n",
    "    test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_gl_a, ds_train_for_combo, seed=cfg.seed, batch_size=128\n",
    "    )\n",
    "\n",
    "    # Load model in worker\n",
    "    device = torch.device(device_str)\n",
    "    try:\n",
    "        model = get_model_by_combo(selected_combo_key, cfg, device)\n",
    "    except Exception as e:\n",
    "        return (\"err\", f\"[{rgi_gl} {year}] model load '{selected_combo_key}': {e}\")\n",
    "\n",
    "    # Predict\n",
    "    try:\n",
    "        df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "    except Exception as e:\n",
    "        return (\"err\", f\"[{rgi_gl} {year}] predict '{selected_combo_key}': {e}\")\n",
    "\n",
    "    # Aggregate mean\n",
    "    data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "    meta_cols = [c for c in ['YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID'] if c in df_grid_monthly_a.columns]\n",
    "    grouped_ids_a = (\n",
    "        df_grid_monthly_a.groupby('ID')[meta_cols].first()\n",
    "        .merge(data_a, left_index=True, right_index=True, how='left')\n",
    "    )\n",
    "    months_per_id_a = df_grid_monthly_a.groupby('ID')['MONTHS'].unique()\n",
    "    grouped_ids_a = grouped_ids_a.merge(months_per_id_a, left_index=True, right_index=True)\n",
    "\n",
    "    grouped_ids_a.reset_index(inplace=True)\n",
    "    grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "    pred_y_annual = grouped_ids_a.copy()\n",
    "    pred_y_annual['PERIOD'] = 'annual'\n",
    "    pred_y_annual = pred_y_annual.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "    mean_MB = float(pred_y_annual['pred'].mean())\n",
    "    return (\"ok\", (rgi_gl, year, mean_MB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "path_rgi_alps = os.path.join(cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "rgi_id_list = os.listdir(path_rgi_alps)\n",
    "\n",
    "# --- build flat task list ---\n",
    "tasks = []\n",
    "YEAR_RE = re.compile(r\"_grid_(\\d{4})\\.parquet$\")\n",
    "for rgi_gl in rgi_id_list:\n",
    "    glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "    if not os.path.exists(glacier_path):\n",
    "        continue\n",
    "    cand_years = []\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        if fname.endswith(\".parquet\") and rgi_gl in fname:\n",
    "            m = YEAR_RE.search(fname)\n",
    "            if m:\n",
    "                cand_years.append((int(m.group(1)), os.path.join(glacier_path, fname)))\n",
    "    for year, file_path in sorted(cand_years):\n",
    "        tasks.append({\n",
    "            \"rgi_gl\": rgi_gl,\n",
    "            \"year\": year,\n",
    "            \"file_path\": file_path,\n",
    "            \"cfg\": cfg,                     # must be picklable\n",
    "            #\"device_str\": str(device),      # e.g. \"cpu\" for CPU-parallel\n",
    "            \"device_str\": 'cpu'\n",
    "        })\n",
    "\n",
    "# --- run in parallel (CPU) ---\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_file = os.path.join(\"logs\", f\"glacier_mean_MB_{current_date}.csv\")\n",
    "\n",
    "# write header once\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "\n",
    "index_counter = 0\n",
    "max_workers = min( max(1, os.cpu_count() - 1), 32 )  # cap so we don't overload\n",
    "with ProcessPoolExecutor(\n",
    "    max_workers=max_workers,\n",
    "    initializer=_worker_init,\n",
    "    initargs=(cfg.seed,),\n",
    ") as ex:\n",
    "    futures = [ex.submit(_process_one_file, t) for t in tasks]\n",
    "    for fut in as_completed(futures):\n",
    "        status, payload = fut.result()\n",
    "        if status == \"ok\":\n",
    "            rgi_gl, year, mean_MB = payload\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "            index_counter += 1\n",
    "        else:\n",
    "            # \"skip\" or \"err\"\n",
    "            print(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_rgi = cfg.dataPath+'GLAMOS/RGI/RGI2000-v7.0-G-11_central_europe/RGI2000-v7.0-G-11_central_europe.shp'\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "\n",
    "# check CRS\n",
    "print(gdf.crs)\n",
    "\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.set_index('RGIId', inplace=True, drop=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_df = pd.read_csv(\"logs/glacier_mean_MB_2025-10-08.csv\").drop(['Index'],\n",
    "                                                                    axis=1)\n",
    "\n",
    "output_df['area_gl'] = output_df['RGIId'].map(\n",
    "    lambda x: gdf_proj.loc[x, 'area_km2'])\n",
    "\n",
    "# yearly_mean_mb_CA = output_df.groupby('Year',\n",
    "#                                       as_index=False).agg({'Mean_MB': 'mean'})\n",
    "# yearly_cum_mb_CA = output_df.groupby('Year',\n",
    "#                                      as_index=False).agg({'Mean_MB': 'sum'})\n",
    "# yearly_cum_mb_CA['Cum_MB'] = yearly_cum_mb_CA['Mean_MB'].cumsum()\n",
    "# yearly_cum_mb_CA['Mean_MB'] = yearly_mean_mb_CA['Mean_MB']\n",
    "# # yearly_cum_mb_CA['Mean_MB'] = yearly_cum_mb_CA['Mean_MB'] / total_area\n",
    "# yearly_cum_mb_CA.head()\n",
    "\n",
    "df = output_df.copy()\n",
    "\n",
    "# annual change per glacier in Gt\n",
    "df[\"annual_change_gt\"] = (df[\"Mean_MB\"] * df[\"area_gl\"]) / 1e9\n",
    "\n",
    "# total annual change in Gt (sum across glaciers)\n",
    "annual_gt = df.groupby(\"Year\")[\"annual_change_gt\"].sum().reset_index(\n",
    "    name=\"Annual_MB_Gt\")\n",
    "\n",
    "# cumulative MB in Gt\n",
    "annual_gt[\"Cumulative_MB_Gt\"] = annual_gt[\"Annual_MB_Gt\"].cumsum()\n",
    "\n",
    "# compute weighted mean MB per year\n",
    "yearly_weighted = (output_df.groupby(\"Year\").apply(lambda g: (g[\"Mean_MB\"] * g[\n",
    "    \"area_gl\"]).sum() / g[\"area_gl\"].sum()).reset_index(name=\"Weighted_MB\"))\n",
    "\n",
    "print(yearly_weighted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glambie_df = pd.read_csv('glambie_values.csv')\n",
    "date_columns = [\n",
    "    'central_europe_dates', 'central_europe_start_dates',\n",
    "    'central_europe_end_dates'\n",
    "]\n",
    "\n",
    "glambie_df[date_columns] = glambie_df[date_columns].apply(\n",
    "    lambda x: x.round() - 1)\n",
    "glambie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plotting ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "\n",
    "# --------------------\n",
    "# Left: LSTM results\n",
    "# --------------------\n",
    "ax1 = axs[0]\n",
    "years = yearly_weighted['Year']\n",
    "\n",
    "# barplot: annual weighted MB (m w.e.)\n",
    "ax1.bar(years,\n",
    "        yearly_weighted['Weighted_MB'],\n",
    "        color=\"skyblue\",\n",
    "        label=\"Area-weighted annual MB\")\n",
    "ax1.set_ylabel(\"Annual MB (m w.e.)\", color=\"skyblue\")\n",
    "\n",
    "# lineplot: cumulative MB in Gt (secondary axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(annual_gt['Year'],\n",
    "         annual_gt['Cumulative_MB_Gt'],\n",
    "         color=\"red\",\n",
    "         marker=\"o\",\n",
    "         label=\"Cumulative MB\")\n",
    "ax2.set_ylabel(\"Cumulative MB (Gt)\", color=\"red\")\n",
    "\n",
    "ax1.set_title(\"Central Alps annual MB (LSTM)\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Right: GLAMBIE results\n",
    "# --------------------\n",
    "ax3 = axs[1]\n",
    "\n",
    "# annual MB (bars)\n",
    "ax3.bar(glambie_df['central_europe_end_dates'],\n",
    "        glambie_df['central_europe_annual_change_mwe'],\n",
    "        color=\"lightgreen\",\n",
    "        label=\"Annual MB (GLAMBIE)\")\n",
    "ax3.set_ylabel(\"Annual MB (m w.e.)\", color=\"lightgreen\")\n",
    "\n",
    "# cumulative MB (line, secondary axis)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(glambie_df['central_europe_dates'],\n",
    "         glambie_df['central_europe_cumulative_change_gt'],\n",
    "         color=\"darkgreen\",\n",
    "         marker=\"s\",\n",
    "         label=\"Cumulative MB (GLAMBIE)\")\n",
    "ax4.set_ylabel(\"Cumulative MB (Gt)\", color=\"darkgreen\")\n",
    "\n",
    "ax3.set_title(\"Central Europe MB (GLAMBIE)\")\n",
    "ax3.legend(loc=\"upper left\")\n",
    "ax4.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Formatting\n",
    "# --------------------\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both datasets use the same x-axis type\n",
    "years_lstm = yearly_weighted['Year']\n",
    "\n",
    "years_glambie = glambie_df['central_europe_end_dates']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# bar width\n",
    "width = 0.4\n",
    "\n",
    "# LSTM bars (slightly shifted left)\n",
    "ax.bar(years_lstm - 0.2,\n",
    "       yearly_weighted['Weighted_MB'],\n",
    "       width=width,\n",
    "       color=\"skyblue\",\n",
    "       label=\"LSTM Annual MB\")\n",
    "\n",
    "# GLAMBIE bars (slightly shifted right)\n",
    "ax.bar(years_glambie + 0.2,\n",
    "       glambie_df['central_europe_annual_change_mwe'],\n",
    "       width=width,\n",
    "       color=\"lightgreen\",\n",
    "       label=\"GLAMBIE Annual MB\")\n",
    "\n",
    "# formatting\n",
    "ax.set_ylabel(\"Annual MB (m w.e.)\")\n",
    "ax.set_title(\"Annual Mass Balance: LSTM vs GLAMBIE\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
