{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v'\n",
    "]\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "cfg.setFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': 9,\n",
    "    'Fs': 5,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': True,\n",
    "    'head_dropout': 0.0\n",
    "}\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# model_filename = f\"models/lstm_model_{current_date}_two_heads.pt\"\n",
    "model_filename = f\"models/lstm_model_2022-09-22_two_heads.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "# Evaluate on test\n",
    "model_filename = 'models/lstm_model_2025-09-22_two_heads.pt'\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation on PMB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(20, 15), sharex=True)\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                 axs=axs,\n",
    "                                 color_annual=color_dark_blue,\n",
    "                                 color_winter=color_pink,\n",
    "                                 custom_order=test_gl_per_el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass balance gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elv_per_id = data_monthly.groupby('ID').POINT_ELEVATION.mean()\n",
    "df_pred = test_df_preds.merge(elv_per_id,\n",
    "                              left_on='ID',\n",
    "                              right_index=True,\n",
    "                              how='left')\n",
    "\n",
    "# Stake data\n",
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)\n",
    "\n",
    "PATH_PREDICTIONS_LSTM_two_heads = os.path.join(\n",
    "    cfg.dataPath, \"GLAMOS\", \"distributed_MB_grids\",\n",
    "    \"MBM/testing_LSTM/LSTM_two_heads_best\")\n",
    "PATH_PREDICTIONS_NN = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "\n",
    "def list_years_from_lstm(glacier_name, path_pred_lstm):\n",
    "    base_lstm = os.path.join(path_pred_lstm, glacier_name)\n",
    "    pattern = os.path.join(base_lstm, \"*_annual.zarr\")\n",
    "    years = []\n",
    "    for f in glob.glob(pattern):\n",
    "        # Match filenames like 'aletsch_2010_annual.zarr'\n",
    "        m = re.match(r\".*[\\\\/](\\D+)?(\\d{4})_annual\\.zarr$\", f)\n",
    "        if m:\n",
    "            years.append(int(m.group(2)))\n",
    "    return sorted(set(years))\n",
    "\n",
    "\n",
    "# ---- helper: build full paths for a given year and check existence ----\n",
    "def paths_for_year(path_pred_lstm, glacier_name, year):\n",
    "    file_ann = f\"{year}_ann_fix_lv95.grid\"\n",
    "    grid_path_ann = os.path.join(cfg.dataPath, path_distributed_MB_glamos,\n",
    "                                 \"GLAMOS\", glacier_name, file_ann)\n",
    "    mbm_file_lstm = os.path.join(path_pred_lstm, glacier_name,\n",
    "                                 f\"{glacier_name}_{year}_annual.zarr\")\n",
    "    return grid_path_ann, mbm_file_lstm\n",
    "\n",
    "\n",
    "# ---- main per-year function (your original logic, wrapped) ----\n",
    "def process_year(glacier_name, path_pred_lstm, year):\n",
    "    # ---- paths ----\n",
    "    grid_path_ann, mbm_file_lstm = paths_for_year(path_pred_lstm, glacier_name,\n",
    "                                                  year)\n",
    "\n",
    "    # ---- GLAMOS (load + WGS84) ----\n",
    "    metadata_ann, grid_data_ann = load_grid_file(grid_path_ann)\n",
    "    ds_glamos_ann = convert_to_xarray_geodata(grid_data_ann, metadata_ann)\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv95_to_wgs84(ds_glamos_ann)\n",
    "\n",
    "    # pick GLAMOS data var (assume single data var)\n",
    "    glamos_var = 'grid_data'\n",
    "\n",
    "    # ---- LSTM (load + smooth) ----\n",
    "    ds_mbm_lstm = apply_gaussian_filter(\n",
    "        xr.open_dataset(mbm_file_lstm, engine=\"zarr\"))\n",
    "\n",
    "    # ---- coord name resolution ----\n",
    "    lon_lstm = \"lon\" if \"lon\" in ds_mbm_lstm.coords else \"longitude\"\n",
    "    lat_lstm = \"lat\" if \"lat\" in ds_mbm_lstm.coords else \"latitude\"\n",
    "\n",
    "    lon_gl = \"lon\" if \"lon\" in ds_glamos_wgs84_ann.coords else \"longitude\"\n",
    "    lat_gl = \"lat\" if \"lat\" in ds_glamos_wgs84_ann.coords else \"latitude\"\n",
    "\n",
    "    # ---- LSTM: raster -> dataframe + elevation merge ----\n",
    "    df_pred_lstm = (\n",
    "        ds_mbm_lstm[\"pred_masked\"].to_dataframe().reset_index().drop(\n",
    "            [\"x\", \"y\"], axis=1, errors=\"ignore\").merge(\n",
    "                ds_mbm_lstm[\"masked_elev\"].to_dataframe().reset_index().drop(\n",
    "                    [\"x\", \"y\"], axis=1, errors=\"ignore\"),\n",
    "                on=[lat_lstm, lon_lstm],\n",
    "                how=\"left\",\n",
    "            ).dropna().rename(\n",
    "                columns={\n",
    "                    \"pred_masked\": \"pred\",\n",
    "                    \"masked_elev\": \"POINT_ELEVATION\",\n",
    "                    lat_lstm: \"lat\",\n",
    "                    lon_lstm: \"lon\"\n",
    "                }))\n",
    "    df_pred_lstm[\"YEAR\"] = year\n",
    "    df_pred_lstm[\"PERIOD\"] = \"annual\"\n",
    "\n",
    "    # ---- 100 m binning (LSTM) ----\n",
    "    min_alt = np.floor(df_pred_lstm[\"POINT_ELEVATION\"].min() / 100) * 100\n",
    "    max_alt = np.ceil(df_pred_lstm[\"POINT_ELEVATION\"].max() / 100) * 100\n",
    "    bins = np.arange(min_alt, max_alt + 100, 100)\n",
    "    df_pred_lstm[\"altitude_interval\"] = pd.cut(df_pred_lstm[\"POINT_ELEVATION\"],\n",
    "                                               bins=bins,\n",
    "                                               right=False)\n",
    "    centers = {\n",
    "        iv: round((iv.left + iv.right) / 2)\n",
    "        for iv in df_pred_lstm[\"altitude_interval\"].cat.categories\n",
    "    }\n",
    "    df_pred_lstm[\"altitude_interval\"] = df_pred_lstm[\"altitude_interval\"].map(\n",
    "        centers)\n",
    "\n",
    "    # ---- GLAMOS: sample elevation from LSTM masked_elev (nearest) ----\n",
    "    elev_da = ds_mbm_lstm[\"masked_elev\"]\n",
    "    # standardize coord names to 'lat','lon' to make interp simple\n",
    "    elev_da_std = elev_da.rename({lat_lstm: \"lat\", lon_lstm: \"lon\"})\n",
    "    glamos_da = ds_glamos_wgs84_ann.rename({lat_gl: \"lat\", lon_gl: \"lon\"})\n",
    "\n",
    "    # Interp elevation onto GLAMOS grid\n",
    "    elev_on_glamos = elev_da_std.interp(lat=glamos_da[\"lat\"],\n",
    "                                        lon=glamos_da[\"lon\"],\n",
    "                                        method=\"nearest\")\n",
    "\n",
    "    # Build GLAMOS dataframe (with lat/lon, POINT_ELEVATION)\n",
    "    df_pred_glamos = (\n",
    "        glamos_da.to_dataframe(name=\"pred\").reset_index().drop(\n",
    "            [\"x\", \"y\"], axis=1, errors=\"ignore\").merge(\n",
    "                elev_on_glamos.to_dataframe(\n",
    "                    name=\"POINT_ELEVATION\").reset_index(),\n",
    "                on=[\"lat\", \"lon\"],\n",
    "                how=\"left\",\n",
    "            ).dropna(subset=[\"POINT_ELEVATION\"])  # ensure we can bin\n",
    "    )\n",
    "    df_pred_glamos[\"YEAR\"] = year\n",
    "    df_pred_glamos[\"PERIOD\"] = \"annual\"\n",
    "    df_pred_glamos[\"SOURCE\"] = \"GLAMOS\"  # optional tag; remove if not needed\n",
    "\n",
    "    # ---- 100 m binning (GLAMOS, same scheme) ----\n",
    "    # Use same bins as LSTM for direct comparability:\n",
    "    df_pred_glamos[\"altitude_interval\"] = pd.cut(\n",
    "        df_pred_glamos[\"POINT_ELEVATION\"], bins=bins, right=False)\n",
    "    # Some GLAMOS points may fall outside LSTM min/max by a tiny margin due to interp; drop NA bins then map centers\n",
    "    df_pred_glamos = df_pred_glamos.dropna(subset=[\"altitude_interval\"]).copy()\n",
    "    df_pred_glamos[\"altitude_interval\"] = df_pred_glamos[\n",
    "        \"altitude_interval\"].map(centers)\n",
    "\n",
    "    return df_pred_lstm, df_pred_glamos\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_all_years_df(glacier_name, path_pred_lstm):\n",
    "    \"\"\"\n",
    "    Process all available years for a glacier and return concatenated\n",
    "    prediction DataFrames for LSTM, GLAMOS, and both combined.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    glacier_name : str\n",
    "        Glacier name (used in file paths).\n",
    "    path_pred_lstm : str\n",
    "        Base path to LSTM predictions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_all_years_lstm : pd.DataFrame\n",
    "        Concatenated LSTM predictions for all years.\n",
    "    df_all_years_glamos : pd.DataFrame\n",
    "        Concatenated GLAMOS predictions for all years.\n",
    "    df_all_years : pd.DataFrame\n",
    "        Combined dataframe (LSTM + GLAMOS), with SOURCE column.\n",
    "    \"\"\"\n",
    "\n",
    "    years = list_years_from_lstm(glacier_name, path_pred_lstm)\n",
    "    print(years)\n",
    "\n",
    "    dfs_lstm, dfs_glamos = [], []\n",
    "\n",
    "    for y in years:\n",
    "        grid_path_ann, mbm_file_lstm = paths_for_year(path_pred_lstm,\n",
    "                                                      glacier_name, y)\n",
    "        if not os.path.exists(grid_path_ann):\n",
    "            print(f\"[skip] {y}: GLAMOS grid missing: {grid_path_ann}\")\n",
    "            continue\n",
    "        if not os.path.exists(mbm_file_lstm):\n",
    "            print(f\"[skip] {y}: LSTM zarr missing: {mbm_file_lstm}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_pred_lstm, df_pred_glamos = process_year(\n",
    "                glacier_name, path_pred_lstm, y)\n",
    "\n",
    "            if df_pred_lstm is not None and len(df_pred_lstm):\n",
    "                dfs_lstm.append(df_pred_lstm)\n",
    "            else:\n",
    "                print(f\"[warn] {y}: empty LSTM dataframe.\")\n",
    "\n",
    "            if df_pred_glamos is not None and len(df_pred_glamos):\n",
    "                dfs_glamos.append(df_pred_glamos)\n",
    "            else:\n",
    "                print(f\"[warn] {y}: empty GLAMOS dataframe.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[error] {y}: {e}\")\n",
    "\n",
    "    # --- Concatenate results ---\n",
    "    df_all_years_lstm = pd.concat(\n",
    "        dfs_lstm, ignore_index=True) if dfs_lstm else pd.DataFrame()\n",
    "\n",
    "    df_all_years_glamos = pd.concat(\n",
    "        dfs_glamos, ignore_index=True) if dfs_glamos else pd.DataFrame()\n",
    "\n",
    "    # Ensure SOURCE column exists and is consistent\n",
    "    if not df_all_years_lstm.empty:\n",
    "        df_all_years_lstm = df_all_years_lstm.assign(SOURCE=\"LSTM\")\n",
    "    if not df_all_years_glamos.empty:\n",
    "        if \"SOURCE\" not in df_all_years_glamos.columns:\n",
    "            df_all_years_glamos = df_all_years_glamos.assign(SOURCE=\"GLAMOS\")\n",
    "\n",
    "    # Combined dataframe (LSTM + GLAMOS)\n",
    "    if not df_all_years_lstm.empty or not df_all_years_glamos.empty:\n",
    "        df_all_years = (pd.concat([df_all_years_lstm, df_all_years_glamos],\n",
    "                                  ignore_index=True).drop(columns=['x', 'y'],\n",
    "                                                          errors='ignore'))\n",
    "    else:\n",
    "        df_all_years = pd.DataFrame()\n",
    "\n",
    "    # Quick check\n",
    "    if not df_all_years.empty:\n",
    "        print(f\"Years processed: {sorted(df_all_years['YEAR'].unique())}\")\n",
    "    else:\n",
    "        print(\"No years processed.\")\n",
    "\n",
    "    return df_all_years_lstm, df_all_years_glamos, df_all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin1 = ['Basodino', 'Adler', 'Hohlaub', 'Silvretta', 'Gries', 'Clariden']\n",
    "bin2 = ['Gietro', 'Schwarzberg', 'Allalin']\n",
    "bin3 = ['Findelen', 'Rhone', 'Corbassiere', 'Aletsch']\n",
    "fig, axs = plt.subplots(3, 2, figsize=(8, 10), sharex=True)\n",
    "axs = axs.flatten()\n",
    "for i, gl in enumerate(bin1):\n",
    "    df_lstm_gl, df_glamos_gl, df_all_gl = build_all_years_df(\n",
    "        gl.lower(), PATH_PREDICTIONS_LSTM_two_heads)\n",
    "    # if dataframe not None\n",
    "    if df_all_gl.empty:\n",
    "        print(f\"No data for glacier: {gl}\")\n",
    "        continue\n",
    "    ax = plot_mb_by_elevation(df_all_gl, df_stakes, glacier_name=gl, ax=axs[i])\n",
    "    axs[i].set_title(gl.capitalize())\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin0 = ['Schwarzbach', 'Sexrouge', 'Murtel']\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5), sharex=True)\n",
    "axs = axs.flatten()\n",
    "for i, gl in enumerate(bin0):\n",
    "    df_lstm_gl, df_glamos_gl, df_all_gl = build_all_years_df(\n",
    "        gl.lower(), PATH_PREDICTIONS_LSTM_two_heads)\n",
    "    # if dataframe not None\n",
    "    if df_all_gl.empty:\n",
    "        print(f\"No data for glacier: {gl}\")\n",
    "        continue\n",
    "    ax = plot_mb_by_elevation(df_all_gl, df_stakes, glacier_name=gl, ax=axs[i])\n",
    "    axs[i].set_title(gl.capitalize())\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glacier-wide MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_LSTM_two_heads = os.path.join(\n",
    "    cfg.dataPath, \"GLAMOS\", \"distributed_MB_grids\",\n",
    "    \"MBM/testing_LSTM/LSTM_two_heads_best\")\n",
    "PATH_PREDICTIONS_NN = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "    'MBM/testing_combis/glamos_dems_NN_SEB_full_OGGM')\n",
    "\n",
    "# Available glaciers (those with LSTM predictions)\n",
    "glaciers_in_glamos = set(os.listdir(PATH_PREDICTIONS_LSTM_two_heads))\n",
    "\n",
    "# Geodetic MB + per-glacier periods\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Areas (with clariden alias fix)\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area[\"clariden\"] = gl_area[\"claridenL\"]\n",
    "\n",
    "# Glaciers present in both geodetic periods and predictions, sorted by area (asc)\n",
    "glacier_list = sorted(\n",
    "    (g for g in periods_per_glacier.keys() if g in glaciers_in_glamos),\n",
    "    key=lambda g: gl_area.get(g, 0))\n",
    "print(\"Number of glaciers:\", len(glacier_list))\n",
    "print(\"Glaciers:\", glacier_list)\n",
    "\n",
    "# Run comparison\n",
    "df_lstm_two_heads = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=os.path.join(cfg.dataPath, path_SMB_GLAMOS_csv),\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_LSTM_two_heads,\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "df_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=os.path.join(cfg.dataPath, path_SMB_GLAMOS_csv),\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=PATH_PREDICTIONS_NN,\n",
    "    cfg=cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where any required columns are NaN\n",
    "df_lstm_two_heads = df_lstm_two_heads.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_lstm_two_heads = df_lstm_two_heads.sort_values(by='Area')\n",
    "df_lstm_two_heads['GLACIER'] = df_lstm_two_heads['GLACIER'].apply(\n",
    "    lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_lstm_two_heads[\"Geodetic MB\"],\n",
    "                                  df_lstm_two_heads[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_lstm_two_heads[\"Geodetic MB\"],\n",
    "                      df_lstm_two_heads[\"MBM MB\"])[0, 1]\n",
    "\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_lstm_two_heads,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered categorical bins\n",
    "bins=[0, 1, 5, 10, 100, np.inf]\n",
    "labels=['<1', '1-5', '5–10', '>10', '>100']\n",
    "df_lstm_two_heads = df_lstm_two_heads.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df_lstm_two_heads[\"Area_bin\"] = pd.cut(\n",
    "    df_lstm_two_heads[\"Area\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=False,\n",
    "    include_lowest=True,\n",
    "    ordered=True,\n",
    ")\n",
    "categories = list(df_lstm_two_heads[\"Area_bin\"].cat.categories)\n",
    "bins_in_use = [b for b in categories if (df_lstm_two_heads[\"Area_bin\"] == b).any()]\n",
    "\n",
    "df_lstm_two_heads.groupby(by=\"Area_bin\").GLACIER.unique().reset_index().GLACIER.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Prep (same as before, but copy to avoid SettingWithCopy) ---\n",
    "df = df_lstm_two_heads.dropna(subset=[\"Geodetic MB\", \"MBM MB\"]).copy()\n",
    "df[\"GLACIER\"] = df[\"GLACIER\"].str.capitalize()\n",
    "df = df.sort_values(by=\"Area\")\n",
    "\n",
    "# --- Per-glacier bias and corrected predictions ---\n",
    "# bias_g = E[MBM - Geodetic | glacier]\n",
    "df[\"bias_gl\"] = (df[\"MBM MB\"] - df[\"Geodetic MB\"]).groupby(\n",
    "    df[\"GLACIER\"]).transform(\"mean\")\n",
    "df[\"MBM MB_corr\"] = df[\"MBM MB\"] - df[\"bias_gl\"]\n",
    "\n",
    "# --- Metrics (original vs corrected) ---\n",
    "rmse_nn = root_mean_squared_error(df[\"Geodetic MB\"], df[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df[\"Geodetic MB\"], df[\"MBM MB\"])[0, 1]\n",
    "\n",
    "rmse_corr = root_mean_squared_error(df[\"Geodetic MB\"], df[\"MBM MB_corr\"])\n",
    "corr_corr = np.corrcoef(df[\"Geodetic MB\"], df[\"MBM MB_corr\"])[0, 1]\n",
    "\n",
    "print(f\"Original  RMSE={rmse_nn:.3f}, r={corr_nn:.3f}\")\n",
    "print(f\"Corrected RMSE={rmse_corr:.3f}, r={corr_corr:.3f}\")\n",
    "\n",
    "# --- Replot using your existing function ---\n",
    "# If plot_mbm_vs_geodetic_by_area_bin expects the column name \"MBM MB\",\n",
    "# make a copy with that column replaced by the corrected series.\n",
    "df_corr = df.copy()\n",
    "df_corr[\"MBM MB\"] = df_corr[\"MBM MB_corr\"]\n",
    "\n",
    "plot_mbm_vs_geodetic_by_area_bin(\n",
    "    df_corr,\n",
    "    bins=[0, 1, 5, 10, 100, np.inf],\n",
    "    labels=[\"<1\", \"1-5\", \"5–10\", \">10\", \">100\"],\n",
    "    max_bins=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.GLACIER == 'Schwarzberg'].bias_gl.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stake data ONCE instead of for every glacier\n",
    "stake_file = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                          \"CH_wgms_dataset_all.csv\")\n",
    "df_stakes = pd.read_csv(stake_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "GLACIER_NAME = 'clariden'\n",
    "bias_gl = df[df.GLACIER == GLACIER_NAME.capitalize()].bias_gl.unique()[0]\n",
    "df_lstm_two_heads_gl = df_lstm_two_heads[df_lstm_two_heads.GLACIER ==\n",
    "                                         GLACIER_NAME]\n",
    "df_nn_gl = df_nn[df_nn.GLACIER == GLACIER_NAME]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "plot_scatter_comparison(axs[0],\n",
    "                        df_lstm_two_heads_gl,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_annual,\n",
    "                        color_glamos=color_winter,\n",
    "                        title_suffix=\"(LSTM two heads)\")\n",
    "plot_scatter_comparison(axs[1],\n",
    "                        df_nn_gl,\n",
    "                        GLACIER_NAME,\n",
    "                        color_mbm=color_annual,\n",
    "                        color_glamos=color_winter,\n",
    "                        title_suffix=\"(MLP)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GLAMOS data\n",
    "GLAMOS_glwmb = get_GLAMOS_glwmb(GLACIER_NAME, cfg)\n",
    "\n",
    "MBM_glwmb_nn = mbm_glwd_pred(PATH_PREDICTIONS_NN, GLACIER_NAME)\n",
    "MBM_glwmb_nn.rename(columns={\"MBM Balance\": \"MBM Balance MLP\"}, inplace=True)\n",
    "\n",
    "MBM_glwmb_lstm = mbm_glwd_pred(PATH_PREDICTIONS_LSTM_two_heads, GLACIER_NAME)\n",
    "MBM_glwmb_lstm.rename(columns={\"MBM Balance\": \"MBM Balance LSTM\"},\n",
    "                      inplace=True)\n",
    "\n",
    "# Merge with GLAMOS data\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.join(GLAMOS_glwmb)\n",
    "MBM_glwmb_nn = MBM_glwmb_nn.dropna()\n",
    "\n",
    "MBM_glwmb = MBM_glwmb_nn.join(MBM_glwmb_lstm)\n",
    "\n",
    "# Plot the data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "MBM_glwmb.plot(ax=axs[0],\n",
    "               y=['MBM Balance LSTM', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_annual, color_winter])\n",
    "MBM_glwmb.plot(ax=axs[1],\n",
    "               y=['MBM Balance MLP', 'GLAMOS Balance'],\n",
    "               marker=\"o\",\n",
    "               color=[color_annual, color_winter])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_title(f\"{GLACIER_NAME.capitalize()} Glacier\", fontsize=24)\n",
    "    ax.set_ylabel(\"Mass Balance [m w.e.]\", fontsize=18)\n",
    "    ax.set_xlabel(\"Year\", fontsize=18)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "axs[0].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (LSTM)\", fontsize=16)\n",
    "axs[1].set_title(f\"{GLACIER_NAME.capitalize()} Glacier (MLP)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in MBM_glwmb_nn.index:\n",
    "    plot_mass_balance_comparison_annual(\n",
    "        glacier_name=GLACIER_NAME,\n",
    "        year=year,\n",
    "        cfg=cfg,\n",
    "        df_stakes=df_stakes,\n",
    "        path_distributed_mb=path_distributed_MB_glamos,\n",
    "        path_pred_lstm=PATH_PREDICTIONS_LSTM_two_heads,\n",
    "        path_pred_nn=PATH_PREDICTIONS_NN,\n",
    "        # bias_correction=bias_gl\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
