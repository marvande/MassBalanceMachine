{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "    \"millan_v\", \"svf\"\n",
    "]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = True\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf_IS.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking on glaciers:\n",
    "\n",
    "Model is trained on all glaciers --> \"Within sample\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "#                                             test_split_on='GLACIER',\n",
    "#                                             test_splits=TEST_GLACIERS,\n",
    "#                                             random_state=cfg.seed)\n",
    "\n",
    "# print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "#                                       test_set['splits_vals']))\n",
    "# test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "# print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "# print('Size of test set:', len(test_set['df_X']))\n",
    "# print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "#                                        train_set['splits_vals']))\n",
    "# print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v', \"svf\"\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# Look at the padding for one example\n",
    "key = ('adler', 2009, 11, 'winter')\n",
    "\n",
    "# find the index of this key\n",
    "try:\n",
    "    idx = ds_train.keys.index(key)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Key {key} not found in dataset.\")\n",
    "\n",
    "# fetch the corresponding sequence\n",
    "sequence = ds_train[idx]\n",
    "sequence['mv'], sequence['mw'], sequence['ma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_two_heads_param_search_progress_svf_IS_2025-10-20.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='avg_test_loss')\n",
    "df = pd.read_csv(log_path)\n",
    "df[\"avg_test_loss\"] = (df[\"test_rmse_a\"] + df[\"test_rmse_w\"]) / 2\n",
    "df.sort_values(by=\"avg_test_loss\", inplace=True)\n",
    "print(best_params)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_params = {\n",
    "#     'Fm': 8,\n",
    "#     'Fs': 6,\n",
    "#     'hidden_size': 128,\n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': False,\n",
    "#     'dropout': 0.1,\n",
    "#     'static_layers': 2,\n",
    "#     'static_hidden': [128, 64],\n",
    "#     'static_dropout': 0.1,\n",
    "#     'lr': 0.001,\n",
    "#     'weight_decay': 0.0,\n",
    "#     'loss_name': 'neutral',\n",
    "#     'two_heads': True,\n",
    "#     'head_dropout': 0.0,\n",
    "#     'loss_spec': None\n",
    "# }\n",
    "\n",
    "custom_params = best_params\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_two_heads_svf_IS.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "    \n",
    "model_filename = f\"models/lstm_model_2025-10-21_two_heads_svf_IS.pt\"\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_area = get_gl_area(cfg)\n",
    "gl_area[\"clariden\"] = gl_area[\"claridenL\"]\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "train_glaciers = {\n",
    "    'adler', 'albigna', 'aletsch', 'allalin', 'basodino', 'clariden',\n",
    "    'corbassiere', 'corvatsch', 'findelen', 'forno', 'gietro', 'gorner',\n",
    "    'gries', 'hohlaub', 'joeri', 'limmern', 'morteratsch', 'murtel', 'oberaar',\n",
    "    'otemma', 'pizol', 'plattalva', 'rhone', 'sanktanna', 'schwarzberg',\n",
    "    'sexrouge', 'silvretta', 'tortin', 'tsanfleuron'\n",
    "}\n",
    "test_gl_per_el = gl_per_el[list(train_glaciers)].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(7, 3, figsize=(25, 30), sharex=False)\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                       axs=axs,\n",
    "                                       color_annual=color_dark_blue,\n",
    "                                       color_winter=color_pink,\n",
    "                                       custom_order=test_gl_per_el,\n",
    "                                       add_text=True,\n",
    "                                       ax_xlim=None,\n",
    "                                       gl_area = gl_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.parallel_mb import MBJobConfig, run_glacier_mb\n",
    "\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/glamos_dems_LSTM_svf_IS')\n",
    "job = MBJobConfig(\n",
    "    cfg=cfg,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    fields_not_features=cfg.fieldsNotFeatures,\n",
    "    model_filename=model_filename,\n",
    "    custom_params=custom_params,\n",
    "    ds_train=ds_train,\n",
    "    train_idx=train_idx,\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    data_path=cfg.dataPath,\n",
    "    path_glacier_grid_glamos=path_glacier_grid_glamos,\n",
    "    path_xr_grids=os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM',\n",
    "                               'xr_masked_grids'),\n",
    "    path_save_glw=path_save_glw,\n",
    "    seed=cfg.seed,\n",
    "    max_workers=None,  # or an int\n",
    "    cpu_only=True,\n",
    "    ONLY_GEODETIC=False)\n",
    "\n",
    "# 3) Run\n",
    "summary = run_glacier_mb(job, glacier_list, periods_per_glacier)\n",
    "print(\"SUMMARY:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                                  df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "fig = plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
