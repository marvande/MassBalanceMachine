{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "    \"millan_v\", \"svf\"\n",
    "]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = True\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf_IS.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking on glaciers:\n",
    "\n",
    "Model is trained on all glaciers --> \"Within sample\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'ELEVATION_DIFFERENCE',\n",
    "    'pcsr'\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi',\n",
    "    'slope_sgi',\n",
    "    \"svf\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True,\n",
    "    normalize_target=False)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# Look at the padding for one example\n",
    "key = ('adler', 2009, 11, 'winter')\n",
    "\n",
    "# find the index of this key\n",
    "try:\n",
    "    idx = ds_train.keys.index(key)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Key {key} not found in dataset.\")\n",
    "\n",
    "# fetch the corresponding sequence\n",
    "sequence = ds_train[idx]\n",
    "sequence['mv'], sequence['mw'], sequence['ma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_two_heads_param_search_progress_no_oggm_IS_2025-10-22.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='test_rmse_a')\n",
    "df = pd.read_csv(log_path)\n",
    "df[\"avg_test_loss\"] = (df[\"test_rmse_a\"] + df[\"test_rmse_w\"]) / 2\n",
    "df.sort_values(by=\"avg_test_loss\", inplace=True)\n",
    "print(best_params)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_params = {\n",
    "#     'Fm': 8,\n",
    "#     'Fs': 6,\n",
    "#     'hidden_size': 128,\n",
    "#     'num_layers': 2,\n",
    "#     'bidirectional': False,\n",
    "#     'dropout': 0.1,\n",
    "#     'static_layers': 2,\n",
    "#     'static_hidden': [128, 64],\n",
    "#     'static_dropout': 0.1,\n",
    "#     'lr': 0.001,\n",
    "#     'weight_decay': 0.0,\n",
    "#     'loss_name': 'neutral',\n",
    "#     'two_heads': True,\n",
    "#     'head_dropout': 0.0,\n",
    "#     'loss_spec': None\n",
    "# }\n",
    "\n",
    "custom_params = best_params\n",
    "custom_params['Fm'] = 9\n",
    "custom_params['Fs'] = 6\n",
    "custom_params['two_heads'] = False\n",
    "\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_with_oggm_IS_one_head.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "#model_filename = f\"models/lstm_model_2025-10-31_no_oggm_IS.pt\"\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Evaluate on test\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_item(x):\n",
    "    return x.item() if x is not None else None\n",
    "\n",
    "\n",
    "print(\"Train dataset (after make_loaders):\")\n",
    "print(f\"  normalize_target = {ds_train_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_train_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_train_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_train_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_train_copy.y.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nTest dataset (after make_test_loader):\")\n",
    "print(f\"  normalize_target = {ds_test_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_test_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_test_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_test_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_test_copy.y.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = ds_train_copy.y.cpu().numpy()\n",
    "y_test = ds_test_copy.y.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(y_train, bins=30, alpha=0.6, label=\"Train\", density=True)\n",
    "plt.hist(y_test, bins=30, alpha=0.6, label=\"Test\", density=True)\n",
    "plt.axvline(y_train.mean(),\n",
    "            color='k',\n",
    "            linestyle='--',\n",
    "            lw=1,\n",
    "            label='mean (train)')\n",
    "plt.xlabel(\"Target (y)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\n",
    "    f\"Target distribution ({'normalized' if ds_train_copy.normalize_target else 'physical'} units)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_area = get_gl_area(cfg)\n",
    "gl_area[\"clariden\"] = gl_area[\"claridenL\"]\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "train_glaciers = {\n",
    "    'adler', 'albigna', 'aletsch', 'allalin', 'basodino', 'clariden',\n",
    "    'corbassiere', 'corvatsch', 'findelen', 'forno', 'gietro', 'gorner',\n",
    "    'gries', 'hohlaub', 'joeri', 'limmern', 'morteratsch', 'murtel', 'oberaar',\n",
    "    'otemma', 'pizol', 'plattalva', 'rhone', 'sanktanna', 'schwarzberg',\n",
    "    'sexrouge', 'silvretta', 'tortin', 'tsanfleuron'\n",
    "}\n",
    "test_gl_per_el = gl_per_el[list(train_glaciers)].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(7, 3, figsize=(25, 30), sharex=False)\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                       axs=axs,\n",
    "                                       color_annual=color_dark_blue,\n",
    "                                       color_winter=color_pink,\n",
    "                                       custom_order=test_gl_per_el,\n",
    "                                       add_text=True,\n",
    "                                       ax_xlim=None,\n",
    "                                       gl_area=gl_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.parallel_mb import MBJobConfig, run_glacier_mb\n",
    "\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/testing_LSTM/LSTM_with_oggm_IS_original_y_one_head')\n",
    "RUN = True\n",
    "if RUN:\n",
    "    job = MBJobConfig(\n",
    "        cfg=cfg,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        fields_not_features=cfg.fieldsNotFeatures,\n",
    "        model_filename=model_filename,\n",
    "        custom_params=custom_params,\n",
    "        ds_train=ds_train,\n",
    "        train_idx=train_idx,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        data_path=cfg.dataPath,\n",
    "        path_glacier_grid_glamos=path_glacier_grid_glamos,\n",
    "        path_xr_grids=os.path.join(cfg.dataPath, 'GLAMOS', 'topo',\n",
    "                                   'GLAMOS_DEM', 'xr_masked_grids'),\n",
    "        path_save_glw=path_save_glw,\n",
    "        seed=cfg.seed,\n",
    "        max_workers=20,  # or an int\n",
    "        cpu_only=True,\n",
    "        ONLY_GEODETIC=True,\n",
    "        denorm=ds_train_copy.normalize_target,\n",
    "        save_monthly=True)\n",
    "\n",
    "    # 3) Run\n",
    "    summary = run_glacier_mb(job, glacier_list, periods_per_glacier)\n",
    "    print(\"SUMMARY:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_months = [\n",
    "    'oct',\n",
    "    'nov',\n",
    "    'dec',\n",
    "    'jan',\n",
    "    'feb',\n",
    "    'mar',\n",
    "    'apr',\n",
    "    'may',\n",
    "    'jun',\n",
    "    'jul',\n",
    "    'aug',\n",
    "    'sep',\n",
    "]\n",
    "\n",
    "glaciers = os.listdir(path_save_glw)\n",
    "\n",
    "# Initialize final storage for all glacier data\n",
    "all_glacier_data = []\n",
    "\n",
    "# Loop over glaciers\n",
    "for glacier_name in tqdm(glaciers):\n",
    "    glacier_path = os.path.join(path_save_glw, glacier_name)\n",
    "    if not os.path.isdir(glacier_path):\n",
    "        continue  # skip non-directories\n",
    "\n",
    "    # Regex pattern adapted for current glacier name\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_[a-z]{{3}}\\.zarr')\n",
    "\n",
    "    # Extract available years\n",
    "    years = set()\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            years.add(int(match.group(1)))\n",
    "    years = sorted(years)\n",
    "\n",
    "    # Collect all year-month data\n",
    "    all_years_data = []\n",
    "    for year in years:\n",
    "        monthly_data = {}\n",
    "        for month in hydro_months:\n",
    "            zarr_path = os.path.join(glacier_path,\n",
    "                                     f'{glacier_name}_{year}_{month}.zarr')\n",
    "            if not os.path.exists(zarr_path):\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(zarr_path)\n",
    "            df = ds.pred_masked.to_dataframe().drop(['x', 'y'],\n",
    "                                                    axis=1).reset_index()\n",
    "            df_pred_months = df[df.pred_masked.notna()]\n",
    "\n",
    "            df_el = ds.masked_elev.to_dataframe().drop(['x', 'y'],\n",
    "                                                       axis=1).reset_index()\n",
    "            df_elv_months = df_el[df.pred_masked.notna()]\n",
    "\n",
    "            df_pred_months['elevation'] = df_elv_months.masked_elev.values\n",
    "\n",
    "            monthly_data[month] = df_pred_months.pred_masked.values\n",
    "\n",
    "        if monthly_data:\n",
    "            df_months = pd.DataFrame(monthly_data)\n",
    "            df_months['year'] = year\n",
    "            df_months['glacier'] = glacier_name  # add glacier name\n",
    "            df_months['elevation'] = df_pred_months.elevation.values\n",
    "            all_years_data.append(df_months)\n",
    "\n",
    "    # Concatenate this glacier's data\n",
    "    if all_years_data:\n",
    "        df_glacier = pd.concat(all_years_data, axis=0, ignore_index=True)\n",
    "        all_glacier_data.append(df_glacier)\n",
    "\n",
    "# Final full DataFrame for all glaciers\n",
    "df_months_LSTM = pd.concat(all_glacier_data, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_file_glamos(glacier, year, period=\"winter\"):\n",
    "    suffix = \"ann\" if period == \"annual\" else \"win\"\n",
    "    base = os.path.join(cfg.dataPath, path_distributed_MB_glamos, \"GLAMOS\",\n",
    "                        glacier)\n",
    "    cand_lv95 = os.path.join(base, f\"{year}_{suffix}_fix_lv95.grid\")\n",
    "    cand_lv03 = os.path.join(base, f\"{year}_{suffix}_fix_lv03.grid\")\n",
    "    if os.path.exists(cand_lv95):\n",
    "        return cand_lv95, \"lv95\"\n",
    "    if os.path.exists(cand_lv03):\n",
    "        return cand_lv03, \"lv03\"\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_glamos_wgs84(glacier, year, period):\n",
    "    \"\"\"Load one GLAMOS .grid file and return it as an xarray in WGS84.\"\"\"\n",
    "    path, cs = pick_file_glamos(glacier, year, period)\n",
    "    if path is None:\n",
    "        return None\n",
    "    meta, arr = load_grid_file(path)\n",
    "    da = convert_to_xarray_geodata(arr, meta)\n",
    "    if cs == \"lv03\":\n",
    "        return transform_xarray_coords_lv03_to_wgs84(da)\n",
    "    elif cs == \"lv95\":\n",
    "        return transform_xarray_coords_lv95_to_wgs84(da)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_glamos(glacier_years, path_glamos):\n",
    "    \"\"\"\n",
    "    Loads both annual and winter GLAMOS grids for all glaciers and years,\n",
    "    interpolates DEM elevation onto MB grids, and returns two DataFrames:\n",
    "    df_GLAMOS_w, df_GLAMOS_a.\n",
    "    \"\"\"\n",
    "    all_glacier_data_w, all_glacier_data_a = [], []\n",
    "\n",
    "    for glacier_name in tqdm(glacier_years.keys(), desc=\"Loading GLAMOS data\"):\n",
    "        glacier_path = os.path.join(path_glamos, glacier_name)\n",
    "        if not os.path.isdir(glacier_path):\n",
    "            continue\n",
    "\n",
    "        years = glacier_years[glacier_name]\n",
    "        all_years_w, all_years_a = [], []\n",
    "\n",
    "        for year in years:\n",
    "            # --- Load DEM ---\n",
    "            dem_path = (cfg.dataPath + path_GLAMOS_topo +\n",
    "                        f\"xr_masked_grids/{glacier_name}_{year}.zarr\")\n",
    "            if not os.path.exists(dem_path):\n",
    "                continue\n",
    "            ds_dem = xr.open_zarr(dem_path)\n",
    "\n",
    "            # --- Load Winter MB ---\n",
    "            ds_w = load_glamos_wgs84(glacier_name, year, period=\"winter\")\n",
    "            if ds_w is not None:\n",
    "                masked_elev_interp = ds_dem[\"masked_elev\"].interp_like(\n",
    "                    ds_w, method=\"nearest\")\n",
    "                masked_elev_interp = masked_elev_interp.assign_coords(x=ds_w.x,\n",
    "                                                                      y=ds_w.y)\n",
    "\n",
    "                ds_merged_w = xr.merge(\n",
    "                    [\n",
    "                        ds_w.to_dataset(name=\"mb\"),\n",
    "                        masked_elev_interp.to_dataset(name=\"masked_elev\"),\n",
    "                    ],\n",
    "                    compat=\"override\",\n",
    "                )\n",
    "\n",
    "                df_w = ds_merged_w.to_dataframe().reset_index()\n",
    "                df_w = df_w[df_w[\"mb\"].notna() & df_w[\"masked_elev\"].notna()]\n",
    "                df_w = df_w[[\"x\", \"y\", \"mb\", \"masked_elev\"]]\n",
    "                df_w[\"year\"] = year\n",
    "                df_w[\"glacier\"] = glacier_name\n",
    "                df_w[\"period\"] = \"winter\"\n",
    "                all_years_w.append(df_w)\n",
    "\n",
    "            # --- Load Annual MB ---\n",
    "            ds_a = load_glamos_wgs84(glacier_name, year, period=\"annual\")\n",
    "            if ds_a is not None:\n",
    "                masked_elev_interp = ds_dem[\"masked_elev\"].interp_like(\n",
    "                    ds_a, method=\"nearest\")\n",
    "                masked_elev_interp = masked_elev_interp.assign_coords(x=ds_a.x,\n",
    "                                                                      y=ds_a.y)\n",
    "\n",
    "                ds_merged_a = xr.merge(\n",
    "                    [\n",
    "                        ds_a.to_dataset(name=\"mb\"),\n",
    "                        masked_elev_interp.to_dataset(name=\"masked_elev\"),\n",
    "                    ],\n",
    "                    compat=\"override\",\n",
    "                )\n",
    "\n",
    "                df_a = ds_merged_a.to_dataframe().reset_index()\n",
    "                df_a = df_a[df_a[\"mb\"].notna() & df_a[\"masked_elev\"].notna()]\n",
    "                df_a = df_a[[\"x\", \"y\", \"mb\", \"masked_elev\"]]\n",
    "                df_a[\"year\"] = year\n",
    "                df_a[\"glacier\"] = glacier_name\n",
    "                df_a[\"period\"] = \"annual\"\n",
    "                all_years_a.append(df_a)\n",
    "\n",
    "        # --- Concatenate per glacier ---\n",
    "        if all_years_w:\n",
    "            df_glacier_w = pd.concat(all_years_w, ignore_index=True)\n",
    "            all_glacier_data_w.append(df_glacier_w)\n",
    "        if all_years_a:\n",
    "            df_glacier_a = pd.concat(all_years_a, ignore_index=True)\n",
    "            all_glacier_data_a.append(df_glacier_a)\n",
    "\n",
    "    # --- Final combined DataFrames ---\n",
    "    df_GLAMOS_w = (pd.concat(all_glacier_data_w, ignore_index=True)\n",
    "                   if all_glacier_data_w else pd.DataFrame())\n",
    "    df_GLAMOS_a = (pd.concat(all_glacier_data_a, ignore_index=True)\n",
    "                   if all_glacier_data_a else pd.DataFrame())\n",
    "\n",
    "    # --- Drop x/y and rename elevation column ---\n",
    "    if not df_GLAMOS_w.empty:\n",
    "        df_GLAMOS_w = df_GLAMOS_w.drop([\"x\", \"y\", \"period\"],\n",
    "                                       axis=1).rename(columns={\n",
    "                                           \"masked_elev\": \"elevation\",\n",
    "                                           \"mb\": \"apr\"\n",
    "                                       })\n",
    "    if not df_GLAMOS_a.empty:\n",
    "        df_GLAMOS_a = df_GLAMOS_a.drop([\"x\", \"y\", \"period\"],\n",
    "                                       axis=1).rename(columns={\n",
    "                                           \"masked_elev\": \"elevation\",\n",
    "                                           \"mb\": \"sep\"\n",
    "                                       })\n",
    "\n",
    "    return df_GLAMOS_w, df_GLAMOS_a\n",
    "\n",
    "\n",
    "PATH_GLAMOS = os.path.join(cfg.dataPath, path_distributed_MB_glamos, 'GLAMOS')\n",
    "glaciers = os.listdir(PATH_GLAMOS)\n",
    "\n",
    "glacier_years = (\n",
    "    df_months_LSTM.groupby('glacier')['year'].unique().apply(sorted).to_dict())\n",
    "\n",
    "df_GLAMOS_w, df_GLAMOS_a = load_all_glamos(glacier_years, PATH_GLAMOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Glacier-wide annual mean MB per year ---\n",
    "glwd_months_LSTM = df_months_LSTM.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_w = df_GLAMOS_w.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_a = df_GLAMOS_a.groupby(['glacier', 'year']).mean().reset_index()\n",
    "\n",
    "# --- 2. Compute the intersection of valid glacier–year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_months_LSTM['glacier'], glwd_months_LSTM['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_w['glacier'], glwd_months_GLAMOS_w['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_a['glacier'], glwd_months_GLAMOS_a['year']))\n",
    ")\n",
    "\n",
    "# --- 3. Helper function for filtering by glacier–year pairs ---\n",
    "def filter_to_valid(df):\n",
    "    return df[df[['glacier', 'year']].apply(tuple, axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "# --- 4. Apply consistent filtering to all datasets ---\n",
    "glwd_months_LSTM_filtered = filter_to_valid(glwd_months_LSTM)\n",
    "glwd_months_GLAMOS_filtered_w = filter_to_valid(glwd_months_GLAMOS_w)\n",
    "glwd_months_GLAMOS_filtered_a = filter_to_valid(glwd_months_GLAMOS_a)\n",
    "\n",
    "print(\n",
    "    len(glwd_months_GLAMOS_filtered_w),\n",
    "    len(glwd_months_GLAMOS_filtered_a),\n",
    "    len(glwd_months_LSTM_filtered),\n",
    ")\n",
    "\n",
    "# --- 5. Prepare for plotting ---\n",
    "df_months_long = prepare_monthly_long_df(\n",
    "    glwd_months_LSTM_filtered,\n",
    "    glwd_months_LSTM_filtered,\n",
    "    glwd_months_GLAMOS_filtered_w,\n",
    "    glwd_months_GLAMOS_filtered_a,\n",
    ")\n",
    "\n",
    "df_months_long.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = df_months_long.min()[[\n",
    "    'mb_nn', 'mb_glamos'\n",
    "]].min(), df_months_long.max()[['mb_nn', 'mb_glamos']].max()\n",
    "fig = plot_monthly_joyplot_single(df_months_long,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)))\n",
    "fig.savefig('figures/CH_LSTM_vs_NN_monthly_joyplot_glwd.png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
