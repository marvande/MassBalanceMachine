{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "    \"millan_v\", \"svf\"\n",
    "]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v', \"svf\"\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "len(existing_glaciers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design of folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
    "\n",
    "def _try_make_strata(gl_stats, q_elev, q_year):\n",
    "    \"\"\"Return strata labels and counts; may drop duplicate bins if ranges are narrow.\"\"\"\n",
    "    elev_bin = pd.qcut(gl_stats[\"med_elev\"], q=q_elev, labels=False, duplicates=\"drop\")\n",
    "    year_bin = pd.qcut(gl_stats[\"med_year\"], q=q_year, labels=False, duplicates=\"drop\")\n",
    "    strata = elev_bin.astype(str) + \"_\" + year_bin.astype(str)\n",
    "    counts = strata.value_counts()\n",
    "    return strata, counts\n",
    "\n",
    "def _try_make_1d_strata(gl_stats, q_bins, on=\"med_elev\"):\n",
    "    \"\"\"One-dimensional fallback stratification.\"\"\"\n",
    "    one_bin = pd.qcut(gl_stats[on], q=q_bins, labels=False, duplicates=\"drop\")\n",
    "    strata = one_bin.astype(str)\n",
    "    counts = strata.value_counts()\n",
    "    return strata, counts\n",
    "\n",
    "def make_stratified_glacier_folds(\n",
    "    data_monthly, n_splits=5, random_state=42, max_bins=5, verbose=True\n",
    "):\n",
    "    dm = data_monthly.copy()\n",
    "\n",
    "    # Per-glacier summaries used for stratification\n",
    "    gl_stats = (\n",
    "        dm.groupby(\"GLACIER\", as_index=False)\n",
    "          .agg(med_elev=(\"POINT_ELEVATION\", \"median\"),\n",
    "               med_year=(\"YEAR\", \"median\"),\n",
    "               n_samples=(\"YEAR\", \"size\"))\n",
    "    )\n",
    "    n_glaciers = len(gl_stats)\n",
    "    if verbose:\n",
    "        print(f\"Total glaciers: {n_glaciers}\")\n",
    "\n",
    "    # Guard: if very few glaciers, reduce n_splits\n",
    "    if n_glaciers < n_splits:\n",
    "        if verbose:\n",
    "            print(f\"Reducing n_splits from {n_splits} to {n_glaciers} (not enough glaciers).\")\n",
    "        n_splits = n_glaciers\n",
    "\n",
    "    # 2D stratification with automatic coarsening\n",
    "    best = None\n",
    "    for q_elev in range(max_bins, 1, -1):\n",
    "        for q_year in range(max_bins, 1, -1):\n",
    "            strata, counts = _try_make_strata(gl_stats, q_elev, q_year)\n",
    "            if counts.min() >= n_splits:\n",
    "                best = (\"2d\", q_elev, q_year, strata)\n",
    "                if verbose:\n",
    "                    print(f\"Using 2D strata: elev_bins={q_elev}, year_bins={q_year}\")\n",
    "                break\n",
    "        if best is not None:\n",
    "            break\n",
    "\n",
    "    # 1D fallback (first try elevation, then year), with coarsening\n",
    "    if best is None:\n",
    "        for on in (\"med_elev\", \"med_year\"):\n",
    "            for q_bins in range(max_bins, 1, -1):\n",
    "                strata, counts = _try_make_1d_strata(gl_stats, q_bins, on=on)\n",
    "                if counts.min() >= n_splits:\n",
    "                    best = (\"1d\", on, q_bins, strata)\n",
    "                    if verbose:\n",
    "                        print(f\"Using 1D strata on {on}: bins={q_bins}\")\n",
    "                    break\n",
    "            if best is not None:\n",
    "                break\n",
    "\n",
    "    folds = []\n",
    "    glacier_to_fold = {}\n",
    "\n",
    "    if best is not None:\n",
    "        # We have viable strata for StratifiedGroupKFold\n",
    "        if best[0] == \"2d\":\n",
    "            _, q_elev, q_year, strata = best\n",
    "        else:\n",
    "            _, on, q_bins, strata = best\n",
    "\n",
    "        sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "        X_dummy = np.zeros((len(gl_stats), 1))\n",
    "        y_strata = np.asarray(strata)\n",
    "        groups = gl_stats[\"GLACIER\"].values\n",
    "\n",
    "        for _, test_idx in sgkf.split(X_dummy, y=y_strata, groups=groups):\n",
    "            test_glaciers = set(gl_stats.loc[test_idx, \"GLACIER\"])\n",
    "            folds.append(test_glaciers)\n",
    "    else:\n",
    "        # Final fallback: plain GroupKFold (no stratification)\n",
    "        if verbose:\n",
    "            print(\"Falling back to GroupKFold (no valid strata with min class size ≥ n_splits).\")\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        # dummy features; groups are glaciers\n",
    "        X_dummy = np.zeros((len(gl_stats), 1))\n",
    "        groups = gl_stats[\"GLACIER\"].values\n",
    "        # Build an index to glacier rows in gl_stats\n",
    "        # We pass per-glacier rows through GroupKFold by expanding to sample-level via merge\n",
    "        # Simpler: just split on the gl_stats rows:\n",
    "        for _, test_idx in gkf.split(X_dummy, groups=groups):\n",
    "            test_glaciers = set(gl_stats.loc[test_idx, \"GLACIER\"])\n",
    "            folds.append(test_glaciers)\n",
    "\n",
    "    # Optional: summary per fold\n",
    "    if verbose:\n",
    "        print(\"\\nFold summary (glaciers, samples, elev range, year span):\")\n",
    "        for i, test_glaciers in enumerate(folds, start=1):\n",
    "            sub = dm[dm[\"GLACIER\"].isin(test_glaciers)]\n",
    "            n_g = len(test_glaciers)\n",
    "            n_s = len(sub)\n",
    "            elev_min, elev_med, elev_max = sub[\"POINT_ELEVATION\"].min(), sub[\"POINT_ELEVATION\"].median(), sub[\"POINT_ELEVATION\"].max()\n",
    "            year_min, year_max = int(sub[\"YEAR\"].min()), int(sub[\"YEAR\"].max())\n",
    "            print(f\"Fold {i}: {n_g:2d} glaciers | {n_s:6d} samples | elev [{elev_min:.0f}, {elev_med:.0f}, {elev_max:.0f}] m | years {year_min}–{year_max}\")\n",
    "\n",
    "    glacier_to_fold = {g: i for i, s in enumerate(folds) for g in s}\n",
    "    return folds, glacier_to_fold\n",
    "\n",
    "folds, glacier_to_fold = make_stratified_glacier_folds(data_monthly, n_splits=5, random_state=42, max_bins=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Balanced GroupKFold split by GLACIER\n",
    "# from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# # Optional: shuffle rows once for reproducibility of the split order\n",
    "# dm = data_monthly.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# groups = dm[\"GLACIER\"]\n",
    "# gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# folds = []  # list of sets of glacier names (test fold per split)\n",
    "# glacier_to_fold = {}  # map glacier -> fold id (0..4)\n",
    "\n",
    "# for fold_id, (train_idx, test_idx) in enumerate(gkf.split(dm, groups=groups)):\n",
    "#     test_glaciers = set(dm.loc[test_idx, \"GLACIER\"].unique())\n",
    "#     folds.append(test_glaciers)\n",
    "#     for g in test_glaciers:\n",
    "#         glacier_to_fold[g] = fold_id\n",
    "\n",
    "# # Example: get train/test masks for fold 0\n",
    "# fold = 0\n",
    "# test_mask = dm[\"GLACIER\"].isin(folds[fold])\n",
    "# train_mask = ~test_mask\n",
    "\n",
    "# # --- Print summary per fold ---\n",
    "# print(\"Fold summary:\\n\")\n",
    "# for fold_id, test_glaciers in enumerate(folds):\n",
    "#     n_glaciers = len(test_glaciers)\n",
    "#     n_samples = dm[dm[\"GLACIER\"].isin(test_glaciers)].shape[0]\n",
    "#     print(\n",
    "#         f\"Fold {fold_id+1}: {n_glaciers:2d} glaciers, {n_samples:5d} samples\")\n",
    "\n",
    "# # Optional: check total consistency\n",
    "# print(\"\\nTotal glaciers:\", len(set.union(*folds)))\n",
    "# print(\"Total samples :\",\n",
    "#       sum(dm[dm[\"GLACIER\"].isin(f)].shape[0] for f in folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random split:\n",
    "# glaciers = np.array(sorted(data_monthly[\"GLACIER\"].unique()))\n",
    "# rng = np.random.default_rng(42)  # reproducible\n",
    "# rng.shuffle(glaciers)\n",
    "\n",
    "# folds_simple = [set(glaciers[i::5]) for i in range(5)]\n",
    "# glacier_to_fold_simple = {g: i for i, s in enumerate(folds_simple) for g in s}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 6,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.1,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': True,\n",
    "    'head_dropout': 0.0,\n",
    "    'loss_spec': None\n",
    "}\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_preds_all = pd.DataFrame()\n",
    "for i, test_glaciers in enumerate(folds):\n",
    "    print(f\"\\n--- Fold {i+1} / {len(folds)} ---\")\n",
    "    print(\"Test glaciers:\", test_glaciers)\n",
    "    seed_all(cfg.seed)\n",
    "\n",
    "    # Define training glaciers correctly\n",
    "    train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "\n",
    "    data_test = data_monthly[data_monthly.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "    data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "\n",
    "    if len(data_train) == 0:\n",
    "        print(\"Warning: No training data available!\")\n",
    "    else:\n",
    "        test_perc = (len(data_test) / len(data_train)) * 100\n",
    "\n",
    "    splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                                test_split_on='GLACIER',\n",
    "                                                test_splits=test_glaciers,\n",
    "                                                random_state=cfg.seed)\n",
    "    # Validation and train split:\n",
    "    data_train = train_set['df_X']\n",
    "    data_train['y'] = train_set['y']\n",
    "\n",
    "    data_test = test_set['df_X']\n",
    "    data_test['y'] = test_set['y']\n",
    "\n",
    "    df_train = data_train.copy()\n",
    "    df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "    df_test = data_test.copy()\n",
    "    df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "    # --- build train dataset from dataframe ---\n",
    "    ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_train,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True)\n",
    "\n",
    "    ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "        df_test,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        months_head_pad=months_head_pad,\n",
    "        expect_target=True)\n",
    "\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "    train_dl, val_dl = ds_train.make_loaders(\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        batch_size_train=64,\n",
    "        batch_size_val=128,\n",
    "        seed=cfg.seed,\n",
    "        fit_and_transform=\n",
    "        True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "        shuffle_train=True,\n",
    "        use_weighted_sampler=True  # use weighted sampler for training\n",
    "    )\n",
    "\n",
    "    # --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "    test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_test, ds_train, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "    # --- build model, resolve loss, train, reload best ---\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params,\n",
    "                                                       device)\n",
    "    loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "    model_filename = f\"models/lstm_model_{current_date}_fold_{i}.pt\"\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test\n",
    "    state = torch.load(model_filename, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "        device, test_dl, ds_test)\n",
    "    test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "        'RMSE_winter']\n",
    "\n",
    "    print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "        test_rmse_a, test_rmse_w))\n",
    "    \n",
    "    test_df_preds[\"fold\"] = i\n",
    "    test_df_preds_all = pd.concat([test_df_preds_all, test_df_preds],\n",
    "                                  axis=0,\n",
    "                                  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds_all,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds_all,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "test_gl_per_el = list(folds[4])\n",
    "fig, axs = plt.subplots(3, 3, figsize=(25, 18), sharex=True)\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds_all['gl_elv'] = test_df_preds_all['GLACIER'].map(gl_per_el)\n",
    "\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(\n",
    "    test_df_preds_all,\n",
    "    axs=axs,\n",
    "    color_annual=color_dark_blue,\n",
    "    color_winter=color_pink,\n",
    "    custom_order=test_gl_per_el)\n",
    "\n",
    "axs[3].set_ylabel(\"Modelled PMB [m w.e.]\", fontsize=20)\n",
    "\n",
    "fig.supxlabel('Observed PMB [m w.e.]', fontsize=20, y=0.06)\n",
    "# two distinct handles\n",
    "legend_scatter_annual = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_annual,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Annual')\n",
    "\n",
    "legend_scatter_winter = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_winter,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Winter')\n",
    "\n",
    "# if you already have other handles (e.g., bands/means), append these:\n",
    "# handles = existing_handles + [legend_scatter_annual, legend_scatter_winter]\n",
    "handles = [legend_scatter_annual, legend_scatter_winter]\n",
    "\n",
    "# You can let matplotlib use the labels from the handles; no need to pass `labels=...`\n",
    "fig.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 0.05),\n",
    "           ncol=4,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
