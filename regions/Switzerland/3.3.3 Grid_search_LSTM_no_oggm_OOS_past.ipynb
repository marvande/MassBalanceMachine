{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\"aspect_sgi\", \"slope_sgi\", \"svf\"]\n",
    "\n",
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_gs_no_oggm.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_gs_no_oggm_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "data_test_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    TEST_GLACIERS)]\n",
    "data_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "\n",
    "splits_Aug_, test_set_Aug_, train_set_Aug_ = get_CV_splits(\n",
    "    dataloader_gl_Aug_,\n",
    "    test_split_on='GLACIER',\n",
    "    test_splits=TEST_GLACIERS,\n",
    "    random_state=cfg.seed)\n",
    "\n",
    "# # Validation and train split:\n",
    "data_train_Aug_ = train_set_Aug_['df_X']\n",
    "data_train_Aug_['y'] = train_set_Aug_['y']\n",
    "data_test_Aug_ = test_set_Aug_['df_X']\n",
    "data_test_Aug_['y'] = test_set_Aug_['y']\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train_Aug_ = data_train_Aug_.copy()\n",
    "df_train_Aug_['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test_Aug_ = data_test_Aug_.copy()\n",
    "df_test_Aug_['PERIOD'] = df_test_Aug_['PERIOD'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = build_combined_LSTM_dataset(df_loss=data_train,\n",
    "                                       df_full=data_train_Aug_,\n",
    "                                       monthly_cols=MONTHLY_COLS,\n",
    "                                       static_cols=STATIC_COLS,\n",
    "                                       months_head_pad=months_head_pad_Aug_,\n",
    "                                       months_tail_pad=months_tail_pad_Aug_,\n",
    "                                       normalize_target=True,\n",
    "                                       expect_target=True)\n",
    "\n",
    "ds_test = build_combined_LSTM_dataset(df_loss=data_test,\n",
    "                                      df_full=data_test_Aug_,\n",
    "                                      monthly_cols=MONTHLY_COLS,\n",
    "                                      static_cols=STATIC_COLS,\n",
    "                                      months_head_pad=months_head_pad_Aug_,\n",
    "                                      months_tail_pad=months_tail_pad_Aug_,\n",
    "                                      normalize_target=True,\n",
    "                                      expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list, month_pos = mbm.data_processing.utils._rebuild_month_index(\n",
    "    months_head_pad_Aug_, months_tail_pad_Aug_)\n",
    "month_order = [m for m, _ in sorted(month_pos.items(), key=lambda x: x[1])]\n",
    "print(\"Month order used in sequences:\", month_order)\n",
    "\n",
    "inspect_LSTM_sample(ds_train, 0, month_labels=month_order)\n",
    "inspect_LSTM_sample(ds_train, 10, month_labels=month_order)\n",
    "inspect_LSTM_sample(ds_train, 150, month_labels=month_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_item(x):\n",
    "    return x.item() if x is not None else None\n",
    "\n",
    "\n",
    "print(\"Train dataset (after make_loaders):\")\n",
    "print(f\"  normalize_target = {ds_train.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_train.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_train.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_train.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_train.y.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nTest dataset (after make_test_loader):\")\n",
    "print(f\"  normalize_target = {ds_test.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_test.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_test.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_test.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_test.y.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "    \"weight_decay\": [0.0, 1e-5, 1e-4],\n",
    "    \"hidden_size\": [64, 128],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"dropout\": [0.1, 0.2],  # force some dropout\n",
    "    \"head_dropout\": [0, 0.1],\n",
    "}\n",
    "\n",
    "static = [\n",
    "    #(0, 0, None),  # identity (use 0 here for robustness)\n",
    "    (2, [128, 64], 0.1),  # small two-layer MLP\n",
    "]\n",
    "\n",
    "\n",
    "def pack(static_triplet):\n",
    "    sl, sh, sd = static_triplet\n",
    "    return dict(\n",
    "        static_layers=sl,\n",
    "        static_hidden=sh,\n",
    "        static_dropout=sd,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---- constants that should be the same for every sample ----\n",
    "const_params = {\n",
    "    \"Fm\": ds_train.Xm.shape[-1],  # monthly features\n",
    "    \"Fs\": ds_train.Xs.shape[-1],  # static features\n",
    "    \"bidirectional\": False,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"loss_spec\": None,\n",
    "    \"two_heads\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def grid_iter_with_static_and_const(grid, static_list, const):\n",
    "    keys = list(grid.keys())\n",
    "    for values in product(*(grid[k] for k in keys), static_list):\n",
    "        params = dict(zip(keys, values[:-1]))  # non-static hyperparams\n",
    "        params.update(pack(values[-1]))  # add static config\n",
    "        params.update(const)  # add constants\n",
    "        yield params\n",
    "\n",
    "\n",
    "# ---- generate all sampled param sets ----\n",
    "sampled_params = list(\n",
    "    grid_iter_with_static_and_const(param_grid, static, const_params))\n",
    "print(len(sampled_params))\n",
    "print(sampled_params[0])  # preview one combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    log_filename = f'logs/lstm_one_heads_param_search_progress_no_oggm_{datetime.now().strftime(\"%Y-%m-%d\")}.csv'\n",
    "\n",
    "    # create log with header\n",
    "    with open(log_filename, mode='w', newline='') as log_file:\n",
    "        writer = csv.DictWriter(log_file,\n",
    "                                fieldnames=list(sampled_params[0].keys()) +\n",
    "                                ['valid_loss', 'test_rmse_a', 'test_rmse_w'])\n",
    "        writer.writeheader()\n",
    "\n",
    "    results = []\n",
    "    best_overall = {\"val\": float('inf'), \"row\": None, \"params\": None}\n",
    "\n",
    "\n",
    "    for i, params in enumerate(sampled_params):\n",
    "        seed_all(cfg.seed)\n",
    "        model_filename = 'models/best_lstm_mb_gs_one_heads_svf_pcsr_OOS.pt'\n",
    "\n",
    "        # delete existing model file:\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "            print(f\"Deleted existing model file: {model_filename}\")\n",
    "\n",
    "        # --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "        seed_all(cfg.seed)\n",
    "        ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train)\n",
    "        ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_test)\n",
    "\n",
    "        train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "            train_idx=train_idx,\n",
    "            val_idx=val_idx,\n",
    "            batch_size_train=64,\n",
    "            batch_size_val=128,\n",
    "            seed=cfg.seed,\n",
    "            fit_and_transform=\n",
    "            True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "            shuffle_train=True,\n",
    "            use_weighted_sampler=True  # use weighted sampler for training\n",
    "        )\n",
    "\n",
    "        # --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "        test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "        print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "        print(params)\n",
    "\n",
    "        # Build model\n",
    "        seed_all(cfg.seed)\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(cfg, params, device)\n",
    "        # Choose loss\n",
    "        loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(params)\n",
    "\n",
    "        # Train\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            clip_val=1,\n",
    "            # scheduler\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            # early stopping\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            # logging\n",
    "            log_every=5,\n",
    "            verbose=True,\n",
    "            # checkpoint\n",
    "            save_best_path=model_filename,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "        # Load the best weights\n",
    "        best_state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(best_state)\n",
    "        test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "            device, test_dl, ds_test_copy)\n",
    "        test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "            'RMSE_winter']\n",
    "\n",
    "        # Log row\n",
    "        row = {\n",
    "            **params, 'valid_loss': float(best_val),\n",
    "            'test_rmse_a': float(test_rmse_a),\n",
    "            'test_rmse_w': float(test_rmse_w)\n",
    "        }\n",
    "\n",
    "        print(test_rmse_a, test_rmse_w)\n",
    "\n",
    "        with open(log_filename, mode='a', newline='') as log_file:\n",
    "            writer = csv.DictWriter(log_file, fieldnames=list(row.keys()))\n",
    "            writer.writerow(row)\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # Track best by validation loss\n",
    "        if best_val < best_overall['val']:\n",
    "            best_overall = {\"val\": best_val, \"row\": row, \"params\": params}\n",
    "\n",
    "    print(\"\\n=== Best config by validation loss ===\")\n",
    "    print(best_overall['params'])\n",
    "    print(best_overall['row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
