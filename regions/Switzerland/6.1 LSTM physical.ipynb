{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "from cmcrameri import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import joypy\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# Add root of repo to import MBM!\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# Local modules\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GLAMOS stake data\n",
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Remove 2025\n",
    "data_monthly = data_monthly[data_monthly['YEAR']\n",
    "                            < 2025]  # Used elsewhere for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREDICTIONS_LSTM_IS = os.path.join(cfg.dataPath, \"GLAMOS\",\n",
    "                                        \"distributed_MB_grids\",\n",
    "                                        \"MBM/paper/LSTM_IS_original_y\")\n",
    "\n",
    "PATH_PREDICTIONS_NN = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                   'distributed_MB_grids', 'MBM/paper/NN')\n",
    "\n",
    "PATH_PREDICTIONS_XGB = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                    'distributed_MB_grids', 'MBM/paper/XGB')\n",
    "\n",
    "hydro_months = [\n",
    "    'oct',\n",
    "    'nov',\n",
    "    'dec',\n",
    "    'jan',\n",
    "    'feb',\n",
    "    'mar',\n",
    "    'apr',\n",
    "    'may',\n",
    "    'jun',\n",
    "    'jul',\n",
    "    'aug',\n",
    "    'sep',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "#     glacier_name=\"rhone\",\n",
    "#     year=2008,\n",
    "#     path_pred_lstm=PATH_PREDICTIONS_NN,\n",
    "#     apply_smoothing_fn=apply_gaussian_filter,\n",
    "# )\n",
    "\n",
    "# fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "#     glacier_name=\"rhone\",\n",
    "#     year=2008,\n",
    "#     path_pred_lstm=PATH_PREDICTIONS_LSTM_IS,\n",
    "#     apply_smoothing_fn=apply_gaussian_filter,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(\n",
    "    cfg.dataPath,\n",
    "    \"GLAMOS/distributed_MB_grids/MBM/paper/processed_dfs\",\n",
    ")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "REBUILD_CACHE = True  # set True only when inputs change\n",
    "\n",
    "paths = {\n",
    "    \"LSTM\": os.path.join(CACHE_DIR, \"df_months_LSTM_.parquet\"),\n",
    "    \"NN\": os.path.join(CACHE_DIR, \"df_months_NN_.parquet\"),\n",
    "    \"XGB\": os.path.join(CACHE_DIR, \"df_months_XGB_.parquet\"),\n",
    "    \"GW\": os.path.join(CACHE_DIR, \"df_GLAMOS_w_.parquet\"),\n",
    "    \"GA\": os.path.join(CACHE_DIR, \"df_GLAMOS_a_.parquet\"),\n",
    "}\n",
    "\n",
    "if REBUILD_CACHE or not all(os.path.exists(p) for p in paths.values()):\n",
    "\n",
    "    print(\"Building monthly prediction DataFrames...\")\n",
    "\n",
    "    df_months_LSTM = load_glwd_lstm_predictions(PATH_PREDICTIONS_LSTM_IS,\n",
    "                                                hydro_months)\n",
    "    df_months_NN = load_glwd_nn_predictions(PATH_PREDICTIONS_NN, hydro_months)\n",
    "    df_months_XGB = load_glwd_nn_predictions(PATH_PREDICTIONS_XGB,\n",
    "                                             hydro_months)\n",
    "\n",
    "    PATH_GLAMOS = os.path.join(cfg.dataPath, path_distributed_MB_glamos,\n",
    "                               \"GLAMOS\")\n",
    "    glaciers = os.listdir(PATH_GLAMOS)\n",
    "\n",
    "    glacier_years = (df_months_LSTM.groupby(\"glacier\")[\"year\"].unique().apply(\n",
    "        sorted).to_dict())\n",
    "\n",
    "    df_GLAMOS_w, df_GLAMOS_a = load_all_glamos(cfg, glacier_years, PATH_GLAMOS)\n",
    "\n",
    "    # Save cache\n",
    "    df_months_LSTM.to_parquet(paths[\"LSTM\"])\n",
    "    df_months_NN.to_parquet(paths[\"NN\"])\n",
    "    df_months_XGB.to_parquet(paths[\"XGB\"])\n",
    "    df_GLAMOS_w.to_parquet(paths[\"GW\"])\n",
    "    df_GLAMOS_a.to_parquet(paths[\"GA\"])\n",
    "\n",
    "    print(f\"Cached DataFrames to {CACHE_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading cached monthly prediction DataFrames...\")\n",
    "\n",
    "    df_months_LSTM = pd.read_parquet(paths[\"LSTM\"])\n",
    "    df_months_NN = pd.read_parquet(paths[\"NN\"])\n",
    "    df_months_XGB = pd.read_parquet(paths[\"XGB\"])\n",
    "    df_GLAMOS_w = pd.read_parquet(paths[\"GW\"])\n",
    "    df_GLAMOS_a = pd.read_parquet(paths[\"GA\"])\n",
    "\n",
    "    print(f\"Loaded DataFrames from {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glacier-wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Glacier-wide annual mean MB per year ---\n",
    "glwd_months_NN = df_months_NN.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_XGB = df_months_XGB.groupby(['glacier',\n",
    "                                         'year']).mean().reset_index()\n",
    "glwd_months_LSTM = df_months_LSTM.groupby(['glacier',\n",
    "                                           'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_w = df_GLAMOS_w.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_a = df_GLAMOS_a.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "\n",
    "# --- 2. Compute the intersection of valid glacier–year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_months_NN['glacier'], glwd_months_NN['year']))\n",
    "    & set(zip(glwd_months_XGB['glacier'], glwd_months_XGB['year']))\n",
    "    & set(zip(glwd_months_LSTM['glacier'], glwd_months_LSTM['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_w['glacier'], glwd_months_GLAMOS_w['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_a['glacier'], glwd_months_GLAMOS_a['year'])))\n",
    "\n",
    "\n",
    "# --- 3. Helper function for filtering by glacier–year pairs ---\n",
    "def filter_to_valid(df):\n",
    "    return df[df[['glacier', 'year'\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 4. Apply filtering to all datasets ---\n",
    "glwd_months_NN_filtered = filter_to_valid(glwd_months_NN)\n",
    "glwd_months_XGB_filtered = filter_to_valid(glwd_months_XGB)\n",
    "glwd_months_LSTM_filtered = filter_to_valid(glwd_months_LSTM)\n",
    "glwd_months_GLAMOS_filtered_w = filter_to_valid(glwd_months_GLAMOS_w)\n",
    "glwd_months_GLAMOS_filtered_a = filter_to_valid(glwd_months_GLAMOS_a)\n",
    "\n",
    "print(\n",
    "    len(glwd_months_GLAMOS_filtered_w),\n",
    "    len(glwd_months_GLAMOS_filtered_a),\n",
    "    len(glwd_months_NN_filtered),\n",
    "    len(glwd_months_XGB_filtered),\n",
    "    len(glwd_months_LSTM_filtered),\n",
    ")\n",
    "\n",
    "# --- 5. Prepare for plotting ---\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_months_LSTM_filtered,\n",
    "    glwd_months_NN_filtered,\n",
    "    glwd_months_XGB_filtered,\n",
    "    glwd_months_GLAMOS_filtered_w,\n",
    "    glwd_months_GLAMOS_filtered_a,\n",
    ")\n",
    "\n",
    "df_months_nn_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Compute min & max across models being plotted ----\n",
    "min_ = df_months_nn_long[['mb_nn', 'mb_lstm', 'mb_xgb']].min().min()\n",
    "max_ = df_months_nn_long[['mb_nn', 'mb_lstm', 'mb_xgb']].max().max()\n",
    "\n",
    "# ---- Plot ----\n",
    "fig = plot_monthly_joyplot(\n",
    "    df_months_nn_long,\n",
    "    x_range=(np.floor(min_), np.ceil(max_)),\n",
    "    color_lstm=color_annual,  # or rename to your liking\n",
    "    color_nn=color_winter,\n",
    "    color_xgb=\"darkgreen\",\n",
    "    color_glamos=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = df_months_nn_long.min()[[\n",
    "    'mb_nn', 'mb_glamos'\n",
    "]].min(), df_months_nn_long.max()[['mb_nn', 'mb_glamos']].max()\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)),\n",
    "                                  model_name='MBM')\n",
    "fig.savefig('figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_glwd.png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation bands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = 200\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# Copy datasets\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_XGB_ = df_months_XGB.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# Assign elevation bands\n",
    "for df_ in [\n",
    "        df_months_NN_,\n",
    "        df_months_XGB_,\n",
    "        df_months_LSTM_,\n",
    "        df_GLAMOS_a_,\n",
    "        df_GLAMOS_w_,\n",
    "]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "def extract_highest_band(df, bin_width):\n",
    "    max_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"max\")\n",
    "    highest_band = df[df[\"elevation\"] >= (max_elev - bin_width)]\n",
    "    return (highest_band.groupby([\"glacier\", \"year\"\n",
    "                                  ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "glwd_high_NN = extract_highest_band(df_months_NN_, bin)\n",
    "glwd_high_XGB = extract_highest_band(df_months_XGB_, bin)\n",
    "glwd_high_LSTM = extract_highest_band(df_months_LSTM_, bin)\n",
    "glwd_high_GLAMOS_a = extract_highest_band(df_GLAMOS_a_, bin)\n",
    "glwd_high_GLAMOS_w = extract_highest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_high_NN[\"glacier\"], glwd_high_NN[\"year\"]))\n",
    "    & set(zip(glwd_high_XGB[\"glacier\"], glwd_high_XGB[\"year\"]))\n",
    "    & set(zip(glwd_high_LSTM[\"glacier\"], glwd_high_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_w[\"glacier\"], glwd_high_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_a[\"glacier\"], glwd_high_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return (df[df[[\"glacier\", \"year\"\n",
    "                   ]].apply(tuple,\n",
    "                            axis=1).isin(valid_pairs)].reset_index(drop=True))\n",
    "\n",
    "\n",
    "glwd_high_NN_filt = filter_to_valid(glwd_high_NN)\n",
    "glwd_high_XGB_filt = filter_to_valid(glwd_high_XGB)\n",
    "glwd_high_LSTM_filt = filter_to_valid(glwd_high_LSTM)\n",
    "glwd_high_GLAMOS_a_filt = filter_to_valid(glwd_high_GLAMOS_a)\n",
    "glwd_high_GLAMOS_w_filt = filter_to_valid(glwd_high_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_high_GLAMOS_w_filt),\n",
    "    len(glwd_high_GLAMOS_a_filt),\n",
    "    len(glwd_high_NN_filt),\n",
    "    len(glwd_high_XGB_filt),\n",
    "    len(glwd_high_LSTM_filt),\n",
    ")\n",
    "\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_high_LSTM_filt,\n",
    "    glwd_high_NN_filt,\n",
    "    glwd_high_XGB_filt,\n",
    "    glwd_high_GLAMOS_w_filt,\n",
    "    glwd_high_GLAMOS_a_filt,\n",
    ")\n",
    "\n",
    "min_, max_ = (\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\", \"mb_glamos\"]].min().min(),\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\", \"mb_glamos\"]].max().max(),\n",
    ")\n",
    "\n",
    "fig = plot_monthly_joyplot_single(\n",
    "    df_months_nn_long,\n",
    "    variable=\"mb_lstm\",\n",
    "    color_model=color_annual,\n",
    "    x_range=(np.floor(min_), np.ceil(max_)),\n",
    "    model_name=\"MBM\",\n",
    ")\n",
    "\n",
    "fig.savefig(\n",
    "    \"figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_high_elv.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Lowest-elevation band\n",
    "# =========================\n",
    "\n",
    "bin = 250\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# --- Copy to avoid modifying originals ---\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_XGB_ = df_months_XGB.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# --- Assign elevation bands ---\n",
    "for df_ in [\n",
    "        df_months_NN_,\n",
    "        df_months_XGB_,\n",
    "        df_months_LSTM_,\n",
    "        df_GLAMOS_a_,\n",
    "        df_GLAMOS_w_,\n",
    "]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# --- Helper: extract lowest-elevation band per glacier ---\n",
    "def extract_lowest_band(df, bin_width):\n",
    "    min_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"min\")\n",
    "    lowest_band = df[df[\"elevation\"] <= (min_elev + bin_width)]\n",
    "    return (lowest_band.groupby([\"glacier\", \"year\"\n",
    "                                 ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "# --- Compute lowest-elevation bands ---\n",
    "glwd_low_NN = extract_lowest_band(df_months_NN_, bin)\n",
    "glwd_low_XGB = extract_lowest_band(df_months_XGB_, bin)\n",
    "glwd_low_LSTM = extract_lowest_band(df_months_LSTM_, bin)\n",
    "glwd_low_GLAMOS_a = extract_lowest_band(df_GLAMOS_a_, bin)\n",
    "glwd_low_GLAMOS_w = extract_lowest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "# --- Define common glacier–year pairs ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_low_NN[\"glacier\"], glwd_low_NN[\"year\"]))\n",
    "    & set(zip(glwd_low_XGB[\"glacier\"], glwd_low_XGB[\"year\"]))\n",
    "    & set(zip(glwd_low_LSTM[\"glacier\"], glwd_low_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_w[\"glacier\"], glwd_low_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_a[\"glacier\"], glwd_low_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return (df[df[[\"glacier\", \"year\"\n",
    "                   ]].apply(tuple,\n",
    "                            axis=1).isin(valid_pairs)].reset_index(drop=True))\n",
    "\n",
    "\n",
    "# --- Apply consistent filtering ---\n",
    "glwd_low_NN_filt = filter_to_valid(glwd_low_NN)\n",
    "glwd_low_XGB_filt = filter_to_valid(glwd_low_XGB)\n",
    "glwd_low_LSTM_filt = filter_to_valid(glwd_low_LSTM)\n",
    "glwd_low_GLAMOS_a_filt = filter_to_valid(glwd_low_GLAMOS_a)\n",
    "glwd_low_GLAMOS_w_filt = filter_to_valid(glwd_low_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_low_GLAMOS_w_filt),\n",
    "    len(glwd_low_GLAMOS_a_filt),\n",
    "    len(glwd_low_NN_filt),\n",
    "    len(glwd_low_XGB_filt),\n",
    "    len(glwd_low_LSTM_filt),\n",
    ")\n",
    "\n",
    "# --- Prepare long-format dataframe for plotting ---\n",
    "df_months_nn_long_low = prepare_monthly_long_df(\n",
    "    glwd_low_LSTM_filt,\n",
    "    glwd_low_NN_filt,\n",
    "    glwd_low_XGB_filt,\n",
    "    glwd_low_GLAMOS_w_filt,\n",
    "    glwd_low_GLAMOS_a_filt,\n",
    ")\n",
    "\n",
    "# --- Determine x-axis limits ---\n",
    "min_, max_ = (\n",
    "    df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\",\n",
    "                           \"mb_glamos\"]].min().min(),\n",
    "    df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\", \"mb_xgb\",\n",
    "                           \"mb_glamos\"]].max().max(),\n",
    ")\n",
    "\n",
    "# Optional manual clamp for ablation-dominated lowest band\n",
    "min_ = -10\n",
    "\n",
    "# --- Plot ---\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long_low,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)),\n",
    "                                  model_name=\"MBM\",\n",
    "                                  y_offset=0.15)\n",
    "\n",
    "# --- Save figure ---\n",
    "fig.savefig(\n",
    "    \"figures/paper/CH_LSTM_vs_GLAMOS_monthly_joyplot_low_elv.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance:\n",
    "\n",
    "Feature importance is done IN-SAMPLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True,\n",
    "    normalize_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, PARAMS_LSTM_IS, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(PARAMS_LSTM_IS)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Load and evaluate on test\n",
    "state = torch.load(LSTM_IS_NORM_Y, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.PFI_all import permutation_feature_importance_mbm_parallel\n",
    "\n",
    "# RUN_PFI = True  # Set False to load existing results\n",
    "\n",
    "# # Define save path\n",
    "# save_dir = os.path.join(\n",
    "#     cfg.dataPath,\n",
    "#     \"GLAMOS/distributed_MB_grids/MBM/testing_LSTM/processed_dfs\",\n",
    "# )\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# pfi_path = os.path.join(save_dir, \"pfi_parallel.parquet\")\n",
    "\n",
    "# if RUN_PFI:\n",
    "#     # --- Compute permutation feature importance ---\n",
    "#     print(\"▶️ Running permutation feature importance (PFI)...\")\n",
    "#     pfi_parallel = permutation_feature_importance_mbm_parallel(\n",
    "#         cfg=cfg,\n",
    "#         custom_params=PARAMS_LSTM_IS,\n",
    "#         model_filename=LSTM_IS_NORM_Y,\n",
    "#         df_eval=\n",
    "#         df_train,  # evaluation DataFrame WITH TARGETS aligned to predictions\n",
    "#         MONTHLY_COLS=MONTHLY_COLS,\n",
    "#         STATIC_COLS=STATIC_COLS,\n",
    "#         ds_train=ds_train,\n",
    "#         train_idx=train_idx,\n",
    "#         target_col=\"POINT_BALANCE\",  # <-- target column name\n",
    "#         months_head_pad=months_head_pad,\n",
    "#         months_tail_pad=months_tail_pad,\n",
    "#         seed=cfg.seed,\n",
    "#         n_repeats=5,\n",
    "#         batch_size=256,\n",
    "#         max_workers=None,  # auto: n_cpus-1 (cap 32)\n",
    "#     )\n",
    "\n",
    "#     # Rename features to readable names\n",
    "#     pfi_parallel[\"feature\"] = pfi_parallel[\"feature\"].apply(\n",
    "#         lambda x: vois_climate_long_name.get(x, x))\n",
    "\n",
    "#     # Save\n",
    "#     pfi_parallel.to_parquet(pfi_path)\n",
    "#     print(f\"PFI results saved to {pfi_path}\")\n",
    "\n",
    "# else:\n",
    "#     # --- Load previously saved results ---\n",
    "#     pfi_parallel = pd.read_parquet(pfi_path)\n",
    "#     print(f\"PFI results loaded from {pfi_path}\")\n",
    "\n",
    "# # --- Plot ---\n",
    "# plt.figure(figsize=(8, max(3, 0.35 * len(pfi_parallel))))\n",
    "# plt.barh(\n",
    "#     pfi_parallel[\"feature\"],\n",
    "#     pfi_parallel[\"mean_delta\"],\n",
    "#     xerr=pfi_parallel[\"std_delta\"],\n",
    "# )\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title(\n",
    "#     f\"Permutation Feature Importance (Δ{pfi_parallel['metric_name'].iloc[0]}; \"\n",
    "#     f\"baseline={pfi_parallel['baseline'].iloc[0]:.3f})\")\n",
    "# plt.xlabel(\n",
    "#     f\"Increase in {pfi_parallel['metric_name'].iloc[0]} (higher = more important)\"\n",
    "# )\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.PFI_monthly import permutation_feature_importance_mbm_monthly_parallel\n",
    "\n",
    "# --- Define monthly mapping ---\n",
    "month_map = {\n",
    "    \"aug_\": 0,\n",
    "    \"sep_\": 1,\n",
    "    \"oct\": 2,\n",
    "    \"nov\": 3,\n",
    "    \"dec\": 4,\n",
    "    \"jan\": 5,\n",
    "    \"feb\": 6,\n",
    "    \"mar\": 7,\n",
    "    \"apr\": 8,\n",
    "    \"may\": 9,\n",
    "    \"jun\": 10,\n",
    "    \"jul\": 11,\n",
    "    \"aug\": 12,\n",
    "    \"sep\": 13,\n",
    "    \"oct_\": 14,\n",
    "}\n",
    "\n",
    "# --- Prepare evaluation DataFrame ---\n",
    "# df_eval = pd.concat([df_train.copy(), df_train.copy()],\n",
    "#                     axis=0).reset_index(drop=True)\n",
    "df_eval = df_train.copy()\n",
    "df_eval[\"MONTH_IDX\"] = df_eval[\"MONTHS\"].str.lower().map(month_map)\n",
    "\n",
    "# --- Run or load ---\n",
    "RUN_PFI_MONTHLY = True  # set to False to just load saved results\n",
    "\n",
    "save_dir = os.path.join(\n",
    "    cfg.dataPath,\n",
    "    \"GLAMOS/distributed_MB_grids/MBM/testing_LSTM/processed_dfs\",\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "pfi_monthly_path = os.path.join(save_dir, \"pfi_monthly.parquet\")\n",
    "\n",
    "if RUN_PFI_MONTHLY:\n",
    "    print(\"▶️ Running monthly permutation feature importance (PFI)...\")\n",
    "\n",
    "    pfi_monthly = permutation_feature_importance_mbm_monthly_parallel(\n",
    "        cfg,\n",
    "        PARAMS_LSTM_IS,\n",
    "        LSTM_IS_NORM_Y,\n",
    "        df_eval,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        ds_train,\n",
    "        train_idx,\n",
    "        months_head_pad,\n",
    "        months_tail_pad,\n",
    "        seed=cfg.seed,\n",
    "        n_repeats=3,\n",
    "        batch_size=256,\n",
    "        denorm=True,\n",
    "        max_workers=None,\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    pfi_monthly.to_parquet(pfi_monthly_path)\n",
    "    print(f\"Monthly PFI results saved to {pfi_monthly_path}\")\n",
    "\n",
    "else:\n",
    "    # --- Load previously saved results ---\n",
    "    pfi_monthly = pd.read_parquet(pfi_monthly_path)\n",
    "    print(f\"Monthly PFI results loaded from {pfi_monthly_path}\")\n",
    "\n",
    "# --- Optional: Quick preview ---\n",
    "print(pfi_monthly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Month order ---\n",
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "    \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "\n",
    "# --- Map features to long names ---\n",
    "pfi_monthly[\"feature_long\"] = pfi_monthly[\"feature\"].apply(\n",
    "    lambda x: vois_climate_long_name.get(x, x))\n",
    "pfi_monthly = pfi_monthly[~pfi_monthly.month.isin(\n",
    "    np.concatenate([months_tail_pad, months_head_pad]))]\n",
    "\n",
    "# --- Prepare pivot table for global ΔRMSE ---\n",
    "piv_global = pfi_monthly.pivot(index=\"feature_long\",\n",
    "                               columns=\"month\",\n",
    "                               values=\"mean_delta_global\")\n",
    "\n",
    "# --- Reorder columns (months) ---\n",
    "piv_global = piv_global[[m for m in month_order if m in piv_global.columns]]\n",
    "\n",
    "# --- Order features by average global importance (optional, makes it clean) ---\n",
    "feat_order = (pfi_monthly.groupby(\"feature_long\")\n",
    "              [\"mean_delta_global\"].mean().sort_values(ascending=False).index)\n",
    "piv_global = piv_global.loc[feat_order]\n",
    "\n",
    "# --- Plot single heatmap ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(piv_global,\n",
    "            cmap=\"magma\",\n",
    "            linewidths=0.3,\n",
    "            cbar_kws={\"label\": \"ΔRMSE (global)\"})\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Monthly Permutation Feature Importance – Global RMSE Δ\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# --- Month order ---\n",
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\",\n",
    "    \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "\n",
    "# --- Map features to long names ---\n",
    "pfi_monthly[\"feature_long\"] = pfi_monthly[\"feature\"].apply(\n",
    "    lambda x: vois_climate_long_name.get(x, x))\n",
    "pfi_monthly = pfi_monthly[~pfi_monthly.month.isin(\n",
    "    np.concatenate([months_tail_pad, months_head_pad]))]\n",
    "\n",
    "# --- Pivot table for global ΔRMSE ---\n",
    "piv_global = pfi_monthly.pivot(index=\"feature_long\",\n",
    "                               columns=\"month\",\n",
    "                               values=\"mean_delta_global\")\n",
    "\n",
    "# --- Reorder months ---\n",
    "piv_global = piv_global[[m for m in month_order if m in piv_global.columns]]\n",
    "\n",
    "# --- Order features by average importance (ascending = top = most important visually) ---\n",
    "feat_order = piv_global.mean(axis=1).sort_values(ascending=True).index\n",
    "\n",
    "# --- Smooth for nicer ridges ---\n",
    "piv_smooth = pd.DataFrame(\n",
    "    np.vstack(\n",
    "        [gaussian_filter1d(piv_global.loc[f], sigma=1) for f in feat_order]),\n",
    "    index=feat_order,\n",
    "    columns=piv_global.columns,\n",
    ")\n",
    "\n",
    "# --- Plot setup ---\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "palette = sns.color_palette(\"magma\", n_colors=len(feat_order))\n",
    "month_idx = np.arange(len(piv_smooth.columns))\n",
    "\n",
    "offset_step_small = np.nanmax(piv_smooth.values) * 0.6\n",
    "offset_step_big = np.nanmax(piv_smooth.values) * 1.\n",
    "big_features = [\"Temp.\", \"Precip.\"]\n",
    "\n",
    "current_offset = 0.0\n",
    "\n",
    "# --- Compute max ΔRMSE per feature ---\n",
    "max_importance = piv_global.max(axis=1)\n",
    "\n",
    "for i, (feat, color) in enumerate(zip(feat_order, palette)):\n",
    "    y = piv_smooth.loc[feat].values\n",
    "\n",
    "    # Plot ridge\n",
    "    ax.plot(month_idx, y + current_offset, color=color, lw=2)\n",
    "    ax.fill_between(month_idx,\n",
    "                    current_offset,\n",
    "                    y + current_offset,\n",
    "                    color=color,\n",
    "                    alpha=0.4)\n",
    "\n",
    "    # Feature label\n",
    "    ax.text(-0.6,\n",
    "            current_offset,\n",
    "            feat,\n",
    "            va='center',\n",
    "            ha='right',\n",
    "            fontsize=13,\n",
    "            color='black')\n",
    "\n",
    "    # --- Add ΔRMSE annotation at ridge's highest point ---\n",
    "    max_idx = np.argmax(y)\n",
    "    max_x = month_idx[max_idx]\n",
    "    max_y = y[max_idx] + current_offset\n",
    "\n",
    "    # Offset annotation if too close to edges\n",
    "    if max_x == 0:\n",
    "        text_x = max_x + 0.1  # shift right if peak is at first position\n",
    "        ha = 'left'\n",
    "    elif max_x == len(month_idx) - 1:\n",
    "        text_x = max_x - 0.4  # shift left if at last position\n",
    "        ha = 'right'\n",
    "    else:\n",
    "        text_x = max_x\n",
    "        ha = 'center'\n",
    "\n",
    "    ax.text(text_x,\n",
    "            max_y + 0.05 * offset_step_small,\n",
    "            f\"ΔRMSE={max_importance[feat]:.3f}\",\n",
    "            ha=ha,\n",
    "            va='bottom',\n",
    "            fontsize=11,\n",
    "            color='black',\n",
    "            rotation=0,\n",
    "            bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1.5))\n",
    "\n",
    "    # Increment vertical offset\n",
    "    current_offset += offset_step_big if feat in big_features else offset_step_small\n",
    "\n",
    "# --- Styling ---\n",
    "ax.set_yticks([])\n",
    "ax.set_xlim(0, len(month_idx) - 1)\n",
    "ax.set_xticks(month_idx)\n",
    "ax.set_xticklabels([m.strip(\"_\").capitalize() for m in piv_smooth.columns],\n",
    "                   rotation=45,\n",
    "                   ha='right')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_title(\"Monthly Permutation Feature Importance\", pad=20)\n",
    "\n",
    "ax.set_facecolor(\"white\")\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "ax.tick_params(colors='black')\n",
    "for spine in [\"top\", \"right\", \"left\"]:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_color(\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Save figure ---\n",
    "fig.savefig(\"figures/paper/CH_LSTM_monthly_PFI.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
