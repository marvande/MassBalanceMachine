{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]\n",
    "\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    # base_url=\n",
    "    # \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "df_missing = export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# gdf_proj.set_index('rgi_id', inplace=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# total glacier area\n",
    "total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# explode the list of missing vars into rows (one var per row)\n",
    "df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# 1) COUNT: number of glaciers missing each variable\n",
    "counts_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "        ascending=False))\n",
    "\n",
    "# 2) TOTAL % AREA with ANY missing var\n",
    "total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "print(f\"Total glacier area with ANY missing variable: \"\n",
    "      f\"{total_missing_area_km2:,.2f} kmÂ² \"\n",
    "      f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# Optional: also show % area per variable (kept from your earlier logic)\n",
    "area_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "        ascending=False))\n",
    "perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "print(\"\\n% of total glacier area missing per variable:\")\n",
    "for var, pct in perc_missing_per_var.items():\n",
    "    print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# ---- barplot: number of glaciers missing each variable ----\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "plt.xlabel(\"Missing variable\")\n",
    "plt.ylabel(\"Number of glaciers\")\n",
    "plt.title(\"Count of glaciers missing each variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open an example\n",
    "# # rgi_gl = gdirs[0].rgi_id\n",
    "# rgi_gl = 'RGI60-11.01238'\n",
    "\n",
    "# ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "# glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "#                         ds['glacier_mask'].values)\n",
    "\n",
    "# # Create glacier mask\n",
    "# ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "# ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "# ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "# ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "# # Assign other variables only if available\n",
    "# if 'hugonnet_dhdt' in ds:\n",
    "#     ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "# if 'consensus_ice_thickness' in ds:\n",
    "#     ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "# if 'millan_v' in ds:\n",
    "#     ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "# glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(16, 8), sharey=True)\n",
    "\n",
    "# ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "# ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "# ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "# ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "# axs[0].set_title(\"Aspect OGGM\")\n",
    "# axs[1].set_title(\"Slope OGGM\")\n",
    "# axs[2].set_title(\"DEM OGGM\")\n",
    "# axs[3].set_title(\"Glacier mask OGGM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked xarray grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_glacier(path_RGIs, rgi_gl):\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "\n",
    "    # Check if 'glacier_mask' exists\n",
    "    if 'glacier_mask' not in ds:\n",
    "        raise ValueError(\n",
    "            f\"'glacier_mask' variable not found in dataset {rgi_gl}\")\n",
    "\n",
    "    # Create glacier mask\n",
    "    glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                            ds['glacier_mask'].values)\n",
    "\n",
    "    # Apply mask to core variables\n",
    "    ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "    ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "    ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "    ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "    # Apply mask to optional variables if present\n",
    "    if 'hugonnet_dhdt' in ds:\n",
    "        ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "    if 'consensus_ice_thickness' in ds:\n",
    "        ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "    if 'millan_v' in ds:\n",
    "        ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "    # Indices where glacier_mask == 1\n",
    "    glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "    return ds, glacier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/',\n",
    "                             'xr_masked_grids/')\n",
    "path_RGIs = cfg.dataPath + path_OGGM + 'xr_grids/'\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11.6\")\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Create masked glacier dataset\n",
    "            ds, glacier_indices = create_masked_glacier(path_RGIs, rgi_gl)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue  # Skip to next glacier\n",
    "\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "\n",
    "        # Coarsen to 50 m resolution if needed\n",
    "        if 20 < dx_m < 50:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=50)\n",
    "            dx_m, dy_m = get_res_from_projected(ds)\n",
    "        else:\n",
    "            ds = ds\n",
    "\n",
    "        # Change coordinates to Lat/Lon projection\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\")\n",
    "        ds_latlon = ds_latlon.rename({'x': 'lon', 'y': 'lat'})\n",
    "\n",
    "        # Save xarray dataset\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path)\n",
    "\n",
    "# open example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl_rhone = gdir_rhone.rgi_id\n",
    "ds = xr.open_dataset(path_xr_grids + rgi_gl_rhone + '.zarr')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "path_xr_grids = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\", \"xr_masked_grids/\")\n",
    "path_geotiff  = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\", \"geotiff_meters_lv95/\")\n",
    "os.makedirs(path_geotiff, exist_ok=True)\n",
    "emptyfolder(path_geotiff)\n",
    "\n",
    "# target projected CRS in meters (Switzerland)\n",
    "TARGET_CRS = \"EPSG:2056\"  # LV95\n",
    "# optionally set a target pixel size in meters (None keeps native res after reprojection)\n",
    "TARGET_RES = None  # e.g., 10.0 for ~10 m pixels\n",
    "\n",
    "for rgi_gl in tqdm([f for f in os.listdir(path_xr_grids) if f.endswith(\".zarr\")]):\n",
    "    rgi_id = rgi_gl[:-5]  # strip \".zarr\"\n",
    "    out_tif = os.path.join(path_geotiff, f\"{rgi_id}.tif\")\n",
    "    if os.path.exists(out_tif):\n",
    "        continue  # idempotent\n",
    "\n",
    "    # open Zarr (try consolidated first)\n",
    "    zarr_path = os.path.join(path_xr_grids, rgi_gl)\n",
    "    try:\n",
    "        ds = xr.open_zarr(zarr_path, consolidated=True)\n",
    "    except Exception:\n",
    "        ds = xr.open_zarr(zarr_path)\n",
    "\n",
    "    dem = ds[\"masked_elev\"]\n",
    "\n",
    "    # ensure a CRS is set for the source (WGS84 if lat/lon)\n",
    "    # If your DataArray already has CRS, you can skip this line.\n",
    "    if not dem.rio.crs:\n",
    "        dem = dem.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # propagate nodata if present; otherwise use NaN\n",
    "    if \"_FillValue\" in dem.attrs:\n",
    "        dem = dem.rio.write_nodata(dem.attrs[\"_FillValue\"])\n",
    "    elif \"nodata\" in dem.attrs:\n",
    "        dem = dem.rio.write_nodata(dem.attrs[\"nodata\"])\n",
    "    else:\n",
    "        dem = dem.rio.write_nodata(np.nan)\n",
    "\n",
    "    # reproject to meters (LV95). You can pass resolution=TARGET_RES to enforce a grid size.\n",
    "    if TARGET_RES is None:\n",
    "        dem_m = dem.rio.reproject(TARGET_CRS)\n",
    "    else:\n",
    "        dem_m = dem.rio.reproject(TARGET_CRS, resolution=TARGET_RES)\n",
    "\n",
    "    # save a compact, tiled, compressed GeoTIFF\n",
    "    dem_m.rio.to_raster(\n",
    "        out_tif,\n",
    "        dtype=\"float32\",\n",
    "        compress=\"LZW\",\n",
    "        BIGTIFF=\"IF_SAFER\",\n",
    "        tiled=True,\n",
    "        predictor=3  # better compression for float rasters\n",
    "    )\n",
    "\n",
    "# quick sanity plot of the last DEM (projected)\n",
    "plt.figure(figsize=(7,6))\n",
    "dem_m.plot(cmap=\"terrain\")\n",
    "plt.title(f\"Projected DEM (meters) â {rgi_id}\")\n",
    "plt.xlabel(\"x (m, LV95)\")\n",
    "plt.ylabel(\"y (m, LV95)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "RUN = True\n",
    "path_rgi_alps = os.path.join(cfg.dataPath, 'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11/')\n",
    "\n",
    "# ---- helpers ----\n",
    "def expected_fname(rgi_gl: str, year: int) -> str:\n",
    "    # Expected: RGI60-11.00001_grid_1999.parquet\n",
    "    return f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "\n",
    "def years_present_for_glacier(folder_path: str, rgi_gl: str) -> set:\n",
    "    \"\"\"Return the set of 4-digit years found for this glacier in its output folder.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return set()\n",
    "    rx = re.compile(rf\"^{re.escape(rgi_gl)}_grid_(\\d{{4}})\\.parquet$\")\n",
    "    years_found = set()\n",
    "    for f in os.listdir(folder_path):\n",
    "        m = rx.match(f)\n",
    "        if m:\n",
    "            years_found.add(int(m.group(1)))\n",
    "    return years_found\n",
    "\n",
    "def glacier_is_complete(rgi_gl: str, years: range) -> bool:\n",
    "    folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "    found = years_present_for_glacier(folder_path, rgi_gl)\n",
    "    return set(years).issubset(found)\n",
    "\n",
    "# ---- main ----\n",
    "if RUN:\n",
    "    # inclusive 1999..2024\n",
    "    years = range(1999, 2025)\n",
    "\n",
    "    os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "\n",
    "    valid_rgis = [\n",
    "        f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "        if f.endswith('.zarr')\n",
    "    ]\n",
    "\n",
    "    # Glaciers that are already complete (all yearly files exist)\n",
    "    complete_rgis = [r for r in valid_rgis if glacier_is_complete(r, years)]\n",
    "    # Glaciers that still need work\n",
    "    rest_rgis = list(set(valid_rgis) - set(complete_rgis))\n",
    "\n",
    "    print(f\"Glaciers already complete: {len(complete_rgis)}\")\n",
    "    print(f\"Number of glaciers to process: {len(rest_rgis)}\")\n",
    "\n",
    "    for gdir in tqdm(gdirs, desc=\"Processing glaciers\"):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        if rgi_gl not in valid_rgis:\n",
    "            print(f\"Skipping {rgi_gl}: not found in valid RGI glaciers\")\n",
    "            continue\n",
    "\n",
    "        # Skip if already fully complete\n",
    "        if glacier_is_complete(rgi_gl, years):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "\n",
    "            # Open Zarr\n",
    "            try:\n",
    "                ds = xr.open_zarr(file_path, consolidated=True)\n",
    "            except Exception:\n",
    "                ds = xr.open_zarr(file_path)\n",
    "\n",
    "            # Build grid for all years once\n",
    "            try:\n",
    "                df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed creating glacier grid for {rgi_gl}: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_grid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Add GLWD_ID and GLACIER columns\n",
    "            df_grid['GLWD_ID'] = [\n",
    "                mbm.data_processing.utils.get_hash(f\"{r}_{y}\")\n",
    "                for r, y in zip(df_grid['RGIId'].astype(str),\n",
    "                                df_grid['YEAR'].astype(str))\n",
    "            ]\n",
    "            df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "            df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "            # Output folder\n",
    "            folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            # Determine missing years for this glacier (idempotent)\n",
    "            existing_years = years_present_for_glacier(folder_path, rgi_gl)\n",
    "            missing_years = [y for y in years if y not in existing_years]\n",
    "\n",
    "            if not missing_years:\n",
    "                # Another process may have finished meanwhile\n",
    "                continue\n",
    "\n",
    "            for year in missing_years:\n",
    "                try:\n",
    "                    df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "                    if df_grid_y.empty:\n",
    "                        # No data for that year; keep going\n",
    "                        continue\n",
    "\n",
    "                    # Build dataset & add climate features\n",
    "                    try:\n",
    "                        dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                            cfg=cfg,\n",
    "                            data=df_grid_y,\n",
    "                            region_name='CH',\n",
    "                            region_id=11,\n",
    "                            data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv)\n",
    "                        )\n",
    "\n",
    "                        era5_climate_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw, 'era5_monthly_averaged_data_Alps.nc'\n",
    "                        )\n",
    "                        geopotential_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw, 'era5_geopotential_pressure_Alps.nc'\n",
    "                        )\n",
    "\n",
    "                        dataset_grid_yearly.get_climate_features(\n",
    "                            climate_data=era5_climate_data,\n",
    "                            geopotential_data=geopotential_data,\n",
    "                            change_units=True,\n",
    "                            smoothing_vois={'vois_climate': vois_climate,\n",
    "                                            'vois_other': ['ALTITUDE_CLIMATE']}\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed adding climate features for {rgi_gl} (year {year}): {e}\")\n",
    "                        continue\n",
    "\n",
    "                    vois_topographical_sub = [voi for voi in vois_topographical\n",
    "                                              if voi in df_grid_y.columns]\n",
    "\n",
    "                    dataset_grid_yearly.convert_to_monthly(\n",
    "                        meta_data_columns=cfg.metaData,\n",
    "                        vois_climate=vois_climate,\n",
    "                        vois_topographical=vois_topographical_sub\n",
    "                    )\n",
    "\n",
    "                    save_path = os.path.join(folder_path, expected_fname(rgi_gl, year))\n",
    "                    # If a stale/partial file exists, overwrite it\n",
    "                    dataset_grid_yearly.data.to_parquet(\n",
    "                        save_path, engine=\"pyarrow\", compression=\"snappy\"\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed processing {rgi_gl} for year {year}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with glacier {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2004\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "year = 2000\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_ids = os.listdir(path_rgi_alps)\n",
    "pos_gl = []\n",
    "for rgi_gl in tqdm(rgi_ids):\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "    pos_gl.append((df.POINT_LAT.mean(), df.POINT_LON.mean()))\n",
    "df_pos_all = pd.DataFrame(pos_gl, columns=['lat', 'lon'])\n",
    "df_pos_all['rgi_id'] = rgi_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of glaciers in RGI region 11.6:', len(df_pos_all))\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "latN, latS = 48, 44\n",
    "lonW, lonE = 4, 14\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=df_pos_all,\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    alpha=0.6,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "glacier_outline_rgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_CA.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_simple, val_idx_simple = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_simple), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 2,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_simple_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_simple.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_simple) ---\n",
    "ds_train_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_simple)\n",
    "\n",
    "ds_test_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_simple)\n",
    "\n",
    "train_dl, val_dl = ds_train_simple_copy.make_loaders(\n",
    "    train_idx=train_idx_simple,\n",
    "    val_idx=val_idx_simple,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_simple and transforms it) ---\n",
    "test_dl_simple = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_simple_copy, ds_train_simple_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "model_filename = f\"models/lstm_model_2025-09-30_CA_simple.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_simple, ds_test_simple_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "vars = ['hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v']\n",
    "\n",
    "all_combos = [('hugonnet_dhdt', ), ('consensus_ice_thickness', ),\n",
    "              ('millan_v', ), ('hugonnet_dhdt', 'consensus_ice_thickness'),\n",
    "              ('hugonnet_dhdt', 'millan_v'),\n",
    "              ('consensus_ice_thickness', 'millan_v')]\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    for combo in all_combos:\n",
    "        print(combo)\n",
    "\n",
    "        STATIC_COLS = ['aspect', 'slope', *combo]\n",
    "        feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "        seed_all(cfg.seed)\n",
    "\n",
    "        # prepare train/test data\n",
    "        df_train = data_train.copy()\n",
    "        df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "        df_test = data_test.copy()\n",
    "        df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "        # datasets\n",
    "        ds_train_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_train,\n",
    "            MONTHLY_COLS,\n",
    "            STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad,\n",
    "            months_head_pad=months_head_pad,\n",
    "            expect_target=True)\n",
    "        ds_test_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_test,\n",
    "            MONTHLY_COLS,\n",
    "            STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad,\n",
    "            months_head_pad=months_head_pad,\n",
    "            expect_target=True)\n",
    "\n",
    "        # split train/val\n",
    "        train_idx_full, val_idx_full = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "            len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "        # params\n",
    "        custom_params = {\n",
    "            'Fm': 8,\n",
    "            'Fs': len(STATIC_COLS),\n",
    "            'hidden_size': 128,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.0,\n",
    "            'static_layers': 0,\n",
    "            'static_hidden': None,\n",
    "            'static_dropout': None,\n",
    "            'lr': 0.001,\n",
    "            'weight_decay': 0.0,\n",
    "            'loss_name': 'neutral',\n",
    "            'loss_spec': None,\n",
    "            'two_heads': True,\n",
    "            'head_dropout': 0.0\n",
    "        }\n",
    "        params_full_model = custom_params.copy()\n",
    "\n",
    "        # --- model filename ---\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        combo_str = \"_\".join(combo)\n",
    "        model_filename = f\"models/lstm_model_{current_date}_CA_{combo_str}.pt\"\n",
    "\n",
    "        # loaders\n",
    "        ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train_full)\n",
    "        ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_test_full)\n",
    "\n",
    "        train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "            train_idx=train_idx_full,\n",
    "            val_idx=val_idx_full,\n",
    "            batch_size_train=64,\n",
    "            batch_size_val=128,\n",
    "            seed=cfg.seed,\n",
    "            fit_and_transform=True,\n",
    "            shuffle_train=True,\n",
    "            use_weighted_sampler=True)\n",
    "\n",
    "        test_dl_full = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_test_full_copy,\n",
    "            ds_train_full_copy,\n",
    "            batch_size=128,\n",
    "            seed=cfg.seed)\n",
    "\n",
    "        # model\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(\n",
    "            cfg, custom_params, device)\n",
    "        loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=custom_params['lr'],\n",
    "            weight_decay=custom_params['weight_decay'],\n",
    "            clip_val=1,\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            log_every=5,\n",
    "            verbose=True,\n",
    "            save_best_path=model_filename,\n",
    "            loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model (with OGGM variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "STATIC_COLS = [\n",
    "    'aspect', 'slope', 'hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_full, val_idx_full = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 5,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_full_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_full.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_full) ---\n",
    "ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_full)\n",
    "\n",
    "ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_full)\n",
    "\n",
    "train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "    train_idx=train_idx_full,\n",
    "    val_idx=val_idx_full,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_full and transforms it) ---\n",
    "test_dl_full = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_full_copy, ds_train_full_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "model_filename = f\"models/lstm_model_2025-09-30_CA_full.pt\"\n",
    "\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_full, ds_test_full_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_rgi = cfg.dataPath+'GLAMOS/RGI/RGI2000-v7.0-G-11_central_europe/RGI2000-v7.0-G-11_central_europe.shp'\n",
    "path_rgi = cfg.dataPath + 'GLAMOS/RGI/nsidc0770_11.rgi60.CentralEurope/11_rgi60_CentralEurope.shp'\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(path_rgi)\n",
    "\n",
    "# check CRS\n",
    "print(gdf.crs)\n",
    "\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.set_index('RGIId', inplace=True, drop=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_df = pd.read_csv(\"logs/glacier_mean_MB_old.csv\").drop(['Index'], axis=1)\n",
    "\n",
    "output_df['area_gl'] = output_df['RGIId'].map(\n",
    "    lambda x: gdf_proj.loc[x, 'area_km2'])\n",
    "\n",
    "# yearly_mean_mb_CA = output_df.groupby('Year',\n",
    "#                                       as_index=False).agg({'Mean_MB': 'mean'})\n",
    "# yearly_cum_mb_CA = output_df.groupby('Year',\n",
    "#                                      as_index=False).agg({'Mean_MB': 'sum'})\n",
    "# yearly_cum_mb_CA['Cum_MB'] = yearly_cum_mb_CA['Mean_MB'].cumsum()\n",
    "# yearly_cum_mb_CA['Mean_MB'] = yearly_mean_mb_CA['Mean_MB']\n",
    "# # yearly_cum_mb_CA['Mean_MB'] = yearly_cum_mb_CA['Mean_MB'] / total_area\n",
    "# yearly_cum_mb_CA.head()\n",
    "\n",
    "df = output_df.copy()\n",
    "\n",
    "# annual change per glacier in Gt\n",
    "df[\"annual_change_gt\"] = (df[\"Mean_MB\"] * df[\"area_gl\"]) / 1e9\n",
    "\n",
    "# total annual change in Gt (sum across glaciers)\n",
    "annual_gt = df.groupby(\"Year\")[\"annual_change_gt\"].sum().reset_index(\n",
    "    name=\"Annual_MB_Gt\")\n",
    "\n",
    "# cumulative MB in Gt\n",
    "annual_gt[\"Cumulative_MB_Gt\"] = annual_gt[\"Annual_MB_Gt\"].cumsum()\n",
    "\n",
    "# compute weighted mean MB per year\n",
    "yearly_weighted = (output_df.groupby(\"Year\").apply(lambda g: (g[\"Mean_MB\"] * g[\n",
    "    \"area_gl\"]).sum() / g[\"area_gl\"].sum()).reset_index(name=\"Weighted_MB\"))\n",
    "\n",
    "print(yearly_weighted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glambie_df = pd.read_csv('glambie_values.csv')\n",
    "date_columns = [\n",
    "    'central_europe_dates', 'central_europe_start_dates',\n",
    "    'central_europe_end_dates'\n",
    "]\n",
    "\n",
    "glambie_df[date_columns] = glambie_df[date_columns].apply(\n",
    "    lambda x: x.round() - 1)\n",
    "glambie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plotting ---\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "\n",
    "# --------------------\n",
    "# Left: LSTM results\n",
    "# --------------------\n",
    "ax1 = axs[0]\n",
    "years = yearly_weighted['Year']\n",
    "\n",
    "# barplot: annual weighted MB (m w.e.)\n",
    "ax1.bar(years,\n",
    "        yearly_weighted['Weighted_MB'],\n",
    "        color=\"skyblue\",\n",
    "        label=\"Area-weighted annual MB\")\n",
    "ax1.set_ylabel(\"Annual MB (m w.e.)\", color=\"skyblue\")\n",
    "\n",
    "# lineplot: cumulative MB in Gt (secondary axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(annual_gt['Year'],\n",
    "         annual_gt['Cumulative_MB_Gt'],\n",
    "         color=\"red\",\n",
    "         marker=\"o\",\n",
    "         label=\"Cumulative MB\")\n",
    "ax2.set_ylabel(\"Cumulative MB (Gt)\", color=\"red\")\n",
    "\n",
    "ax1.set_title(\"Central Alps annual MB (LSTM)\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Right: GLAMBIE results\n",
    "# --------------------\n",
    "ax3 = axs[1]\n",
    "\n",
    "# annual MB (bars)\n",
    "ax3.bar(glambie_df['central_europe_end_dates'],\n",
    "        glambie_df['central_europe_annual_change_mwe'],\n",
    "        color=\"lightgreen\",\n",
    "        label=\"Annual MB (GLAMBIE)\")\n",
    "ax3.set_ylabel(\"Annual MB (m w.e.)\", color=\"lightgreen\")\n",
    "\n",
    "# cumulative MB (line, secondary axis)\n",
    "ax4 = ax3.twinx()\n",
    "ax4.plot(glambie_df['central_europe_dates'],\n",
    "         glambie_df['central_europe_cumulative_change_gt'],\n",
    "         color=\"darkgreen\",\n",
    "         marker=\"s\",\n",
    "         label=\"Cumulative MB (GLAMBIE)\")\n",
    "ax4.set_ylabel(\"Cumulative MB (Gt)\", color=\"darkgreen\")\n",
    "\n",
    "ax3.set_title(\"Central Europe MB (GLAMBIE)\")\n",
    "ax3.legend(loc=\"upper left\")\n",
    "ax4.legend(loc=\"upper right\")\n",
    "\n",
    "# --------------------\n",
    "# Formatting\n",
    "# --------------------\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both datasets use the same x-axis type\n",
    "years_lstm = yearly_weighted['Year']\n",
    "\n",
    "years_glambie = glambie_df['central_europe_end_dates']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# bar width\n",
    "width = 0.4\n",
    "\n",
    "# LSTM bars (slightly shifted left)\n",
    "ax.bar(years_lstm - 0.2,\n",
    "       yearly_weighted['Weighted_MB'],\n",
    "       width=width,\n",
    "       color=\"skyblue\",\n",
    "       label=\"LSTM Annual MB\")\n",
    "\n",
    "# GLAMBIE bars (slightly shifted right)\n",
    "ax.bar(years_glambie + 0.2,\n",
    "       glambie_df['central_europe_annual_change_mwe'],\n",
    "       width=width,\n",
    "       color=\"lightgreen\",\n",
    "       label=\"GLAMBIE Annual MB\")\n",
    "\n",
    "# formatting\n",
    "ax.set_ylabel(\"Annual MB (m w.e.)\")\n",
    "ax.set_title(\"Annual Mass Balance: LSTM vs GLAMBIE\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
