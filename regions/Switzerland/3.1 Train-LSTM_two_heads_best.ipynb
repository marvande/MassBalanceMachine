{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect_sgi\",\n",
    "    \"slope_sgi\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check:\n",
    "df = dataloader_gl.data\n",
    "df[(df.POINT_ID == 'adler_26') & (df.YEAR == 2006)].MONTHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi',\n",
    "    'slope_sgi',\n",
    "    'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness',\n",
    "    'millan_v',\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ('adler', 2009, 11, 'winter')\n",
    "\n",
    "# find the index of this key\n",
    "try:\n",
    "    idx = ds_train.keys.index(key)\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Key {key} not found in dataset.\")\n",
    "\n",
    "# fetch the corresponding sequence\n",
    "sequence = ds_train[idx]\n",
    "sequence['mv'], sequence['mw'], sequence['ma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'logs/lstm_two_heads_param_search_progress_2025-10-09.csv'\n",
    "best_params = get_best_params_for_lstm(log_path, select_by='avg_test_loss')\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = Path(log_path)\n",
    "df = pd.read_csv(log_path)\n",
    "df[\"avg_test_loss\"] = (df[\"test_rmse_a\"] + df[\"test_rmse_w\"]) / 2\n",
    "df.sort_values(by=\"avg_test_loss\", inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topk_param_distributions(log_path, k=5, metric=\"valid_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': len(MONTHLY_COLS),\n",
    "    'Fs': len(STATIC_COLS),\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.2,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None,\n",
    "    'two_heads': True,\n",
    "    'head_dropout': 0.0\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_two_heads.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "# Evaluate on test\n",
    "model_filename = 'models/lstm_model_2025-10-10_two_heads.pt'\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "\n",
    "test_gl_per_el = gl_per_el[TEST_GLACIERS].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(25, 18), sharex=True)\n",
    "\n",
    "gl_per_el = data_glamos[data_glamos.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_df_preds['gl_elv'] = test_df_preds['GLACIER'].map(gl_per_el)\n",
    "\n",
    "subplot_labels = [\n",
    "    '(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)', '(i)'\n",
    "]\n",
    "\n",
    "axs = PlotIndividualGlacierPredVsTruth(test_df_preds,\n",
    "                                       axs=axs,\n",
    "                                       subplot_labels=subplot_labels,\n",
    "                                       color_annual=color_dark_blue,\n",
    "                                       color_winter=color_pink,\n",
    "                                       custom_order=test_gl_per_el)\n",
    "\n",
    "axs[3].set_ylabel(\"Modelled PMB [m w.e.]\", fontsize=20)\n",
    "\n",
    "fig.supxlabel('Observed PMB [m w.e.]', fontsize=20, y=0.06)\n",
    "# two distinct handles\n",
    "legend_scatter_annual = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_annual,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Annual')\n",
    "\n",
    "legend_scatter_winter = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=color_winter,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Winter')\n",
    "\n",
    "# if you already have other handles (e.g., bands/means), append these:\n",
    "# handles = existing_handles + [legend_scatter_annual, legend_scatter_winter]\n",
    "handles = [legend_scatter_annual, legend_scatter_winter]\n",
    "\n",
    "# You can let matplotlib use the labels from the handles; no need to pass `labels=...`\n",
    "fig.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 0.05),\n",
    "           ncol=4,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate in space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "glacier_list = list(data_glamos.GLACIER.unique())\n",
    "print('Number of glaciers with pcsr:', len(glacier_list))\n",
    "\n",
    "geodetic_glaciers = periods_per_glacier.keys()\n",
    "print('Number of glaciers with geodetic MB:', len(geodetic_glaciers))\n",
    "\n",
    "# Intersection of both\n",
    "common_glaciers = list(set(geodetic_glaciers) & set(glacier_list))\n",
    "print('Number of common glaciers:', len(common_glaciers))\n",
    "\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = sort_by_area(common_glaciers, gl_area)\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# # Required by the dataset builder regardless of your feature list\n",
    "# REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "\n",
    "# # Paths\n",
    "# path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "#                              'MBM/glamos_dems_LSTM_two_heads_svf')\n",
    "# os.makedirs(path_save_glw, exist_ok=True)\n",
    "# path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM',\n",
    "#                              'xr_masked_grids')\n",
    "\n",
    "# # Load model once\n",
    "# best_state = torch.load(model_filename, map_location=device)\n",
    "# model.load_state_dict(best_state)\n",
    "# model.eval()\n",
    "\n",
    "# RUN = False\n",
    "# if RUN:\n",
    "#     emptyfolder(path_save_glw)\n",
    "#     for glacier_name in glacier_list:\n",
    "#         glacier_path = os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "#                                     glacier_name)\n",
    "#         if not os.path.exists(glacier_path):\n",
    "#             print(f\"Folder not found for {glacier_name}, skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         glacier_files = sorted(\n",
    "#             [f for f in os.listdir(glacier_path) if glacier_name in f])\n",
    "\n",
    "#         geodetic_range = range(np.min(periods_per_glacier[glacier_name]),\n",
    "#                                np.max(periods_per_glacier[glacier_name]) + 1)\n",
    "\n",
    "#         years = [int(f.split('_')[2].split('.')[0]) for f in glacier_files]\n",
    "#         years = [y for y in years if y in geodetic_range]\n",
    "\n",
    "#         print(\n",
    "#             f\"Processing {glacier_name} ({len(years)} files): {geodetic_range}\"\n",
    "#         )\n",
    "\n",
    "#         for year in tqdm(years, desc=f\"Processing {glacier_name}\",\n",
    "#                          leave=False):\n",
    "#             seed_all(cfg.seed)\n",
    "\n",
    "#             file_name = f\"{glacier_name}_grid_{year}.parquet\"\n",
    "#             df_grid_monthly = pd.read_parquet(\n",
    "#                 os.path.join(cfg.dataPath + path_glacier_grid_glamos,\n",
    "#                              glacier_name, file_name)).copy()\n",
    "\n",
    "#             df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "#             # Keep required + feature columns; DON'T drop PERIOD/MONTHS/YEAR/ID/GLACIER\n",
    "#             keep = [\n",
    "#                 c for c in (set(all_columns) | set(REQUIRED))\n",
    "#                 if c in df_grid_monthly.columns\n",
    "#             ]\n",
    "#             df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "#             # --- Build winter subset (Sep–Apr) ---\n",
    "#             winter_months = [\n",
    "#                 'sep', 'oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr'\n",
    "#             ]\n",
    "#             df_grid_monthly_w = (\n",
    "#                 df_grid_monthly[df_grid_monthly['MONTHS'].str.lower().isin(\n",
    "#                     winter_months)].copy().dropna(subset=['ID', 'MONTHS']))\n",
    "#             # tag period\n",
    "#             df_grid_monthly_w['PERIOD'] = 'winter'\n",
    "\n",
    "#             # Minimal NaN clean-up for annual too\n",
    "#             df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "\n",
    "#             # --- Build ds_gl_a WITHOUT targets (annual) ---\n",
    "#             ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#                 ds_train)\n",
    "#             ds_train_copy.fit_scalers(train_idx)  # fit only\n",
    "\n",
    "#             ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "#                 df_grid_monthly_a,\n",
    "#                 MONTHLY_COLS,\n",
    "#                 STATIC_COLS,\n",
    "#                 months_tail_pad=months_tail_pad,\n",
    "#                 months_head_pad=months_head_pad,\n",
    "#                 expect_target=False,\n",
    "#                 show_progress=False,\n",
    "#             )\n",
    "\n",
    "#             test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#                 ds_gl_a, ds_train_copy, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "#             # Predict annual (no metrics)\n",
    "#             df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "\n",
    "#             # Join preds back to unique cell IDs for saving (annual)\n",
    "#             data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "#             grouped_ids_a = (df_grid_monthly_a.groupby('ID')[[\n",
    "#                 'YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID'\n",
    "#             ]].first().merge(data_a,\n",
    "#                              left_index=True,\n",
    "#                              right_index=True,\n",
    "#                              how='left'))\n",
    "#             months_per_id_a = df_grid_monthly_a.groupby(\n",
    "#                 'ID')['MONTHS'].unique()\n",
    "#             grouped_ids_a = grouped_ids_a.merge(months_per_id_a,\n",
    "#                                                 left_index=True,\n",
    "#                                                 right_index=True)\n",
    "\n",
    "#             grouped_ids_a.reset_index(inplace=True)\n",
    "#             grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "#             # Annual output\n",
    "#             grouped_ids_annual = grouped_ids_a.copy()\n",
    "#             grouped_ids_annual['PERIOD'] = 'annual'\n",
    "#             pred_y_annual = grouped_ids_annual.drop(columns=['YEAR'],\n",
    "#                                                     errors='ignore')\n",
    "\n",
    "#             # Load geo grid once per year\n",
    "#             path_glacier_dem = os.path.join(cfg.dataPath, path_xr_grids,\n",
    "#                                             f\"{glacier_name}_{year}.zarr\")\n",
    "#             ds = xr.open_dataset(path_glacier_dem)\n",
    "#             geoData = mbm.geodata.GeoData(df_grid_monthly_a,\n",
    "#                                           months_head_pad=months_head_pad,\n",
    "#                                           months_tail_pad=months_tail_pad)\n",
    "\n",
    "#             # Save annual\n",
    "#             geoData._save_prediction(ds, pred_y_annual, glacier_name, year,\n",
    "#                                      path_save_glw, \"annual\")\n",
    "\n",
    "#             # ----------------------------\n",
    "#             # WINTER PREDICTIONS (new)\n",
    "#             # ----------------------------\n",
    "#             if len(df_grid_monthly_w) == 0:\n",
    "#                 print(\n",
    "#                     f\"[skip-winter] {glacier_name} {year}: no winter months present.\"\n",
    "#                 )\n",
    "#                 continue\n",
    "\n",
    "#             # Build ds_gl_w WITHOUT targets (winter), reusing same fitted scalers\n",
    "#             ds_gl_w = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "#                 df_grid_monthly_w,\n",
    "#                 MONTHLY_COLS,\n",
    "#                 STATIC_COLS,\n",
    "#                 months_tail_pad=months_tail_pad,\n",
    "#                 months_head_pad=months_head_pad,\n",
    "#                 expect_target=False,\n",
    "#                 show_progress=False,\n",
    "#             )\n",
    "\n",
    "#             test_gl_dl_w = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#                 ds_gl_w, ds_train_copy, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "#             # Predict winter (no metrics)\n",
    "#             df_preds_w = model.predict_with_keys(device, test_gl_dl_w, ds_gl_w)\n",
    "\n",
    "#             # Join preds back to unique cell IDs for saving (winter)\n",
    "#             data_w = df_preds_w[['ID', 'pred']].set_index('ID')\n",
    "#             grouped_ids_w = (df_grid_monthly_w.groupby('ID')[[\n",
    "#                 'YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID'\n",
    "#             ]].first().merge(data_w,\n",
    "#                              left_index=True,\n",
    "#                              right_index=True,\n",
    "#                              how='left'))\n",
    "#             months_per_id_w = df_grid_monthly_w.groupby(\n",
    "#                 'ID')['MONTHS'].unique()\n",
    "#             grouped_ids_w = grouped_ids_w.merge(months_per_id_w,\n",
    "#                                                 left_index=True,\n",
    "#                                                 right_index=True)\n",
    "\n",
    "#             grouped_ids_w.reset_index(inplace=True)\n",
    "#             grouped_ids_w.sort_values(by='ID', inplace=True)\n",
    "\n",
    "#             # Winter output\n",
    "#             grouped_ids_winter = grouped_ids_w.copy()\n",
    "#             grouped_ids_winter['PERIOD'] = 'winter'\n",
    "#             pred_y_winter = grouped_ids_winter.drop(columns=['YEAR'],\n",
    "#                                                     errors='ignore')\n",
    "\n",
    "#             # Save winter (reuse ds and geoData)\n",
    "#             # Note: pass the winter df to GeoData if you need month filtering inside _save_prediction\n",
    "#             geoData_w = mbm.geodata.GeoData(df_grid_monthly_w,\n",
    "#                                             months_head_pad=months_head_pad,\n",
    "#                                             months_tail_pad=months_tail_pad)\n",
    "#             geoData_w._save_prediction(ds, pred_y_winter, glacier_name, year,\n",
    "#                                        path_save_glw, \"winter\")\n",
    "\n",
    "# # quick viz\n",
    "# glacier_name = 'aletsch'\n",
    "# year = 2008\n",
    "# xr.open_dataset(os.path.join(path_save_glw, f'{glacier_name}/{glacier_name}_{year}_annual.zarr'))\\\n",
    "#   .pred_masked.plot(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parallelized glacier-year inference & save (CPU, Linux) ===\n",
    "import os, sys, io, logging, multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from contextlib import redirect_stdout\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "\n",
    "# ----------------- quiet main logging (optional) -----------------\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_PATH = f\"logs/predict_glaciers_{datetime.now():%Y%m%d_%H%M%S}.log\"\n",
    "logging.basicConfig(\n",
    "    filename=LOG_PATH, level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "log = logging.getLogger(\"predict\")\n",
    "\n",
    "# ----------------- constants & paths -----------------\n",
    "REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "all_columns = MONTHLY_COLS + STATIC_COLS + cfg.fieldsNotFeatures\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/glamos_dems_LSTM_two_heads')\n",
    "os.makedirs(path_save_glw, exist_ok=True)\n",
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS', 'topo', 'GLAMOS_DEM', 'xr_masked_grids')\n",
    "\n",
    "# ----------------- worker init (quiet + CPU threads cap) -----------------\n",
    "def _worker_init_quiet():\n",
    "    # keep stderr for tqdm in main; silence worker prints\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    sys.stderr = open(os.devnull, \"w\")\n",
    "    os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")   # CPU only\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"1\")\n",
    "    try:\n",
    "        torch.set_num_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ----------------- per-process model cache -----------------\n",
    "_MODEL = None\n",
    "def _get_model_cpu(cfg, params_used, model_filename):\n",
    "    \"\"\"Build+load the model once per worker (cached).\"\"\"\n",
    "    global _MODEL\n",
    "    if _MODEL is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(cfg, params_used, device, verbose=False)\n",
    "        state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval()\n",
    "        _MODEL = model\n",
    "    return _MODEL\n",
    "\n",
    "# ----------------- one glacier-year task -----------------\n",
    "def _process_glacier_year(args):\n",
    "    glacier_name, year = args  # everything else taken from globals via fork\n",
    "    try:\n",
    "        # Seed for reproducibility if you wish\n",
    "        seed_all(cfg.seed)\n",
    "\n",
    "        glacier_path = os.path.join(cfg.dataPath + path_glacier_grid_glamos, glacier_name)\n",
    "        if not os.path.exists(glacier_path):\n",
    "            return (\"skip\", glacier_name, year, \"glacier folder missing\")\n",
    "\n",
    "        file_name = f\"{glacier_name}_grid_{year}.parquet\"\n",
    "        parquet_path = os.path.join(glacier_path, file_name)\n",
    "        if not os.path.exists(parquet_path):\n",
    "            return (\"skip\", glacier_name, year, \"parquet missing\")\n",
    "\n",
    "        df_grid_monthly = pd.read_parquet(parquet_path).copy()\n",
    "        df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Keep required + feature columns; preserve order\n",
    "        needed = set(all_columns) | set(REQUIRED)\n",
    "        keep = [c for c in df_grid_monthly.columns if c in needed]\n",
    "        df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "        # Build winter subset (Sep–Apr)\n",
    "        winter_months = ['sep','oct','nov','dec','jan','feb','mar','apr']\n",
    "        df_grid_monthly_w = (df_grid_monthly[df_grid_monthly['MONTHS'].str.lower().isin(winter_months)]\n",
    "                             .copy().dropna(subset=['ID','MONTHS']))\n",
    "        df_grid_monthly_w['PERIOD'] = 'winter'\n",
    "\n",
    "        # Minimal NaN cleanup for annual\n",
    "        df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID','MONTHS'])\n",
    "\n",
    "        # Fit scalers on TRAIN only (clone/train ds are global via fork)\n",
    "        ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(ds_train)\n",
    "        ds_train_copy.fit_scalers(train_idx)\n",
    "\n",
    "        # Annual dataset/loader\n",
    "        ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_grid_monthly_a, MONTHLY_COLS, STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad, months_head_pad=months_head_pad,\n",
    "            expect_target=False, show_progress=False\n",
    "        )\n",
    "        test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_gl_a, ds_train_copy, seed=cfg.seed, batch_size=128\n",
    "        )\n",
    "\n",
    "        # Model (cached per worker)\n",
    "        model = _get_model_cpu(cfg, custom_params, model_filename)\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        # Predict annual\n",
    "        df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "\n",
    "        # Aggregate annual\n",
    "        data_a = df_preds_a[['ID','pred']].set_index('ID')\n",
    "        meta_cols = [c for c in ['YEAR','POINT_LAT','POINT_LON','GLWD_ID'] if c in df_grid_monthly_a.columns]\n",
    "        grouped_ids_a = (df_grid_monthly_a.groupby('ID')[meta_cols].first()\n",
    "                         .merge(data_a, left_index=True, right_index=True, how='left'))\n",
    "        months_per_id_a = df_grid_monthly_a.groupby('ID')['MONTHS'].unique()\n",
    "        grouped_ids_a = grouped_ids_a.merge(months_per_id_a, left_index=True, right_index=True)\n",
    "        grouped_ids_a.reset_index(inplace=True)\n",
    "        grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "        pred_y_annual = grouped_ids_a.copy()\n",
    "        pred_y_annual['PERIOD'] = 'annual'\n",
    "        pred_y_annual = pred_y_annual.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        # Load per-year DEM grid and save annual\n",
    "        path_glacier_dem = os.path.join(path_xr_grids, f\"{glacier_name}_{year}.zarr\")\n",
    "        if not os.path.exists(path_glacier_dem):\n",
    "            return (\"skip\", glacier_name, year, \"DEM zarr missing\")\n",
    "        ds = xr.open_dataset(path_glacier_dem)\n",
    "\n",
    "        geoData = mbm.geodata.GeoData(df_grid_monthly_a,\n",
    "                                      months_head_pad=months_head_pad,\n",
    "                                      months_tail_pad=months_tail_pad)\n",
    "        geoData._save_prediction(ds, pred_y_annual, glacier_name, year, path_save_glw, \"annual\")\n",
    "\n",
    "        # Winter branch\n",
    "        if len(df_grid_monthly_w) == 0:\n",
    "            return (\"ok\", glacier_name, year, \"no winter months\")\n",
    "\n",
    "        ds_gl_w = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "            df_grid_monthly_w, MONTHLY_COLS, STATIC_COLS,\n",
    "            months_tail_pad=months_tail_pad, months_head_pad=months_head_pad,\n",
    "            expect_target=False, show_progress=False\n",
    "        )\n",
    "        test_gl_dl_w = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_gl_w, ds_train_copy, seed=cfg.seed, batch_size=128\n",
    "        )\n",
    "\n",
    "        df_preds_w = model.predict_with_keys(device, test_gl_dl_w, ds_gl_w)\n",
    "\n",
    "        data_w = df_preds_w[['ID','pred']].set_index('ID')\n",
    "        grouped_ids_w = (df_grid_monthly_w.groupby('ID')[meta_cols].first()\n",
    "                         .merge(data_w, left_index=True, right_index=True, how='left'))\n",
    "        months_per_id_w = df_grid_monthly_w.groupby('ID')['MONTHS'].unique()\n",
    "        grouped_ids_w = grouped_ids_w.merge(months_per_id_w, left_index=True, right_index=True)\n",
    "        grouped_ids_w.reset_index(inplace=True)\n",
    "        grouped_ids_w.sort_values(by='ID', inplace=True)\n",
    "\n",
    "        pred_y_winter = grouped_ids_w.copy()\n",
    "        pred_y_winter['PERIOD'] = 'winter'\n",
    "        pred_y_winter = pred_y_winter.drop(columns=['YEAR'], errors='ignore')\n",
    "\n",
    "        geoData_w = mbm.geodata.GeoData(df_grid_monthly_w,\n",
    "                                        months_head_pad=months_head_pad,\n",
    "                                        months_tail_pad=months_tail_pad)\n",
    "        geoData_w._save_prediction(ds, pred_y_winter, glacier_name, year, path_save_glw, \"winter\")\n",
    "\n",
    "        return (\"ok\", glacier_name, year, \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (\"err\", glacier_name, year, str(e))\n",
    "\n",
    "# ----------------- build tasks -----------------\n",
    "tasks = []\n",
    "for glacier_name in glacier_list:\n",
    "    glacier_path = os.path.join(cfg.dataPath + path_glacier_grid_glamos, glacier_name)\n",
    "    if not os.path.exists(glacier_path):\n",
    "        continue\n",
    "    glacier_files = sorted([f for f in os.listdir(glacier_path) if glacier_name in f and f.endswith(\".parquet\")])\n",
    "    if not glacier_files:\n",
    "        continue\n",
    "    geodetic_range = range(np.min(periods_per_glacier[glacier_name]),\n",
    "                           np.max(periods_per_glacier[glacier_name]) + 1)\n",
    "    years = [int(f.split('_')[2].split('.')[0]) for f in glacier_files]\n",
    "    years = [y for y in years if y in geodetic_range]\n",
    "    for y in years:\n",
    "        tasks.append((glacier_name, y))\n",
    "\n",
    "# ----------------- run in parallel (quiet stdout, keep tqdm) -----------------\n",
    "class _Devnull(io.StringIO):\n",
    "    def write(self, *args, **kwargs): return 0\n",
    "\n",
    "ctx = mp.get_context(\"fork\")  # Linux\n",
    "max_workers = min(max(1, (os.cpu_count() or 2) - 1), 32)\n",
    "\n",
    "with redirect_stdout(_Devnull()):  # keep stderr so tqdm is visible\n",
    "    ok = skip = err = 0\n",
    "    with ProcessPoolExecutor(max_workers=max_workers,\n",
    "                             initializer=_worker_init_quiet,\n",
    "                             mp_context=ctx) as ex:\n",
    "        futures = [ex.submit(_process_glacier_year, t) for t in tasks]\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures),\n",
    "                        desc=f\"Predicting ({max_workers} workers)\"):\n",
    "            status, g, y, msg = fut.result()\n",
    "            if status == \"ok\":\n",
    "                ok += 1\n",
    "                if msg:  # no winter months, non-fatal\n",
    "                    log.info(f\"OK {g} {y}: {msg}\")\n",
    "            elif status == \"skip\":\n",
    "                skip += 1\n",
    "                log.warning(f\"SKIP {g} {y}: {msg}\")\n",
    "            else:\n",
    "                err += 1\n",
    "                log.error(f\"ERR {g} {y}: {msg}\")\n",
    "\n",
    "log.info(f\"SUMMARY: ok={ok} skip={skip} err={err} total={len(tasks)}\")\n",
    "print(f\"Done. Logs → {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers_in_glamos = os.listdir(path_save_glw)\n",
    "\n",
    "geodetic_mb = get_geodetic_MB(cfg)\n",
    "\n",
    "# get years per glacier\n",
    "years_start_per_gl = geodetic_mb.groupby(\n",
    "    'glacier_name')['Astart'].unique().apply(list).to_dict()\n",
    "years_end_per_gl = geodetic_mb.groupby('glacier_name')['Aend'].unique().apply(\n",
    "    list).to_dict()\n",
    "\n",
    "periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "\n",
    "# Glaciers with geodetic MB data:\n",
    "# Sort glaciers by area\n",
    "gl_area = get_gl_area(cfg)\n",
    "gl_area['clariden'] = gl_area['claridenL']\n",
    "\n",
    "\n",
    "# Sort the lists by area if available in gl_area\n",
    "def sort_by_area(glacier_list, gl_area):\n",
    "    return sorted(glacier_list, key=lambda g: gl_area.get(g, 0), reverse=False)\n",
    "\n",
    "\n",
    "glacier_list = [\n",
    "    f for f in list(periods_per_glacier.keys()) if f in glaciers_in_glamos\n",
    "]\n",
    "glacier_list = sort_by_area(glacier_list, gl_area)\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "print('Glaciers:', glacier_list)\n",
    "\n",
    "df_all_nn = process_geodetic_mass_balance_comparison(\n",
    "    glacier_list=glacier_list,\n",
    "    path_SMB_GLAMOS_csv=cfg.dataPath + path_SMB_GLAMOS_csv,\n",
    "    periods_per_glacier=periods_per_glacier,\n",
    "    geoMB_per_glacier=geoMB_per_glacier,\n",
    "    gl_area=gl_area,\n",
    "    test_glaciers=TEST_GLACIERS,\n",
    "    path_predictions=path_save_glw,  # or another path if needed\n",
    "    cfg=cfg)\n",
    "\n",
    "# Drop rows where any required columns are NaN\n",
    "df_all_nn = df_all_nn.dropna(subset=['Geodetic MB', 'MBM MB'])\n",
    "df_all_nn = df_all_nn.sort_values(by='Area')\n",
    "df_all_nn['GLACIER'] = df_all_nn['GLACIER'].apply(lambda x: x.capitalize())\n",
    "\n",
    "# Compute RMSE and Pearson correlation\n",
    "rmse_nn = root_mean_squared_error(df_all_nn[\"Geodetic MB\"],\n",
    "                                  df_all_nn[\"MBM MB\"])\n",
    "corr_nn = np.corrcoef(df_all_nn[\"Geodetic MB\"], df_all_nn[\"MBM MB\"])[0, 1]\n",
    "\n",
    "plot_mbm_vs_geodetic_by_area_bin(df_all_nn,\n",
    "                                 bins=[0, 1, 5, 10, 100, np.inf],\n",
    "                                 labels=['<1', '1-5', '5–10', '>10', '>100'],\n",
    "                                 max_bins=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def permutation_importance_lstm(\n",
    "    model,\n",
    "    device,\n",
    "    ds_test,                 # MBSequenceDataset (already tensorized)\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    *,\n",
    "    ds_train_with_scalers=None,   # <<< NEW: provide a train ds that has y_mean/y_std set\n",
    "    batch_size: int = 128,\n",
    "    n_repeats: int = 10,\n",
    "    seed: int = 42,\n",
    "    metric: str = \"RMSE_mean\",    # \"RMSE_annual\" | \"RMSE_winter\" | \"RMSE_mean\"\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Permutation importance for an LSTM with sequence + static features.\n",
    "\n",
    "    IMPORTANT:\n",
    "      - `evaluate_with_preds` needs ds.y_mean / ds.y_std to de-normalize targets.\n",
    "      - Pass `ds_train_with_scalers` (a dataset that already ran fit_scalers(...))\n",
    "        so we can inject y_mean/y_std into ds_test and its permuted copies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_imp, baseline_score\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    model.eval()\n",
    "\n",
    "    # --- ensure target scalers exist on the base test dataset ---\n",
    "    if ds_train_with_scalers is not None:\n",
    "        # copy target scalers (tensors) if missing\n",
    "        if getattr(ds_test, \"y_mean\", None) is None:\n",
    "            ds_test.y_mean = ds_train_with_scalers.y_mean.clone()\n",
    "        if getattr(ds_test, \"y_std\", None) is None:\n",
    "            ds_test.y_std = ds_train_with_scalers.y_std.clone()\n",
    "\n",
    "    # --- helper to compute scalar score from metrics dict ---\n",
    "    def score_from_metrics(metrics: dict) -> float:\n",
    "        if metric == \"RMSE_annual\":\n",
    "            return float(metrics[\"RMSE_annual\"])\n",
    "        elif metric == \"RMSE_winter\":\n",
    "            return float(metrics[\"RMSE_winter\"])\n",
    "        elif metric == \"RMSE_mean\":\n",
    "            vals = [float(v) for k, v in metrics.items()\n",
    "                    if k.startswith(\"RMSE_\") and np.isfinite(v)]\n",
    "            return float(np.mean(vals)) if len(vals) else float(\"nan\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric '{metric}'\")\n",
    "\n",
    "    # --- baseline ---\n",
    "    base_dl = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    base_metrics, _ = model.evaluate_with_preds(device, base_dl, ds_test)\n",
    "    baseline_score = score_from_metrics(base_metrics)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ---- STATIC FEATURES ----\n",
    "    for feat in STATIC_COLS:\n",
    "        col_idx = STATIC_COLS.index(feat)\n",
    "        deltas = []\n",
    "        for _ in range(n_repeats):\n",
    "            # shallow-copy dataset object; clone only tensors we mutate\n",
    "            ds_perm = copy.copy(ds_test)\n",
    "            Xs_perm = ds_test.Xs.clone()\n",
    "\n",
    "            # inject target scalers for this permuted copy as well\n",
    "            if ds_train_with_scalers is not None:\n",
    "                ds_perm.y_mean = ds_train_with_scalers.y_mean.clone()\n",
    "                ds_perm.y_std  = ds_train_with_scalers.y_std.clone()\n",
    "\n",
    "            col = Xs_perm[:, col_idx].cpu().numpy()\n",
    "            Xs_perm[:, col_idx] = torch.from_numpy(rng.permutation(col)).to(Xs_perm.dtype)\n",
    "            ds_perm.Xs = Xs_perm\n",
    "\n",
    "            dl = DataLoader(ds_perm, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "            m, _ = model.evaluate_with_preds(device, dl, ds_perm)\n",
    "            deltas.append(score_from_metrics(m) - baseline_score)\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": feat,\n",
    "            \"group\": \"static\",\n",
    "            \"metric\": metric,\n",
    "            \"mean_importance\": float(np.mean(deltas)),\n",
    "            \"std_importance\": float(np.std(deltas, ddof=1) if len(deltas) > 1 else 0.0),\n",
    "        })\n",
    "\n",
    "    # ---- MONTHLY FEATURES (permute full 12-mo sequences across samples) ----\n",
    "    for feat in MONTHLY_COLS:\n",
    "        fidx = MONTHLY_COLS.index(feat)\n",
    "        deltas = []\n",
    "        for _ in range(n_repeats):\n",
    "            ds_perm = copy.copy(ds_test)\n",
    "            Xm_perm = ds_test.Xm.clone()  # (N, 12, Fm)\n",
    "\n",
    "            if ds_train_with_scalers is not None:\n",
    "                ds_perm.y_mean = ds_train_with_scalers.y_mean.clone()\n",
    "                ds_perm.y_std  = ds_train_with_scalers.y_std.clone()\n",
    "\n",
    "            # sequences shape (N, 12)\n",
    "            seqs = Xm_perm[:, :, fidx].cpu().numpy()\n",
    "            seqs_perm = rng.permutation(seqs)  # permute across samples\n",
    "            Xm_perm[:, :, fidx] = torch.from_numpy(seqs_perm).to(Xm_perm.dtype)\n",
    "            ds_perm.Xm = Xm_perm\n",
    "\n",
    "            dl = DataLoader(ds_perm, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory)\n",
    "            m, _ = model.evaluate_with_preds(device, dl, ds_perm)\n",
    "            deltas.append(score_from_metrics(m) - baseline_score)\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": feat,\n",
    "            \"group\": \"monthly\",\n",
    "            \"metric\": metric,\n",
    "            \"mean_importance\": float(np.mean(deltas)),\n",
    "            \"std_importance\": float(np.std(deltas, ddof=1) if len(deltas) > 1 else 0.0),\n",
    "        })\n",
    "\n",
    "    df_imp = (pd.DataFrame(results)\n",
    "              .sort_values(\"mean_importance\", ascending=False)\n",
    "              .reset_index(drop=True))\n",
    "    return df_imp, baseline_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scalers on TRAIN (as you do for inference)\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(ds_train)\n",
    "ds_train_copy.fit_scalers(train_idx)\n",
    "\n",
    "# IMPORTANT: ds_test must already be *tensorized* (has Xm/Xs),\n",
    "# and should be transformed with the same scalers you use at inference.\n",
    "# If you normally construct a test loader like this:\n",
    "#   test_dl = MBSequenceDataset.make_test_loader(ds_test, ds_train_copy, ...)\n",
    "# you can run that once up-front to ensure ds_test.Xm/Xs are standardized.\n",
    "\n",
    "df_imp, baseline = permutation_importance_lstm(\n",
    "    model,\n",
    "    device=torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\"),\n",
    "    ds_test=ds_test,                         # the tensorized/standardized test dataset\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    ds_train_with_scalers=ds_train_copy,     # <<< gives us y_mean/y_std\n",
    "    batch_size=128,\n",
    "    n_repeats=10,\n",
    "    seed=cfg.seed,\n",
    "    metric=\"RMSE_mean\",\n",
    ")\n",
    "\n",
    "print(f\"Baseline {df_imp.metric.iloc[0]}: {baseline:.4f}\")\n",
    "print(df_imp.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_importance(df_imp, top_n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
