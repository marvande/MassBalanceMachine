{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from contextlib import redirect_stdout\n",
    "from datetime import datetime\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Make repo root importable (for MBM & scripts/*)\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# --- Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import xarray as xr\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Project-local\n",
    "from scripts.utils import *\n",
    "from scripts.glamos import *\n",
    "from scripts.models import *\n",
    "from scripts.geo_data import *\n",
    "from scripts.dataset import *\n",
    "from scripts.geodetic import *\n",
    "from scripts.plotting import *\n",
    "\n",
    "# --- Notebook settings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "use_mbm_style()\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read GLAMOS stake data\n",
    "data_glamos = get_stakes_data(cfg)\n",
    "\n",
    "# Compute padding for monthly data\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    df=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Remove 2025\n",
    "data_monthly = data_monthly[data_monthly['YEAR']\n",
    "                            < 2025]  # Used elsewhere for validation\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = data_train\n",
    "data_train['y'] = data_train['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to start of August instead:\n",
    "# Convert to str → parse → replace month/day → convert back to int\n",
    "data_glamos_Aug_ = data_glamos.copy()\n",
    "data_glamos_Aug_[\"FROM_DATE\"] = (\n",
    "    data_glamos_Aug_[\"FROM_DATE\"].astype(str).str.slice(0,\n",
    "                                                        4)  # extract year YYYY\n",
    "    .astype(int).astype(str) + \"0801\"  # append \"0801\"\n",
    ").astype(int)\n",
    "\n",
    "# Same for full temporal resolution (run or load data):\n",
    "# Compute padding for monthly data\n",
    "months_head_pad_Aug_, months_tail_pad_Aug_ = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos_Aug_)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "RUN = False\n",
    "data_monthly_Aug_ = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos_Aug_,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_Aug_.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl_Aug_ = mbm.dataloader.DataLoader(cfg,\n",
    "                                               data=data_monthly_Aug_,\n",
    "                                               random_seed=cfg.seed,\n",
    "                                               meta_data_columns=cfg.metaData)\n",
    "\n",
    "# Remove 2025\n",
    "data_monthly_Aug_ = data_monthly_Aug_[data_monthly_Aug_['YEAR']\n",
    "                                      < 2025]  # Used elsewhere for validation\n",
    "\n",
    "# Blocking on glaciers:\n",
    "# Model is trained on all glaciers --> \"Within sample\"\n",
    "\n",
    "existing_glaciers = set(data_monthly_Aug_.GLACIER.unique())\n",
    "train_glaciers = existing_glaciers\n",
    "data_train_Aug_ = data_monthly_Aug_[data_monthly_Aug_.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train_Aug_))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train_Aug_ = data_train_Aug_\n",
    "data_train_Aug_['y'] = data_train_Aug_['POINT_BALANCE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "ds_train = build_combined_LSTM_dataset(df_loss=data_train,\n",
    "                                       df_full=data_train_Aug_,\n",
    "                                       monthly_cols=MONTHLY_COLS,\n",
    "                                       static_cols=STATIC_COLS,\n",
    "                                       months_head_pad=months_head_pad_Aug_,\n",
    "                                       months_tail_pad=months_tail_pad_Aug_,\n",
    "                                       normalize_target=True,\n",
    "                                       expect_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list, month_pos = mbm.data_processing.utils._rebuild_month_index(\n",
    "    months_head_pad_Aug_, months_tail_pad_Aug_)\n",
    "month_order = [m for m, _ in sorted(month_pos.items(), key=lambda x: x[1])]\n",
    "print(\"Month order used in sequences:\", month_order)\n",
    "inspect_LSTM_sample(ds_train, 0, month_labels=month_order)\n",
    "inspect_LSTM_sample(ds_train, 10, month_labels=month_order)\n",
    "inspect_LSTM_sample(ds_train, 150, month_labels=month_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_LSTM_padded_months(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def generate_param_sets(const_params, param_grid):\n",
    "    \"\"\"\n",
    "    Generate all valid parameter combinations.\n",
    "    Returns: List[dict]\n",
    "    \"\"\"\n",
    "    keys = list(param_grid.keys())\n",
    "    values = [param_grid[k] for k in keys]\n",
    "\n",
    "    param_sets = []\n",
    "\n",
    "    for combo in product(*values):\n",
    "        params = dict(zip(keys, combo))\n",
    "\n",
    "        # ---- conditional logic ----\n",
    "\n",
    "        # 1) If no static MLP, ignore static_hidden & static_dropout\n",
    "        if params[\"static_layers\"] == 0:\n",
    "            params[\"static_hidden\"] = None\n",
    "            params[\"static_dropout\"] = None\n",
    "\n",
    "        # 2) LSTM dropout only relevant if stacked\n",
    "        if params[\"num_layers\"] == 1:\n",
    "            params[\"dropout\"] = 0.0\n",
    "\n",
    "        # 3) Head dropout sanity\n",
    "        if params[\"head_dropout\"] < 0.0:\n",
    "            continue\n",
    "\n",
    "        # Merge constants\n",
    "        full_params = {**const_params, **params}\n",
    "        param_sets.append(full_params)\n",
    "\n",
    "    return param_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_sets(\n",
    "    const_params,\n",
    "    param_grid,\n",
    "    n_samples: int,\n",
    "    seed: int = cfg.seed,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate all valid parameter combinations, then randomly sample n_samples.\n",
    "    \"\"\"\n",
    "    all_params = generate_param_sets(const_params, param_grid)\n",
    "\n",
    "    if n_samples >= len(all_params):\n",
    "        print(f\"Requested {n_samples}, but only {len(all_params)} available. Using all.\")\n",
    "        return all_params\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    return rng.sample(all_params, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_params = {\n",
    "    \"Fm\": ds_train.Xm.shape[-1],\n",
    "    \"Fs\": ds_train.Xs.shape[-1],\n",
    "    \"bidirectional\": False,\n",
    "    \"two_heads\": False,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    # ----- LSTM -----\n",
    "    \"hidden_size\": [64, 96, 128, 160],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"dropout\": [0.1, 0.2, 0.3],\n",
    "\n",
    "    # ----- static MLP -----\n",
    "    \"static_layers\": [0, 1, 2],\n",
    "    \"static_hidden\": [32, 64, 128],\n",
    "    \"static_dropout\": [0.1, 0.2, 0.3],\n",
    "\n",
    "    # ----- head -----\n",
    "    \"head_dropout\": [0.0, 0.05, 0.1],\n",
    "\n",
    "    # ----- optimization -----\n",
    "    \"lr\": [5e-4, 1e-3, 2e-3],\n",
    "    \"weight_decay\": [1e-5, 1e-4],\n",
    "}\n",
    "\n",
    "# sampled_params = generate_param_sets(const_params, param_grid)\n",
    "# print(f\"Total runs: {len(sampled_params)}\")\n",
    "\n",
    "N_SAMPLES = 300  # or 50, 200, etc.\n",
    "\n",
    "sampled_params = sample_param_sets(\n",
    "    const_params,\n",
    "    param_grid,\n",
    "    n_samples=N_SAMPLES,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(f\"Running {len(sampled_params)} random configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib, json\n",
    "RUN = True\n",
    "if RUN:\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    log_filename = f'logs/lstm_param_search_progress_IS_past_{datetime.now().strftime(\"%Y-%m-%d\")}.csv'\n",
    "\n",
    "    # create log with header\n",
    "    with open(log_filename, mode='w', newline='') as log_file:\n",
    "        writer = csv.DictWriter(log_file,\n",
    "                                fieldnames=list(sampled_params[0].keys()) +\n",
    "                                ['valid_loss', 'test_rmse_a', 'test_rmse_w'])\n",
    "        writer.writeheader()\n",
    "\n",
    "    results = []\n",
    "    best_overall = {\"val\": float('inf'), \"row\": None, \"params\": None}\n",
    "\n",
    "    for i, params in enumerate(sampled_params):\n",
    "        seed_all(cfg.seed)\n",
    "\n",
    "        def param_hash(params):\n",
    "            s = json.dumps(params, sort_keys=True)\n",
    "            return hashlib.md5(s.encode()).hexdigest()[:8]\n",
    "\n",
    "        run_id = param_hash(params)\n",
    "        model_filename = f\"models/GS_past/best_lstm_IS_{run_id}.pt\"\n",
    "\n",
    "        # delete existing model file:\n",
    "        if os.path.exists(model_filename):\n",
    "            os.remove(model_filename)\n",
    "            print(f\"Deleted existing model file: {model_filename}\")\n",
    "\n",
    "        # --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "        ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train)\n",
    "\n",
    "        ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "            ds_train)\n",
    "\n",
    "        train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "            train_idx=train_idx,\n",
    "            val_idx=val_idx,\n",
    "            batch_size_train=64,\n",
    "            batch_size_val=128,\n",
    "            seed=cfg.seed,\n",
    "            fit_and_transform=\n",
    "            True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "            shuffle_train=True,\n",
    "            use_weighted_sampler=True  # use weighted sampler for training\n",
    "        )\n",
    "\n",
    "        # --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "        test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "            ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "        print(f\"\\n--- Running config {i+1}/{len(sampled_params)} ---\")\n",
    "        print(params)\n",
    "\n",
    "        # Build model\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(cfg, params, device)\n",
    "\n",
    "        # Choose loss\n",
    "        loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(params)\n",
    "\n",
    "        # Train\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=train_dl,\n",
    "            val_dl=val_dl,\n",
    "            epochs=150,\n",
    "            lr=params['lr'],\n",
    "            weight_decay=params['weight_decay'],\n",
    "            clip_val=1,\n",
    "            # scheduler\n",
    "            sched_factor=0.5,\n",
    "            sched_patience=6,\n",
    "            sched_threshold=0.01,\n",
    "            sched_threshold_mode=\"rel\",\n",
    "            sched_cooldown=1,\n",
    "            sched_min_lr=1e-6,\n",
    "            # early stopping\n",
    "            es_patience=15,\n",
    "            es_min_delta=1e-4,\n",
    "            # logging\n",
    "            log_every=5,\n",
    "            verbose=True,\n",
    "            # checkpoint\n",
    "            save_best_path=model_filename,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "        # Load the best weights\n",
    "        best_state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(best_state)\n",
    "        test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "            device, test_dl, ds_test_copy)\n",
    "        test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "            'RMSE_winter']\n",
    "\n",
    "        # Log row\n",
    "        row = {\n",
    "            **params, 'valid_loss': float(best_val),\n",
    "            'test_rmse_a': float(test_rmse_a),\n",
    "            'test_rmse_w': float(test_rmse_w)\n",
    "        }\n",
    "\n",
    "        print(test_rmse_a, test_rmse_w)\n",
    "\n",
    "        with open(log_filename, mode='a', newline='') as log_file:\n",
    "            writer = csv.DictWriter(log_file, fieldnames=list(row.keys()))\n",
    "            writer.writerow(row)\n",
    "\n",
    "        # Track best by validation loss\n",
    "        if best_val < best_overall['val']:\n",
    "            best_overall = {\"val\": best_val, \"row\": row, \"params\": params}\n",
    "\n",
    "    print(\"\\n=== Best config by validation loss ===\")\n",
    "    print(best_overall['params'])\n",
    "    print(best_overall['row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
