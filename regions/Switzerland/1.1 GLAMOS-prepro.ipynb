{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLAMOS MB data:\n",
    "\n",
    "Does the pre-processing of the point MB measurements from GLAMOS (winter and summer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Mass Balance data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "import matplotlib as mpl\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from cmcrameri import cm\n",
    "from pathlib import Path\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.geodata import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "path_save_glw = os.path.join(cfg.dataPath, 'GLAMOS', 'distributed_MB_grids',\n",
    "                             'MBM/testing_LSTM/LSTM_two_heads')\n",
    "path_save_glw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "cmap = cm.devon\n",
    "\n",
    "# For bars and lines:\n",
    "color_diff_xgb = '#4d4d4d'\n",
    "\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_1 = colors[0]\n",
    "color_2 = '#c51b7d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform .dat files to .csv:\n",
    "\n",
    "Transform the seasonal and winter PMB .dat files to .csv for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pmb_dat_files(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assemble measurement periods:\n",
    "### Annual measurements: \n",
    "Process annual measurements and put all stakes into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first two rows\n",
    "df_annual_raw = process_annual_stake_data(cfg.dataPath + path_PMB_GLAMOS_csv_a)\n",
    "df_annual_raw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winter measurements:\n",
    "For each point in annual meas., take winter meas that was taken closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_winter_stake_data(df_annual_raw, cfg.dataPath + path_PMB_GLAMOS_csv_w,\n",
    "                          cfg.dataPath + path_PMB_GLAMOS_csv_w_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble both periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_raw = assemble_all_stake_data(\n",
    "    df_annual_raw, cfg.dataPath + path_PMB_GLAMOS_csv_w_clean,\n",
    "    cfg.dataPath + path_PMB_GLAMOS_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Number of measurements per year\n",
    "df_measurements_per_year = df_all_raw.groupby(['YEAR',\n",
    "                                               'PERIOD']).size().unstack()\n",
    "df_measurements_per_year.plot(kind='bar',\n",
    "                              stacked=True,\n",
    "                              figsize=(20, 5),\n",
    "                              color=[color_1, color_2])\n",
    "plt.title('Number of measurements per year for all glaciers')\n",
    "plt.ylabel('Number of Measurements')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Period')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add RGIs Ids:\n",
    "\n",
    "For each PMB measurement, we want to add the RGI ID (v6) of the shapefile it belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb = add_rgi_ids_to_df(df_all_raw, cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "rgiids6 = df_pmb[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "if check_multiple_rgi_ids(rgiids6):\n",
    "    print(\n",
    "        \"-- Alert: The following glaciers have more than one RGIId. Cleaning up.\"\n",
    "    )\n",
    "    df_pmb_clean = clean_rgi_ids(df_pmb.copy())\n",
    "    df_pmb_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    rgiids6_clean = df_pmb_clean[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "    if check_multiple_rgi_ids(rgiids6_clean):\n",
    "        print(\"-- Error: Some glaciers still have more than one RGIId.\")\n",
    "    else:\n",
    "        print(\"-- All glaciers are correctly associated with a single RGIId.\")\n",
    "else:\n",
    "    print(\"-- All glaciers are correctly associated with a single RGIId.\")\n",
    "    df_pmb_clean = df_pmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut from 1951:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to start of MS data (1951) or ERA5-Land data (1950):\n",
    "df_pmb_50s = df_pmb_clean[df_pmb_clean.YEAR > 1950].sort_values(\n",
    "    by=['GLACIER', 'YEAR'], ascending=[True, True])\n",
    "\n",
    "# Change from mm w.e. to m w.e.\n",
    "df_pmb_50s['POINT_BALANCE'] = df_pmb_50s['POINT_BALANCE'] / 1000\n",
    "\n",
    "# merge ClaridenL and ClaridenU into one glacier:\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenU', 'GLACIER'] = 'clariden'\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenL', 'GLACIER'] = 'clariden'\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_50s))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'winter']))\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s.groupby(['YEAR',\n",
    "                    'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                                     stacked=True,\n",
    "                                                     color=[color_1, color_2],\n",
    "                                                     ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge stakes that are close: \n",
    "Especially with winter probes, a lot of measurements were done at the same place in the raw data and this leads to noise. We merge the stakes that are very close and keep the mean of the measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_50s_clean_pts = pd.DataFrame()\n",
    "for gl in tqdm(df_pmb_50s.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_pmb_50s[df_pmb_50s.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_50s_clean_pts = pd.concat([df_pmb_50s_clean_pts, df_gl_cleaned])\n",
    "df_pmb_50s_clean_pts.drop(['x', 'y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for wrong elevations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def find_mismatch_by_year(\n",
    "        df_gl: pd.DataFrame,\n",
    "        path_xr_grids: str,\n",
    "        var_name: str = \"masked_elev\",\n",
    "        lon_name: str = \"lon\",\n",
    "        lat_name: str = \"lat\",\n",
    "        year_col: str = \"YEAR\",\n",
    "        glacier_col: str = \"GLACIER\",\n",
    "        threshold: float = 500.0,\n",
    "        file_pattern: str = \"{glacier}_{year}.zarr\",  # pattern of your files\n",
    "        strict:\n",
    "    bool = False,  # if True, raise if any glacier-year file is missing\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare each point's elevation to the nearest DEM cell from the glacier-year Zarr.\n",
    "\n",
    "    Returns:\n",
    "        mismatch_idx (pd.Index): indices in df_gl where |POINT_ELEVATION - DEM| >= threshold\n",
    "        mismatch_df (pd.DataFrame): rows of df_gl for mismatches with columns:\n",
    "                                    ['DEM_elv', 'elev_diff'] appended (sorted by elev_diff)\n",
    "    Notes:\n",
    "        - Expects df_gl to contain columns:\n",
    "            ['POINT_LON','POINT_LAT','POINT_ELEVATION', year_col, glacier_col]\n",
    "        - Zarr filenames are constructed with `file_pattern` inside `path_xr_grids`.\n",
    "          Example default: f\"{glacier}_{year}.zarr\"\n",
    "    \"\"\"\n",
    "    required_cols = {\n",
    "        \"POINT_LON\", \"POINT_LAT\", \"POINT_ELEVATION\", year_col, glacier_col\n",
    "    }\n",
    "    missing = required_cols - set(df_gl.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"df_gl is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    base = Path(path_xr_grids)\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {base}\")\n",
    "\n",
    "    all_mismatch_idx = []\n",
    "    mismatch_frames = []\n",
    "\n",
    "    # Cache opened datasets per (glacier, year) to avoid re-opening\n",
    "    ds_cache = {}\n",
    "\n",
    "    # Work on current index space (no reset), so returned indices match df_gl\n",
    "    grouped = df_gl.groupby([glacier_col, year_col], sort=False)\n",
    "\n",
    "    for (glacier, year), df_group in grouped:\n",
    "        # Build path for this glacier-year\n",
    "        fname = file_pattern.format(glacier=str(glacier), year=int(year))\n",
    "        zarr_path = base / fname\n",
    "\n",
    "        if not zarr_path.exists():\n",
    "            msg = f\"Missing DEM for glacier '{glacier}', year {year}: {zarr_path}\"\n",
    "            if strict:\n",
    "                raise FileNotFoundError(msg)\n",
    "            else:\n",
    "                # Skip gracefully\n",
    "                # print(msg)\n",
    "                continue\n",
    "\n",
    "        # Open or reuse the dataset\n",
    "        key = (glacier, int(year))\n",
    "        if key not in ds_cache:\n",
    "            ds_cache[key] = xr.open_zarr(str(zarr_path))\n",
    "        ds = ds_cache[key]\n",
    "\n",
    "        # Sanity checks\n",
    "        if lon_name not in ds.coords or lat_name not in ds.coords:\n",
    "            raise KeyError(\n",
    "                f\"Dataset {zarr_path} missing coords '{lon_name}'/'{lat_name}'. \"\n",
    "                \"Rename coords or pass lon_name/lat_name correctly.\")\n",
    "        if var_name not in ds.variables:\n",
    "            raise KeyError(\n",
    "                f\"Variable '{var_name}' not found in dataset {zarr_path}.\")\n",
    "\n",
    "        # Vectorized nearest sampling for this group\n",
    "        lons = xr.DataArray(df_group[\"POINT_LON\"].to_numpy(), dims=\"points\")\n",
    "        lats = xr.DataArray(df_group[\"POINT_LAT\"].to_numpy(), dims=\"points\")\n",
    "        xr_elev_da = ds[var_name].sel({\n",
    "            lon_name: lons,\n",
    "            lat_name: lats\n",
    "        },\n",
    "                                      method=\"nearest\")\n",
    "        xr_elev = xr_elev_da.to_numpy()\n",
    "\n",
    "        pt_elev = df_group[\"POINT_ELEVATION\"].to_numpy()\n",
    "        elev_diff = pt_elev - xr_elev\n",
    "\n",
    "        # Assemble a small result frame aligned to original indices\n",
    "        res = df_group.copy()\n",
    "        res[\"DEM_elv\"] = xr_elev\n",
    "        res[\"elev_diff\"] = elev_diff\n",
    "\n",
    "        # Drop NaNs (on either DEM or diff)\n",
    "        res = res.dropna(subset=[\"DEM_elv\", \"elev_diff\"])\n",
    "\n",
    "        # Flag mismatches (abs diff >= threshold)\n",
    "        mismatch_mask = (np.abs(res[\"elev_diff\"]) >= threshold)\n",
    "\n",
    "        if mismatch_mask.any():\n",
    "            # Collect indices and rows\n",
    "            these_idx = res.index[mismatch_mask]\n",
    "            all_mismatch_idx.append(these_idx)\n",
    "            mismatch_frames.append(res.loc[these_idx])\n",
    "\n",
    "    if len(all_mismatch_idx) == 0:\n",
    "        # Nothing found\n",
    "        return pd.Index([], dtype=df_gl.index.dtype), pd.DataFrame(\n",
    "            columns=list(df_gl.columns) + [\"DEM_elv\", \"elev_diff\"])\n",
    "\n",
    "    mismatch_idx = all_mismatch_idx[0].append(all_mismatch_idx[1:]) if len(\n",
    "        all_mismatch_idx) > 1 else all_mismatch_idx[0]\n",
    "    mismatch_df = pd.concat(mismatch_frames,\n",
    "                            axis=0).sort_values(by=\"elev_diff\", ascending=True)\n",
    "\n",
    "    return mismatch_idx, mismatch_df\n",
    "\n",
    "\n",
    "def reconcile_points_by_year(\n",
    "    df: pd.DataFrame,\n",
    "    path_xr_grids: str,\n",
    "    var_name: str = \"masked_elev\",\n",
    "    lon_name: str = \"lon\",\n",
    "    lat_name: str = \"lat\",\n",
    "    year_col: str = \"YEAR\",\n",
    "    glacier_col: str = \"GLACIER\",\n",
    "    point_elev_col: str = \"POINT_ELEVATION\",\n",
    "    threshold: float = 500.0,\n",
    "    file_pattern: str = \"{glacier}_{year}.zarr\",\n",
    "    replace_glaciers: Optional[Iterable[str]] = None,  # e.g., {\"aletsch\"}\n",
    "    strict: bool = False,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reconcile point elevations against glacier-year DEMs stored as Zarrs.\n",
    "\n",
    "    For each (GLACIER, YEAR) group:\n",
    "      - Open <path_xr_grids>/<glacier>_<year>.zarr if present.\n",
    "      - If missing, fall back to the earliest available <glacier>_YYYY.zarr in that folder.\n",
    "      - Sample nearest DEM cell at (POINT_LON, POINT_LAT).\n",
    "      - If |POINT_ELEVATION - DEM| >= threshold:\n",
    "          * If GLACIER ∈ replace_glaciers: REPLACE POINT_ELEVATION with DEM value.\n",
    "          * Else: DROP the row.\n",
    "\n",
    "    Prints per-glacier counts of dropped/replaced points and fallback uses.\n",
    "\n",
    "    Returns:\n",
    "      df_clean    : DataFrame after drops/replacements.\n",
    "      df_mismatch : All mismatching rows with ['DEM_elv','elev_diff'] added.\n",
    "      summary     : Per-glacier counts (dropped, replaced, fallback_groups_used, missing_dem_groups).\n",
    "    \"\"\"\n",
    "    # --- validations ---\n",
    "    required = {\n",
    "        glacier_col, year_col, \"POINT_LON\", \"POINT_LAT\", point_elev_col\n",
    "    }\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Input df is missing columns: {sorted(missing)}\")\n",
    "\n",
    "    base = Path(path_xr_grids)\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {base}\")\n",
    "\n",
    "    replace_set = set(g.lower() for g in (replace_glaciers or set()))\n",
    "\n",
    "    # Work on a unique-index copy so drops/replacements are precise\n",
    "    df_clean = df.reset_index(drop=True).copy()\n",
    "\n",
    "    # Caches and accumulators\n",
    "    ds_cache: dict[Tuple[str, int], xr.Dataset] = {}\n",
    "    mismatch_frames = []\n",
    "    dropped_indices = []\n",
    "    drop_counts = defaultdict(int)\n",
    "    replace_counts = defaultdict(int)\n",
    "    missing_dem_groups = defaultdict(int)\n",
    "    fallback_counts = defaultdict(int)\n",
    "\n",
    "    def _find_existing_dem_path(\n",
    "            base_dir: Path, glacier_name: str, requested_year: int,\n",
    "            patt: str) -> Tuple[Optional[Path], Optional[int], bool]:\n",
    "        \"\"\"\n",
    "        Return (path, used_year, used_fallback).\n",
    "          - Exact match -> (exact_path, requested_year, False)\n",
    "          - Else earliest glacier_YYYY.zarr -> (fallback_path, year, True)\n",
    "          - Else -> (None, None, False)\n",
    "        \"\"\"\n",
    "        exact_name = patt.format(glacier=str(glacier_name),\n",
    "                                 year=int(requested_year))\n",
    "        exact_path = base_dir / exact_name\n",
    "        if exact_path.exists():\n",
    "            return exact_path, int(requested_year), False\n",
    "\n",
    "        rgx = re.compile(rf\"^{re.escape(str(glacier_name))}_(\\d{{4}})\\.zarr$\",\n",
    "                         re.IGNORECASE)\n",
    "        candidates = []\n",
    "        for entry in base_dir.iterdir():\n",
    "            if not entry.name.lower().endswith(\".zarr\"):\n",
    "                continue\n",
    "            m = rgx.match(entry.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            try:\n",
    "                y = int(m.group(1))\n",
    "                candidates.append((y, entry))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if not candidates:\n",
    "            return None, None, False\n",
    "        candidates.sort(key=lambda t: t[0])  # earliest year first\n",
    "        y_min, path_min = candidates[0]\n",
    "        return path_min, y_min, True\n",
    "\n",
    "    # Iterate by (GLACIER, YEAR) groups; indices remain aligned with df_clean\n",
    "    for (glacier, year), grp in df_clean.groupby([glacier_col, year_col],\n",
    "                                                 sort=False):\n",
    "        # Resolve DEM path with fallback if needed\n",
    "        zarr_path, used_year, used_fallback = _find_existing_dem_path(\n",
    "            base_dir=base,\n",
    "            glacier_name=str(glacier),\n",
    "            requested_year=int(year),\n",
    "            patt=file_pattern)\n",
    "\n",
    "        if zarr_path is None:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"[WARN] No DEMs found at all for glacier='{glacier}' in {base}\"\n",
    "                )\n",
    "            missing_dem_groups[str(glacier)] += 1\n",
    "            if strict:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Missing DEM for glacier '{glacier}' (no files like {glacier}_YYYY.zarr)\"\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        if used_fallback and verbose:\n",
    "            print(\n",
    "                f\"[INFO] Fallback DEM for glacier='{glacier}': requested {year} -> using {used_year} ({zarr_path.name})\"\n",
    "            )\n",
    "            fallback_counts[str(glacier)] += 1\n",
    "\n",
    "        key = (str(glacier), int(used_year))\n",
    "        if key not in ds_cache:\n",
    "            ds_cache[key] = xr.open_zarr(str(zarr_path))\n",
    "        ds = ds_cache[key]\n",
    "\n",
    "        # sanity checks\n",
    "        if lon_name not in ds.coords or lat_name not in ds.coords:\n",
    "            raise KeyError(\n",
    "                f\"{zarr_path} missing coords '{lon_name}'/'{lat_name}'. \"\n",
    "                \"Rename dataset coords or pass correct names.\")\n",
    "        if var_name not in ds.variables:\n",
    "            raise KeyError(f\"Variable '{var_name}' not found in {zarr_path}.\")\n",
    "\n",
    "        # vectorized nearest sampling\n",
    "        lons = xr.DataArray(grp[\"POINT_LON\"].to_numpy(), dims=\"points\")\n",
    "        lats = xr.DataArray(grp[\"POINT_LAT\"].to_numpy(), dims=\"points\")\n",
    "        xr_elev = ds[var_name].sel({\n",
    "            lon_name: lons,\n",
    "            lat_name: lats\n",
    "        },\n",
    "                                   method=\"nearest\").to_numpy()\n",
    "\n",
    "        pt_elev = grp[point_elev_col].to_numpy()\n",
    "        elev_diff = pt_elev - xr_elev\n",
    "\n",
    "        res = grp.copy()\n",
    "        res[\"DEM_elv\"] = xr_elev\n",
    "        res[\"elev_diff\"] = elev_diff\n",
    "\n",
    "        # Remove rows where DEM or diff is NaN before thresholding\n",
    "        res = res.dropna(subset=[\"DEM_elv\", \"elev_diff\"])\n",
    "\n",
    "        # mismatches for this (glacier, year)\n",
    "        mask = (np.abs(res[\"elev_diff\"]) >= threshold)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        # rows to act on (indices in df_clean)\n",
    "        mm = res.loc[mask]\n",
    "        mismatch_frames.append(mm)\n",
    "\n",
    "        gkey = str(glacier)\n",
    "        if gkey.lower() in replace_set:\n",
    "            # Replace POINT_ELEVATION with DEM value for those rows\n",
    "            df_clean.loc[mm.index, point_elev_col] = mm[\"DEM_elv\"].values\n",
    "            replace_counts[gkey] += len(mm)\n",
    "        else:\n",
    "            # Drop mismatched rows\n",
    "            dropped_indices.extend(mm.index.tolist())\n",
    "            drop_counts[gkey] += len(mm)\n",
    "\n",
    "    # Apply drops once\n",
    "    if dropped_indices:\n",
    "        df_clean = df_clean.drop(index=dropped_indices)\n",
    "\n",
    "    # Build mismatch table\n",
    "    if mismatch_frames:\n",
    "        df_mismatch = pd.concat(mismatch_frames,\n",
    "                                axis=0).sort_values(\"elev_diff\")\n",
    "    else:\n",
    "        df_mismatch = pd.DataFrame(columns=list(df.columns) +\n",
    "                                   [\"DEM_elv\", \"elev_diff\"])\n",
    "\n",
    "    # Tidy\n",
    "    df_clean = df_clean.sort_index().reset_index(drop=True)\n",
    "\n",
    "    # Summary dataframe\n",
    "    glaciers = sorted(\n",
    "        set(\n",
    "            list(drop_counts.keys()) + list(replace_counts.keys()) +\n",
    "            list(missing_dem_groups.keys()) + list(fallback_counts.keys())))\n",
    "    summary = pd.DataFrame({\n",
    "        \"GLACIER\":\n",
    "        glaciers,\n",
    "        \"dropped\": [drop_counts[g] for g in glaciers],\n",
    "        \"replaced\": [replace_counts[g] for g in glaciers],\n",
    "        \"fallback_groups_used\": [fallback_counts[g] for g in glaciers],\n",
    "        \"missing_dem_groups\": [missing_dem_groups[g] for g in glaciers],\n",
    "    })\n",
    "\n",
    "    # Print per-glacier info\n",
    "    if verbose and len(summary):\n",
    "        print(\"\\n=== Reconcile summary (per glacier) ===\")\n",
    "        for _, row in summary.iterrows():\n",
    "            g = row[\"GLACIER\"]\n",
    "            d = int(row[\"dropped\"])\n",
    "            r = int(row[\"replaced\"])\n",
    "            f = int(row[\"fallback_groups_used\"])\n",
    "            m = int(row[\"missing_dem_groups\"])\n",
    "            msg = f\"{g}: removed {d} point(s)\"\n",
    "            if r: msg += f\", replaced {r} point(s)\"\n",
    "            if f: msg += f\", fallback DEM groups: {f}\"\n",
    "            if m: msg += f\", missing DEM groups: {m}\"\n",
    "            print(msg)\n",
    "\n",
    "    return df_clean, df_mismatch, summary\n",
    "\n",
    "\n",
    "def first_year_per_glacier(path_xr_grids: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan a directory for Zarr folders named <glacier>_<year>.zarr and\n",
    "    return a DataFrame with the earliest year per glacier and the path.\n",
    "\n",
    "    Returns columns:\n",
    "      - glacier\n",
    "      - first_year (int)\n",
    "      - first_year_path (str)\n",
    "    \"\"\"\n",
    "    p = Path(path_xr_grids)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {p}\")\n",
    "\n",
    "    # Match anything up to the last underscore, then 4-digit year, then .zarr\n",
    "    # e.g. \"aletsch_1951.zarr\", \"some_glacier_name_2008.zarr\"\n",
    "    pat = re.compile(r\"^(?P<glacier>.+)_(?P<year>\\d{4})\\.zarr$\", re.IGNORECASE)\n",
    "\n",
    "    # Keep min year and path per glacier\n",
    "    best = {}  # glacier -> (year, full_path)\n",
    "    for entry in p.iterdir():\n",
    "        # Zarr datasets are often directories; accept both dir and file if present\n",
    "        if not entry.name.lower().endswith(\".zarr\"):\n",
    "            continue\n",
    "        m = pat.match(entry.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        glacier = m.group(\"glacier\")\n",
    "        try:\n",
    "            year = int(m.group(\"year\"))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if glacier not in best or year < best[glacier][0]:\n",
    "            best[glacier] = (year, str(entry.resolve()))\n",
    "\n",
    "    if not best:\n",
    "        # No valid matches found\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"glacier\", \"first_year\", \"first_year_path\"])\n",
    "\n",
    "    rows = [{\n",
    "        \"glacier\": g,\n",
    "        \"first_year\": y,\n",
    "        \"first_year_path\": path\n",
    "    } for g, (y, path) in best.items()]\n",
    "    df = pd.DataFrame(rows).sort_values([\"glacier\",\n",
    "                                         \"first_year\"]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique-index working copy\n",
    "df_clean = df_pmb_50s_clean_pts.reset_index(drop=True).copy()\n",
    "print(\"Initial number of rows:\", len(df_clean))\n",
    "\n",
    "path_xr_grids = os.path.join(cfg.dataPath, path_GLAMOS_topo,\n",
    "                             \"xr_masked_grids/\")\n",
    "\n",
    "df_clean, df_mismatch, summary = reconcile_points_by_year(\n",
    "    df=df_pmb_50s_clean_pts,\n",
    "    path_xr_grids=path_xr_grids,\n",
    "    var_name=\"masked_elev\",\n",
    "    lon_name=\"lon\",\n",
    "    lat_name=\"lat\",\n",
    "    year_col=\"YEAR\",\n",
    "    glacier_col=\"GLACIER\",\n",
    "    point_elev_col=\"POINT_ELEVATION\",\n",
    "    threshold=400.0,\n",
    "    file_pattern=\"{glacier}_{year}.zarr\",\n",
    "    replace_glaciers={\"aletsch\"},  # replace for Aletsch, drop for others\n",
    "    strict=False,\n",
    "    verbose=True,  # prints counts per glacier\n",
    ")\n",
    "\n",
    "print(\"Final number of rows:\", len(df_clean))\n",
    "\n",
    "# Save mismatches to CSV\n",
    "out_csv = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                       \"GLAMOS_elev_mismatch.csv\")\n",
    "df_mismatch.sort_values(by=\"elev_diff\", ascending=False, inplace=True)\n",
    "df_mismatch.to_csv(out_csv, index=False)\n",
    "print(\"Saved mismatches to:\", out_csv)\n",
    "\n",
    "# df_clean is your final cleaned dataframe (all glaciers, mismatches removed)\n",
    "df_pmb_50s_clean_elv = df_clean\n",
    "\n",
    "# reset_index\n",
    "df_pmb_50s_clean_elv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save intermediate output\n",
    "print('Saving intermediate output df_pmb_50s.csv to {path_PMB_GLAMOS_csv}')\n",
    "df_pmb_50s_clean_elv.to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                                         'df_pmb_50s.csv'),\n",
    "                            index=False)\n",
    "df_pmb_50s_clean_elv[[\n",
    "    'GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON', 'PERIOD'\n",
    "]].to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                       'coordinate_50s.csv'),\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_name = 'rhone'\n",
    "df_clean = df_pmb_50s_clean_pts.reset_index(drop=True).copy()\n",
    "df_first = first_year_per_glacier(path_xr_grids)\n",
    "\n",
    "df_gl = df_clean[(df_clean.GLACIER == glacier_name)]\n",
    "ds = xr.open_zarr(\n",
    "    df_first[df_first.glacier == glacier_name].first_year_path.values[0])\n",
    "\n",
    "threshold = 400.0,\n",
    "mismatch_idx, df_with_diffs = find_mismatch_by_year(\n",
    "    df_gl=df_gl,  # must include GLACIER and YEAR columns\n",
    "    path_xr_grids=path_xr_grids,\n",
    "    var_name=\"masked_elev\",\n",
    "    lon_name=\"lon\",\n",
    "    lat_name=\"lat\",\n",
    "    year_col=\"YEAR\",  # change if your year column is named differently\n",
    "    glacier_col=\"GLACIER\",\n",
    "    threshold=threshold,  # meters\n",
    "    file_pattern=\"{glacier}_{year}.zarr\",  # e.g. \"aletsch_1951.zarr\"\n",
    "    strict=False,\n",
    ")\n",
    "print(\n",
    "    f\"Number of POINT indices with >={threshold} m mismatch: {len(mismatch_idx)}\"\n",
    ")\n",
    "\n",
    "\n",
    "def pick_ann_file(cfg, glacier_name, year, period=\"annual\"):\n",
    "    if period == \"annual\":\n",
    "        suffix = \"ann\"\n",
    "    elif period == \"winter\":\n",
    "        suffix = \"win\"\n",
    "    base = os.path.join(cfg.dataPath, path_distributed_MB_glamos, \"GLAMOS\",\n",
    "                        glacier_name)\n",
    "    cand_lv95 = os.path.join(base, f\"{year}_{suffix}_fix_lv95.grid\")\n",
    "    cand_lv03 = os.path.join(base, f\"{year}_{suffix}_fix_lv03.grid\")\n",
    "    if os.path.exists(cand_lv95):\n",
    "        return cand_lv95, \"lv95\"\n",
    "    if os.path.exists(cand_lv03):\n",
    "        return cand_lv03, \"lv03\"\n",
    "    return None, None\n",
    "\n",
    "\n",
    "glacier_name = glacier_name\n",
    "year = df_first[df_first.glacier == glacier_name].first_year.values[0]\n",
    "period = 'annual'\n",
    "file_ann, coord_system = pick_ann_file(cfg, glacier_name, year, period)\n",
    "grid_path_ann = os.path.join(cfg.dataPath, path_distributed_MB_glamos,\n",
    "                             \"GLAMOS\", glacier_name, file_ann)\n",
    "\n",
    "# Load GLAMOS data and convert to WGS84\n",
    "metadata_ann, grid_data_ann = load_grid_file(grid_path_ann)\n",
    "ds_glamos_ann = convert_to_xarray_geodata(grid_data_ann, metadata_ann)\n",
    "if coord_system == \"lv03\":\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv03_to_wgs84(ds_glamos_ann)\n",
    "elif coord_system == \"lv95\":\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv95_to_wgs84(ds_glamos_ann)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Shared normalization across both plots\n",
    "vmin = min(df_with_diffs[\"POINT_ELEVATION\"].min(), float(ds.masked_elev.min()))\n",
    "vmax = max(df_with_diffs[\"POINT_ELEVATION\"].max(), float(ds.masked_elev.max()))\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.cm.terrain\n",
    "\n",
    "# ---- First subplot ----\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ds_glamos_wgs84_ann.plot.imshow(\n",
    "    ax=ax1,\n",
    "    cmap=\"Greys\",\n",
    "    cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"},\n",
    ")\n",
    "\n",
    "# scatter using same cmap + norm\n",
    "sc = ax1.scatter(\n",
    "    df_with_diffs[\"POINT_LON\"],\n",
    "    df_with_diffs[\"POINT_LAT\"],\n",
    "    c=df_with_diffs[\"POINT_ELEVATION\"],\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=25,\n",
    ")\n",
    "ax1.set_title(f\"{glacier_name.capitalize()} {year} GLAMOS glacier-wide MB\")\n",
    "\n",
    "# ---- Second subplot ----\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "im = ds.masked_elev.plot(\n",
    "    ax=ax2,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    add_colorbar=False,  # don’t add duplicate colorbar\n",
    ")\n",
    "ax2.set_title(f\"{glacier_name.capitalize()} {year} DEM\")\n",
    "\n",
    "# ---- Shared colorbar for elevation ----\n",
    "cbar = figure.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "    ax=ax2,\n",
    "    orientation=\"vertical\",\n",
    "    fraction=0.02,\n",
    "    pad=0.02,\n",
    ")\n",
    "cbar.set_label(\"Elevation [m a.s.l.]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "df_with_diffs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s_clean_elv.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[color_1, color_2], ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s_clean_elv.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_list = list(df_pmb_50s_clean_elv.GLACIER.unique())\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "glacier_list.sort()\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per glacier per year:\n",
    "num_gl_yr = df_pmb_50s_clean_elv.groupby(['GLACIER', 'YEAR', 'PERIOD'\n",
    "                                          ]).size().unstack().reset_index()\n",
    "\n",
    "num_gl_annual = df_pmb_50s_clean_elv[\n",
    "    df_pmb_50s_clean_elv.PERIOD == 'annual'].groupby(['GLACIER'\n",
    "                                                      ]).size().sort_values()\n",
    "\n",
    "# Plot one glacier per column:\n",
    "big_gl = num_gl_annual[num_gl_annual > 250].index.sort_values()\n",
    "num_glaciers = len(big_gl)\n",
    "fig, ax = plt.subplots(num_glaciers, 1, figsize=(15, 5 * num_glaciers))\n",
    "for i, gl in enumerate(big_gl):\n",
    "    num_gl_yr[num_gl_yr.GLACIER == gl].plot(x='YEAR',\n",
    "                                            kind='bar',\n",
    "                                            stacked=True,\n",
    "                                            ax=ax[i],\n",
    "                                            title=gl)\n",
    "    ax[i].set_ylabel('Number of measurements')\n",
    "    ax[i].set_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_50s_clean_elv))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s_clean_elv[df_pmb_50s_clean_elv.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s_clean_elv[df_pmb_50s_clean_elv.PERIOD == 'winter']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_50s_clean_elv.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add topographical information from OGGM & SGI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skyview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "path_svf_latlon = os.path.join(cfg.dataPath, \"GLAMOS/topo/RGI_v6_11\",\n",
    "                               \"svf_nc_latlon\")\n",
    "\n",
    "\n",
    "def sample_svf_for_points(df_points: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in df_points (must contain ['POINT_LAT','POINT_LON','RGIId']),\n",
    "    open the corresponding *_svf_latlon.nc and sample SVF/ASVF/OPNS at the point.\n",
    "    Returns a copy with new columns: 'SVF','ASVF','OPNS'.\n",
    "    \"\"\"\n",
    "    out = df_points.copy()\n",
    "    out[\"SVF\"] = np.nan\n",
    "    out[\"ASVF\"] = np.nan\n",
    "    out[\"OPNS\"] = np.nan\n",
    "\n",
    "    # group by glacier to open each dataset once\n",
    "    for rgi_id, df_g in out.groupby(\"RGIId\", sort=False):\n",
    "        svf_path = os.path.join(path_svf_latlon, f\"{rgi_id}_svf_latlon.nc\")\n",
    "        if not os.path.exists(svf_path):\n",
    "            # no SVF available for this glacier; leave NaNs\n",
    "            print(f\"[warn] Missing SVF file for {rgi_id}: {svf_path}\")\n",
    "            continue\n",
    "\n",
    "        # open & prep once per glacier\n",
    "        ds = xr.open_dataset(svf_path)\n",
    "\n",
    "        # ensure expected var names exist\n",
    "        vars_present = [\n",
    "            v for v in [\"svf\", \"asvf\", \"opns\"] if v in ds.data_vars\n",
    "        ]\n",
    "        if not vars_present:\n",
    "            print(f\"[warn] No SVF variables in {svf_path}\")\n",
    "            ds.close()\n",
    "            continue\n",
    "\n",
    "        # make sure coords are named 'lat'/'lon' and sorted ascending (required by interp)\n",
    "        ren = {}\n",
    "        if \"x\" in ds.dims or \"y\" in ds.dims:\n",
    "            ren.update({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "        if \"latitude\" in ds.dims or \"longitude\" in ds.dims:\n",
    "            ren.update({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "        if ren:\n",
    "            ds = ds.rename(ren)\n",
    "        if ds.lon[0] > ds.lon[-1]:\n",
    "            ds = ds.sortby(\"lon\")\n",
    "        if ds.lat[0] > ds.lat[-1]:\n",
    "            ds = ds.sortby(\"lat\")\n",
    "\n",
    "        # vectorized sampling points\n",
    "        lats = xr.DataArray(df_g[\"POINT_LAT\"].values, dims=\"p\")\n",
    "        lons = xr.DataArray(df_g[\"POINT_LON\"].values, dims=\"p\")\n",
    "\n",
    "        # clip to grid bounds to avoid all-NaNs near edges\n",
    "        lons = xr.apply_ufunc(np.clip, lons, ds.lon.values.min(),\n",
    "                              ds.lon.values.max())\n",
    "        lats = xr.apply_ufunc(np.clip, lats, ds.lat.values.min(),\n",
    "                              ds.lat.values.max())\n",
    "\n",
    "        # choose method: \"nearest\" (robust) or \"linear\" (smooth)\n",
    "        # nearest avoids NaNs if points slightly off-grid due to reprojection\n",
    "        sampled = {}\n",
    "        if \"svf\" in ds:\n",
    "            sampled[\"SVF\"] = ds[\"svf\"].interp(lon=lons,\n",
    "                                              lat=lats,\n",
    "                                              method=\"nearest\").values\n",
    "        if \"asvf\" in ds:\n",
    "            sampled[\"ASVF\"] = ds[\"asvf\"].interp(lon=lons,\n",
    "                                                lat=lats,\n",
    "                                                method=\"nearest\").values\n",
    "        if \"opns\" in ds:\n",
    "            sampled[\"OPNS\"] = ds[\"opns\"].interp(lon=lons,\n",
    "                                                lat=lats,\n",
    "                                                method=\"nearest\").values\n",
    "\n",
    "        # write back into the corresponding rows\n",
    "        out.loc[df_g.index, \"SVF\"] = sampled.get(\"SVF\", np.nan)\n",
    "        out.loc[df_g.index, \"ASVF\"] = sampled.get(\"ASVF\", np.nan)\n",
    "        out.loc[df_g.index, \"OPNS\"] = sampled.get(\"OPNS\", np.nan)\n",
    "\n",
    "        ds.close()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "df_pmb_50s_clean = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                               'df_pmb_50s.csv')\n",
    "df_with_svf = sample_svf_for_points(df_pmb_50s_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize OGGM glacier directories\n",
    "# df_pmb_50s_clean = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "#                                'df_pmb_50s.csv')\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    "    from_prepro_level=3,\n",
    "    prepro_border=10,\n",
    ")\n",
    "unique_rgis = df_with_svf['RGIId'].unique()\n",
    "\n",
    "export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "df_pmb_topo = merge_pmb_with_oggm_data(\n",
    "    df_pmb=df_with_svf,\n",
    "    gdirs=gdirs,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to within glacier shape\n",
    "df_pmb_topo = df_pmb_topo[df_pmb_topo['within_glacier_shape']]\n",
    "df_pmb_topo = df_pmb_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the masked topographical arrays per glacier:\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "create_sgi_topo_masks(cfg,\n",
    "                      glacier_list,\n",
    "                      type='glacier_name',\n",
    "                      path_save=os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                                             'xr_masked_grids/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "i = 0\n",
    "glacier_name = 'clariden'\n",
    "df_pmb_gl = df_pmb_50s_clean[df_pmb_50s_clean.GLACIER == glacier_name]\n",
    "\n",
    "stake_coordinates = df_pmb_gl[['POINT_LON', 'POINT_LAT']].values\n",
    "\n",
    "# Open SGI grid:\n",
    "ds_sgi = xr.open_dataset(cfg.dataPath + path_SGI_topo + 'xr_masked_grids/' +\n",
    "                         f'{glacier_name}.zarr')\n",
    "\n",
    "# Plot the masked data\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds_sgi.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted')\n",
    "ds_sgi.masked_slope.plot(ax=axs[1], cmap='cividis')\n",
    "ds_sgi.masked_elev.plot(ax=axs[2], cmap='terrain')\n",
    "ds_sgi.glacier_mask.plot(ax=axs[3], cmap='binary')\n",
    "axs[3].scatter(stake_coordinates[:, 0], stake_coordinates[:, 1], c='r', s=10)\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_grids = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                                 'xr_masked_grids/')\n",
    "\n",
    "# Merge PMB with SGI data\n",
    "df_pmb_sgi = merge_pmb_with_sgi_data(\n",
    "    df_pmb_topo,  # cleaned PMB DataFrame\n",
    "    path_masked_grids,  # path to SGI grids\n",
    "    voi=[\"masked_aspect\", \"masked_slope\", \"masked_elev\"])\n",
    "\n",
    "# Drop points that have no intersection with SGI mask: (have NaN values)\n",
    "df_pmb_sgi = df_pmb_sgi.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and display the number of samples\n",
    "print(f\"Total number of winter and annual samples: {len(df_pmb_sgi)}\")\n",
    "\n",
    "# Count occurrences of 'PERIOD' values\n",
    "period_counts = df_pmb_sgi['PERIOD'].value_counts()\n",
    "print(f\"Number of annual samples: {period_counts.get('annual', 0)}\")\n",
    "print(f\"Number of winter samples: {period_counts.get('winter', 0)}\")\n",
    "\n",
    "# Unique years, sorted\n",
    "unique_years = np.sort(df_pmb_sgi.YEAR.unique())\n",
    "print(f\"Unique years: {unique_years}\")\n",
    "\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_sgi.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'clariden'\n",
    "# stakes\n",
    "df_stakes = df_pmb_topo.copy()\n",
    "df_stakes = df_stakes[(df_stakes['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes.RGIId.unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath}/OGGM/xr_grids/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes[\"POINT_LON\"].values, df_stakes[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes['x'] = x_stake\n",
    "df_stakes['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(121)\n",
    "ds_oggm.glacier_mask.plot(cmap='binary', ax=ax)\n",
    "sns.scatterplot(\n",
    "    df_stakes,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    # hue='within_glacier_shape',\n",
    "    ax=ax,\n",
    "    palette=['r', 'b'])\n",
    "ax.set_title('Stakes on glacier OGGM')\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "path_SGI_topo = f'{cfg.dataPath}/GLAMOS/topo/SGI2020/'\n",
    "sgi_grid = xr.open_dataset(path_SGI_topo +\n",
    "                           f'xr_masked_grids/{glacierName}.zarr')\n",
    "sgi_grid.glacier_mask.plot(cmap='binary', ax=ax)\n",
    "sns.scatterplot(\n",
    "    df_stakes,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    # hue='within_glacier_shape',\n",
    "    ax=ax,\n",
    "    palette=['r', 'b'])\n",
    "ax.set_title('Stakes on glacier SGI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_sgi.groupby(['YEAR',\n",
    "                    'PERIOD']).size().unstack().plot(kind='bar',\n",
    "                                                     stacked=True,\n",
    "                                                     color=[color_1, color_2],\n",
    "                                                     ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_sgi.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'clariden'\n",
    "df_pmb_gl = df_pmb_sgi[(df_pmb_sgi.GLACIER == glacierName)]\n",
    "\n",
    "# Plot aspect and sgi aspect\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "axs[0].scatter(df_pmb_gl.aspect, df_pmb_gl.aspect_sgi)\n",
    "axs[0].set_xlabel('aspect oggm')\n",
    "axs[0].set_ylabel('aspect sgi')\n",
    "axs[0].set_title('Aspect')\n",
    "\n",
    "axs[1].scatter(df_pmb_gl.slope, df_pmb_gl.slope_sgi)\n",
    "axs[1].set_xlabel('slope oggm')\n",
    "axs[1].set_ylabel('slope sgi')\n",
    "axs[1].set_title('Slope')\n",
    "\n",
    "# same for topo\n",
    "axs[2].scatter(df_pmb_gl.topo, df_pmb_gl.topo_sgi)\n",
    "axs[2].set_xlabel('topo oggm')\n",
    "axs[2].set_ylabel('topo sgi')\n",
    "axs[2].set_title('Topo')\n",
    "# add 1:1 line\n",
    "for ax in axs:\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), ls=\"--\", c=\".3\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give new stake IDs:\n",
    "Give new stake IDs with glacier name and then a number according to the elevation. This is because accross glaciers some stakes have the same ID which is not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop taelliboden (only one measurement)\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi.GLACIER != 'taelliboden']\n",
    "\n",
    "# drop taelliboden (big outlier)\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi.GLACIER != 'plainemorte']\n",
    "\n",
    "df_pmb_sgi = rename_stakes_by_elevation(df_pmb_sgi)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_sgi)\n",
    "\n",
    "# Save to CSV\n",
    "fname = 'CH_wgms_dataset_all.csv'\n",
    "df_pmb_sgi.to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv, fname),\n",
    "                  index=False)\n",
    "log.info(f\"-- Saved pmb & oggm dataset {fname} to: {path_PMB_GLAMOS_csv}\")\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_sgi))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_sgi[df_pmb_sgi.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_sgi[df_pmb_sgi.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_sgi['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_sgi['MONTH_START'] = [str(date)[4:6] for date in df_pmb_sgi.FROM_DATE]\n",
    "df_pmb_sgi['MONTH_END'] = [str(date)[4:6] for date in df_pmb_sgi.TO_DATE]\n",
    "\n",
    "# drop rows where month_start is '07'\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi['MONTH_START'] != '07']\n",
    "\n",
    "# drop\n",
    "df_pmb_sgi = df_pmb_sgi.loc[~((df_pmb_sgi['MONTH_END'] == '06') &\n",
    "                              (df_pmb_sgi['PERIOD'] == 'annual'))]\n",
    "\n",
    "df_pmb_sgi = df_pmb_sgi.loc[~((df_pmb_sgi['MONTH_END'] == '11') &\n",
    "                              (df_pmb_sgi['PERIOD'] == 'annual'))]\n",
    "\n",
    "# Rows where month_end is '05' and period is 'annual', rename period to 'winter'\n",
    "df_pmb_sgi.loc[(df_pmb_sgi['MONTH_END'] == '05') &\n",
    "               (df_pmb_sgi['PERIOD'] == 'annual'), 'PERIOD'] = 'winter'\n",
    "\n",
    "# Rows where month_end is '08' and period is 'winter', rename period to 'winter'\n",
    "df_pmb_sgi.loc[(df_pmb_sgi['MONTH_END'] == '08') &\n",
    "               (df_pmb_sgi['PERIOD'] == 'winter'), 'PERIOD'] = 'annual'\n",
    "\n",
    "# Save to csv:\n",
    "df_pmb_sgi.to_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                  f'CH_wgms_dataset_all.csv',\n",
    "                  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                 f'CH_wgms_dataset_all.csv')\n",
    "df.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier wide MB:\n",
    "Pre-processing of glacier wide SMB data from GLAMOS. Transform .dat files to .csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_SMB_GLAMOS(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs: no fixed dates, but using observed periods.\n",
    "# Example:\n",
    "fileName = 'aletsch_obs.csv'\n",
    "aletsch_csv = pd.read_csv(cfg.dataPath + path_SMB_GLAMOS_csv + 'obs/' +\n",
    "                          fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: with fixed periods (hydrological year).\n",
    "# # Example:\n",
    "fileName = 'aletsch_fix.csv'\n",
    "aletsch_csv = pd.read_csv(cfg.dataPath + path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                          fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential incoming clear sky solar radiation:\n",
    "\n",
    "Pre-process glamos data of \"potential incoming clear sky solar radiation (pcsr)\" used as a topographical variable. One per day grid per glacier for one year only, depends on the glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    glDirect = np.sort(os.listdir(cfg.dataPath + path_pcsr +\n",
    "                                  'raw/'))  # Glaciers with data\n",
    "\n",
    "    print('Number of glacier with clear sky radiation data:', len(glDirect))\n",
    "    print('Glaciers with clear sky radiation data:', glDirect)\n",
    "\n",
    "    process_pcsr(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an plot one file\n",
    "xr_file = xr.open_dataset(cfg.dataPath + path_pcsr + 'zarr/' +\n",
    "                          'xr_direct_aletsch.zarr')\n",
    "xr_file['grid_data'].plot(x='x', y='y', col='time', col_wrap=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcsr_glaciers = os.listdir(cfg.dataPath + path_pcsr + 'raw/')\n",
    "len(pcsr_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years available per glacier\n",
    "geod_glaciers = [\n",
    "    'schwarzbach', 'joeri', 'sanktanna', 'corvatsch', 'sexrouge', 'murtel',\n",
    "    'plattalva', 'tortin', 'basodino', 'limmern', 'adler', 'hohlaub',\n",
    "    'albigna', 'tsanfleuron', 'silvretta', 'oberaar', 'gries', 'clariden',\n",
    "    'gietro', 'schwarzberg', 'forno', 'allalin', 'otemma', 'findelen', 'rhone',\n",
    "    'morteratsch', 'corbassiere', 'gorner', 'aletsch'\n",
    "]\n",
    "\n",
    "base_dir = os.path.join(cfg.dataPath, path_pcsr, 'raw')\n",
    "\n",
    "glacier_years = {}\n",
    "\n",
    "for glacier_name in geod_glaciers:\n",
    "    glacier_path = os.path.join(base_dir, glacier_name)\n",
    "    if os.path.isdir(glacier_path):\n",
    "        years = []\n",
    "        for fname in os.listdir(glacier_path):\n",
    "            match = re.search(r'(\\d{4})', fname)  # look for a 4-digit year\n",
    "            if match:\n",
    "                years.append(int(match.group(1)))\n",
    "        glacier_years[glacier_name] = sorted(set(years))\n",
    "\n",
    "pd.DataFrame(glacier_years).transpose().sort_values(by=0).reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'glacier_name',\n",
    "        0: 'pcsr year'\n",
    "    }).to_csv('pcsr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glacier_years).transpose().sort_values(by=0).reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'glacier_name',\n",
    "        0: 'pcsr year'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
