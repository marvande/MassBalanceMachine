{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLAMOS MB data:\n",
    "\n",
    "Does the pre-processing of the point MB measurements from GLAMOS (winter and summer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Mass Balance data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "import matplotlib as mpl\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from cmcrameri import cm\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import *\n",
    "from scripts.glamos import *\n",
    "from scripts.geo_data import *\n",
    "from scripts.oggm import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.plotting import *\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.plots.use_mbm_style()\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    mbm.utils.free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform .dat files to .csv:\n",
    "\n",
    "Transform the seasonal and winter PMB .dat files to .csv for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pmb_dat_files(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assemble measurement periods:\n",
    "### Annual measurements: \n",
    "Process annual measurements and put all stakes into one csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first two rows\n",
    "df_annual_raw = process_annual_stake_data(cfg.dataPath + path_PMB_GLAMOS_csv_a)\n",
    "df_annual_raw.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winter measurements:\n",
    "For each point in annual meas., take winter meas that was taken closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_winter_stake_data(df_annual_raw, cfg.dataPath + path_PMB_GLAMOS_csv_w,\n",
    "                          cfg.dataPath + path_PMB_GLAMOS_csv_w_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble both periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_raw = assemble_all_stake_data(\n",
    "    df_annual_raw, cfg.dataPath + path_PMB_GLAMOS_csv_w_clean,\n",
    "    cfg.dataPath + path_PMB_GLAMOS_csv)\n",
    "\n",
    "# Plot: Number of measurements per year\n",
    "df_measurements_per_year = df_all_raw.groupby(['YEAR',\n",
    "                                               'PERIOD']).size().unstack()\n",
    "df_measurements_per_year.plot(kind='bar',\n",
    "                              stacked=True,\n",
    "                              figsize=(20, 5),\n",
    "                              color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER])\n",
    "plt.title('Number of measurements per year for all glaciers')\n",
    "plt.ylabel('Number of Measurements')\n",
    "plt.xlabel('Year')\n",
    "plt.legend(title='Period')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add RGIs Ids:\n",
    "\n",
    "For each PMB measurement, we want to add the RGI ID (v6) of the shapefile it belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb = add_rgi_ids_to_df(df_all_raw, cfg.dataPath + path_rgi_outlines)\n",
    "\n",
    "rgiids6 = df_pmb[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "if check_multiple_rgi_ids(rgiids6):\n",
    "    print(\n",
    "        \"-- Alert: The following glaciers have more than one RGIId. Cleaning up.\"\n",
    "    )\n",
    "    df_pmb_clean = clean_rgi_ids(df_pmb.copy())\n",
    "    df_pmb_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    rgiids6_clean = df_pmb_clean[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "    if check_multiple_rgi_ids(rgiids6_clean):\n",
    "        print(\"-- Error: Some glaciers still have more than one RGIId.\")\n",
    "    else:\n",
    "        print(\"-- All glaciers are correctly associated with a single RGIId.\")\n",
    "else:\n",
    "    print(\"-- All glaciers are correctly associated with a single RGIId.\")\n",
    "    df_pmb_clean = df_pmb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut from 1951:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to start of MS data (1951) or ERA5-Land data (1950):\n",
    "df_pmb_50s = df_pmb_clean[df_pmb_clean.YEAR > 1950].sort_values(\n",
    "    by=['GLACIER', 'YEAR'], ascending=[True, True])\n",
    "\n",
    "# Change from mm w.e. to m w.e.\n",
    "df_pmb_50s['POINT_BALANCE'] = df_pmb_50s['POINT_BALANCE'] / 1000\n",
    "\n",
    "# merge ClaridenL and ClaridenU into one glacier:\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenU', 'GLACIER'] = 'clariden'\n",
    "df_pmb_50s.loc[df_pmb_50s.GLACIER == 'claridenL', 'GLACIER'] = 'clariden'\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_50s))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s[df_pmb_50s.PERIOD == 'winter']))\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER], ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge stakes that are close: \n",
    "Especially with winter probes, a lot of measurements were done at the same place in the raw data and this leads to noise. We merge the stakes that are very close and keep the mean of the measurement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_50s_clean_pts = pd.DataFrame()\n",
    "for gl in tqdm(df_pmb_50s.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_pmb_50s[df_pmb_50s.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_50s_clean_pts = pd.concat([df_pmb_50s_clean_pts, df_gl_cleaned])\n",
    "df_pmb_50s_clean_pts.drop(['x', 'y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for wrong elevations:\n",
    "Some PMB data is stored with the wrong altitudes. We compare them to the DEMs and correct them otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a unique-index working copy\n",
    "df_clean = df_pmb_50s_clean_pts.reset_index(drop=True).copy()\n",
    "print(\"Initial number of rows:\", len(df_clean))\n",
    "\n",
    "path_xr_grids = os.path.join(cfg.dataPath, path_GLAMOS_topo,\n",
    "                             \"xr_masked_grids/\")\n",
    "\n",
    "df_clean, df_mismatch, summary = reconcile_points_by_year(\n",
    "    df=df_pmb_50s_clean_pts,\n",
    "    path_xr_grids=path_xr_grids,\n",
    "    var_name=\"masked_elev\",\n",
    "    lon_name=\"lon\",\n",
    "    lat_name=\"lat\",\n",
    "    year_col=\"YEAR\",\n",
    "    glacier_col=\"GLACIER\",\n",
    "    point_elev_col=\"POINT_ELEVATION\",\n",
    "    threshold=400.0,\n",
    "    file_pattern=\"{glacier}_{year}.zarr\",\n",
    "    replace_glaciers={\"aletsch\"},  # replace for Aletsch, drop for others\n",
    "    strict=False,\n",
    "    verbose=True,  # prints counts per glacier\n",
    ")\n",
    "\n",
    "print(\"Final number of rows:\", len(df_clean))\n",
    "\n",
    "# Save mismatches to CSV\n",
    "out_csv = os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                       \"GLAMOS_elev_mismatch.csv\")\n",
    "df_mismatch.sort_values(by=\"elev_diff\", ascending=False, inplace=True)\n",
    "df_mismatch.to_csv(out_csv, index=False)\n",
    "print(\"Saved mismatches to:\", out_csv)\n",
    "\n",
    "# df_clean is your final cleaned dataframe (all glaciers, mismatches removed)\n",
    "df_pmb_50s_clean_elv = df_clean\n",
    "\n",
    "# reset_index\n",
    "df_pmb_50s_clean_elv.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save intermediate output\n",
    "print('Saving intermediate output df_pmb_50s.csv to {path_PMB_GLAMOS_csv}')\n",
    "df_pmb_50s_clean_elv.to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                                         'df_pmb_50s.csv'),\n",
    "                            index=False)\n",
    "df_pmb_50s_clean_elv[[\n",
    "    'GLACIER', 'POINT_ID', 'POINT_LAT', 'POINT_LON', 'PERIOD'\n",
    "]].to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                       'coordinate_50s.csv'),\n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacier_name = 'rhone'\n",
    "df_clean = df_pmb_50s_clean_pts.reset_index(drop=True).copy()\n",
    "df_first = first_year_per_glacier(path_xr_grids)\n",
    "\n",
    "df_gl = df_clean[(df_clean.GLACIER == glacier_name)]\n",
    "ds = xr.open_zarr(\n",
    "    df_first[df_first.glacier == glacier_name].first_year_path.values[0])\n",
    "\n",
    "threshold = 400.0,\n",
    "mismatch_idx, df_with_diffs = find_mismatch_by_year(\n",
    "    df_gl=df_gl,  # must include GLACIER and YEAR columns\n",
    "    path_xr_grids=path_xr_grids,\n",
    "    var_name=\"masked_elev\",\n",
    "    lon_name=\"lon\",\n",
    "    lat_name=\"lat\",\n",
    "    year_col=\"YEAR\",  # change if your year column is named differently\n",
    "    glacier_col=\"GLACIER\",\n",
    "    threshold=threshold,  # meters\n",
    "    file_pattern=\"{glacier}_{year}.zarr\",  # e.g. \"aletsch_1951.zarr\"\n",
    "    strict=False,\n",
    ")\n",
    "print(\n",
    "    f\"Number of POINT indices with >={threshold} m mismatch: {len(mismatch_idx)}\"\n",
    ")\n",
    "\n",
    "\n",
    "def pick_ann_file(cfg, glacier_name, year, period=\"annual\"):\n",
    "    if period == \"annual\":\n",
    "        suffix = \"ann\"\n",
    "    elif period == \"winter\":\n",
    "        suffix = \"win\"\n",
    "    base = os.path.join(cfg.dataPath, path_distributed_MB_glamos, \"GLAMOS\",\n",
    "                        glacier_name)\n",
    "    cand_lv95 = os.path.join(base, f\"{year}_{suffix}_fix_lv95.grid\")\n",
    "    cand_lv03 = os.path.join(base, f\"{year}_{suffix}_fix_lv03.grid\")\n",
    "    if os.path.exists(cand_lv95):\n",
    "        return cand_lv95, \"lv95\"\n",
    "    if os.path.exists(cand_lv03):\n",
    "        return cand_lv03, \"lv03\"\n",
    "    return None, None\n",
    "\n",
    "\n",
    "glacier_name = glacier_name\n",
    "year = df_first[df_first.glacier == glacier_name].first_year.values[0]\n",
    "period = 'annual'\n",
    "file_ann, coord_system = pick_ann_file(cfg, glacier_name, year, period)\n",
    "grid_path_ann = os.path.join(cfg.dataPath, path_distributed_MB_glamos,\n",
    "                             \"GLAMOS\", glacier_name, file_ann)\n",
    "\n",
    "# Load GLAMOS data and convert to WGS84\n",
    "metadata_ann, grid_data_ann = load_grid_file(grid_path_ann)\n",
    "ds_glamos_ann = convert_to_xarray_geodata(grid_data_ann, metadata_ann)\n",
    "if coord_system == \"lv03\":\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv03_to_wgs84(ds_glamos_ann)\n",
    "elif coord_system == \"lv95\":\n",
    "    ds_glamos_wgs84_ann = transform_xarray_coords_lv95_to_wgs84(ds_glamos_ann)\n",
    "\n",
    "figure = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Shared normalization across both plots\n",
    "vmin = min(df_with_diffs[\"POINT_ELEVATION\"].min(), float(ds.masked_elev.min()))\n",
    "vmax = max(df_with_diffs[\"POINT_ELEVATION\"].max(), float(ds.masked_elev.max()))\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = plt.cm.terrain\n",
    "\n",
    "# ---- First subplot ----\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ds_glamos_wgs84_ann.plot.imshow(\n",
    "    ax=ax1,\n",
    "    cmap=\"Greys\",\n",
    "    cbar_kwargs={\"label\": \"Mass Balance [m w.e.]\"},\n",
    ")\n",
    "\n",
    "# scatter using same cmap + norm\n",
    "sc = ax1.scatter(\n",
    "    df_with_diffs[\"POINT_LON\"],\n",
    "    df_with_diffs[\"POINT_LAT\"],\n",
    "    c=df_with_diffs[\"POINT_ELEVATION\"],\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=25,\n",
    ")\n",
    "ax1.set_title(f\"{glacier_name.capitalize()} {year} GLAMOS glacier-wide MB\")\n",
    "\n",
    "# ---- Second subplot ----\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "im = ds.masked_elev.plot(\n",
    "    ax=ax2,\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    add_colorbar=False,  # donâ€™t add duplicate colorbar\n",
    ")\n",
    "ax2.set_title(f\"{glacier_name.capitalize()} {year} DEM\")\n",
    "\n",
    "# ---- Shared colorbar for elevation ----\n",
    "cbar = figure.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "    ax=ax2,\n",
    "    orientation=\"vertical\",\n",
    "    fraction=0.02,\n",
    "    pad=0.02,\n",
    ")\n",
    "cbar.set_label(\"Elevation [m a.s.l.]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "df_with_diffs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_50s_clean_elv.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER], ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_50s_clean_elv.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacier_list = list(df_pmb_50s_clean_elv.GLACIER.unique())\n",
    "print('Number of glaciers:', len(glacier_list))\n",
    "glacier_list.sort()\n",
    "glacier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per glacier per year:\n",
    "num_gl_yr = df_pmb_50s_clean_elv.groupby(['GLACIER', 'YEAR', 'PERIOD'\n",
    "                                          ]).size().unstack().reset_index()\n",
    "\n",
    "num_gl_annual = df_pmb_50s_clean_elv[\n",
    "    df_pmb_50s_clean_elv.PERIOD == 'annual'].groupby(['GLACIER'\n",
    "                                                      ]).size().sort_values()\n",
    "\n",
    "# Plot one glacier per column:\n",
    "big_gl = num_gl_annual[num_gl_annual > 250].index.sort_values()\n",
    "num_glaciers = len(big_gl)\n",
    "fig, ax = plt.subplots(num_glaciers, 1, figsize=(15, 5 * num_glaciers))\n",
    "for i, gl in enumerate(big_gl):\n",
    "    num_gl_yr[num_gl_yr.GLACIER == gl].plot(x='YEAR',\n",
    "                                            kind='bar',\n",
    "                                            stacked=True,\n",
    "                                            ax=ax[i],\n",
    "                                            title=gl)\n",
    "    ax[i].set_ylabel('Number of measurements')\n",
    "    ax[i].set_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_50s_clean_elv))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_50s_clean_elv[df_pmb_50s_clean_elv.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_50s_clean_elv[df_pmb_50s_clean_elv.PERIOD == 'winter']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_50s_clean_elv.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add topographical information from OGGM & SGI:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OGGM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_50s_clean = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                               'df_pmb_50s.csv')\n",
    "\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "unique_rgis = df_pmb_50s_clean['RGIId'].unique()\n",
    "\n",
    "export_oggm_grids(cfg, gdirs)\n",
    "\n",
    "df_pmb_topo = merge_pmb_with_oggm_data(\n",
    "    df_pmb=df_pmb_50s_clean,\n",
    "    gdirs=gdirs,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict to within glacier shape\n",
    "df_pmb_topo = df_pmb_topo[df_pmb_topo['within_glacier_shape']]\n",
    "df_pmb_topo = df_pmb_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving intermediate output df_pmb_50s.csv to {path_PMB_GLAMOS_csv}')\n",
    "df_pmb_topo.to_csv(os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                                'df_pmb_oggm_intermediate.csv'),\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_topo = pd.read_csv(\n",
    "    os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv,\n",
    "                 'df_pmb_oggm_intermediate.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the masked topographical arrays per glacier:\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "create_sgi_topo_masks(cfg,\n",
    "                      glacier_list,\n",
    "                      type='glacier_name',\n",
    "                      path_save=os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                                             'xr_masked_grids/'),\n",
    "                      path_xr_svf=os.path.join(\n",
    "                          cfg.dataPath, \"GLAMOS/topo/SGI2020/svf_nc_latlon/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "i = 0\n",
    "glacier_name = 'adler'\n",
    "df_pmb_gl = df_pmb_50s_clean[df_pmb_50s_clean.GLACIER == glacier_name]\n",
    "\n",
    "stake_coordinates = df_pmb_gl[['POINT_LON', 'POINT_LAT']].values\n",
    "\n",
    "# Open SGI grid:\n",
    "ds_sgi = xr.open_dataset(\n",
    "    os.path.join(cfg.dataPath, path_SGI_topo, 'xr_masked_grids/',\n",
    "                 f'{glacier_name}.zarr'))\n",
    "\n",
    "# Plot the masked data\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds_sgi.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted')\n",
    "ds_sgi.masked_slope.plot(ax=axs[1], cmap='cividis')\n",
    "ds_sgi.masked_elev.plot(ax=axs[2], cmap='terrain')\n",
    "ds_sgi.glacier_mask.plot(ax=axs[3], cmap='binary')\n",
    "axs[3].scatter(stake_coordinates[:, 0], stake_coordinates[:, 1], c='r', s=10)\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"SVF\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_grids = os.path.join(cfg.dataPath, path_SGI_topo,\n",
    "                                 'xr_masked_grids/')\n",
    "\n",
    "# Merge PMB with SGI data\n",
    "df_pmb_sgi = merge_pmb_with_sgi_data(\n",
    "    df_pmb_topo,  # cleaned PMB DataFrame\n",
    "    path_masked_grids,  # path to SGI grids\n",
    "    voi=[\n",
    "        \"masked_aspect\", \"masked_slope\", \"masked_elev\", \"masked_svf\",\n",
    "        \"masked_asvf\", \"masked_opns\"\n",
    "    ])\n",
    "\n",
    "df_pmb_sgi.rename(columns={\n",
    "    'masked_svf': 'svf',\n",
    "    'masked_asvf': 'asvf',\n",
    "    'masked_opns': 'opns',\n",
    "},\n",
    "                  inplace=True)\n",
    "\n",
    "# Drop points that have no intersection with SGI mask: (have NaN values)\n",
    "df_pmb_sgi = df_pmb_sgi.dropna()\n",
    "df_pmb_sgi.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and display the number of samples\n",
    "print(f\"Total number of winter and annual samples: {len(df_pmb_sgi)}\")\n",
    "\n",
    "# Count occurrences of 'PERIOD' values\n",
    "period_counts = df_pmb_sgi['PERIOD'].value_counts()\n",
    "print(f\"Number of annual samples: {period_counts.get('annual', 0)}\")\n",
    "print(f\"Number of winter samples: {period_counts.get('winter', 0)}\")\n",
    "\n",
    "# Unique years, sorted\n",
    "unique_years = np.sort(df_pmb_sgi.YEAR.unique())\n",
    "print(f\"Unique years: {unique_years}\")\n",
    "\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_sgi.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'clariden'\n",
    "# stakes\n",
    "df_stakes = df_pmb_topo.copy()\n",
    "df_stakes = df_stakes[(df_stakes['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes.RGIId.unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath}/OGGM/xr_grids/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes[\"POINT_LON\"].values, df_stakes[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes['x'] = x_stake\n",
    "df_stakes['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(121)\n",
    "ds_oggm.glacier_mask.plot(cmap='binary', ax=ax)\n",
    "sns.scatterplot(\n",
    "    df_stakes,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    # hue='within_glacier_shape',\n",
    "    ax=ax,\n",
    "    palette=['r', 'b'])\n",
    "ax.set_title('Stakes on glacier OGGM')\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "path_SGI_topo = f'{cfg.dataPath}/GLAMOS/topo/SGI2020/'\n",
    "sgi_grid = xr.open_dataset(path_SGI_topo +\n",
    "                           f'xr_masked_grids/{glacierName}.zarr')\n",
    "sgi_grid.glacier_mask.plot(cmap='binary', ax=ax)\n",
    "sns.scatterplot(\n",
    "    df_stakes,\n",
    "    x='POINT_LON',\n",
    "    y='POINT_LAT',\n",
    "    # hue='within_glacier_shape',\n",
    "    ax=ax,\n",
    "    palette=['r', 'b'])\n",
    "ax.set_title('Stakes on glacier SGI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_sgi.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER], ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_sgi.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierName = 'clariden'\n",
    "df_pmb_gl = df_pmb_sgi[(df_pmb_sgi.GLACIER == glacierName)]\n",
    "\n",
    "# Plot aspect and sgi aspect\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "axs[0].scatter(df_pmb_gl.aspect, df_pmb_gl.aspect_sgi)\n",
    "axs[0].set_xlabel('aspect oggm')\n",
    "axs[0].set_ylabel('aspect sgi')\n",
    "axs[0].set_title('Aspect')\n",
    "\n",
    "axs[1].scatter(df_pmb_gl.slope, df_pmb_gl.slope_sgi)\n",
    "axs[1].set_xlabel('slope oggm')\n",
    "axs[1].set_ylabel('slope sgi')\n",
    "axs[1].set_title('Slope')\n",
    "\n",
    "# same for topo\n",
    "axs[2].scatter(df_pmb_gl.topo, df_pmb_gl.topo_sgi)\n",
    "axs[2].set_xlabel('topo oggm')\n",
    "axs[2].set_ylabel('topo sgi')\n",
    "axs[2].set_title('Topo')\n",
    "# add 1:1 line\n",
    "for ax in axs:\n",
    "    ax.plot(ax.get_xlim(), ax.get_xlim(), ls=\"--\", c=\".3\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give new stake IDs:\n",
    "Give new stake IDs with glacier name and then a number according to the elevation. This is because accross glaciers some stakes have the same ID which is not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop taelliboden (only one measurement)\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi.GLACIER != 'taelliboden']\n",
    "\n",
    "# drop taelliboden (big outlier)\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi.GLACIER != 'plainemorte']\n",
    "\n",
    "df_pmb_sgi = rename_stakes_by_elevation(df_pmb_sgi)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_sgi)\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_sgi))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_sgi[df_pmb_sgi.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_sgi[df_pmb_sgi.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_sgi['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_sgi['MONTH_START'] = [str(date)[4:6] for date in df_pmb_sgi.FROM_DATE]\n",
    "df_pmb_sgi['MONTH_END'] = [str(date)[4:6] for date in df_pmb_sgi.TO_DATE]\n",
    "\n",
    "# drop rows where month_start is '07'\n",
    "df_pmb_sgi = df_pmb_sgi[df_pmb_sgi['MONTH_START'] != '07']\n",
    "\n",
    "# drop\n",
    "df_pmb_sgi = df_pmb_sgi.loc[~((df_pmb_sgi['MONTH_END'] == '06') &\n",
    "                              (df_pmb_sgi['PERIOD'] == 'annual'))]\n",
    "\n",
    "df_pmb_sgi = df_pmb_sgi.loc[~((df_pmb_sgi['MONTH_END'] == '11') &\n",
    "                              (df_pmb_sgi['PERIOD'] == 'annual'))]\n",
    "\n",
    "# Rows where month_end is '05' and period is 'annual', rename period to 'winter'\n",
    "df_pmb_sgi.loc[(df_pmb_sgi['MONTH_END'] == '05') &\n",
    "               (df_pmb_sgi['PERIOD'] == 'annual'), 'PERIOD'] = 'winter'\n",
    "\n",
    "# Rows where month_end is '08' and period is 'winter', rename period to 'winter'\n",
    "df_pmb_sgi.loc[(df_pmb_sgi['MONTH_END'] == '08') &\n",
    "               (df_pmb_sgi['PERIOD'] == 'winter'), 'PERIOD'] = 'annual'\n",
    "\n",
    "POINT_ID_to_drop = [\n",
    "    'schwarzberg_12', 'schwarzberg_13', 'schwarzberg_6', 'schwarzberg_5',\n",
    "    'schwarzberg_12', 'plattalva_7', 'plattalva_11', 'plattalva_10',\n",
    "    'plattalva_1'\n",
    "]\n",
    "\n",
    "# remove points with too much missing data\n",
    "df_pmb_sgi = df_pmb_sgi[~df_pmb_sgi['POINT_ID'].isin(POINT_ID_to_drop)]\n",
    "\n",
    "# Save to csv:\n",
    "df_pmb_sgi.to_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                  f'CH_wgms_dataset_all.csv',\n",
    "                  index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cfg.dataPath + path_PMB_GLAMOS_csv +\n",
    "                 f'CH_wgms_dataset_all.csv')\n",
    "df.GLACIER.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier wide MB:\n",
    "Pre-processing of glacier wide SMB data from GLAMOS. Transform .dat files to .csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_SMB_GLAMOS(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs: no fixed dates, but using observed periods.\n",
    "# Example:\n",
    "fileName = 'aletsch_obs.csv'\n",
    "aletsch_csv = pd.read_csv(cfg.dataPath + path_SMB_GLAMOS_csv + 'obs/' +\n",
    "                          fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: with fixed periods (hydrological year).\n",
    "# # Example:\n",
    "fileName = 'aletsch_fix.csv'\n",
    "aletsch_csv = pd.read_csv(cfg.dataPath + path_SMB_GLAMOS_csv + 'fix/' +\n",
    "                          fileName,\n",
    "                          sep=',',\n",
    "                          header=0,\n",
    "                          encoding='latin-1')\n",
    "aletsch_csv.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential incoming clear sky solar radiation:\n",
    "\n",
    "Pre-process glamos data of \"potential incoming clear sky solar radiation (pcsr)\" used as a topographical variable. One per day grid per glacier for one year only, depends on the glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "if RUN:\n",
    "    glDirect = np.sort(os.listdir(cfg.dataPath + path_pcsr +\n",
    "                                  'raw/'))  # Glaciers with data\n",
    "\n",
    "    print('Number of glacier with clear sky radiation data:', len(glDirect))\n",
    "    print('Glaciers with clear sky radiation data:', glDirect)\n",
    "\n",
    "    process_pcsr(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an plot one file\n",
    "xr_file = xr.open_dataset(cfg.dataPath + path_pcsr + 'zarr/' +\n",
    "                          'xr_direct_aletsch.zarr')\n",
    "xr_file['grid_data'].plot(x='x', y='y', col='time', col_wrap=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcsr_glaciers = os.listdir(cfg.dataPath + path_pcsr + 'raw/')\n",
    "len(pcsr_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years available per glacier\n",
    "geod_glaciers = [\n",
    "    'schwarzbach', 'joeri', 'sanktanna', 'corvatsch', 'sexrouge', 'murtel',\n",
    "    'plattalva', 'tortin', 'basodino', 'limmern', 'adler', 'hohlaub',\n",
    "    'albigna', 'tsanfleuron', 'silvretta', 'oberaar', 'gries', 'clariden',\n",
    "    'gietro', 'schwarzberg', 'forno', 'allalin', 'otemma', 'findelen', 'rhone',\n",
    "    'morteratsch', 'corbassiere', 'gorner', 'aletsch'\n",
    "]\n",
    "\n",
    "base_dir = os.path.join(cfg.dataPath, path_pcsr, 'raw')\n",
    "\n",
    "glacier_years = {}\n",
    "\n",
    "for glacier_name in geod_glaciers:\n",
    "    glacier_path = os.path.join(base_dir, glacier_name)\n",
    "    if os.path.isdir(glacier_path):\n",
    "        years = []\n",
    "        for fname in os.listdir(glacier_path):\n",
    "            match = re.search(r'(\\d{4})', fname)  # look for a 4-digit year\n",
    "            if match:\n",
    "                years.append(int(match.group(1)))\n",
    "        glacier_years[glacier_name] = sorted(set(years))\n",
    "\n",
    "pd.DataFrame(glacier_years).transpose().sort_values(by=0).reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'glacier_name',\n",
    "        0: 'pcsr year'\n",
    "    }).to_csv('pcsr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glacier_years).transpose().sort_values(by=0).reset_index().rename(\n",
    "    columns={\n",
    "        'index': 'glacier_name',\n",
    "        0: 'pcsr year'\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
