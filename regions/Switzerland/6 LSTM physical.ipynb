{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from tqdm.notebook import tqdm\n",
    "from cmcrameri import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import joypy\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# Add root of repo to import MBM\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# Local modules\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "# RGI Ids:\n",
    "# Read rgi ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "]\n",
    "\n",
    "# vois_topographical = [\n",
    "#     \"aspect_sgi\", \"slope_sgi\", \"hugonnet_dhdt\", \"consensus_ice_thickness\",\n",
    "#     \"millan_v\", \"svf\"\n",
    "# ]\n",
    "\n",
    "vois_topographical = [\"aspect_sgi\", \"slope_sgi\", \"svf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "# Number of winter and annual measurements:\n",
    "print(\"Number of winter measurements:\",\n",
    "      data_glamos.groupby('PERIOD').count().YEAR.loc['winter'])\n",
    "print(\"Number of annual measurements:\",\n",
    "      data_glamos.groupby('PERIOD').count().YEAR.loc['annual'])\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM_svf.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution of test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'pcsr',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "STATIC_COLS = ['aspect_sgi', 'slope_sgi', 'svf']\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "cfg.setFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True,\n",
    "    normalize_target=True)\n",
    "\n",
    "ds_test = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True,\n",
    "    normalize_target=True)\n",
    "\n",
    "train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'Fm': 9,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 2,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.2,\n",
    "    'static_layers': 2,\n",
    "    'static_hidden': [128, 64],\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.0,\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test)\n",
    "\n",
    "train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test and transforms it) ---\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "# Evaluate on test\n",
    "model_filename = f\"models/lstm_model_2025-11-04_no_oggm_norm_y.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_item(x):\n",
    "    return x.item() if x is not None else None\n",
    "\n",
    "\n",
    "print(\"Train dataset (after make_loaders):\")\n",
    "print(f\"  normalize_target = {ds_train_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_train_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_train_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_train_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_train_copy.y.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nTest dataset (after make_test_loader):\")\n",
    "print(f\"  normalize_target = {ds_test_copy.normalize_target}\")\n",
    "print(f\"  y_mean (scaler)  = {safe_item(ds_test_copy.y_mean)}\")\n",
    "print(f\"  y_std  (scaler)  = {safe_item(ds_test_copy.y_std)}\")\n",
    "print(f\"  Actual y.mean()  = {ds_test_copy.y.mean().item():.4f}\")\n",
    "print(f\"  Actual y.std()   = {ds_test_copy.y.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_PREDICTIONS_LSTM_OOS = os.path.join(\n",
    "#     cfg.dataPath, \"GLAMOS\", \"distributed_MB_grids\",\n",
    "#     \"MBM/testing_LSTM/LSTM_no_oggm_OOS_original_y\")\n",
    "\n",
    "PATH_PREDICTIONS_LSTM_IS = os.path.join(\n",
    "    cfg.dataPath, \"GLAMOS\", \"distributed_MB_grids\",\n",
    "    \"MBM/testing_LSTM/LSTM_with_oggm_IS_original_y_one_head\")\n",
    "\n",
    "PATH_PREDICTIONS_NN = os.path.join(cfg.dataPath, 'GLAMOS',\n",
    "                                   'distributed_MB_grids',\n",
    "                                   'MBM/testing_LSTM/NN')\n",
    "\n",
    "hydro_months = [\n",
    "    'oct',\n",
    "    'nov',\n",
    "    'dec',\n",
    "    'jan',\n",
    "    'feb',\n",
    "    'mar',\n",
    "    'apr',\n",
    "    'may',\n",
    "    'jun',\n",
    "    'jul',\n",
    "    'aug',\n",
    "    'sep',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "    glacier_name=\"rhone\",\n",
    "    year=2008,\n",
    "    path_pred_lstm=PATH_PREDICTIONS_NN,\n",
    "    apply_smoothing_fn=apply_gaussian_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_glacier_monthly_series_lstm_sharedcmap_center0(\n",
    "    glacier_name=\"rhone\",\n",
    "    year=2008,\n",
    "    path_pred_lstm=PATH_PREDICTIONS_LSTM_IS,\n",
    "    apply_smoothing_fn=apply_gaussian_filter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers = os.listdir(PATH_PREDICTIONS_LSTM_IS)\n",
    "\n",
    "# Initialize final storage for all glacier data\n",
    "all_glacier_data = []\n",
    "\n",
    "# Loop over glaciers\n",
    "for glacier_name in tqdm(glaciers):\n",
    "    glacier_path = os.path.join(PATH_PREDICTIONS_LSTM_IS, glacier_name)\n",
    "    if not os.path.isdir(glacier_path):\n",
    "        continue  # skip non-directories\n",
    "\n",
    "    # Regex pattern adapted for current glacier name\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_[a-z]{{3}}\\.zarr')\n",
    "\n",
    "    # Extract available years\n",
    "    years = set()\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            years.add(int(match.group(1)))\n",
    "    years = sorted(years)\n",
    "\n",
    "    # Collect all year-month data\n",
    "    all_years_data = []\n",
    "    for year in years:\n",
    "        monthly_data = {}\n",
    "        for month in hydro_months:\n",
    "            zarr_path = os.path.join(glacier_path,\n",
    "                                     f'{glacier_name}_{year}_{month}.zarr')\n",
    "            if not os.path.exists(zarr_path):\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(zarr_path)\n",
    "            df = ds.pred_masked.to_dataframe().drop(['x', 'y'],\n",
    "                                                    axis=1).reset_index()\n",
    "            df_pred_months = df[df.pred_masked.notna()]\n",
    "\n",
    "            df_el = ds.masked_elev.to_dataframe().drop(['x', 'y'],\n",
    "                                                       axis=1).reset_index()\n",
    "            df_elv_months = df_el[df.pred_masked.notna()]\n",
    "\n",
    "            df_pred_months['elevation'] = df_elv_months.masked_elev.values\n",
    "\n",
    "            monthly_data[month] = df_pred_months.pred_masked.values\n",
    "\n",
    "        if monthly_data:\n",
    "            df_months = pd.DataFrame(monthly_data)\n",
    "            df_months['year'] = year\n",
    "            df_months['glacier'] = glacier_name  # add glacier name\n",
    "            df_months['elevation'] = df_pred_months.elevation.values\n",
    "            all_years_data.append(df_months)\n",
    "\n",
    "    # Concatenate this glacier's data\n",
    "    if all_years_data:\n",
    "        df_glacier = pd.concat(all_years_data, axis=0, ignore_index=True)\n",
    "        all_glacier_data.append(df_glacier)\n",
    "\n",
    "# Final full DataFrame for all glaciers\n",
    "df_months_LSTM = pd.concat(all_glacier_data, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers = os.listdir(PATH_PREDICTIONS_NN)\n",
    "\n",
    "# Initialize final storage for all glacier data\n",
    "all_glacier_data = []\n",
    "\n",
    "# Loop over glaciers\n",
    "for glacier_name in tqdm(glaciers):\n",
    "    glacier_path = os.path.join(PATH_PREDICTIONS_NN, glacier_name)\n",
    "    if not os.path.isdir(glacier_path):\n",
    "        continue  # skip non-directories\n",
    "\n",
    "    # Regex pattern adapted for current glacier name\n",
    "    pattern = re.compile(rf'{glacier_name}_(\\d{{4}})_[a-z]{{3}}\\.zarr')\n",
    "\n",
    "    # Extract available years\n",
    "    years = set()\n",
    "    for fname in os.listdir(glacier_path):\n",
    "        match = pattern.match(fname)\n",
    "        if match:\n",
    "            years.add(int(match.group(1)))\n",
    "    years = sorted(years)\n",
    "\n",
    "    # Collect all year-month data\n",
    "    all_years_data = []\n",
    "    for year in years:\n",
    "        monthly_data = {}\n",
    "        for month in hydro_months:\n",
    "            zarr_path = os.path.join(glacier_path,\n",
    "                                     f'{glacier_name}_{year}_{month}.zarr')\n",
    "            if not os.path.exists(zarr_path):\n",
    "                continue\n",
    "\n",
    "            ds = xr.open_dataset(zarr_path)\n",
    "            df = ds.pred_masked.to_dataframe().drop(['x', 'y'],\n",
    "                                                    axis=1).reset_index()\n",
    "            df_pred_months = df[df.pred_masked.notna()]\n",
    "\n",
    "            df_el = ds.masked_elev.to_dataframe().drop(['x', 'y'],\n",
    "                                                       axis=1).reset_index()\n",
    "            df_elv_months = df_el[df.pred_masked.notna()]\n",
    "\n",
    "            df_pred_months['elevation'] = df_elv_months.masked_elev.values\n",
    "\n",
    "            monthly_data[month] = df_pred_months.pred_masked.values\n",
    "\n",
    "        if monthly_data:\n",
    "            df_months = pd.DataFrame(monthly_data)\n",
    "            df_months['year'] = year\n",
    "            df_months['glacier'] = glacier_name  # add glacier name\n",
    "            df_months['elevation'] = df_pred_months.elevation.values\n",
    "            all_years_data.append(df_months)\n",
    "\n",
    "    # Concatenate this glacier's data\n",
    "    if all_years_data:\n",
    "        df_glacier = pd.concat(all_years_data, axis=0, ignore_index=True)\n",
    "        all_glacier_data.append(df_glacier)\n",
    "\n",
    "# Final full DataFrame for all glaciers\n",
    "df_months_NN = pd.concat(all_glacier_data, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_file_glamos(glacier, year, period=\"winter\"):\n",
    "    suffix = \"ann\" if period == \"annual\" else \"win\"\n",
    "    base = os.path.join(cfg.dataPath, path_distributed_MB_glamos, \"GLAMOS\",\n",
    "                        glacier)\n",
    "    cand_lv95 = os.path.join(base, f\"{year}_{suffix}_fix_lv95.grid\")\n",
    "    cand_lv03 = os.path.join(base, f\"{year}_{suffix}_fix_lv03.grid\")\n",
    "    if os.path.exists(cand_lv95):\n",
    "        return cand_lv95, \"lv95\"\n",
    "    if os.path.exists(cand_lv03):\n",
    "        return cand_lv03, \"lv03\"\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_glamos_wgs84(glacier, year, period):\n",
    "    \"\"\"Load one GLAMOS .grid file and return it as an xarray in WGS84.\"\"\"\n",
    "    path, cs = pick_file_glamos(glacier, year, period)\n",
    "    if path is None:\n",
    "        return None\n",
    "    meta, arr = load_grid_file(path)\n",
    "    da = convert_to_xarray_geodata(arr, meta)\n",
    "    if cs == \"lv03\":\n",
    "        return transform_xarray_coords_lv03_to_wgs84(da)\n",
    "    elif cs == \"lv95\":\n",
    "        return transform_xarray_coords_lv95_to_wgs84(da)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_glamos(glacier_years, path_glamos):\n",
    "    \"\"\"\n",
    "    Loads both annual and winter GLAMOS grids for all glaciers and years,\n",
    "    interpolates DEM elevation onto MB grids, and returns two DataFrames:\n",
    "    df_GLAMOS_w, df_GLAMOS_a.\n",
    "    \"\"\"\n",
    "    all_glacier_data_w, all_glacier_data_a = [], []\n",
    "\n",
    "    for glacier_name in tqdm(glacier_years.keys(), desc=\"Loading GLAMOS data\"):\n",
    "        glacier_path = os.path.join(path_glamos, glacier_name)\n",
    "        if not os.path.isdir(glacier_path):\n",
    "            continue\n",
    "\n",
    "        years = glacier_years[glacier_name]\n",
    "        all_years_w, all_years_a = [], []\n",
    "\n",
    "        for year in years:\n",
    "            # --- Load DEM ---\n",
    "            dem_path = (cfg.dataPath + path_GLAMOS_topo +\n",
    "                        f\"xr_masked_grids/{glacier_name}_{year}.zarr\")\n",
    "            if not os.path.exists(dem_path):\n",
    "                continue\n",
    "            ds_dem = xr.open_zarr(dem_path)\n",
    "\n",
    "            # --- Load Winter MB ---\n",
    "            ds_w = load_glamos_wgs84(glacier_name, year, period=\"winter\")\n",
    "            if ds_w is not None:\n",
    "                masked_elev_interp = ds_dem[\"masked_elev\"].interp_like(\n",
    "                    ds_w, method=\"nearest\")\n",
    "                masked_elev_interp = masked_elev_interp.assign_coords(x=ds_w.x,\n",
    "                                                                      y=ds_w.y)\n",
    "\n",
    "                ds_merged_w = xr.merge(\n",
    "                    [\n",
    "                        ds_w.to_dataset(name=\"mb\"),\n",
    "                        masked_elev_interp.to_dataset(name=\"masked_elev\"),\n",
    "                    ],\n",
    "                    compat=\"override\",\n",
    "                )\n",
    "\n",
    "                df_w = ds_merged_w.to_dataframe().reset_index()\n",
    "                df_w = df_w[df_w[\"mb\"].notna() & df_w[\"masked_elev\"].notna()]\n",
    "                df_w = df_w[[\"x\", \"y\", \"mb\", \"masked_elev\"]]\n",
    "                df_w[\"year\"] = year\n",
    "                df_w[\"glacier\"] = glacier_name\n",
    "                df_w[\"period\"] = \"winter\"\n",
    "                all_years_w.append(df_w)\n",
    "\n",
    "            # --- Load Annual MB ---\n",
    "            ds_a = load_glamos_wgs84(glacier_name, year, period=\"annual\")\n",
    "            if ds_a is not None:\n",
    "                masked_elev_interp = ds_dem[\"masked_elev\"].interp_like(\n",
    "                    ds_a, method=\"nearest\")\n",
    "                masked_elev_interp = masked_elev_interp.assign_coords(x=ds_a.x,\n",
    "                                                                      y=ds_a.y)\n",
    "\n",
    "                ds_merged_a = xr.merge(\n",
    "                    [\n",
    "                        ds_a.to_dataset(name=\"mb\"),\n",
    "                        masked_elev_interp.to_dataset(name=\"masked_elev\"),\n",
    "                    ],\n",
    "                    compat=\"override\",\n",
    "                )\n",
    "\n",
    "                df_a = ds_merged_a.to_dataframe().reset_index()\n",
    "                df_a = df_a[df_a[\"mb\"].notna() & df_a[\"masked_elev\"].notna()]\n",
    "                df_a = df_a[[\"x\", \"y\", \"mb\", \"masked_elev\"]]\n",
    "                df_a[\"year\"] = year\n",
    "                df_a[\"glacier\"] = glacier_name\n",
    "                df_a[\"period\"] = \"annual\"\n",
    "                all_years_a.append(df_a)\n",
    "\n",
    "        # --- Concatenate per glacier ---\n",
    "        if all_years_w:\n",
    "            df_glacier_w = pd.concat(all_years_w, ignore_index=True)\n",
    "            all_glacier_data_w.append(df_glacier_w)\n",
    "        if all_years_a:\n",
    "            df_glacier_a = pd.concat(all_years_a, ignore_index=True)\n",
    "            all_glacier_data_a.append(df_glacier_a)\n",
    "\n",
    "    # --- Final combined DataFrames ---\n",
    "    df_GLAMOS_w = (pd.concat(all_glacier_data_w, ignore_index=True)\n",
    "                   if all_glacier_data_w else pd.DataFrame())\n",
    "    df_GLAMOS_a = (pd.concat(all_glacier_data_a, ignore_index=True)\n",
    "                   if all_glacier_data_a else pd.DataFrame())\n",
    "\n",
    "    # --- Drop x/y and rename elevation column ---\n",
    "    if not df_GLAMOS_w.empty:\n",
    "        df_GLAMOS_w = df_GLAMOS_w.drop([\"x\", \"y\", \"period\"],\n",
    "                                       axis=1).rename(columns={\n",
    "                                           \"masked_elev\": \"elevation\",\n",
    "                                           \"mb\": \"apr\"\n",
    "                                       })\n",
    "    if not df_GLAMOS_a.empty:\n",
    "        df_GLAMOS_a = df_GLAMOS_a.drop([\"x\", \"y\", \"period\"],\n",
    "                                       axis=1).rename(columns={\n",
    "                                           \"masked_elev\": \"elevation\",\n",
    "                                           \"mb\": \"sep\"\n",
    "                                       })\n",
    "\n",
    "    return df_GLAMOS_w, df_GLAMOS_a\n",
    "\n",
    "\n",
    "PATH_GLAMOS = os.path.join(cfg.dataPath, path_distributed_MB_glamos, 'GLAMOS')\n",
    "glaciers = os.listdir(PATH_GLAMOS)\n",
    "\n",
    "glacier_years = (\n",
    "    df_months_LSTM.groupby('glacier')['year'].unique().apply(sorted).to_dict())\n",
    "\n",
    "df_GLAMOS_w, df_GLAMOS_a = load_all_glamos(glacier_years, PATH_GLAMOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glacier-wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get glacier-wide MB for every year\n",
    "glwd_months_NN = df_months_NN.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_LSTM = df_months_LSTM.groupby(['glacier',\n",
    "                                           'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_w = df_GLAMOS_w.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_a = df_GLAMOS_a.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "\n",
    "valid_pairs_NN = set(zip(glwd_months_NN['glacier'], glwd_months_NN['year']))\n",
    "\n",
    "\n",
    "# Define a helper function to filter by those pairs\n",
    "def filter_to_NN(df):\n",
    "    return df[df[['glacier', 'year'\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs_NN)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "valid_pairs = set(zip(glwd_months_NN['glacier'], glwd_months_NN['year']))\n",
    "\n",
    "\n",
    "# Define a helper function to filter by those pairs\n",
    "def filter_to_glamos(df):\n",
    "    return df[df[['glacier', 'year'\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 1. Glacier-wide annual mean MB per year ---\n",
    "glwd_months_NN = df_months_NN.groupby(['glacier', 'year']).mean().reset_index()\n",
    "glwd_months_LSTM = df_months_LSTM.groupby(['glacier',\n",
    "                                           'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_w = df_GLAMOS_w.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "glwd_months_GLAMOS_a = df_GLAMOS_a.groupby(['glacier',\n",
    "                                            'year']).mean().reset_index()\n",
    "\n",
    "# --- 2. Compute the intersection of valid glacier–year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_months_NN['glacier'], glwd_months_NN['year']))\n",
    "    & set(zip(glwd_months_LSTM['glacier'], glwd_months_LSTM['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_w['glacier'], glwd_months_GLAMOS_w['year']))\n",
    "    & set(zip(glwd_months_GLAMOS_a['glacier'], glwd_months_GLAMOS_a['year'])))\n",
    "\n",
    "\n",
    "# --- 3. Helper function for filtering by glacier–year pairs ---\n",
    "def filter_to_valid(df):\n",
    "    return df[df[['glacier', 'year'\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 4. Apply consistent filtering to all datasets ---\n",
    "glwd_months_LSTM_filtered = filter_to_valid(glwd_months_LSTM)\n",
    "glwd_months_NN_filtered = filter_to_valid(glwd_months_NN)\n",
    "glwd_months_GLAMOS_filtered_w = filter_to_valid(glwd_months_GLAMOS_w)\n",
    "glwd_months_GLAMOS_filtered_a = filter_to_valid(glwd_months_GLAMOS_a)\n",
    "\n",
    "print(\n",
    "    len(glwd_months_GLAMOS_filtered_w),\n",
    "    len(glwd_months_GLAMOS_filtered_a),\n",
    "    len(glwd_months_NN_filtered),\n",
    "    len(glwd_months_LSTM_filtered),\n",
    ")\n",
    "\n",
    "# --- 5. Prepare for plotting ---\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_months_LSTM_filtered,\n",
    "    glwd_months_NN_filtered,\n",
    "    glwd_months_GLAMOS_filtered_w,\n",
    "    glwd_months_GLAMOS_filtered_a,\n",
    ")\n",
    "\n",
    "df_months_nn_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove bad glaciers:\n",
    "# geodetic_mb = get_geodetic_MB(cfg)\n",
    "# periods_per_glacier, geoMB_per_glacier = build_periods_per_glacier(geodetic_mb)\n",
    "# geogl = [\n",
    "#     'Schwarzbach', 'Sexrouge', 'Murtel', 'Basodino', 'Adler', 'Hohlaub',\n",
    "#     'Tsanfleuron', 'Silvretta', 'Gries', 'Clariden', 'Gietro', 'Schwarzberg',\n",
    "#     'Allalin', 'Findelen', 'Rhone', 'Corbassiere', 'Aletsch'\n",
    "# ]\n",
    "# geogl = [gl.lower() for gl in geogl]\n",
    "# df_months_nn_long = df_months_nn_long[df_months_nn_long.glacier.isin(geogl)]\n",
    "\n",
    "# gl_error = ['schwarzberg', 'gietro']\n",
    "\n",
    "# df_months_nn_long = df_months_nn_long[~df_months_nn_long.glacier.isin(gl_error\n",
    "#                                                                       )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "min_, max_ = df_months_nn_long.min()[[\n",
    "    'mb_nn', 'mb_lstm'\n",
    "]].min(), df_months_nn_long.max()[['mb_nn', 'mb_lstm']].max()\n",
    "fig = plot_monthly_joyplot(df_months_nn_long,\n",
    "                           color_annual=color_annual,\n",
    "                           color_winter=color_winter,\n",
    "                           x_range=(np.floor(min_), np.ceil(max_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_, max_ = df_months_nn_long.min()[[\n",
    "    'mb_nn', 'mb_glamos'\n",
    "]].min(), df_months_nn_long.max()[['mb_nn', 'mb_glamos']].max()\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)))\n",
    "fig.savefig('figures/CH_LSTM_vs_NN_monthly_joyplot_glwd.png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyfolder('figures/joyplots')\n",
    "for gl in df_months_nn_long.glacier.unique():\n",
    "    df_gl = df_months_nn_long[df_months_nn_long.glacier == gl]\n",
    "    min_, max_ = df_months_nn_long.min()[[\n",
    "        'mb_nn', 'mb_glamos'\n",
    "    ]].min(), df_months_nn_long.max()[['mb_nn', 'mb_glamos']].max()\n",
    "    fig = plot_monthly_joyplot_single(df_gl,\n",
    "                                      variable=\"mb_lstm\",\n",
    "                                      color_model=color_annual,\n",
    "                                      x_range=(np.floor(min_), np.ceil(max_)),\n",
    "                                      show=False)\n",
    "\n",
    "    # save figure\n",
    "    fig.savefig(f'figures/joyplots/{gl}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation bands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = 200\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# Copy datasets\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# Assign elevation bands\n",
    "for df_ in [df_months_NN_, df_months_LSTM_, df_GLAMOS_a_, df_GLAMOS_w_]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# --- Helper to extract highest-elevation band per glacier ---\n",
    "def extract_highest_band(df, bin_width):\n",
    "    max_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"max\")\n",
    "    highest_band = df[df[\"elevation\"] >= (max_elev - bin_width)]\n",
    "    return (highest_band.groupby([\"glacier\", \"year\"\n",
    "                                  ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "# --- Compute highest-elevation bands for all datasets ---\n",
    "glwd_high_NN = extract_highest_band(df_months_NN_, bin)\n",
    "glwd_high_LSTM = extract_highest_band(df_months_LSTM_, bin)\n",
    "glwd_high_GLAMOS_a = extract_highest_band(df_GLAMOS_a_, bin)\n",
    "glwd_high_GLAMOS_w = extract_highest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "# --- Define common glacier-year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_high_NN[\"glacier\"], glwd_high_NN[\"year\"]))\n",
    "    & set(zip(glwd_high_LSTM[\"glacier\"], glwd_high_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_w[\"glacier\"], glwd_high_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_high_GLAMOS_a[\"glacier\"], glwd_high_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return df[df[[\"glacier\", \"year\"\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Apply consistent filtering ---\n",
    "glwd_high_NN_filt = filter_to_valid(glwd_high_NN)\n",
    "glwd_high_LSTM_filt = filter_to_valid(glwd_high_LSTM)\n",
    "glwd_high_GLAMOS_a_filt = filter_to_valid(glwd_high_GLAMOS_a)\n",
    "glwd_high_GLAMOS_w_filt = filter_to_valid(glwd_high_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_high_NN_filt),\n",
    "    len(glwd_high_LSTM_filt),\n",
    "    len(glwd_high_GLAMOS_w_filt),\n",
    "    len(glwd_high_GLAMOS_a_filt),\n",
    ")\n",
    "\n",
    "# --- Prepare combined long-format dataframe for plotting ---\n",
    "df_months_nn_long = prepare_monthly_long_df(\n",
    "    glwd_high_LSTM_filt,  # LSTM data\n",
    "    glwd_high_NN_filt,  # NN data\n",
    "    glwd_high_GLAMOS_w_filt,  # GLAMOS winter (April)\n",
    "    glwd_high_GLAMOS_a_filt  # GLAMOS annual (September)\n",
    ")\n",
    "\n",
    "# --- Determine x-axis limits for plotting ---\n",
    "min_, max_ = (\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_glamos\"]].min().min(),\n",
    "    df_months_nn_long[[\"mb_nn\", \"mb_lstm\", \"mb_glamos\"]].max().max(),\n",
    ")\n",
    "\n",
    "# --- Plot ---\n",
    "# fig = plot_monthly_joyplot(\n",
    "#     df_months_nn_long,\n",
    "#     color_annual=color_annual,\n",
    "#     color_winter=color_winter,\n",
    "#     x_range=(np.floor(min_), np.ceil(max_)),\n",
    "# )\n",
    "\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)))\n",
    "\n",
    "# --- Save figure ---\n",
    "fig.savefig(\"figures/CH_LSTM_vs_NN_GLAMOS_monthly_joyplot_high_elv.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin = 250\n",
    "bins = np.arange(1200, 4500, bin)\n",
    "labels = [f\"{b}-{b+bin}\" for b in bins[:-1]]\n",
    "\n",
    "# Copy to avoid modifying original DataFrames\n",
    "df_months_NN_ = df_months_NN.copy()\n",
    "df_months_LSTM_ = df_months_LSTM.copy()\n",
    "df_GLAMOS_a_ = df_GLAMOS_a.copy()\n",
    "df_GLAMOS_w_ = df_GLAMOS_w.copy()\n",
    "\n",
    "# Assign elevation bands to all datasets\n",
    "for df_ in [df_months_NN_, df_months_LSTM_, df_GLAMOS_a_, df_GLAMOS_w_]:\n",
    "    df_[\"elev_band\"] = pd.cut(df_[\"elevation\"], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# --- Helper for extracting lowest-elevation band per glacier ---\n",
    "def extract_lowest_band(df, bin_width):\n",
    "    min_elev = df.groupby(\"glacier\")[\"elevation\"].transform(\"min\")\n",
    "    lowest_band = df[df[\"elevation\"] <= (min_elev + bin_width)]\n",
    "    return (lowest_band.groupby([\"glacier\", \"year\"\n",
    "                                 ]).mean(numeric_only=True).reset_index())\n",
    "\n",
    "\n",
    "# --- Compute for all datasets ---\n",
    "glwd_low_NN = extract_lowest_band(df_months_NN_, bin)\n",
    "glwd_low_LSTM = extract_lowest_band(df_months_LSTM_, bin)\n",
    "glwd_low_GLAMOS_a = extract_lowest_band(df_GLAMOS_a_, bin)\n",
    "glwd_low_GLAMOS_w = extract_lowest_band(df_GLAMOS_w_, bin)\n",
    "\n",
    "# --- Define common glacier-year pairs across all datasets ---\n",
    "valid_pairs = (\n",
    "    set(zip(glwd_low_NN[\"glacier\"], glwd_low_NN[\"year\"]))\n",
    "    & set(zip(glwd_low_LSTM[\"glacier\"], glwd_low_LSTM[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_w[\"glacier\"], glwd_low_GLAMOS_w[\"year\"]))\n",
    "    & set(zip(glwd_low_GLAMOS_a[\"glacier\"], glwd_low_GLAMOS_a[\"year\"])))\n",
    "\n",
    "\n",
    "def filter_to_valid(df):\n",
    "    return df[df[[\"glacier\", \"year\"\n",
    "                  ]].apply(tuple,\n",
    "                           axis=1).isin(valid_pairs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- Apply consistent filtering ---\n",
    "glwd_low_NN_filt = filter_to_valid(glwd_low_NN)\n",
    "glwd_low_LSTM_filt = filter_to_valid(glwd_low_LSTM)\n",
    "glwd_low_GLAMOS_a_filt = filter_to_valid(glwd_low_GLAMOS_a)\n",
    "glwd_low_GLAMOS_w_filt = filter_to_valid(glwd_low_GLAMOS_w)\n",
    "\n",
    "print(\n",
    "    len(glwd_low_NN_filt),\n",
    "    len(glwd_low_LSTM_filt),\n",
    "    len(glwd_low_GLAMOS_w_filt),\n",
    "    len(glwd_low_GLAMOS_a_filt),\n",
    ")\n",
    "\n",
    "# --- Prepare for plotting (includes both GLAMOS winter + annual) ---\n",
    "df_months_nn_long_low = prepare_monthly_long_df(glwd_low_LSTM_filt,\n",
    "                                                glwd_low_NN_filt,\n",
    "                                                glwd_low_GLAMOS_w_filt,\n",
    "                                                glwd_low_GLAMOS_a_filt)\n",
    "\n",
    "# --- Determine x-axis range for plotting ---\n",
    "min_, max_ = (df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\",\n",
    "                                     \"mb_glamos\"]].min().min(),\n",
    "              df_months_nn_long_low[[\"mb_nn\", \"mb_lstm\",\n",
    "                                     \"mb_glamos\"]].max().max())\n",
    "\n",
    "min_ = -10\n",
    "\n",
    "# --- Plot ---\n",
    "# fig = plot_monthly_joyplot(df_months_nn_long_low,\n",
    "#                            color_annual=color_annual,\n",
    "#                            color_winter=color_winter,\n",
    "#                            x_range=(np.floor(min_), np.ceil(max_)))\n",
    "\n",
    "fig = plot_monthly_joyplot_single(df_months_nn_long_low,\n",
    "                                  variable=\"mb_lstm\",\n",
    "                                  color_model=color_annual,\n",
    "                                  x_range=(np.floor(min_), np.ceil(max_)))\n",
    "\n",
    "# --- Save figure ---\n",
    "fig.savefig(\"figures/CH_LSTM_vs_NN_GLAMOS_monthly_joyplot_low_elv.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.PFI_all import permutation_feature_importance_mbm_parallel\n",
    "\n",
    "pfi_parallel = permutation_feature_importance_mbm_parallel(\n",
    "    cfg=cfg,\n",
    "    custom_params=custom_params,\n",
    "    model_filename=model_filename,\n",
    "    df_eval=df_test,  # your eval dataframe WITH TARGETS aligned to predictions\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    ds_train=ds_train,\n",
    "    train_idx=train_idx,\n",
    "    target_col=\"POINT_BALANCE\",  # <-- set your target column name\n",
    "    months_head_pad=months_head_pad,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    seed=cfg.seed,\n",
    "    n_repeats=5,\n",
    "    batch_size=256,\n",
    "    max_workers=None,  # auto: n_cpus-1 (cap 32)\n",
    ")\n",
    "plt.figure(figsize=(8, max(3, 0.35 * len(pfi_parallel))))\n",
    "plt.barh(pfi_parallel[\"feature\"],\n",
    "         pfi_parallel[\"mean_delta\"],\n",
    "         xerr=pfi_parallel[\"std_delta\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\n",
    "    f\"Permutation Feature Importance (Δ{pfi_parallel['metric_name'].iloc[0]}; baseline={pfi_parallel['baseline'].iloc[0]:.3f})\"\n",
    ")\n",
    "plt.xlabel(\n",
    "    f\"Increase in {pfi_parallel['metric_name'].iloc[0]} (higher = more important)\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.PFI_monthly import permutation_feature_importance_mbm_monthly_parallel\n",
    "\n",
    "month_map = {\n",
    "    \"aug_\": 0,\n",
    "    \"sep_\": 1,\n",
    "    \"oct\": 2,\n",
    "    \"nov\": 3,\n",
    "    \"dec\": 4,\n",
    "    \"jan\": 5,\n",
    "    \"feb\": 6,\n",
    "    \"mar\": 7,\n",
    "    \"apr\": 8,\n",
    "    \"may\": 9,\n",
    "    \"jun\": 10,\n",
    "    \"jul\": 11,\n",
    "    \"aug\": 12,\n",
    "    \"sep\": 13,\n",
    "    \"oct_\": 14\n",
    "}\n",
    "df_eval = df_test.copy()\n",
    "df_eval[\"MONTH_IDX\"] = df_eval[\"MONTHS\"].str.lower().map(month_map)\n",
    "\n",
    "pfi_monthly = permutation_feature_importance_mbm_monthly_parallel(\n",
    "    cfg,\n",
    "    custom_params,\n",
    "    model_filename,\n",
    "    df_eval,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    ds_train,\n",
    "    train_idx,\n",
    "    months_head_pad,\n",
    "    months_tail_pad,\n",
    "    seed=cfg.seed,\n",
    "    n_repeats=3,\n",
    "    batch_size=256,\n",
    "    denorm=True,\n",
    "    max_workers=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Month order for columns ---\n",
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\",\n",
    "    \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "\n",
    "# ✅ Correct column name spelling and mapping\n",
    "pfi_monthly[\"feature_long\"] = pfi_monthly[\"feature\"].apply(\n",
    "    lambda x: vois_climate_long_name.get(x, x)\n",
    ")\n",
    "\n",
    "# --- Prepare pivot tables using long names ---\n",
    "piv_winter = pfi_monthly.pivot(index=\"feature_long\", columns=\"month\", values=\"mean_delta_winter\")\n",
    "piv_annual = pfi_monthly.pivot(index=\"feature_long\", columns=\"month\", values=\"mean_delta_annual\")\n",
    "piv_global = pfi_monthly.pivot(index=\"feature_long\", columns=\"month\", values=\"mean_delta_global\")\n",
    "\n",
    "# --- Reorder months for each (keeping only existing ones) ---\n",
    "piv_winter = piv_winter[[m for m in month_order if m in piv_winter.columns]]\n",
    "piv_annual = piv_annual[[m for m in month_order if m in piv_annual.columns]]\n",
    "piv_global = piv_global[[m for m in month_order if m in piv_global.columns]]\n",
    "\n",
    "# --- Order features by combined importance (optional) ---\n",
    "feat_order = (\n",
    "    pfi_monthly.groupby(\"feature_long\")[\"mean_delta_global\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "piv_winter = piv_winter.loc[feat_order]\n",
    "piv_annual = piv_annual.loc[feat_order]\n",
    "piv_global = piv_global.loc[feat_order]\n",
    "\n",
    "# --- Create figure with three side-by-side heatmaps ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    piv_winter,\n",
    "    cmap=\"magma\",\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"label\": \"ΔRMSE (winter)\"},\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Monthly PFI – Winter RMSE Δ\")\n",
    "axes[0].set_xlabel(\"Month\")\n",
    "axes[0].set_ylabel(\"Feature\")\n",
    "\n",
    "sns.heatmap(\n",
    "    piv_annual,\n",
    "    cmap=\"magma\",\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"label\": \"ΔRMSE (annual)\"},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Monthly PFI – Annual RMSE Δ\")\n",
    "axes[1].set_xlabel(\"Month\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "\n",
    "sns.heatmap(\n",
    "    piv_global,\n",
    "    cmap=\"magma\",\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"label\": \"ΔRMSE (global)\"},\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Monthly PFI – Global RMSE Δ\")\n",
    "axes[2].set_xlabel(\"Month\")\n",
    "axes[2].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Month order ---\n",
    "month_order = [\n",
    "    \"aug_\", \"sep_\", \"oct\", \"nov\", \"dec\", \"jan\", \"feb\", \"mar\",\n",
    "    \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct_\"\n",
    "]\n",
    "\n",
    "# --- Map features to long names ---\n",
    "pfi_monthly[\"feature_long\"] = pfi_monthly[\"feature\"].apply(\n",
    "    lambda x: vois_climate_long_name.get(x, x)\n",
    ")\n",
    "\n",
    "# --- Prepare pivot table for global ΔRMSE ---\n",
    "piv_global = pfi_monthly.pivot(\n",
    "    index=\"feature_long\", columns=\"month\", values=\"mean_delta_global\"\n",
    ")\n",
    "\n",
    "# --- Reorder columns (months) ---\n",
    "piv_global = piv_global[[m for m in month_order if m in piv_global.columns]]\n",
    "\n",
    "# --- Order features by average global importance (optional, makes it clean) ---\n",
    "feat_order = (\n",
    "    pfi_monthly.groupby(\"feature_long\")[\"mean_delta_global\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "piv_global = piv_global.loc[feat_order]\n",
    "\n",
    "# --- Plot single heatmap ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    piv_global,\n",
    "    cmap=\"magma\",\n",
    "    linewidths=0.3,\n",
    "    cbar_kws={\"label\": \"ΔRMSE (global)\"}\n",
    ")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Monthly Permutation Feature Importance – Global RMSE Δ\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
