{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             '../../'))  # Add root of repo to import MBM\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import ast\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from scripts.helpers import *\n",
    "from scripts.glamos_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_CH import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.geodata import *\n",
    "from scripts.NN_networks import *\n",
    "from scripts.geodata_plots import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.SwitzerlandConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()  # in case no memory\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "    \"topo\",\n",
    "]\n",
    "\n",
    "glacier_outline_rgi = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"6\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L3-L5_files/2023.1/elev_bands/W5E5_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "export_oggm_grids(cfg, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_RGIs = cfg.dataPath + path_OGGM + 'xr_grids/'\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11.6\")\n",
    "\n",
    "# Open an example\n",
    "# rgi_gl = gdirs[0].rgi_id\n",
    "rgi_gl = 'RGI60-11.01238'\n",
    "\n",
    "ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                        ds['glacier_mask'].values)\n",
    "\n",
    "# Create glacier mask\n",
    "ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "# Assign other variables only if available\n",
    "if 'hugonnet_dhdt' in ds:\n",
    "    ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "if 'consensus_ice_thickness' in ds:\n",
    "    ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "if 'millan_v' in ds:\n",
    "    ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 8), sharey=True)\n",
    "\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=False)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=False)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=False)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect OGGM\")\n",
    "axs[1].set_title(\"Slope OGGM\")\n",
    "axs[2].set_title(\"DEM OGGM\")\n",
    "axs[3].set_title(\"Glacier mask OGGM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_glacier(path_RGIs, rgi_gl):\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(path_RGIs + rgi_gl + '.zarr')\n",
    "\n",
    "    # Check if 'glacier_mask' exists\n",
    "    if 'glacier_mask' not in ds:\n",
    "        raise ValueError(\n",
    "            f\"'glacier_mask' variable not found in dataset {rgi_gl}\")\n",
    "\n",
    "    # Create glacier mask\n",
    "    glacier_mask = np.where(ds['glacier_mask'].values == 0, np.nan,\n",
    "                            ds['glacier_mask'].values)\n",
    "\n",
    "    # Apply mask to core variables\n",
    "    ds = ds.assign(masked_slope=glacier_mask * ds['slope'])\n",
    "    ds = ds.assign(masked_elev=glacier_mask * ds['topo'])\n",
    "    ds = ds.assign(masked_aspect=glacier_mask * ds['aspect'])\n",
    "    ds = ds.assign(masked_dis=glacier_mask * ds['dis_from_border'])\n",
    "\n",
    "    # Apply mask to optional variables if present\n",
    "    if 'hugonnet_dhdt' in ds:\n",
    "        ds = ds.assign(masked_hug=glacier_mask * ds['hugonnet_dhdt'])\n",
    "    if 'consensus_ice_thickness' in ds:\n",
    "        ds = ds.assign(masked_cit=glacier_mask * ds['consensus_ice_thickness'])\n",
    "    if 'millan_v' in ds:\n",
    "        ds = ds.assign(masked_miv=glacier_mask * ds['millan_v'])\n",
    "\n",
    "    # Indices where glacier_mask == 1\n",
    "    glacier_indices = np.where(ds['glacier_mask'].values == 1)\n",
    "\n",
    "    return ds, glacier_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/',\n",
    "                             'xr_masked_grids/')\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_xr_grids)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Create masked glacier dataset\n",
    "            ds, glacier_indices = create_masked_glacier(path_RGIs, rgi_gl)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue  # Skip to next glacier\n",
    "\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "\n",
    "        # Coarsen to 50 m resolution if needed\n",
    "        if 20 < dx_m < 50:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=50)\n",
    "            dx_m, dy_m = get_res_from_projected(ds)\n",
    "        else:\n",
    "            ds = ds\n",
    "\n",
    "        # Change coordinates to Lat/Lon projection\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\")\n",
    "        ds_latlon = ds_latlon.rename({'x': 'lon', 'y': 'lat'})\n",
    "\n",
    "        # Save xarray dataset\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path)\n",
    "\n",
    "# open example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl_rhone = gdir_rhone.rgi_id\n",
    "ds = xr.open_dataset(path_xr_grids + rgi_gl_rhone + '.zarr')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.00878':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl_rhone = gdir_rhone.rgi_id\n",
    "ds = xr.open_dataset(path_xr_grids + rgi_gl_rhone + '.zarr')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "ds.masked_aspect.plot(ax=axs[0], cmap='twilight_shifted', add_colorbar=True)\n",
    "ds.masked_slope.plot(ax=axs[1], cmap='cividis', add_colorbar=True)\n",
    "ds.masked_elev.plot(ax=axs[2], cmap='terrain', add_colorbar=True)\n",
    "ds.glacier_mask.plot(ax=axs[3], cmap='binary', add_colorbar=False)\n",
    "\n",
    "axs[0].set_title(\"Aspect\")\n",
    "axs[1].set_title(\"Slope\")\n",
    "axs[2].set_title(\"DEM\")\n",
    "axs[3].set_title(\"Glacier mask\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "path_rgi_alps = os.path.join(cfg.dataPath,\n",
    "                             'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11/')\n",
    "\n",
    "if RUN:\n",
    "    years = range(2000, 2024)\n",
    "\n",
    "    #os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "    #emptyfolder(path_rgi_alps)\n",
    "\n",
    "    valid_rgis = [\n",
    "        f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "        if f.endswith('.zarr')\n",
    "    ]\n",
    "\n",
    "    processed_rgis = os.listdir(path_rgi_alps)\n",
    "    rest_rgis = list(set(valid_rgis) - set(processed_rgis))\n",
    "    print(f\"Number of glaciers to process: {len(rest_rgis)}\")\n",
    "\n",
    "    for gdir in tqdm(gdirs, desc=\"Processing glaciers\"):\n",
    "        # for gdir in [gdir_rhone]:  # For testing, only process one glacier\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        if rgi_gl not in valid_rgis:\n",
    "            print(f\"Skipping {rgi_gl}: not found in valid RGI glaciers\")\n",
    "            continue\n",
    "        if rgi_gl in processed_rgis:\n",
    "            continue\n",
    "        try:\n",
    "            file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                ds = xr.open_zarr(file_path, consolidated=True)\n",
    "            except Exception:\n",
    "                ds = xr.open_zarr(file_path)\n",
    "\n",
    "            # Create glacier grid\n",
    "            try:\n",
    "                df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed creating glacier grid for {rgi_gl}: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_grid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Add GLWD_ID\n",
    "            df_grid['GLWD_ID'] = [\n",
    "                mbm.data_processing.utils.get_hash(f\"{r}_{y}\") for r, y in zip(\n",
    "                    df_grid['RGIId'].astype(str), df_grid['YEAR'].astype(str))\n",
    "            ]\n",
    "            df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "            df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "            # Prepare output folder\n",
    "            folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            # Process each year\n",
    "            for year in years:\n",
    "                try:\n",
    "                    df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "                    if df_grid_y.empty:\n",
    "                        continue\n",
    "\n",
    "                    # Wrap Dataset creation & climate feature extraction\n",
    "                    try:\n",
    "                        dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                            cfg=cfg,\n",
    "                            data=df_grid_y,\n",
    "                            region_name='CH',\n",
    "                            data_path=os.path.join(cfg.dataPath,\n",
    "                                                   path_PMB_GLAMOS_csv))\n",
    "\n",
    "                        era5_climate_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw,\n",
    "                            'era5_monthly_averaged_data_Alps.nc')\n",
    "                        geopotential_data = os.path.join(\n",
    "                            cfg.dataPath, path_ERA5_raw,\n",
    "                            'era5_geopotential_pressure_Alps.nc')\n",
    "\n",
    "                        dataset_grid_yearly.get_climate_features(\n",
    "                            climate_data=era5_climate_data,\n",
    "                            geopotential_data=geopotential_data,\n",
    "                            change_units=True,\n",
    "                            smoothing_vois={\n",
    "                                'vois_climate': vois_climate,\n",
    "                                'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Failed adding climate features for {rgi_gl}: {e}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    vois_topographical_sub = [\n",
    "                        voi for voi in vois_topographical\n",
    "                        if voi in df_grid_y.columns\n",
    "                    ]\n",
    "\n",
    "                    dataset_grid_yearly.convert_to_monthly(\n",
    "                        meta_data_columns=cfg.metaData,\n",
    "                        vois_climate=vois_climate,\n",
    "                        vois_topographical=vois_topographical_sub)\n",
    "\n",
    "                    save_path = os.path.join(folder_path,\n",
    "                                             f\"{rgi_gl}_grid_{year}.parquet\")\n",
    "                    dataset_grid_yearly.data.to_parquet(save_path,\n",
    "                                                        engine=\"pyarrow\",\n",
    "                                                        compression=\"snappy\")\n",
    "                    #print(f\"Saved: {save_path}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed processing {rgi_gl} for year {year}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with glacier {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "# Look at one example\n",
    "# load the dataset\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2008\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "# Look at one example\n",
    "# load the dataset\n",
    "year = 2008\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of all glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_ids = os.listdir(path_rgi_alps)\n",
    "pos_gl = []\n",
    "for rgi_gl in tqdm(rgi_ids):\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "    pos_gl.append((df.POINT_LAT.mean(), df.POINT_LON.mean()))\n",
    "df_pos_all = pd.DataFrame(pos_gl, columns=['lat', 'lon'])\n",
    "df_pos_all['rgi_id'] = rgi_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of glaciers in RGI region 11.6:', len(df_pos_all))\n",
    "\n",
    "# ---- 2. Create figure and base map ----\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "latN, latS = 48, 44\n",
    "lonW, lonE = 4, 14\n",
    "projPC = ccrs.PlateCarree()\n",
    "ax2 = plt.axes(projection=projPC)\n",
    "ax2.set_extent([lonW, lonE, latS, latN], crs=ccrs.Geodetic())\n",
    "\n",
    "ax2.add_feature(cfeature.COASTLINE)\n",
    "ax2.add_feature(cfeature.LAKES)\n",
    "ax2.add_feature(cfeature.RIVERS)\n",
    "ax2.add_feature(cfeature.BORDERS, linestyle='-', linewidth=1)\n",
    "\n",
    "g = sns.scatterplot(\n",
    "    data=df_pos_all,\n",
    "    x='lon',\n",
    "    y='lat',\n",
    "    alpha=0.6,\n",
    "    transform=projPC,\n",
    "    ax=ax2,\n",
    "    zorder=10,\n",
    "    legend=True  # custom legend added below\n",
    ")\n",
    "\n",
    "glacier_outline_rgi.plot(ax=ax2, transform=projPC, color='black')\n",
    "\n",
    "# ---- 4. Gridlines ----\n",
    "gl = ax2.gridlines(draw_labels=True,\n",
    "                   linewidth=1,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   linestyle='--')\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.xlabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.ylabel_style = {'size': 16, 'color': 'black'}\n",
    "gl.top_labels = gl.right_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glamos = getStakesData(cfg)\n",
    "\n",
    "months_head_pad, months_tail_pad = mbm.data_processing.utils._compute_head_tail_pads_from_df(\n",
    "    data_glamos)\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLAMOS_csv,\n",
    "    'era5_climate_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data.nc',\n",
    "    'geopotential_data':\n",
    "    cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure.nc',\n",
    "    'radiation_save_path': cfg.dataPath + path_pcsr + 'zarr/'\n",
    "}\n",
    "RUN = False\n",
    "data_monthly = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    data_glamos=data_glamos,\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='CH_wgms_dataset_monthly_LSTM.csv')\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                          data=data_monthly,\n",
    "                                          random_seed=cfg.seed,\n",
    "                                          meta_data_columns=cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(data_monthly.GLACIER.unique())\n",
    "missing_glaciers = [g for g in TEST_GLACIERS if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers correctly\n",
    "train_glaciers = [i for i in existing_glaciers if i not in TEST_GLACIERS]\n",
    "\n",
    "data_test = data_monthly[data_monthly.GLACIER.isin(TEST_GLACIERS)]\n",
    "print('Size of monthly test data:', len(data_test))\n",
    "\n",
    "data_train = data_monthly[data_monthly.GLACIER.isin(train_glaciers)]\n",
    "print('Size of monthly train data:', len(data_train))\n",
    "\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=TEST_GLACIERS,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "test_perc = (len(test_set['df_X']) / len(train_set['df_X'])) * 100\n",
    "print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "print('Size of test set:', len(test_set['df_X']))\n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                       train_set['splits_vals']))\n",
    "print('Size of train set:', len(train_set['df_X']))\n",
    "\n",
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "data_test = test_set['df_X']\n",
    "data_test['y'] = test_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi',\n",
    "    'slope_sgi',\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_simple = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_simple, val_idx_simple = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_simple), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 2,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_simple_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_simple.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_simple) ---\n",
    "ds_train_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_simple)\n",
    "\n",
    "ds_test_simple_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_simple)\n",
    "\n",
    "train_dl, val_dl = ds_train_simple_copy.make_loaders(\n",
    "    train_idx=train_idx_simple,\n",
    "    val_idx=val_idx_simple,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_simple and transforms it) ---\n",
    "test_dl_simple = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_simple_copy, ds_train_simple_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_simple, ds_test_simple_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full model (with OGGM variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "STATIC_COLS = [\n",
    "    'aspect_sgi', 'slope_sgi', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
    "    'millan_v'\n",
    "]\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "df_train = data_train.copy()\n",
    "df_train['PERIOD'] = df_train['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "df_test = data_test.copy()\n",
    "df_test['PERIOD'] = df_test['PERIOD'].str.strip().str.lower()\n",
    "\n",
    "# --- build train dataset from dataframe ---\n",
    "ds_train_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_train,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_test_full = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "    df_test,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    months_tail_pad=months_tail_pad,\n",
    "    months_head_pad=months_head_pad,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_full, val_idx_full = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_full), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "custom_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 5,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 0,\n",
    "    'static_hidden': None,\n",
    "    'static_dropout': None,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0,\n",
    "    'loss_name': 'neutral',\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "custom_params['two_heads'] = True\n",
    "custom_params['head_dropout'] = 0.0\n",
    "\n",
    "params_full_model = custom_params.copy()\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_model_{current_date}_CA_full.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train_full) ---\n",
    "ds_train_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_full)\n",
    "\n",
    "ds_test_full_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_test_full)\n",
    "\n",
    "train_dl, val_dl = ds_train_full_copy.make_loaders(\n",
    "    train_idx=train_idx_full,\n",
    "    val_idx=val_idx_full,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- test loader (copies TRAIN scalers into ds_test_full and transforms it) ---\n",
    "test_dl_full = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_full_copy, ds_train_full_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model = mbm.models.LSTM_MB.build_model_from_params(cfg, custom_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(custom_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=150,\n",
    "        lr=custom_params['lr'],\n",
    "        weight_decay=custom_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "# Evaluate on test\n",
    "test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "    device, test_dl_full, ds_test_full_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "fig = plot_predictions_summary(grouped_ids=test_df_preds,\n",
    "                               scores_annual=scores_annual,\n",
    "                               scores_winter=scores_winter,\n",
    "                               ax_xlim=(-8, 6),\n",
    "                               ax_ylim=(-8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolate in space to RGI glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id_list = os.listdir(path_rgi_alps)\n",
    "\n",
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "\n",
    "# Define paths\n",
    "path_save_glw = os.path.join(\n",
    "    cfg.dataPath, 'GLAMOS/distributed_MB_grids/MBM/central_europe/')\n",
    "path_xr_grids = os.path.join(cfg.dataPath, 'GLAMOS/topo/RGI_v6_11/',\n",
    "                             'xr_masked_grids/')\n",
    "\n",
    "# Required by the dataset builder regardless of your feature list\n",
    "REQUIRED = ['GLACIER', 'YEAR', 'ID', 'PERIOD', 'MONTHS']\n",
    "\n",
    "# Paths to your trained weights\n",
    "MODEL_SIMPLE_PATH = \"models/lstm_model_2025-09-26_CA_simple.pt\"\n",
    "MODEL_FULL_PATH = \"models/lstm_model_2025-09-26_CA_full.pt\"\n",
    "\n",
    "# simple cache keyed by kind -> model instance\n",
    "_MODEL_CACHE = {}\n",
    "\n",
    "def get_model(kind: str, cfg, device, params_simple_model: dict,\n",
    "              params_full_model: dict):\n",
    "    \"\"\"\n",
    "    Build and load the LSTM model for the requested kind ('simple'|'full'),\n",
    "    using the corresponding hyperparameter dict and checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kind : {'simple','full'}\n",
    "    cfg : your MBM config\n",
    "    device : torch.device\n",
    "    params_simple_model : dict\n",
    "    params_full_model   : dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module (eval mode)\n",
    "    \"\"\"\n",
    "    kind = kind.lower()\n",
    "    if kind not in (\"simple\", \"full\"):\n",
    "        raise ValueError(f\"Unknown model kind: {kind}\")\n",
    "\n",
    "    if kind in _MODEL_CACHE:\n",
    "        return _MODEL_CACHE[kind]\n",
    "\n",
    "    # pick params + checkpoint\n",
    "    if kind == \"simple\":\n",
    "        params = params_simple_model\n",
    "        ckpt_path = MODEL_SIMPLE_PATH\n",
    "    else:\n",
    "        params = params_full_model\n",
    "        ckpt_path = MODEL_FULL_PATH\n",
    "\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    # build with matching params\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg, params, device, verbose = False)\n",
    "\n",
    "    # load weights\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    _MODEL_CACHE[kind] = model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Safe rename helper\n",
    "def safe_rename(df, mapping):\n",
    "    present = {k: v for k, v in mapping.items() if k in df.columns}\n",
    "    return df.rename(columns=present) if present else df\n",
    "\n",
    "\n",
    "# Robust year regex: RGI60-11.01238_grid_2003.parquet\n",
    "YEAR_RE = re.compile(r\"_grid_(\\d{4})\\.parquet$\")\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    emptyfolder(path_save_glw)\n",
    "\n",
    "    # CSV header\n",
    "    output_file = os.path.join(\"logs/glacier_mean_MB.csv\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"Index,RGIId,Year,Mean_MB\\n\")\n",
    "\n",
    "    index_counter = 0\n",
    "\n",
    "    for rgi_gl in tqdm(rgi_id_list):\n",
    "        seed_all(cfg.seed)\n",
    "        glacier_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "        if not os.path.exists(glacier_path):\n",
    "            print(f\"Folder not found for {rgi_gl}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Collect years robustly\n",
    "        years = []\n",
    "        for fname in os.listdir(glacier_path):\n",
    "            if not (fname.endswith(\".parquet\") and rgi_gl in fname):\n",
    "                continue\n",
    "            m = YEAR_RE.search(fname)\n",
    "            if m:\n",
    "                years.append(int(m.group(1)))\n",
    "        years = sorted(set(years))\n",
    "        if not years:\n",
    "            print(f\"No parquet years for {rgi_gl}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Optionally clear model cache per glacier\n",
    "        _MODEL_CACHE.clear()\n",
    "\n",
    "        for year in years:\n",
    "            file_name = f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "            file_path = os.path.join(glacier_path, file_name)\n",
    "\n",
    "            try:\n",
    "                # Load parquet\n",
    "                df_grid_monthly = pd.read_parquet(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"[{rgi_gl} {year}] read error: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_grid_monthly.drop_duplicates(inplace=True)\n",
    "\n",
    "            # safe rename if present\n",
    "            df_grid_monthly = safe_rename(df_grid_monthly, {\n",
    "                'aspect': 'aspect_sgi',\n",
    "                'slope': 'slope_sgi'\n",
    "            })\n",
    "\n",
    "            oggm_columns = [\n",
    "                'hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v'\n",
    "            ]\n",
    "            has_oggm = all(c in df_grid_monthly.columns for c in oggm_columns)\n",
    "\n",
    "            if has_oggm:\n",
    "                kind = \"full\"\n",
    "                STATIC_COLS = ['aspect_sgi', 'slope_sgi'] + oggm_columns\n",
    "                ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "                    ds_train_full)\n",
    "                ds_train_copy.fit_scalers(train_idx_full)\n",
    "            else:\n",
    "                kind = \"simple\"\n",
    "                STATIC_COLS = ['aspect_sgi', 'slope_sgi']\n",
    "                ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "                    ds_train_simple)\n",
    "                ds_train_copy.fit_scalers(train_idx_simple)\n",
    "\n",
    "            # Build column list in correct schema\n",
    "            feature_columns = MONTHLY_COLS + STATIC_COLS\n",
    "            all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "            # Keep required + feature columns\n",
    "            keep = [\n",
    "                c for c in (set(all_columns) | set(REQUIRED))\n",
    "                if c in df_grid_monthly.columns\n",
    "            ]\n",
    "            df_grid_monthly = df_grid_monthly[keep]\n",
    "\n",
    "            # Ensure required columns exist\n",
    "            missing_req = [\n",
    "                c for c in REQUIRED if c not in df_grid_monthly.columns\n",
    "            ]\n",
    "            if missing_req:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] missing required cols: {missing_req}, skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Minimal NaN clean-up\n",
    "            df_grid_monthly_a = df_grid_monthly.dropna(subset=['ID', 'MONTHS'])\n",
    "            if df_grid_monthly_a.empty:\n",
    "                print(\n",
    "                    f\"[{rgi_gl} {year}] empty after NaN cleanup, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Build inference dataset\n",
    "            ds_gl_a = mbm.data_processing.MBSequenceDataset.from_dataframe(\n",
    "                df_grid_monthly_a,\n",
    "                MONTHLY_COLS,\n",
    "                STATIC_COLS,\n",
    "                months_tail_pad=months_tail_pad,\n",
    "                months_head_pad=months_head_pad,\n",
    "                expect_target=False,\n",
    "                show_progress=False,\n",
    "            )\n",
    "            test_gl_dl_a = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "                ds_gl_a, ds_train_copy, seed=cfg.seed, batch_size=128)\n",
    "\n",
    "            # Load model (cached) and predict\n",
    "            try:\n",
    "                model = get_model(kind, cfg, device, params_simple_model,\n",
    "                                  params_full_model)\n",
    "            except Exception as e:\n",
    "                print(f\"[{rgi_gl} {year}] model load error: {e}\")\n",
    "                continue\n",
    "\n",
    "            df_preds_a = model.predict_with_keys(device, test_gl_dl_a, ds_gl_a)\n",
    "\n",
    "            # Join preds for output\n",
    "            data_a = df_preds_a[['ID', 'pred']].set_index('ID')\n",
    "            meta_cols = [\n",
    "                c for c in ['YEAR', 'POINT_LAT', 'POINT_LON', 'GLWD_ID']\n",
    "                if c in df_grid_monthly_a.columns\n",
    "            ]\n",
    "            grouped_ids_a = (\n",
    "                df_grid_monthly_a.groupby('ID')[meta_cols].first().merge(\n",
    "                    data_a, left_index=True, right_index=True, how='left'))\n",
    "            months_per_id_a = df_grid_monthly_a.groupby(\n",
    "                'ID')['MONTHS'].unique()\n",
    "            grouped_ids_a = grouped_ids_a.merge(months_per_id_a,\n",
    "                                                left_index=True,\n",
    "                                                right_index=True)\n",
    "\n",
    "            grouped_ids_a.reset_index(inplace=True)\n",
    "            grouped_ids_a.sort_values(by='ID', inplace=True)\n",
    "\n",
    "            pred_y_annual = grouped_ids_a.copy()\n",
    "            pred_y_annual['PERIOD'] = 'annual'\n",
    "            pred_y_annual = pred_y_annual.drop(columns=['YEAR'],\n",
    "                                               errors='ignore')\n",
    "\n",
    "            mean_MB = pred_y_annual['pred'].mean()\n",
    "\n",
    "            with open(output_file, 'a') as f:\n",
    "                f.write(f\"{index_counter},{rgi_gl},{year},{mean_MB:.4f}\\n\")\n",
    "\n",
    "            index_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(\"logs/glacier_mean_MB.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df.RGIId == 'RGI60-11.01238']\n",
    "# ensure Year is integer (just in case)\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "\n",
    "# add the new column\n",
    "df[\"LSTM_full\"] = np.nan\n",
    "\n",
    "# set Year as index\n",
    "df.set_index(\"Year\", inplace=True)\n",
    "\n",
    "# your values\n",
    "vals = [\n",
    "    -0.764493, -0.85534433, -0.61580243, -0.6876, -1.81396141, -1.17730185,\n",
    "    -0.28465775, -0.41800559, -0.96153416, -0.74692587, -1.41592501,\n",
    "    -0.97360032, -0.97257272, -1.23755494, -0.51568465, -3.05461539,\n",
    "    -2.76094933\n",
    "]\n",
    "\n",
    "# build matching years dynamically (starts at 2007, matches length of vals)\n",
    "years = list(range(2007, 2007 + len(vals)))  # 2007..2023 inclusive\n",
    "\n",
    "# assign\n",
    "df.loc[years, \"LSTM_full\"] = vals\n",
    "\n",
    "# optional: plot\n",
    "ax = df.plot(y=[\"Mean_MB\", \"LSTM_full\"], marker=\"o\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Mass balance (m w.e.)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean predicted MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open output file\n",
    "output_df = os.path.join(path_save_glw, \"glacier_mean_MB.csv\")\n",
    "output_df = pd.read_csv(output_df)\n",
    "\n",
    "output_df.groupby('Year').agg({'Mean_MB': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
