{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & utilities ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import traceback\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Add repo root for MBM imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../\"))\n",
    "\n",
    "# --- Data science stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Machine learning / DL ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# --- Cartography / plotting ---\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Custom MBM modules ---\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Warnings & autoreload (notebook) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from regions.Switzerland.scripts.geo_data import *\n",
    "from regions.Switzerland.scripts.oggm import initialize_oggm_glacier_directories, export_oggm_grids\n",
    "from regions.Switzerland.scripts.config_CH import * \n",
    "from regions.Switzerland.scripts.utils import * \n",
    "\n",
    "# --- Configuration ---\n",
    "cfg = mbm.SwitzerlandConfig()\n",
    "\n",
    "# Plot styles:\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    mbm.utils.free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "df_missing = export_oggm_grids(cfg, gdirs, rgi_region=\"11\")\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(cfg.dataPath + path_rgi_outlines)\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# gdf_proj.set_index('rgi_id', inplace=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# total glacier area\n",
    "total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# explode the list of missing vars into rows (one var per row)\n",
    "df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# 1) COUNT: number of glaciers missing each variable\n",
    "counts_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "        ascending=False))\n",
    "\n",
    "# 2) TOTAL % AREA with ANY missing var\n",
    "total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "print(f\"Total glacier area with ANY missing variable: \"\n",
    "      f\"{total_missing_area_km2:,.2f} km² \"\n",
    "      f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# Optional: also show % area per variable (kept from your earlier logic)\n",
    "area_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "        ascending=False))\n",
    "perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "print(\"\\n% of total glacier area missing per variable:\")\n",
    "for var, pct in perc_missing_per_var.items():\n",
    "    print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# ---- barplot: number of glaciers missing each variable ----\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "plt.xlabel(\"Missing variable\")\n",
    "plt.ylabel(\"Number of glaciers\")\n",
    "plt.title(\"Count of glaciers missing each variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGI Ids:\n",
    "# Read glacier ids:\n",
    "rgi_df = pd.read_csv(cfg.dataPath + path_glacier_ids, sep=',')\n",
    "rgi_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "rgi_df.sort_values(by='short_name', inplace=True)\n",
    "rgi_df.set_index('short_name', inplace=True)\n",
    "rgi_df.loc['rhone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export geotifs of DEMs (needed for svf in separate notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_RGIs = os.path.join(cfg.dataPath, path_OGGM, \"xr_grids/\")\n",
    "path_geotiff = os.path.join(cfg.dataPath, \"RGI_v6/RGI_11_CentralEurope\",\n",
    "                            \"geotiff/\")\n",
    "\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 11\")\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_geotiff)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Export DEMs to GeoTIFF\n",
    "            out_tif = export_glacier_dems_to_geotiff(path_RGIs, rgi_gl,\n",
    "                                                     path_geotiff)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked xarray grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def process_one_glacier(\n",
    "    rgi_gl: str,\n",
    "    path_RGIs: str,\n",
    "    path_xr_svf: str,\n",
    "    path_xr_grids: str,\n",
    "    target_res_m: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Worker: load OGGM grid, mask, optional coarsen, reproject to lat/lon,\n",
    "    merge SVF, write per-glacier zarr. Returns a small status tuple.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) Masked OGGM grid in projected coords\n",
    "        ds, _ = create_masked_glacier_grid(path_RGIs, rgi_gl)\n",
    "\n",
    "        # 2) Optional coarsen in projected space\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "        if 20 < dx_m < target_res_m:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=target_res_m)\n",
    "\n",
    "        # 3) Reproject to WGS84 lat/lon\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\").rename({\n",
    "            \"x\": \"lon\",\n",
    "            \"y\": \"lat\"\n",
    "        })\n",
    "\n",
    "        # 4) Load SVF + merge (if exists)\n",
    "        svf_path = os.path.join(path_xr_svf, f\"{rgi_gl}_svf_latlon.nc\")\n",
    "        if os.path.exists(svf_path):\n",
    "            ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "            # Normalize coord names\n",
    "            if \"x\" in ds_svf.dims or \"y\" in ds_svf.dims:\n",
    "                ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "            if \"longitude\" in ds_svf.dims or \"latitude\" in ds_svf.dims:\n",
    "                ds_svf = ds_svf.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "\n",
    "            # Sort ascending for interp stability\n",
    "            if ds_latlon.lon[0] > ds_latlon.lon[-1]:\n",
    "                ds_latlon = ds_latlon.sortby(\"lon\")\n",
    "            if ds_latlon.lat[0] > ds_latlon.lat[-1]:\n",
    "                ds_latlon = ds_latlon.sortby(\"lat\")\n",
    "            if ds_svf.lon[0] > ds_svf.lon[-1]:\n",
    "                ds_svf = ds_svf.sortby(\"lon\")\n",
    "            if ds_svf.lat[0] > ds_svf.lat[-1]:\n",
    "                ds_svf = ds_svf.sortby(\"lat\")\n",
    "\n",
    "            svf_vars = [\n",
    "                v for v in (\"svf\", \"asvf\", \"opns\") if v in ds_svf.data_vars\n",
    "            ]\n",
    "\n",
    "            if svf_vars:\n",
    "                # Merge directly if grids match; else interpolate\n",
    "                if (np.array_equal(ds_latlon.lon.values, ds_svf.lon.values)\n",
    "                        and np.array_equal(ds_latlon.lat.values,\n",
    "                                           ds_svf.lat.values)):\n",
    "                    ds_latlon = xr.merge([ds_latlon, ds_svf[svf_vars]])\n",
    "                else:\n",
    "                    svf_on_grid = ds_svf[svf_vars].interp(lon=ds_latlon.lon,\n",
    "                                                          lat=ds_latlon.lat,\n",
    "                                                          method=\"linear\")\n",
    "                    for v in svf_vars:\n",
    "                        svf_on_grid[v] = svf_on_grid[v].astype(\"float32\")\n",
    "                    ds_latlon = ds_latlon.assign(\n",
    "                        **{v: svf_on_grid[v]\n",
    "                           for v in svf_vars})\n",
    "\n",
    "                # Masked SVF versions using glacier_mask (if present)\n",
    "                if \"glacier_mask\" in ds_latlon:\n",
    "                    gmask = xr.where(ds_latlon[\"glacier_mask\"] == 1, 1.0,\n",
    "                                     np.nan)\n",
    "                    for v in svf_vars:\n",
    "                        ds_latlon[f\"masked_{v}\"] = gmask * ds_latlon[v]\n",
    "\n",
    "        # 5) Save final lat/lon grid\n",
    "        os.makedirs(path_xr_grids, exist_ok=True)\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path, mode=\"w\")\n",
    "\n",
    "        return (rgi_gl, \"ok\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (rgi_gl, \"error\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def run_parallel_processing(\n",
    "    gdirs,\n",
    "    path_RGIs,\n",
    "    path_xr_svf,\n",
    "    path_xr_grids,\n",
    "    n_workers=None,\n",
    "    clear_out=False,\n",
    "    target_res_m=50,\n",
    "):\n",
    "    rgi_ids = [g.rgi_id for g in gdirs]\n",
    "\n",
    "    if clear_out:\n",
    "        emptyfolder(path_xr_grids)\n",
    "    else:\n",
    "        os.makedirs(path_xr_grids, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "        futures = {\n",
    "            ex.submit(\n",
    "                process_one_glacier,\n",
    "                rgi_id,\n",
    "                path_RGIs,\n",
    "                path_xr_svf,\n",
    "                path_xr_grids,\n",
    "                target_res_m,\n",
    "            ):\n",
    "            rgi_id\n",
    "            for rgi_id in rgi_ids\n",
    "        }\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(fut.result())\n",
    "\n",
    "    # quick summary\n",
    "    n_ok = sum(r[1] == \"ok\" for r in results)\n",
    "    n_err = sum(r[1] == \"error\" for r in results)\n",
    "    print(f\"Done. ok={n_ok}, error={n_err}\")\n",
    "\n",
    "    if n_err:\n",
    "        for rgi_id, status, msg in results:\n",
    "            if status == \"error\":\n",
    "                print(f\"[{rgi_id}] {msg}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, \"RGI_v6/RGI_11_CentralEurope\",\n",
    "                             \"xr_masked_grids/\")\n",
    "path_xr_svf = os.path.join(cfg.dataPath, \"RGI_v6/RGI_11_CentralEurope\",\n",
    "                           \"svf_nc_latlon/\")\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    results = run_parallel_processing(\n",
    "        gdirs=gdirs,\n",
    "        path_RGIs=path_RGIs,\n",
    "        path_xr_svf=path_xr_svf,\n",
    "        path_xr_grids=path_xr_grids,\n",
    "        n_workers=6,  # start modest (4–8 is usually good)\n",
    "        clear_out=True,  # or False if you want to keep existing zarrs\n",
    "        target_res_m=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id =  \"RGI60-11.01238\"\n",
    "# --- Paths ---\n",
    "dem_path = os.path.join(path_geotiff, f\"{rgi_id}.tif\")\n",
    "zarr_path = os.path.join(path_xr_grids, f\"{rgi_id}.zarr\")\n",
    "svf_path = os.path.join(path_xr_svf, f\"{rgi_id}_svf_latlon.nc\")\n",
    "\n",
    "# --- Load data ---\n",
    "dem = rioxarray.open_rasterio(dem_path).squeeze()\n",
    "ds = xr.open_zarr(zarr_path)\n",
    "ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "# Handle coord naming for SVF\n",
    "if \"lon\" not in ds_svf.coords:\n",
    "    ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "\n",
    "# --- Figure layout ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# 1️⃣ DEM (projected)\n",
    "dem.plot(ax=axes[0], cmap=\"terrain\")\n",
    "axes[0].set_title(\"DEM (projected meters)\")\n",
    "axes[0].set_xlabel(\"Easting [m]\")\n",
    "axes[0].set_ylabel(\"Northing [m]\")\n",
    "\n",
    "# 2️⃣ Masked aspect (projected OGGM grid)\n",
    "ds[\"masked_aspect\"].plot(ax=axes[1])\n",
    "axes[1].set_title(\"Masked Aspect (°)\")\n",
    "axes[1].set_xlabel(\"Longitude (°)\")\n",
    "axes[1].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "# 3️⃣ SVF (lat/lon)\n",
    "ds[\"svf\"].plot(ax=axes[2])\n",
    "\n",
    "axes[2].set_title(\"Sky View Factor (lat/lon)\")\n",
    "axes[2].set_xlabel(\"Longitude (°)\")\n",
    "axes[2].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "plt.suptitle(f\"{rgi_id}\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                             \"xr_masked_grids/\")\n",
    "path_xr_svf = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                           \"svf_nc_latlon/\")\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    results = run_parallel_processing(\n",
    "        gdirs=gdirs,\n",
    "        path_RGIs=path_RGIs,\n",
    "        path_xr_svf=path_xr_svf,\n",
    "        path_xr_grids=path_xr_grids,\n",
    "        n_workers=6,  # start modest (4–8 is usually good)\n",
    "        clear_out=True,  # or False if you want to keep existing zarrs\n",
    "        target_res_m=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.00001':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2004\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_rhone = gdir\n",
    "\n",
    "year = 2000\n",
    "rgi_gl = gdir_rhone.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
