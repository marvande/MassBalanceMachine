{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from regions.French_Alps.scripts.config_FR import *\n",
    "from regions.French_Alps.scripts.dataset import get_stakes_data_FR\n",
    "from regions.French_Alps.scripts.utils import *\n",
    "\n",
    "from regions.Switzerland.scripts.dataset import process_or_load_data, get_CV_splits\n",
    "from regions.Switzerland.scripts.plotting import plot_predictions_summary, plot_individual_glacier_pred, plot_history_lstm, get_cmap_hex,plot_tsne_overlap, plot_feature_kde_overlap, pred_vs_truth_density\n",
    "from regions.Switzerland.scripts.dataset import get_stakes_data, build_combined_LSTM_dataset, inspect_LSTM_sample, prepare_monthly_dfs_with_padding\n",
    "from regions.Switzerland.scripts.models import compute_seasonal_scores, get_best_params_for_lstm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.FranceConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Regional Transfer Learning (Switzerland → France)\n",
    "\n",
    "This approach uses the Swiss dataset to try and model France glaciers.\n",
    "\n",
    "## Create Combined Swiss and France Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_FR = get_stakes_data_FR(cfg)\n",
    "data_CH = get_stakes_data(cfg)\n",
    "\n",
    "# Adjust dfs to match\n",
    "data_CH = data_CH.drop(\n",
    "    columns=['aspect_sgi', 'slope_sgi', 'topo_sgi', 'asvf', 'opns'],\n",
    "    errors='ignore')\n",
    "data_CH['GLACIER_ZONE'] = ''\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "\n",
    "print('Number FR glaciers:', data_FR['GLACIER'].nunique())\n",
    "print('FR glaciers:', data_FR['GLACIER'].unique())\n",
    "print('Number CH glaciers:', data_CH['GLACIER'].nunique())\n",
    "print('CH glaciers:', data_CH['GLACIER'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PERIOD column just in case\n",
    "data_FR[\"PERIOD\"] = data_FR[\"PERIOD\"].str.strip().str.lower()\n",
    "data_CH[\"PERIOD\"] = data_CH[\"PERIOD\"].str.strip().str.lower()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5), sharey=True)\n",
    "\n",
    "for ax, period in zip(axes, [\"annual\", \"winter\"]):\n",
    "    mb_nor = data_FR.loc[data_FR.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "    mb_ch = data_CH.loc[data_CH.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "\n",
    "    # Common bins for fair comparison\n",
    "    all_vals = np.concatenate([mb_nor, mb_ch])\n",
    "    bins = np.linspace(all_vals.min(), all_vals.max(), 21)\n",
    "\n",
    "    ax.hist(mb_nor, bins=bins, alpha=0.6, label=\"France\")\n",
    "    ax.hist(mb_ch, bins=bins, alpha=0.6, label=\"Switzerland\")\n",
    "\n",
    "    ax.axvline(mb_nor.mean(), linestyle=\"--\")\n",
    "    ax.axvline(mb_ch.mean(), linestyle=\"--\")\n",
    "\n",
    "    ax.set_title(f\"{period.capitalize()} Mass Balance\")\n",
    "    ax.set_xlabel(\"Mass balance [m w.e.]\")\n",
    "    ax.legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Number of measurements\")\n",
    "\n",
    "plt.suptitle(\"Seasonal Point Mass Balance Distribution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning FR datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'csv_path':\n",
    "    os.path.join(cfg.dataPath, path_PMB_GLACIOCLIM_csv),\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_NOR_Alps.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_NOR_Alps.nc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5% FINE-TUNING SPLIT (FR)\n",
    "# ---------------------------\n",
    "\n",
    "finetune_glaciers_5pct = ['Sarennes', 'Talefre', 'Grands Montets', 'Leschaux']\n",
    "\n",
    "# All remaining glaciers = holdout set\n",
    "all_nor_glaciers = list(data_FR['GLACIER'].unique())\n",
    "holdout_glaciers_5pct = [\n",
    "    g for g in all_nor_glaciers if g not in finetune_glaciers_5pct\n",
    "]\n",
    "\n",
    "data_FR_ft_5pct = data_FR[data_FR['GLACIER'].isin(\n",
    "    finetune_glaciers_5pct)].copy()\n",
    "data_FR_holdout_5pct = data_FR[~data_FR['GLACIER'].isin(finetune_glaciers_5pct\n",
    "                                                        )].copy()\n",
    "\n",
    "print(\n",
    "    f\"5% fine-tuning glaciers ({len(finetune_glaciers_5pct)}): {finetune_glaciers_5pct}\"\n",
    ")\n",
    "print(\n",
    "    f\"Hold-out glaciers ({len(holdout_glaciers_5pct)}): {holdout_glaciers_5pct}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_FR_5pct = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_FR,\n",
    "    region_name=\"FR\",\n",
    "    region_id=11,\n",
    "    paths=paths,\n",
    "    test_glaciers=holdout_glaciers_5pct,  # holdout = test set\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=True,\n",
    "    output_file_monthly='FR_5pct_ft_dataset_monthly.csv',\n",
    "    output_file_monthly_aug='FR_5pct_ft_dataset_monthly_Aug.csv')\n",
    "\n",
    "df_ft_FR_5pct = res_FR_5pct[\"df_train\"]\n",
    "df_holdout_FR_5pct = res_FR_5pct[\"df_test\"]\n",
    "df_ft_FR_5pct_Aug = res_FR_5pct[\"df_train_aug\"]\n",
    "df_holdout_FR_5pct_Aug = res_FR_5pct[\"df_test_aug\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "ds_ft_FR_5pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_ft_FR_5pct,\n",
    "    df_full=df_ft_FR_5pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_FR_5pct['months_head_pad'],\n",
    "    months_tail_pad=res_FR_5pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_holdout_FR_5pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_holdout_FR_5pct,\n",
    "    df_full=df_holdout_FR_5pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_FR_5pct['months_head_pad'],\n",
    "    months_tail_pad=res_FR_5pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "assert set(df_ft_FR_5pct.GLACIER.unique()) == set(finetune_glaciers_5pct)\n",
    "assert set(df_holdout_FR_5pct.GLACIER.unique()).isdisjoint(\n",
    "    set(finetune_glaciers_5pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In sample CH dataset (used for the pretrained model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_CH = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_CH,\n",
    "    region_name=\"CH\",\n",
    "    region_id=11,\n",
    "    paths=paths,\n",
    "    test_glaciers=[],\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    "    add_pcsr=False,\n",
    "    output_file_monthly='CH_wgms_dataset_monthly_LSTM_IS.csv',\n",
    "    output_file_monthly_aug='CH_wgms_dataset_monthly_LSTM_Aug_IS.csv')\n",
    "\n",
    "df_train = res_CH[\"df_train\"]\n",
    "df_train_Aug = res_CH[\"df_train_aug\"]\n",
    "\n",
    "# Check that train set contains all glaciers\n",
    "existing_glaciers = set(df_train.GLACIER.unique())\n",
    "print('Number of glaciers in train data:', len(existing_glaciers))\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "ds_train_CH = build_combined_LSTM_dataset(\n",
    "    df_loss=df_train,  # hydrological-year POINT_BALANCE\n",
    "    df_full=df_train_Aug,  # August-anchored monthly sequences\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_CH['months_head_pad'],\n",
    "    months_tail_pad=res_CH['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_CH, val_idx_CH = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_CH), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CH model (w/o pcsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"Fm\": 8,\n",
    "    \"Fs\": 3,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"static_layers\": 1,\n",
    "    \"static_hidden\": 128,\n",
    "    \"static_dropout\": 0.3,\n",
    "    \"lr\": 0.0005,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"two_heads\": False,\n",
    "    \"head_dropout\": 0.0,\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_CH_model_{current_date}_IS_norm_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl_CH, val_dl_CH = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model_CH.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl_CH,\n",
    "        val_dl=val_dl_CH,\n",
    "        epochs=150,\n",
    "        lr=best_params['lr'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Load and evaluate on test\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model_CH.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained CH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl, val_dl = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# DATALOADERS for FR 5% fine-tune split\n",
    "# ---------------------------------------\n",
    "\n",
    "# pristine clone (fine-tune set)\n",
    "ds_ft_FR_5pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_ft_FR_5pct)\n",
    "\n",
    "# split indices on FR 5%-ft\n",
    "train_idx_FR_5pct, val_idx_FR_5pct = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_ft_FR_5pct_copy), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# IMPORTANT: copy CH scalers -> FR 5%-ft, then transform in-place\n",
    "ds_ft_FR_5pct_copy.set_scalers_from(ds_train_CH_copy)\n",
    "ds_ft_FR_5pct_copy.transform_inplace()\n",
    "\n",
    "# create loaders WITHOUT fitting scalers\n",
    "ft_train_dl_FR_5pct, ft_val_dl_FR_5pct = ds_ft_FR_5pct_copy.make_loaders(\n",
    "    train_idx=train_idx_FR_5pct,\n",
    "    val_idx=val_idx_FR_5pct,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=False,  # <-- key!\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True)\n",
    "\n",
    "# holdout loader (FR 5% split)\n",
    "ds_holdout_FR_5pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_holdout_FR_5pct)\n",
    "\n",
    "holdout_dl_FR_5pct = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_holdout_FR_5pct_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Safe” fine-tune for small FR-ft set (freeze LSTM, train only static+head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Safe” fine-tune for small FR-ft set (freeze LSTM, train only static+head):\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft.load_state_dict(state)\n",
    "\n",
    "# 1) freeze recurrent encoder\n",
    "for name, p in model_CH_ft.named_parameters():\n",
    "    if name.startswith(\"lstm.\"):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# 2) new optimizer on trainable params only (small LR)\n",
    "opt = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_CH_ft.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# 3) fine-tune\n",
    "history, best_val, best_state = model_CH_ft.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_FR_5pct,\n",
    "    val_dl=ft_val_dl_FR_5pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=8,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_FR_5pct.pt\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Full” fine-tune (unfreeze everything, very small LR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_2 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_2.load_state_dict(state)\n",
    "\n",
    "# unfreeze everything\n",
    "for p in model_CH_ft_2.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    model_CH_ft_2.parameters(),\n",
    "    lr=1e-5,  # smaller because we’re updating the LSTM too\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_2.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_FR_5pct,\n",
    "    val_dl=ft_val_dl_FR_5pct,\n",
    "    epochs=80,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_FR_full_5pct.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practice: two-stage fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_3 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_3.load_state_dict(state)\n",
    "\n",
    "# Stage 1: freeze LSTM, tune heads\n",
    "for name, p in model_CH_ft_3.named_parameters():\n",
    "    p.requires_grad = not name.startswith(\"lstm.\")\n",
    "\n",
    "opt1 = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                model_CH_ft_3.parameters()),\n",
    "                         lr=2e-4,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "model_CH_ft_3.train_loop(device,\n",
    "                         ft_train_dl_FR_5pct,\n",
    "                         ft_val_dl_FR_5pct,\n",
    "                         epochs=20,\n",
    "                         optimizer=opt1,\n",
    "                         loss_fn=loss_fn,\n",
    "                         es_patience=5,\n",
    "                         save_best_path=\"models/tmp_stage1.pt\")\n",
    "\n",
    "# Stage 2: unfreeze all, very small LR\n",
    "for p in model_CH_ft_3.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt2 = torch.optim.AdamW(model_CH_ft_3.parameters(),\n",
    "                         lr=1e-5,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_3.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_FR_5pct,\n",
    "    val_dl=ft_val_dl_FR_5pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt2,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_FR_2stage_5pct.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare fine-tuning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_scores(model_, holdout_dl, ds_holdout_copy):\n",
    "    test_metrics, df_preds = model_.evaluate_with_preds(\n",
    "        device, holdout_dl, ds_holdout_copy)\n",
    "    scores_annual, scores_winter = compute_seasonal_scores(df_preds,\n",
    "                                                           target_col=\"target\",\n",
    "                                                           pred_col=\"pred\")\n",
    "    return test_metrics, df_preds, scores_annual, scores_winter\n",
    "\n",
    "\n",
    "def add_metrics_box(ax, scores_annual, scores_winter, title=None):\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=18)\n",
    "\n",
    "    legend_txt = \"\\n\".join([\n",
    "        r\"$\\mathrm{RMSE_a}=%.2f$, $\\mathrm{RMSE_w}=%.2f$\" %\n",
    "        (scores_annual[\"rmse\"], scores_winter[\"rmse\"]),\n",
    "        r\"$\\mathrm{R^2_a}=%.2f$, $\\mathrm{R^2_w}=%.2f$\" %\n",
    "        (scores_annual[\"R2\"], scores_winter[\"R2\"]),\n",
    "        r\"$\\mathrm{Bias_a}=%.2f$, $\\mathrm{Bias_w}=%.2f$\" %\n",
    "        (scores_annual[\"Bias\"], scores_winter[\"Bias\"]),\n",
    "    ])\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        legend_txt,\n",
    "        transform=ax.transAxes,\n",
    "        va=\"top\",\n",
    "        fontsize=14,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6),\n",
    "    )\n",
    "\n",
    "\n",
    "methods = [\n",
    "    (\"No fine-tune (CH→FR)\", model_CH),\n",
    "    (\"Heads-only FT (freeze LSTM)\", model_CH_ft),\n",
    "    (\"Full FT (unfreeze all)\", model_CH_ft_2),\n",
    "    (\"Two-stage FT\", model_CH_ft_3),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, m in methods:\n",
    "    test_metrics, df_preds, s_a, s_w = eval_and_scores(\n",
    "        m, holdout_dl_FR_5pct, ds_holdout_FR_5pct_copy)\n",
    "    results.append((name, df_preds, s_a, s_w))\n",
    "    print(name, \"| RMSE annual:\", test_metrics[\"RMSE_annual\"],\n",
    "          \"| RMSE winter:\", test_metrics[\"RMSE_winter\"])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (name, df_preds, s_a, s_w) in zip(axes, results):\n",
    "    pred_vs_truth_density(\n",
    "        ax,\n",
    "        df_preds,\n",
    "        s_a,\n",
    "        add_legend=False,\n",
    "        palette=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "        ax_xlim=(-14, 8),\n",
    "        ax_ylim=(-14, 8),\n",
    "    )\n",
    "    add_metrics_box(ax, s_a, s_w, title=name)\n",
    "\n",
    "fig.supxlabel(\"Observed PMB [m w.e.]\", fontsize=20)\n",
    "fig.supylabel(\"Modeled PMB [m w.e.]\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
