{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Glacier Mass Balance Prediction\n",
    "\n",
    "1. **Load a pre-trained neural network** trained on Swiss glacier data\n",
    "2. **Fine-tune it on French glacier data** using various strategies\n",
    "3. **Evaluate performance** on unseen French glaciers\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Pre-trained model on all Swiss data from ../regions/Switzerland/3.2.2 Train-ML-model-NN.ipynb e.g. `nn_model_2025-07-14_CH_flexible.pt`\n",
    "- French glacier dataset from ../regions/France/1.1. GLACIOCLIM-prepro.ipynb\n",
    "- ERA5 climate data of France from ../regions/France/1.2. ERA5Land-prepro.ipynb\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glacioclim_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_FR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.FranceConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\", # OGGM\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "### Create French Glacier Dataset\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm_with_blanc.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glacioclim['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_glacioclim[data_glacioclim.PERIOD == 'annual']) + len(data_glacioclim[data_glacioclim.PERIOD == 'winter']) + len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'winter']))\n",
    "#print('Number of summer samples:',\n",
    "      #len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "\n",
    "data_glacioclim = data_glacioclim[data_glacioclim['PERIOD'] != 'summer']\n",
    "\n",
    "data_glacioclim.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim_test = data_glacioclim.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_glacioclim_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file = 'FR_wgms_dataset_monthly_full_no_summer_with_blanc.csv')\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Data Split (5-10% for fine-tuning)\n",
    "\n",
    "**Data**: About 5-10 % of the available data is used as fine-tuning set\n",
    "\n",
    "**Use case**: Test performance with minimal fine-tuning data (~500 measurements) to simulate data-scarce scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFER LEARNING SETUP 5-10%\n",
    "# Fine-tuning glaciers 5-10%\n",
    "train_glaciers = ['FR4N01163A02 de Sarennes 1', 'FR4N01236A07 de Talefre', 'FR4N01236A02 des Grands Montets', 'FR4N01236A01 Leschaux']\n",
    "\n",
    "# Test glaciers (all remaining France glaciers)\n",
    "all_france_glaciers = list(data_glacioclim['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_france_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split\n",
    "\n",
    "### Random 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Configuration\n",
    "\n",
    "### Feature Engineering and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks and Training Configuration\n",
    "Set up training callbacks and configuration for optimal performance and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',    # Monitor validation loss\n",
    "    patience=15,             # Stop after 15 epochs without improvement\n",
    "    threshold=1e-4,          # Minimum change threshold\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for adaptive training\n",
    "lr_scheduler_cb = LRScheduler(\n",
    "    policy=ReduceLROnPlateau,\n",
    "    monitor='valid_loss',\n",
    "    mode='min',\n",
    "    factor=0.5,              # Reduce LR by half\n",
    "    patience=5,              # Wait 5 epochs before reducing\n",
    "    threshold=0.01,\n",
    "    threshold_mode='rel',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Global variables for dataset management\n",
    "dataset = dataset_val = None\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    \"\"\"Custom train/validation split function for skorch.\"\"\"\n",
    "    return dataset, dataset_val\n",
    "\n",
    "# Model configuration parameters\n",
    "param_init = {'device': 'cpu'}\n",
    "nInp = len(feature_columns)  # Number of input features\n",
    "\n",
    "# Model checkpointing to save best model during training\n",
    "checkpoint_cb = Checkpoint(\n",
    "    monitor='valid_loss_best',\n",
    "    f_params='best_model.pt',\n",
    "    f_optimizer=None,        # Don't save optimizer state\n",
    "    f_history=None,          # Don't save training history\n",
    "    f_criterion=None,        # Don't save criterion state\n",
    "    load_best=True,          # Load best model after training\n",
    ")\n",
    "\n",
    "# Custom callback to save models at specific epochs for analysis\n",
    "save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\n",
    "\n",
    "print('Callbacks and configuration ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "Datasets will be created in the training loop after loading the pre-trained Swiss model to ensure compatible preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't create datasets here, create them after loading the Swiss model\n",
    "features = features_val = None\n",
    "metadata = metadata_val = None\n",
    "dataset = dataset_val = None\n",
    "print(\"Datasets will be created after loading Swiss model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Execution\n",
    "\n",
    "### Loading Pre-trained Swiss Model and Fine-tuning on French Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Standard Fine-tuning with Selective Layer Freezing\n",
    "\n",
    "After loading the model, all layers will be frozen by default, to unfreeze a layer you have to include it in \"if name not in [...]\" in Step 3.\n",
    "\n",
    " The SaveBestAtEpochs callback automatically saves the current best model at epochs [10, 15, 20, 30, 50, 100], which can then be evaluated in the Epoch-wise model evalution section. Comment out the callback if you don't want this feature. If you do and you continuously want to retrain models at different learning rates, you have to reexecute the \"save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True  # Set to True to actually train\n",
    "if TRAIN:\n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True, # Important!!! this tells skorch to not re-initialize the weights etc.\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "            #('save_best_at_epochs', save_best_epochs_cb)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using the loaded Swiss model\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 2.5: Freeze layers\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        # Freeze layers\n",
    "        if name not in [#'model.0.weight', 'model.0.bias',\n",
    "                        'model.1.weight', 'model.1.bias',\n",
    "                        #'model.4.weight', 'model.4.bias',\n",
    "                        'model.5.weight', 'model.5.bias',\n",
    "                        #'model.8.weight', 'model.8.bias',\n",
    "                        'model.9.weight', 'model.9.bias',\n",
    "                        #'model.12.weight', 'model.12.bias',\n",
    "                        'model.13.weight', 'model.13.bias',\n",
    "                        #'model.16.weight', 'model.16.bias'\n",
    "                        ]:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # STEP 3: Update for fine-tuning\n",
    "    print(\"Updating model for fine-tuning...\")\n",
    "    loaded_model = loaded_model.set_params(\n",
    "        lr=0.05,\n",
    "        max_epochs=1,\n",
    "    )\n",
    "    \n",
    "    # STEP 4: Fine-tune\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    loaded_model.fit(features, y_train)\n",
    "    \n",
    "    # STEP 5: Save\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"✓ Fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Progressive Layer Unfreezing\n",
    "This advanced approach gradually unfreezes layers during training for more controlled adaptation to the French data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True  # Set to True to actually train\n",
    "if TRAIN:\n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True, # Important!!! this tells skorch not re-initialize the weights etc.\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using the loaded Swiss model\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 2.5: Freeze layers\n",
    "    \n",
    "    # Helper to freeze/unfreeze layers\n",
    "    def set_requires_grad(layer_names, requires_grad=True):\n",
    "        for name, param in loaded_model.module_.named_parameters():\n",
    "            if name in layer_names:\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    # List of layer groups to progressively unfreeze\n",
    "    layer_groups = [\n",
    "        (\n",
    "        [\n",
    "            'model.1.weight', 'model.1.bias',\n",
    "            'model.5.weight', 'model.5.bias',\n",
    "            'model.9.weight', 'model.9.bias',\n",
    "            'model.13.weight', 'model.13.bias'\n",
    "        ],200,  0.1\n",
    "    ),\n",
    "        (['model.16.weight', 'model.16.bias'], 200, 0.001),\n",
    "        (['model.12.weight', 'model.12.bias'], 200, 0.001),\n",
    "        (['model.8.weight', 'model.8.bias'], 200, 0.001)\n",
    "    ]\n",
    "\n",
    "    # Freeze all layers first\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Progressive unfreezing loop with custom learning rates\n",
    "    for layers, epochs, lr in layer_groups:\n",
    "        set_requires_grad(layers, True)\n",
    "        print(f\"Fine-tuning layers: {layers} for {epochs} epochs with lr={lr}...\")\n",
    "        loaded_model = loaded_model.set_params(lr=lr, max_epochs=epochs)\n",
    "        loaded_model.fit(features, y_train)\n",
    "\n",
    "        val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "        print(\"Validation score:\", val_score)\n",
    "    \n",
    "    # STEP 3: Save\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"✓ Fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Performance Evaluation\n",
    "Get immediate performance metrics on the test set using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comprehensive evaluation of the fine-tuned model\n",
    "print(\"Evaluating fine-tuned model performance...\")\n",
    "\n",
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm\n",
    ")\n",
    "\n",
    "print(\"Test Set Performance Metrics:\")\n",
    "display(scores_NN)\n",
    "\n",
    "# Validation score for confirmation that model with the best val_loss is used\n",
    "val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "print(f\"Validation score (for reference): {val_score:.4f}\")\n",
    "\n",
    "# Calculate additional performance metrics by glacier\n",
    "print(\"\\nPerformance by glacier:\")\n",
    "glacier_performance = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_samples': len(x),\n",
    "        'rmse': np.sqrt(np.mean((x['target'] - x['pred'])**2)),\n",
    "        'mae': np.mean(np.abs(x['target'] - x['pred'])),\n",
    "        'r2': 1 - np.sum((x['target'] - x['pred'])**2) / np.sum((x['target'] - x['target'].mean())**2),\n",
    "        'Bias': np.mean(x['pred'].values - x['target'].values)\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "display(glacier_performance.sort_values('rmse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch-wise Model Evaluation\n",
    "Evaluate models saved at different training epochs to understand training dynamics and optimal stopping points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models saved at different epochs to analyze training dynamics\n",
    "print(\"Evaluating models saved at different training epochs...\")\n",
    "\n",
    "epochs_to_evaluate = [10, 15, 20, 30, 50, 100]\n",
    "model_prefix = \"nn_model_best_epoch\"\n",
    "\n",
    "epoch_results = {}\n",
    "\n",
    "for epoch in epochs_to_evaluate:\n",
    "    model_name = f\"{model_prefix}_{epoch}.pt\"\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_name):\n",
    "        print(f\"Model for epoch {epoch} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model at epoch {epoch}...\")\n",
    "    \n",
    "    # Load model with same architecture as Swiss model\n",
    "    epoch_model = mbm.models.CustomNeuralNetRegressor(\n",
    "        cfg, **swiss_args, **param_init\n",
    "    )\n",
    "    epoch_model = epoch_model.set_params(device='cpu').to('cpu')\n",
    "    epoch_model.initialize()\n",
    "    \n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(model_name, map_location='cpu')\n",
    "    epoch_model.module_.load_state_dict(state_dict)\n",
    "\n",
    "    # Evaluate the model\n",
    "    grouped_ids_epoch, scores_NN_epoch, ids_NN_epoch, y_pred_NN_epoch = evaluate_model_and_group_predictions(\n",
    "        epoch_model, df_X_test_subset, test_set['y'], cfg, mbm\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    epoch_results[epoch] = scores_NN_epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch} performance:\")\n",
    "    display(scores_NN_epoch)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Epoch-wise evaluation completed!\")\n",
    "\n",
    "# Compare performance across epochs\n",
    "if epoch_results:\n",
    "    print(\"\\nPerformance comparison across epochs:\")\n",
    "    comparison_df = pd.DataFrame(epoch_results).T\n",
    "    comparison_df.index.name = 'Epoch'\n",
    "    display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Visualization and Analysis\n",
    "Generate detailed visualizations to understand model performance across different glaciers and time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive visualization data\n",
    "print(\"Preparing data for comprehensive visualizations...\")\n",
    "\n",
    "# Create features and metadata for final evaluation\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU for visualization\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "if hasattr(test_set['y'], 'cpu'):\n",
    "    targets_test = test_set['y'].cpu()\n",
    "else:\n",
    "    targets_test = test_set['y']\n",
    "\n",
    "# Create final test dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),  # Features\n",
    "    SliceDataset(dataset_test, idx=1)   # Targets\n",
    "]\n",
    "\n",
    "# Generate final predictions\n",
    "print(\"Generating final predictions for visualization...\")\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "# Prepare evaluation metrics\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate comprehensive performance metrics\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "print(f\"Final Model Performance Summary:\")\n",
    "print(f\"   R² Score: {score:.4f}\")\n",
    "print(f\"   RMSE: {rmse:.4f} mm w.e.\")\n",
    "print(f\"   MAE: {mae:.4f} mm w.e.\")\n",
    "print(f\"   Pearson r: {pearson:.4f}\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "grouped_ids = pd.DataFrame({\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "})\n",
    "\n",
    "# Add comprehensive metadata\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')\n",
    "\n",
    "print(\"Visualization data prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "\n",
    "PlotPredictionsCombined_NN_additional(grouped_ids)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of all model strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN components: Gradient Reversal, network, regressor wrapper, and dataset bindings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skorch.utils import to_tensor\n",
    "import massbalancemachine as mbm\n",
    "from pathlib import Path\n",
    "\n",
    "# Gradient Reversal Layer\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class GradReverse(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class DANNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor identical to FlexibleNetwork's trunk, with two heads:\n",
    "    - Regressor head for SMB (label)\n",
    "    - Domain classifier head (binary)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout=0.2, use_batchnorm=False, domain_hidden=64, grl_lambda=1.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(hidden_layers)\n",
    "        for hidden_dim, drop_rate in zip(hidden_layers, dropout):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            current_dim = hidden_dim\n",
    "        # trunk outputs the last hidden representation\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        # heads\n",
    "        self.regressor = nn.Linear(current_dim, 1)\n",
    "        self.grl = GradReverse(lambda_=grl_lambda)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(current_dim, domain_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout[-1] if isinstance(dropout, list) else dropout),\n",
    "            nn.Linear(domain_hidden, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)\n",
    "        y_pred = self.regressor(h)\n",
    "        d_logits = self.domain_classifier(self.grl(h))\n",
    "        return y_pred, d_logits\n",
    "\n",
    "# Dataset that yields domain labels padded per ID to match monthly padding\n",
    "class DomainTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, aggregated_dataset):\n",
    "        self.base = aggregated_dataset\n",
    "        self.meta_has_domain = 'DOMAIN' in self.base.metadataColumns\n",
    "        if not self.meta_has_domain:\n",
    "            # fallback: try to read from features\n",
    "            assert 'DOMAIN' in self.base.cfg.featureColumns, \"DOMAIN must be in metadata or featureColumns\"\n",
    "            self.domain_feat_idx = self.base.cfg.featureColumns.index('DOMAIN')\n",
    "        else:\n",
    "            self.domain_idx = self.base.metadataColumns.index('DOMAIN')\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.base._getInd(index)\n",
    "        if self.meta_has_domain:\n",
    "            dval = self.base.metadata[ind[0]][self.domain_idx]\n",
    "        else:\n",
    "            dval = self.base.features[ind[0], self.domain_feat_idx]\n",
    "        dpad = np.empty(self.base.maxConcatNb, dtype=np.float32)\n",
    "        dpad.fill(np.nan)\n",
    "        dpad[:len(ind)] = dval\n",
    "        return dpad\n",
    "\n",
    "# Binding that returns (X, (y, d)) so y_true in get_loss can contain both\n",
    "class CombinedTargetBinding(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_slice, y_slice, d_dataset):\n",
    "        self.X = X_slice\n",
    "        self.y = y_slice\n",
    "        self.d = d_dataset\n",
    "        assert len(self.X) == len(self.y) == len(self.d)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], (self.y[idx], self.d[idx])\n",
    "\n",
    "# Skorch regressor that adds domain-adversarial loss on top of the SMB loss\n",
    "class CustomDANNRegressor(mbm.models.CustomNeuralNetRegressor):\n",
    "    def __init__(self, cfg, *args, dan_lambda=0.1, **kwargs):\n",
    "        super().__init__(cfg, *args, **kwargs)\n",
    "        self.dan_lambda = dan_lambda\n",
    "        self._last_domain_logits = None\n",
    "\n",
    "    def infer(self, x, **fit_params):\n",
    "        x = to_tensor(x, device=self.device)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        x, indNonNan = self._unpack_inp(x)\n",
    "        if self.modelDtype is not None:\n",
    "            x = x.type(self.modelDtype)\n",
    "        outputs = self.module_(x, **fit_params)\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_monthly, d_monthly = outputs\n",
    "            y_packed = self._pack_out(y_monthly, indNonNan)\n",
    "            d_packed = self._pack_out(d_monthly, indNonNan)\n",
    "            self._last_domain_logits = d_packed\n",
    "            return y_packed\n",
    "        else:\n",
    "            return self._pack_out(outputs, indNonNan)\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        # y_true can be (label_pad, domain_pad) from CombinedTargetBinding\n",
    "        if isinstance(y_true, (tuple, list)) and len(y_true) == 2:\n",
    "            y_true_labels, y_true_domain = y_true\n",
    "        else:\n",
    "            y_true_labels, y_true_domain = y_true, None\n",
    "\n",
    "        # Label loss (same as base implementation)\n",
    "        loss = 0.0\n",
    "        cnt = 0\n",
    "        for yi_pred, yi_true in zip(y_pred, y_true_labels):\n",
    "            valid = ~torch.isnan(yi_pred)\n",
    "            if valid.any():\n",
    "                pred_sum = yi_pred[valid].sum()\n",
    "                true_mean = yi_true[valid].mean()\n",
    "                loss = loss + (pred_sum - true_mean) ** 2\n",
    "                cnt += 1\n",
    "        label_loss = loss / max(cnt, 1)\n",
    "\n",
    "        # Domain loss (optional during training)\n",
    "        \"\"\"\n",
    "        # Simple domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits\n",
    "            mask = ~torch.isnan(y_true_domain)\n",
    "            if mask.any():\n",
    "                domain_loss = F.binary_cross_entropy_with_logits(d_logits[mask], y_true_domain[mask].float())\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean, otherwise IDs with longer months have higher domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # shape: [batch, max_months] (or [batch, max_months, 1])\n",
    "            per_id_losses = []\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                # squeeze trailing dim if present\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                # mask valid months\n",
    "                mask = ~torch.isnan(d_true_row) # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    per_id_losses.append(loss_i)\n",
    "\n",
    "            if len(per_id_losses) > 0:\n",
    "                domain_loss = torch.stack(per_id_losses).mean()  # mean over IDs\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean for each domain\n",
    "        # otherwise IDs with longer months have higher domain loss and CH domain loss with more data is exaggerated\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # [batch_ids, max_months(,1)]\n",
    "            per_id_losses_ch, per_id_losses_nor = [], []\n",
    "\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                mask = ~torch.isnan(d_true_row)  # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    # domain for this ID (same across valid months)\n",
    "                    dom_i = int(d_true_row[mask][0].item())  # 0=CH, 1=NOR\n",
    "                    (per_id_losses_ch if dom_i == 0 else per_id_losses_nor).append(loss_i)\n",
    "\n",
    "            parts = []\n",
    "            if len(per_id_losses_ch) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_ch).mean())\n",
    "            if len(per_id_losses_nor) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_nor).mean())\n",
    "            if len(parts) > 0:\n",
    "                domain_loss = torch.stack(parts).mean()\n",
    "\n",
    "        return label_loss + self.dan_lambda * domain_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(cfg, fname: str, *args, **kwargs):\n",
    "        \"\"\"Loads a pre-trained DANN model from a file.\"\"\"\n",
    "        model = CustomDANNRegressor(cfg, *args, **kwargs)\n",
    "        model.initialize()\n",
    "        models_dir = Path(\"./models\")\n",
    "        model.load_params(f_params=models_dir / fname)\n",
    "        return model\n",
    "\n",
    "params_dann = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "    'module__domain_hidden': 64,\n",
    "    'module__grl_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# Use DANN network\n",
    "args_dann = {\n",
    "    'module': DANNNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params_dann['module__dropout'],\n",
    "    'module__hidden_layers': params_dann['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params_dann['module__use_batchnorm'],\n",
    "    'module__domain_hidden': params_dann['module__domain_hidden'],\n",
    "    'module__grl_lambda': params_dann['module__grl_lambda'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params_dann['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params_dann['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params_dann['optimizer'],\n",
    "    'optimizer__weight_decay': params_dann['optimizer__weight_decay'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_pmb_multi_fixed_lines(models_grouped, title_prefix='Mean PMB'):\n",
    "\n",
    "    first_label = next(iter(models_grouped))\n",
    "    df_ref = models_grouped[first_label].copy()\n",
    "\n",
    "\n",
    "    fig, (ax_ann, ax_win) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "\n",
    "    ax_ann.set_title('Mean annual point mass balance', fontsize=14, fontweight='bold', pad=30)\n",
    "    ax_win.set_title('Mean winter point mass balance', fontsize=14, fontweight='bold', pad=30)\n",
    "\n",
    "\n",
    "    palette = sns.color_palette(\"colorblind\", n_colors=len(models_grouped))\n",
    "    label_to_color = {label: palette[i] for i, label in enumerate(models_grouped.keys())}\n",
    "\n",
    "\n",
    "    regional_models = ['Regional Baseline', 'CH and Regional Baseline']\n",
    "    transfer_models = ['Fine-tuning', 'Batchnorm Unfrozen', 'DANN']\n",
    "    line_styles = {model: '-' for model in regional_models} \n",
    "    line_styles.update({model: '--' for model in transfer_models}) \n",
    "\n",
    "    def _compute_metrics(df_y):\n",
    "        valid = df_y[['pred', 'target']].dropna()\n",
    "        if len(valid) == 0:\n",
    "            return np.nan, np.nan\n",
    "        y_pred = valid['pred'].values\n",
    "        y_true = valid['target'].values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "        rho = np.corrcoef(y_pred, y_true)[0, 1] if y_pred.size > 1 else np.nan\n",
    "        return rmse, rho\n",
    "\n",
    "    def _plot_panel(ax, period_name):\n",
    "        df_period = df_ref[df_ref['PERIOD'] == period_name].copy()\n",
    "        years = np.sort(df_period['YEAR'].unique())\n",
    "\n",
    "\n",
    "        tgt_mean = df_period.groupby('YEAR')['target'].mean()\n",
    "        available_years = tgt_mean.index.values\n",
    "        available_values = tgt_mean.values\n",
    "        \n",
    "        target_line_plotted = False\n",
    "        for i in range(len(available_years) - 1):\n",
    "            if available_years[i+1] - available_years[i] == 1:  \n",
    "                if not target_line_plotted:\n",
    "\n",
    "                    ax.plot([available_years[i], available_years[i+1]], \n",
    "                           [available_values[i], available_values[i+1]], \n",
    "                           color=\"black\", linewidth=2, marker='x', markersize=6,\n",
    "                           label=\"target mean\")\n",
    "                    target_line_plotted = True\n",
    "                else:\n",
    "\n",
    "                    ax.plot([available_years[i], available_years[i+1]], \n",
    "                           [available_values[i], available_values[i+1]], \n",
    "                           color=\"black\", linewidth=2, marker='x', markersize=6)\n",
    "        \n",
    "        if not target_line_plotted:\n",
    "            ax.plot(available_years, available_values, color=\"black\", marker='x', markersize=6,\n",
    "                   linestyle='None', label=\"target mean\")\n",
    "\n",
    "        for label, gdf in models_grouped.items():\n",
    "            gdf_p = gdf[gdf['PERIOD'] == period_name].copy()\n",
    "            pred_mean = gdf_p.groupby('YEAR')['pred'].mean()\n",
    "\n",
    "            metrics_df = pd.DataFrame({\n",
    "                'YEAR': pred_mean.index,\n",
    "                'pred': pred_mean.values,\n",
    "            }).merge(\n",
    "                df_period.groupby('YEAR')['target'].mean().reset_index(), on='YEAR', how='left'\n",
    "            )\n",
    "            rmse, rho = _compute_metrics(metrics_df)\n",
    "\n",
    "            label_with_metrics = f\"{label} (RMSE={np.nan_to_num(rmse):.2f}, \\u03C1={np.nan_to_num(rho):.2f})\"\n",
    "            \n",
    "            pred_years = pred_mean.index.values\n",
    "            pred_values = pred_mean.values\n",
    "            \n",
    "            label_added = False  \n",
    "            \n",
    "            for i in range(len(pred_years) - 1):\n",
    "                if pred_years[i+1] - pred_years[i] == 1:  \n",
    "                    ax.plot([pred_years[i], pred_years[i+1]], \n",
    "                           [pred_values[i], pred_values[i+1]], \n",
    "                           color=label_to_color.get(label, None), \n",
    "                           linestyle=line_styles.get(label, '-'),\n",
    "                           label=label_with_metrics if not label_added else \"\")\n",
    "                    label_added = True\n",
    "            \n",
    "\n",
    "            if not label_added and len(pred_years) > 0:\n",
    "                ax.plot([], [], \n",
    "                       color=label_to_color.get(label, None), \n",
    "                       linestyle=line_styles.get(label, '-'),\n",
    "                       label=label_with_metrics)\n",
    "\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    _plot_panel(ax_ann, 'annual')\n",
    "    _plot_panel(ax_win, 'winter')\n",
    "\n",
    "\n",
    "    ax_ann.legend(fontsize=8, loc='upper center', ncol=3, frameon=True, bbox_to_anchor=(0.5, 1.22))\n",
    "    ax_win.legend(fontsize=8, loc='upper center', ncol=3, frameon=True, bbox_to_anchor=(0.5, 1.22))\n",
    "\n",
    "\n",
    "    ax_ann.set_ylabel('[m w.e.]')\n",
    "    ax_win.set_ylabel('[m w.e.]')\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.8))  \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "base = grouped_ids[['ID', 'target', 'PERIOD', 'GLACIER', 'YEAR']].copy()\n",
    "\n",
    "# List models (optionally mix NN and DANN). Provide up to 5.\n",
    "#5-10%\n",
    "models = [\n",
    "    #('CH Baseline',  'nn_model_2025-07-14_CH_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Regional Baseline',  'nn_model_2025-08-24_5-10%_3rd_try_regional_baseline.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('CH and Regional Baseline',  'nn_model_2025-08-24_5-10%_CH_and_5-10%_baseline_only_FR_val.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Fine-tuning',  'nn_model_finetuned_2025-08-24_5-10%__3rd_try_fine_tuning_lr0.0005_epoch_10.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Batchnorm Unfrozen',  'nn_model_finetuned_2025-08-24_5-10%_3rd_try_batchnorm_unfrozen_linear_frozen_lr0.05_epoch200.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('DANN',      'dann_model_2025-08-24_5-10%_lamba0_05_mean_domainloss_even_domainloss_only_FR_val_50_50epochsplit.pt', CustomDANNRegressor),\n",
    "]\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "nn_loader_args = {**swiss_args, **param_init}\n",
    "try:\n",
    "    _ = DANNNetwork\n",
    "    dann_loader_args = {\n",
    "        **{\n",
    "            **args_dann,\n",
    "            'module': DANNNetwork,\n",
    "            'module__domain_hidden': params_dann.get('module__domain_hidden', 64),\n",
    "            'module__grl_lambda': params_dann.get('module__grl_lambda', 1.0),\n",
    "        },\n",
    "        'dan_lambda': 0.05,  # inference-only, safe if different from train\n",
    "        **param_init,\n",
    "    }\n",
    "except NameError:\n",
    "    dann_loader_args = None\n",
    "\n",
    "models_grouped = {}\n",
    "\n",
    "for label, fname, model_cls in models:\n",
    "    print(f'Loading {label}: {fname}')\n",
    "\n",
    "    if model_cls.__name__ == 'CustomDANNRegressor':\n",
    "        if dann_loader_args is None:\n",
    "            raise NameError(\"DANNNetwork not available. Run the DANN components cell before loading DANN models.\")\n",
    "        mdl = model_cls.load_model(cfg, fname, **dann_loader_args)\n",
    "    else:\n",
    "        mdl = model_cls.load_model(cfg, fname, **nn_loader_args)\n",
    "\n",
    "    mdl = mdl.set_params(device=device).to(device)\n",
    "\n",
    "    # Predict aggregated by measurement ID using the same dataset_test\n",
    "    y_pred_agg = mdl.aggrPredict(dataset_test[0])\n",
    "\n",
    "    assert len(y_pred_agg) == len(base), f'Length mismatch for {label}'\n",
    "    gdf = base.copy()\n",
    "    gdf['pred'] = y_pred_agg\n",
    "    models_grouped[label] = gdf\n",
    "\n",
    "# Plot mean annual and mean winter PMB for all models together\n",
    "plot_mean_pmb_multi_fixed_lines(models_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_obs_grid(models_grouped, ncols=3, point_size=45, region_name='CH->FR'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    period_colors = {'annual': '#e31a1c', 'winter': '#1f78b4'}\n",
    "    labels = list(models_grouped.keys())\n",
    "    n_models = len(labels)\n",
    "    ncols_eff = min(ncols, n_models)\n",
    "    nrows = int(np.ceil(n_models / ncols_eff))\n",
    "\n",
    "    # Global axis limits\n",
    "    all_targets = np.concatenate([models_grouped[l].target.values for l in labels])\n",
    "    all_preds   = np.concatenate([models_grouped[l].pred.values   for l in labels])\n",
    "    gmin = np.nanmin([all_targets.min(), all_preds.min()])\n",
    "    gmax = np.nanmax([all_targets.max(), all_preds.max()])\n",
    "    pad = 0.05 * (gmax - gmin)\n",
    "    gmin -= pad\n",
    "    gmax += pad\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols_eff,\n",
    "                             figsize=(4.4 * ncols_eff, 4.6 * nrows),\n",
    "                             squeeze=False)\n",
    "    metrics_all = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        r, c = divmod(i, ncols_eff)\n",
    "        ax = axes[r][c]\n",
    "        df_plot = models_grouped[label]\n",
    "        df_plot = df_plot[df_plot.PERIOD.isin(['annual','winter'])].copy()\n",
    "\n",
    "        # Metrics per period\n",
    "        panel_metrics = {}\n",
    "        for period in ['annual','winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                y_t = sub.target.values\n",
    "                y_p = sub.pred.values\n",
    "                rmse = np.sqrt(np.mean((y_p - y_t)**2))\n",
    "                rho  = np.corrcoef(y_t, y_p)[0,1] if len(sub) > 1 else np.nan\n",
    "                panel_metrics[period] = (rmse, rho)\n",
    "\n",
    "        # Combined\n",
    "        y_t_all = df_plot.target.values\n",
    "        y_p_all = df_plot.pred.values\n",
    "        rmse_all = np.sqrt(np.mean((y_p_all - y_t_all)**2))\n",
    "        rho_all  = np.corrcoef(y_t_all, y_p_all)[0,1] if len(df_plot) > 1 else np.nan\n",
    "        panel_metrics['combined'] = (rmse_all, rho_all)\n",
    "        metrics_all[label] = panel_metrics\n",
    "\n",
    "        # Scatter (no per‑subplot point handles; handled globally)\n",
    "        for period in ['annual','winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                ax.scatter(sub.target, sub.pred,\n",
    "                           s=point_size, alpha=0.65, edgecolor='none',\n",
    "                           color=period_colors[period])\n",
    "\n",
    "        # 1:1 line + handle for subplot legend\n",
    "        ax.plot([gmin,gmax],[gmin,gmax],'k--',linewidth=1,alpha=0.55)\n",
    "\n",
    "        # Metric legend entries\n",
    "        def metric_handle(name, key):\n",
    "            rmse, rho = panel_metrics[key]\n",
    "            return Line2D([],[], linestyle='',\n",
    "                          label=f\"{name}: RMSE {rmse:.2f} m w.e., ρ:{rho:.2f}\",\n",
    "                          color='none')\n",
    "\n",
    "        metric_handles = [\n",
    "            metric_handle(\"Combined\", 'combined'),\n",
    "            *( [metric_handle(\"Annual\", 'annual')] if 'annual' in panel_metrics else [] ),\n",
    "            *( [metric_handle(\"Winter\", 'winter')] if 'winter' in panel_metrics else [] ),\n",
    "        ]\n",
    "\n",
    "        ax.set_xlim(gmin,gmax)\n",
    "        ax.set_ylim(gmin,gmax)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xlabel('Observed PMB', fontsize=9)\n",
    "        ax.set_ylabel('Predicted PMB', fontsize=9)\n",
    "\n",
    "        # Subplot legend: only 1:1 + metrics\n",
    "        ax.legend(handles=metric_handles,\n",
    "                  fontsize=7, loc='upper left',\n",
    "                  frameon=True, handlelength=1.2, borderpad=0.5)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(n_models, nrows * ncols_eff):\n",
    "        r, c = divmod(j, ncols_eff)\n",
    "        axes[r][c].axis('off')\n",
    "\n",
    "    # Global legend for point types (annual / winter)\n",
    "    global_point_handles = [\n",
    "        Line2D([0],[0], marker='o', linestyle='', color=period_colors['annual'],\n",
    "               label='Annual'),\n",
    "        Line2D([0],[0], marker='o', linestyle='', color=period_colors['winter'],\n",
    "               label='Winter')\n",
    "    ]\n",
    "    fig.legend(handles=global_point_handles,\n",
    "               loc='upper center',\n",
    "               ncol=2,\n",
    "               frameon=True,\n",
    "               fontsize=9,\n",
    "               bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    plt.tight_layout(rect=(0,0,1,0.95))\n",
    "    return metrics_all\n",
    "\n",
    "pred_grid_metrics = plot_pred_vs_obs_grid(models_grouped, ncols=3, region_name='CH->FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
