{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modelling for Glacier Mass Balance Prediction\n",
    "\n",
    "This notebook implements **neural network models** for predicting glacier point mass balance measurements using meteorological and topographical features. The notebook supports two main modelling approaches:\n",
    "\n",
    "##  **Modelling Strategies**\n",
    "\n",
    "### 1. **Regional Modelling (French Alps Only)**\n",
    "- Train and test exclusively on French Alps glacier data\n",
    "- Uses glacier-based train/test splits to ensure spatial generalization\n",
    "\n",
    "### 2. **Cross-Regional Modelling (Switzerland → French Alps)**\n",
    "- Combines data from Swiss glaciers and French Alps glaciers\n",
    "- Can be used as either a cross-regional model, training exclusively on Swiss data and testing on French Alps data\n",
    "- or as a baseline for transfer learning models, training on Swiss data and a subset of Icelandic data and testing on remaining Icelandic data\n",
    "\n",
    "---\n",
    "\n",
    "##  **Prerequisites**\n",
    "- French Alps glacier dataset from `../1.1. France-prepro.ipynb`\n",
    "- Swiss glacier dataset from `regions/Switzerland/1.1. GLAMOS-prepro.ipynb`\n",
    "- ERA5 climate data for both regions from `../1.2. ERA5Land-prepro.ipynb`\n",
    "---\n",
    "\n",
    "### Feature Definitions\n",
    "\n",
    "**Climate Features (ERA5 Reanalysis):**\n",
    "- `t2m`: 2-meter temperature\n",
    "- `tp`: Total precipitation\n",
    "- `slhf`/`sshf`: Surface heat fluxes\n",
    "- `ssrd`: Surface solar radiation downwards\n",
    "- `fal`: Albedo\n",
    "- `str`: Surface net thermal radiation\n",
    "- `u10`/`v10`: Wind components\n",
    "\n",
    "**Topographical Features (OGGM):**\n",
    "- `aspect`/`slope`: Terrain geometry\n",
    "- `hugonnet_dhdt`: Ice thickness changes\n",
    "- `consensus_ice_thickness`: Ice depth\n",
    "- `millan_v`: Ice surface velocity\n",
    "- `elevation_difference`: Measurement elevation − ERA5-Land grid cell elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glacioclim_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_FR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.FranceConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Regional Modelling (French Alps Only)\n",
    "\n",
    "This approach trains neural networks exclusively on French Alps glacier data.\n",
    "\n",
    "## Data Loading & Initial Processing\n",
    "\n",
    "### Create French Alps Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm_with_blanc.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glacioclim['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_glacioclim[data_glacioclim.PERIOD == 'annual']) + len(data_glacioclim[data_glacioclim.PERIOD == 'winter']) + len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'winter']))\n",
    "#print('Number of summer samples:',\n",
    "      #len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "\n",
    "data_glacioclim = data_glacioclim[data_glacioclim['PERIOD'] != 'summer']\n",
    "\n",
    "data_glacioclim.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transformation to Monthly Format\n",
    "\n",
    "Transform point mass balance data to monthly resolution and integrate with ERA5 climate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim_test = data_glacioclim.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_glacioclim_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file = 'FR_wgms_dataset_monthly_full_no_summer_with_blanc.csv')\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splitting Strategy\n",
    "\n",
    "**Spatial Generalization Approach:** Select test set based on glaciers, remaining glaciers will be the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = ['FR4N01146D09+E06 Gebroulaz']\n",
    "\n",
    "#test_glaciers = ['FR4N01083B21 Blanc']\n",
    "\n",
    "# Regional 50%\n",
    "#test_glaciers = ['FR4N01236A07 de Talefre','FR4N01235A08 dArgentiere', 'FR4N01146D09+E06 Gebroulaz']\n",
    "\n",
    "# 5-10% or 653 IDs in training set\n",
    "#test_glaciers = ['FR4N01235A08 dArgentiere' ,'FR4N01146D09+E06 Gebroulaz' ,'FR4N01236A01 Mer de Glace/Geant','FR4N01162B09+154D03 de Saint Sorlin', 'FR4N01083B21 Blanc']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal Generalization Approach:** Withhold X of the last years for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_years_count = 10\n",
    "\n",
    "all_years = sorted(dataloader_gl.data['YEAR'].unique())\n",
    "\n",
    "# Use the most recent years as test data\n",
    "train_years = all_years[:-test_years_count]\n",
    "test_years = all_years[-test_years_count:]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(test_years))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.YEAR.isin(\n",
    "    train_years)]\n",
    "print('Size of train data:', len(train_years))\n",
    "if len(train_years) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(test_years) / len(train_years)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(\n",
    "                                dataloader_gl,\n",
    "                                test_split_on='YEAR',\n",
    "                                test_splits=test_years,\n",
    "                                random_state=cfg.seed)\n",
    "\n",
    "\n",
    "\n",
    "print('Train year: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test years: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "region_name='Prediction most recent 10 years'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split Strategy (80/20)\n",
    "\n",
    "**Standard Approach:** Random 80/20 split across all available training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Cross-Regional Transfer Learning (Switzerland → French Alps)\n",
    "\n",
    "This approach uses the Swiss dataset to try and model French Alps glaciers.\n",
    "\n",
    "### Create Combined Swiss and French Alps Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_FR = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm_with_blanc.csv')\n",
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "# Adjust dfs to match\n",
    "data_CH['GLACIER_ZONE'] = 'Placeholder'\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "data_CH = data_CH.drop(columns=['aspect_sgi', 'slope_sgi', 'topo_sgi'], errors='ignore')\n",
    "\n",
    "data_FR = data_FR[data_FR['PERIOD'] != 'summer']\n",
    "\n",
    "# Merge FR with CH\n",
    "data_FR_CH = pd.concat([data_FR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(len(data_FR_CH['GLACIER'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_FR_CH,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'FR_CH_wgms_dataset_monthly_full_with_blanc.csv')\n",
    "data_monthly_FR_CH = dataloader_gl.data\n",
    "\n",
    "display(data_monthly_FR_CH.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splitting Strategy\n",
    "\n",
    "**Spatial Generalization Approach:** Select test set based on glaciers, remaining glaciers will be the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_glaciers = list(data_FR['GLACIER'].unique())\n",
    "\n",
    "# 5-10% or 653 IDs in train set\n",
    "test_glaciers = ['FR4N01235A08 dArgentiere' ,'FR4N01146D09+E06 Gebroulaz' ,'FR4N01236A01 Mer de Glace/Geant','FR4N01162B09+154D03 de Saint Sorlin', 'FR4N01083B21 Blanc']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = list(data_CH['GLACIER'].unique())\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split Strategy (80/20)\n",
    "\n",
    "**Standard Approach:** Random 80/20 split across all available training data (Swiss + French Alps training glaciers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split - Target Domain Focus (80/20)\n",
    "\n",
    "**Domain-Aware Approach:** This split will only build the validation set from French data\n",
    "\n",
    "- **Training Set:** All Swiss data + 80% of French Alps training glaciers\n",
    "- **Validation Set:** 20% of French Alps training glaciers only\n",
    "\n",
    "This ensures that the same validation set is used as in fine-tuning and layer freezing in the `../3.2.4 Train-ML-model-NN_progressive_transfer` Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pool = CH + France subset\n",
    "data_train = train_set['df_X'].copy()\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "# France train_glaciers\n",
    "france_train_glacier = [\n",
    "    g for g in data_FR['GLACIER'].unique()\n",
    "    if g not in test_glaciers\n",
    "]\n",
    "display('train glaciers from target domain: ', france_train_glacier)\n",
    "\n",
    "# Find France subset within this pool\n",
    "france_mask = data_train['GLACIER'].isin(france_train_glacier)\n",
    "data_france = data_train.loc[france_mask]\n",
    "\n",
    "# Split only the France subset\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_france)\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "france_train_idx = list(train_itr)\n",
    "france_val_idx = list(val_itr)\n",
    "\n",
    "# Training set = CH + France train portion\n",
    "df_X_train = pd.concat([\n",
    "    data_train.loc[~france_mask],                           # all CH glaciers\n",
    "    data_france.iloc[france_train_idx]                    # France train glaciers\n",
    "])\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Validation set = France val portion only\n",
    "df_X_val = data_france.iloc[france_val_idx]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Configuration & Training\n",
    "\n",
    "Implementing a **Multilayer perceptron deep neural network** designed specifically for glacier mass balance prediction\n",
    "\n",
    "The following cell is optional and initializes a period_indicator variable, which allows the NN to keep track if the current point is from a winter or annual measurement.\n",
    "\n",
    "Uncomment `#+ ['PERIOD_INDICATOR']` in the \"Define features\" cell if you want to use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional, don't run if you dont want to use period_indicator\n",
    "\n",
    "def create_period_indicator(df):\n",
    "    \"\"\"Create numerical PERIOD_INDICATOR feature\"\"\"\n",
    "    df = df.copy()\n",
    "    df['PERIOD_INDICATOR'] = df['PERIOD'].map({'annual': 0, 'winter': 1})\n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "df_X_train = create_period_indicator(df_X_train)\n",
    "df_X_val = create_period_indicator(df_X_val)\n",
    "test_set['df_X'] = create_period_indicator(test_set['df_X'])\n",
    "\n",
    "print(\"PERIOD_INDICATOR created:\")\n",
    "print(\"Annual (0):\", (df_X_train['PERIOD_INDICATOR'] == 0).sum())\n",
    "print(\"Winter (1):\", (df_X_train['PERIOD_INDICATOR'] == 1).sum())\n",
    "print(\"Original PERIOD column preserved:\", df_X_train['PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate) # + ['PERIOD_INDICATOR']\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization & Hyperparameters\n",
    "\n",
    "**NN Configuration:**\n",
    "- **Learning Rate:** 0.001 with ReduceLROnPlateau scheduling\n",
    "- **Batch Size:** 128 samples per gradient update  \n",
    "- **Optimization:** Adam optimizer with L2 weight decay (1e-5)\n",
    "- **Architecture:** [128, 128, 64, 32] hidden layers with 20% dropout\n",
    "- **Regularization:** Batch normalization + early stopping (patience=15)\n",
    "\n",
    "**Callbacks for Robust Training:**\n",
    "- **Early Stopping:** Prevents overfitting by monitoring validation loss\n",
    "- **Learning Rate Scheduler:** Reduces LR when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Checkpointing\n",
    "\n",
    "Set `TRAIN = True` to train new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "    \n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-09-30.pt\"\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation & Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual grouped_ids as CSV\n",
    "split_name = \"cross_regional_asdfasdf\"\n",
    "csv_filename = f\"grouped_ids_{split_name.replace('%', 'pct').replace('-', '_')}.csv\"\n",
    "grouped_ids.to_csv(f\"results/{csv_filename}\", index=False)\n",
    "\n",
    "split_name = \"cross_regional_all_FR_CH_periodindicator\"  \n",
    "csv_filename = f\"results/grouped_ids_{split_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined_NN_additional(grouped_ids, region_name='Blanc', nticks=6, min_val=-11.92, max_val=5.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
