{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of GLACIOCLIM MB data:\n",
    "\n",
    "Does the pre-processing of the point MB measurements from GLACIOCLIM (French Alps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "from regions.French_Alps.scripts.glacioclim_preprocess import *\n",
    "from regions.French_Alps.scripts.config_FR import *\n",
    "from regions.Switzerland.scripts.oggm import initialize_oggm_glacier_directories, export_oggm_grids\n",
    "\n",
    "from regions.Switzerland.scripts.glamos import merge_pmb_with_oggm_data, rename_stakes_by_elevation, check_point_ids_contain_glacier, remove_close_points, check_multiple_rgi_ids\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.FranceConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dictionary of dataframes\n",
    "\n",
    "Unzip GLACIOCLIM Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in = False\n",
    "if read_in == True:\n",
    "    extract_glacioclim_files(cfg.dataPath + path_PMB_GLACIOCLIM_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all csv from \"unzipped\" into single dictionary of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes_csv_all = {}\n",
    "for path, _, files in os.walk(cfg.dataPath + path_PMB_GLACIOCLIM_raw):\n",
    "    # Find all .csv files in the current directory\n",
    "    csv_files = glob.glob(os.path.join(path, '*.csv'))\n",
    "\n",
    "    # Read each CSV file and add to dictionary\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            # Use the filename as the key\n",
    "            key = os.path.splitext(os.path.basename(file))[0]\n",
    "            # Read the CSV file\n",
    "            stakes_csv_all[key] = pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal number of files processed: {len(stakes_csv_all)}\")\n",
    "display(stakes_csv_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case Sarennes. \n",
    "\n",
    "The File is not in a typical csv structure, so cant just be read in. For this code to work, in each sheet in the B_SARENNES_94_20.xls file, make a table with start_date, spring_date, end_date, X, Y, Z, winter mb, summer mb, annual mb (requires xlrd package to read in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarennes_path = cfg.dataPath + 'GLACIOCLIM/point/Glacier de SARENNES_discontinued/B_SARENNES_94-20.xls'\n",
    "\n",
    "# Read all sheets into a dictionary of dfs\n",
    "all_sheets = pd.read_excel(sarennes_path, sheet_name=None)\n",
    "sarennes_dfs = extract_sarennes_data(all_sheets)\n",
    "display(sarennes_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case Glacier Blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blanc_path = cfg.dataPath + 'GLACIOCLIM/point/Glacier BLANC_unusable/GlacierBlanc2000_2024_pointMB_mwe.xls'\n",
    "\n",
    "blanc_data = pd.read_excel(blanc_path, skiprows=2).iloc[:, 1:]\n",
    "\n",
    "blanc_df = extract_blanc_data(blanc_data)\n",
    "\n",
    "display(blanc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from Lambert3 / Lambert2 cooridnates to WGS84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For some reason there is a 2 in front of the y_lambert2e coordinates in certain years, hardcode remove them:\n",
    "stakes_csv_all['mdg_Tacul_winter_smb_abl_2007'][\n",
    "    'y_lambert2e'] = stakes_csv_all['mdg_Tacul_winter_smb_abl_2007'][\n",
    "        'y_lambert2e'].apply(lambda x: x - 2000000 if x > 2000000 else x)\n",
    "stakes_csv_all['mdg_Talefre_annual_smb_abl_2006'][\n",
    "    'y_lambert2e'] = stakes_csv_all['mdg_Talefre_annual_smb_abl_2006'][\n",
    "        'y_lambert2e'].apply(lambda x: x - 2000000 if x > 2000000 else x)\n",
    "\n",
    "stakes_csv_all = lambert_transform(stakes_csv_all)\n",
    "sarennes_dfs = lambert_transform(sarennes_dfs)\n",
    "blanc_df = lamberttoWGS84(blanc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot stakes over map for visual conformation of coordinate transformation\n",
    "This needs additional folium package to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "glacier_dfs = {\n",
    "    'GEBROULAZ': (stakes_csv_all['geb_annual_smb_abl_1979'], 'red'),\n",
    "    'ARGENTIERE': (stakes_csv_all['Argentiere_annual_smb_accu_1995'], 'blue'),\n",
    "    'SAINT_SORLIN': (stakes_csv_all['stso_annual_smb_abl_1957'], 'green'),\n",
    "    'MER_DE_GLACE':\n",
    "    (stakes_csv_all['mdg_Leschaux_winter_smb_abl_2020'], 'purple'),\n",
    "    'SARENNES': (sarennes_dfs['sarennes_complete_winter_2006'], 'orange'),\n",
    "}\n",
    "\n",
    "center_lat, center_lon = 45.8736, 6.8770\n",
    "\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
    "\n",
    "# Add markers for each glacier\n",
    "for glacier_name, (df, color) in glacier_dfs.items():\n",
    "    fg = folium.FeatureGroup(name=glacier_name)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if glacier_name == 'SARENNES':\n",
    "            stake_id = row['POINT_ID'].split('_')[-1]\n",
    "            altitude = row['POINT_ELEVATION']\n",
    "        else:\n",
    "            stake_id = row['stake_number']\n",
    "            altitude = row['altitude']\n",
    "\n",
    "        # Add circle marker with popup\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['lon']],\n",
    "            radius=5,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            popup=f\"{glacier_name} - Stake {stake_id}: {altitude}m\").add_to(fg)\n",
    "\n",
    "    fg.add_to(m)\n",
    "\n",
    "# Add a legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; padding: 10px; border-radius: 5px;\">\n",
    "    <p><strong>Glaciers</strong></p>\n",
    "    <p><span style=\"color: red;\">●</span> GEBROULAZ</p>\n",
    "    <p><span style=\"color: blue;\">●</span> ARGENTIERE</p>\n",
    "    <p><span style=\"color: green;\">●</span> SAINT SORLIN</p>\n",
    "    <p><span style=\"color: purple;\">●</span> MER DE GLACE</p>\n",
    "    <p><span style=\"color: orange;\">●</span> SARENNES</p>\n",
    "</div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGMS Mapping, appending SARENNES and conversion into single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes_csv_WGMS = {}\n",
    "\n",
    "# Process each DataFrame in the original dictionary to WGMS format\n",
    "for key, df in stakes_csv_all.items():\n",
    "    try:\n",
    "        required_cols = {\n",
    "            'lat', 'lon', 'altitude', 'stake_number', 'day_start',\n",
    "            'month_start', 'year_start', 'day_end', 'month_end', 'year_end'\n",
    "        }\n",
    "        if all(col in df.columns for col in required_cols):\n",
    "            stakes_csv_WGMS[key] = transform_WGMS_df(df, key)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {key}: {str(e)}\")\n",
    "\n",
    "# Append SARENNES to the dictionary\n",
    "columns = [\n",
    "    'POINT_ID', 'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION', 'FROM_DATE',\n",
    "    'TO_DATE', 'POINT_BALANCE', 'GLACIER', 'PERIOD', 'GLACIER_ZONE'\n",
    "]\n",
    "for key, df in sarennes_dfs.items():\n",
    "    stakes_csv_WGMS[key] = df.rename(columns={\n",
    "        'lat': 'POINT_LAT',\n",
    "        'lon': 'POINT_LON'\n",
    "    })[columns]\n",
    "\n",
    "# Rename Glacier Blanc\n",
    "blanc_df = blanc_df.rename(columns={'lat': 'POINT_LAT', 'lon': 'POINT_LON'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine dictionary of dfs into 1 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to single DataFrame\n",
    "df_pmb_all_raw = pd.concat(stakes_csv_WGMS.values(), ignore_index=True)\n",
    "# Append Glacier Blanc\n",
    "df_pmb_all_raw = pd.concat([df_pmb_all_raw, blanc_df], ignore_index=True)\n",
    "# Sort by date and glacier\n",
    "df_pmb_all_raw = df_pmb_all_raw.sort_values(\n",
    "    ['GLACIER', 'GLACIER_ZONE', 'PERIOD', 'FROM_DATE'])\n",
    "# Add YEAR column to df\n",
    "df_pmb_all_raw['YEAR'] = df_pmb_all_raw['TO_DATE'].astype(str).str[:4].astype(\n",
    "    int)\n",
    "# Add data modification column to keep track of mannual changes\n",
    "df_pmb_all_raw['DATA_MODIFICATION'] = ''\n",
    "# Add remark that glacier blanc dates were modified\n",
    "df_pmb_all_raw.loc[df_pmb_all_raw['GLACIER'] == 'glacier_blanc',\n",
    "                   'DATA_MODIFICATION'] = 'Hydraulic year dates'\n",
    "df_pmb_all_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge stakes that are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_all_raw_dropped = pd.DataFrame()\n",
    "for gl in tqdm(df_pmb_all_raw.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_pmb_all_raw[df_pmb_all_raw.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_all_raw_dropped = pd.concat([df_pmb_all_raw_dropped, df_gl_cleaned])\n",
    "df_pmb_all_raw_dropped.drop(['x', 'y'], axis=1, inplace=True)\n",
    "df_pmb_all_raw_dropped.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. General data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows with NaN values:\")\n",
    "display(df_pmb_all_raw_dropped[df_pmb_all_raw_dropped.isna().any(axis=1)])\n",
    "\n",
    "print(\"Rows with zero values:\")\n",
    "display(df_pmb_all_raw_dropped[df_pmb_all_raw_dropped.eq(0).any(axis=1)])\n",
    "\n",
    "print(\"Rows with extreme POINT_BALANCE values (>5 or <-15):\")\n",
    "display(\n",
    "    df_pmb_all_raw_dropped[(df_pmb_all_raw_dropped['POINT_BALANCE'] > 5) |\n",
    "                           (df_pmb_all_raw_dropped['POINT_BALANCE'] < -15)])\n",
    "\n",
    "# Saint-Sorlin has POINT_ELEVATION 0.0 on 8 stakes, also about 20 stakes have point balance 0.00, remove them all\n",
    "mask_zeros = df_pmb_all_raw_dropped.eq(0).any(axis=1)\n",
    "df_pmb_all_raw_dropped = df_pmb_all_raw_dropped[~mask_zeros]\n",
    "\n",
    "# stso_summer_smb_abl_2017_ablation_setup2015_14 -88 pmb, nonsensical value, remove it\n",
    "mask = df_pmb_all_raw_dropped[\n",
    "    'POINT_ID'] != 'stso_summer_smb_abl_2017_ablation_setup2015_14'\n",
    "df_pmb_all_raw_dropped = df_pmb_all_raw_dropped[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent, summer_inconsistent = check_period_consistency(\n",
    "    df_pmb_all_raw_dropped)\n",
    "\n",
    "display(annual_inconsistent)\n",
    "display(summer_inconsistent)\n",
    "\n",
    "# 7 short summers but nothing majorly inconsistent, leaving them in\n",
    "# Annuual, mdg_langue_annual_smb_abl_2008_setup2008_9 is a faulty measurement, goes from 2009 to 2008, pop it\n",
    "mask = df_pmb_all_raw_dropped[\n",
    "    'POINT_ID'] != 'mdg_langue_annual_smb_abl_2008_Langue_setup2008_9'\n",
    "df_pmb_all_raw_dropped = df_pmb_all_raw_dropped[mask]\n",
    "df_pmb_all_raw_dropped.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_all_raw_dropped))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_all_raw_dropped[df_pmb_all_raw_dropped.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_all_raw_dropped[df_pmb_all_raw_dropped.PERIOD == 'winter']))\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_all_raw_dropped.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "    ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_all_raw_dropped.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RGIId and OGGM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "export_oggm_grids = False\n",
    "if export_oggm_grids:\n",
    "    export_oggm_grids(cfg, gdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glacier outlines\n",
    "rgi_file = utils.get_rgi_region_file(region=\"11\", version=\"62\")\n",
    "glacier_outline = gpd.read_file(rgi_file)\n",
    "\n",
    "# Add RGI IDs through intersection\n",
    "df_pmb = mbm.data_processing.utils.get_rgi(data=df_pmb_all_raw_dropped,\n",
    "                                           glacier_outlines=glacier_outline)\n",
    "\n",
    "print('Number of measurements without RGI:',\n",
    "      len(df_pmb[df_pmb['RGIId'].isna()]))\n",
    "\n",
    "# Remove stakes without RGIId, as they wont have OGGM data anyways\n",
    "print('Drop stakes without RGIs...')\n",
    "df_pmb = df_pmb.dropna(subset=['RGIId'])\n",
    "\n",
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "df_pmb['GLACIER'] = df_pmb['RGIId'].map(rgi_to_name_dict)\n",
    "\n",
    "rgiids6 = df_pmb[['GLACIER', 'RGIId']].drop_duplicates()\n",
    "if check_multiple_rgi_ids(rgiids6):\n",
    "    print(\n",
    "        \"-- Alert: The following glaciers have more than one RGIId. Cleaning up.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"-- All glaciers are correctly associated with a single RGIId.\")\n",
    "    df_pmb_clean = df_pmb\n",
    "\n",
    "df_pmb_clean.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Add OGGM data:\n",
    "df_pmb_topo = merge_pmb_with_oggm_data(df_pmb=df_pmb_clean,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"11\",\n",
    "                                       rgi_version=\"62\",\n",
    "                                       verbose=True)\n",
    "# Restrict to within glacier shape and drop the column\n",
    "df_pmb_topo = df_pmb_topo[df_pmb_topo['within_glacier_shape'] == True]\n",
    "df_pmb_topo = df_pmb_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "# Drop points if aspect or slope is NaN\n",
    "df_pmb_topo = df_pmb_topo[~df_pmb_topo.aspect.isna()]\n",
    "df_pmb_topo = df_pmb_topo[~df_pmb_topo.slope.isna()]\n",
    "df_pmb_topo = df_pmb_topo[~df_pmb_topo.topo.isna()]\n",
    "\n",
    "df_pmb_topo.reset_index(inplace=True, drop=True)\n",
    "df_pmb_topo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_topo.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "    ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_topo.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "print(\n",
    "    f'Saving intermediate output df_pmb_topo.csv to {path_PMB_GLACIOCLIM_csv}')\n",
    "df_pmb_topo.to_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'df_pmb_topo.csv',\n",
    "                   index=False)\n",
    "display(df_pmb_topo.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for wrong elevations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checked, df_bad = flag_elevation_mismatch(df_pmb_topo, threshold=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add skyview factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one svf file\n",
    "rgi_id = df_pmb_topo.loc[0].RGIId\n",
    "\n",
    "# read ds with svf\n",
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              'GLAMOS/topo/RGI_v6_11/xr_masked_grids')\n",
    "\n",
    "xr.open_zarr(path_masked_xr + f'/{rgi_id}.zarr').svf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              \"GLAMOS/topo/RGI_v6_11/xr_masked_grids\")\n",
    "\n",
    "df_pmb_topo_svf = add_svf_from_rgi_zarr(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    ")\n",
    "df_missing = df_pmb_topo_svf[df_pmb_topo_svf[\"svf\"].isna()].copy()\n",
    "print(\"Missing SVF points:\", len(df_missing))\n",
    "print(\"Glaciers affected:\", sorted(df_missing[\"RGIId\"].unique()))\n",
    "\n",
    "# drop rows where svf is NaN\n",
    "df_pmb_topo_svf_clean = df_pmb_topo_svf[~df_pmb_topo_svf.svf.isna()]\n",
    "\n",
    "# Print how many rows were dropped\n",
    "print(\n",
    "    f\"Dropped {len(df_pmb_topo) - len(df_pmb_topo_svf_clean)} rows due to missing SVF data.\"\n",
    ")\n",
    "df_pmb_topo_svf_clean.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_topo_svf_new = add_svf_nearest_valid(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    "    max_radius=30,  # ~30 grid cells search; adjust if needed\n",
    ")\n",
    "\n",
    "print(\"Missing SVF points after nearest-valid fill:\",\n",
    "      df_pmb_topo_svf_new[\"svf\"].isna().sum())\n",
    "\n",
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf_new,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glacier_svf_with_points(\n",
    "    df_with_svf=df_pmb_topo_svf_new,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_col=\"svf\",\n",
    "    svf_var=\"svf\",\n",
    "    save_dir=\n",
    "    None,  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_point_plots\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give new stake IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_new_ids = rename_stakes_by_elevation(df_pmb_topo_svf_new)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_new_ids)\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_new_ids))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_new_ids['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_clean = df_pmb_new_ids.copy()\n",
    "df_pmb_clean['MONTH_START'] = [\n",
    "    str(date)[4:6] for date in df_pmb_clean.FROM_DATE\n",
    "]\n",
    "df_pmb_clean['MONTH_END'] = [str(date)[4:6] for date in df_pmb_clean.TO_DATE]\n",
    "\n",
    "winter_pmb = df_pmb_clean[df_pmb_clean.PERIOD == \"winter\"]\n",
    "annual_pmb = df_pmb_clean[df_pmb_clean.PERIOD == \"annual\"]\n",
    "\n",
    "print(\"Winter measurement months:\")\n",
    "print(\"  Unique start months:\", sorted(winter_pmb[\"MONTH_START\"].unique()))\n",
    "print(\"  Unique end months:  \", sorted(winter_pmb[\"MONTH_END\"].unique()))\n",
    "\n",
    "print(\"\\nAnnual measurement months:\")\n",
    "print(\"  Unique start months:\", sorted(annual_pmb[\"MONTH_START\"].unique()))\n",
    "print(\"  Unique end months:  \", sorted(annual_pmb[\"MONTH_END\"].unique()))\n",
    "\n",
    "# correct mistakes in winter mb. If month end is 06 and mb is negative\n",
    "# correct to \"annual\"\n",
    "\n",
    "# Correct mislabeled winter MB:\n",
    "# If MONTH_END == 6 and MB is negative, it should be annual (not winter)\n",
    "mask_fix = ((df_pmb_clean[\"PERIOD\"] == \"winter\") &\n",
    "            (df_pmb_clean[\"MONTH_END\"] == '06') &\n",
    "            (df_pmb_clean[\"POINT_BALANCE\"] < 0))\n",
    "\n",
    "print(\"Rows to relabel winter -> annual:\", mask_fix.sum())\n",
    "\n",
    "df_pmb_clean.loc[mask_fix, \"PERIOD\"] = \"annual\"\n",
    "\n",
    "# For now, don't remove anything:\n",
    "# Save to csv:\n",
    "df_pmb_clean.to_csv(os.path.join(cfg.dataPath, path_PMB_GLACIOCLIM_csv,\n",
    "                                 'FR_wgms_dataset_all.csv'),\n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
