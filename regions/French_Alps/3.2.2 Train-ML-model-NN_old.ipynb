{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.glacioclim_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_FR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.FranceConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/GLACIOCLIM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "colors = get_cmap_hex(cm.batlow, 10)\n",
    "color_dark_blue = colors[0]\n",
    "color_pink = '#c51b7d'\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\",\n",
    "    \"slope\",\n",
    "    \"hugonnet_dhdt\",\n",
    "    \"consensus_ice_thickness\",\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm_with_blanc.csv')\n",
    "\n",
    "print('Number of glaciers:', len(data_glacioclim['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_glacioclim[data_glacioclim.PERIOD == 'annual']) + len(data_glacioclim[data_glacioclim.PERIOD == 'winter']) + len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_glacioclim[data_glacioclim.PERIOD == 'winter']))\n",
    "#print('Number of summer samples:',\n",
    "      #len(data_glacioclim[data_glacioclim.PERIOD == 'summer']))\n",
    "\n",
    "data_glacioclim = data_glacioclim[data_glacioclim['PERIOD'] != 'summer']\n",
    "\n",
    "data_glacioclim.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "### Input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_glacioclim_test = data_glacioclim.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_glacioclim_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file = 'FR_wgms_dataset_monthly_full_no_summer_with_blanc.csv')\n",
    "data_monthly = dataloader_gl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_glaciers = data_glacioclim_test['GLACIER'].value_counts()\n",
    "print(unique_glaciers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import ks_2samp\n",
    "import random\n",
    "\n",
    "# CONFIG: features to match distributions on and desired test fraction\n",
    "features = ['ELEVATION_DIFFERENCE', 't2m', 'ssrd', 'POINT_LAT', 'POINT_LON']   # extend if needed\n",
    "test_frac = 0.25\n",
    "K = 2   # number of strata/clusters\n",
    "seed = cfg.seed\n",
    "\n",
    "# 1) glacier-level summaries\n",
    "df = dataloader_gl.data.copy()\n",
    "gl_stats = df.groupby('GLACIER')[features].agg(['mean','std','count'])\n",
    "# flatten columns\n",
    "gl_stats.columns = ['_'.join(col).strip() for col in gl_stats.columns.values]\n",
    "gl_stats = gl_stats.reset_index()\n",
    "\n",
    "# use means (and optionally std) to form clustering features\n",
    "cluster_cols = [c for c in gl_stats.columns if c.endswith('_mean')]\n",
    "X = gl_stats[cluster_cols].fillna(0).values\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "\n",
    "# 2) KMeans strata\n",
    "kmeans = KMeans(n_clusters=K, random_state=seed, n_init=10)\n",
    "gl_stats['stratum'] = kmeans.fit_predict(Xs)\n",
    "\n",
    "# 3) stratified sampling of glaciers to get approx test_frac rows\n",
    "test_glaciers = []\n",
    "random.seed(seed)\n",
    "for s, gdf in gl_stats.groupby('stratum'):\n",
    "    glaciers_in_stratum = list(gdf['GLACIER'].values)\n",
    "    # choose by groups (glaciers) but try to maintain approximate row fraction:\n",
    "    # compute number of rows available per glacier then pick until reaching fraction\n",
    "    rows = df[df.GLACIER.isin(glaciers_in_stratum)].groupby('GLACIER').size().to_dict()\n",
    "    total_rows = sum(rows.values())\n",
    "    target_rows = int(total_rows * test_frac)\n",
    "    # greedy random pick until reach target_rows\n",
    "    gls = glaciers_in_stratum.copy()\n",
    "    random.shuffle(gls)\n",
    "    picked = []\n",
    "    acc = 0\n",
    "    for g in gls:\n",
    "        if acc >= target_rows:\n",
    "            break\n",
    "        picked.append(g)\n",
    "        acc += rows[g]\n",
    "    test_glaciers.extend(picked)\n",
    "\n",
    "# Ensure uniqueness and no overlap\n",
    "test_glaciers = list(dict.fromkeys(test_glaciers))\n",
    "existing_glaciers = set(df.GLACIER.unique())\n",
    "test_glaciers = [g for g in test_glaciers if g in existing_glaciers]\n",
    "\n",
    "train_glaciers = [g for g in existing_glaciers if g not in test_glaciers]\n",
    "\n",
    "data_test = df[df.GLACIER.isin(test_glaciers)].copy()\n",
    "data_train = df[df.GLACIER.isin(train_glaciers)].copy()\n",
    "\n",
    "print(\"Train rows:\", len(data_train), \"Test rows:\", len(data_test),\n",
    "      \"Ratio train/test rows ≈\", round(len(data_train)/max(1,len(data_test)),2))\n",
    "print(\"Train glaciers:\", len(train_glaciers), \"Test glaciers:\", len(test_glaciers))\n",
    "\n",
    "# 4) Diagnostics: KS test for each feature\n",
    "print(\"\\nKS p-values (train vs test) for key features:\")\n",
    "for f in features:\n",
    "    p = ks_2samp(data_train[f].dropna(), data_test[f].dropna()).pvalue\n",
    "    print(f, \"KS p:\", p, \"train mean/std:\", data_train[f].mean(), data_train[f].std(),\n",
    "          \"test mean/std:\", data_test[f].mean(), data_test[f].std())\n",
    "\n",
    "display('test :', test_glaciers)\n",
    "display('train :', train_glaciers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking on glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_glaciers = ['FR4N01146D09+E06 Gebroulaz']\n",
    "\n",
    "# Regional 50%\n",
    "test_glaciers = ['FR4N01236A07 de Talefre','FR4N01235A08 dArgentiere', 'FR4N01146D09+E06 Gebroulaz']\n",
    "\n",
    "# 3rd try 5-10% or 653 IDs\n",
    "#test_glaciers = ['FR4N01235A08 dArgentiere' ,'FR4N01146D09+E06 Gebroulaz' ,'FR4N01236A01 Mer de Glace/Geant','FR4N01162B09+154D03 de Saint Sorlin', 'FR4N01083B21 Blanc']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fr_mask = data_monthly['GLACIER'].isin(train_glaciers)\n",
    "train_fr_point_ids = (data_monthly.loc[train_fr_mask, 'POINT_ID']\n",
    "                      .dropna()\n",
    "                      .astype(str)\n",
    "                      .unique())\n",
    "n_train_fr_point_ids = len(train_fr_point_ids)\n",
    "n_train_fr_rows = int(train_fr_mask.sum())\n",
    "print(f\"Rows from FR train glaciers: {n_train_fr_rows}\")\n",
    "\n",
    "print(f\"Unique POINT_IDs in FR train glaciers: {n_train_fr_point_ids}\")\n",
    "\n",
    "\n",
    "display(data_monthly['GLACIER'].unique())\n",
    "\n",
    "glacier_counts = data_monthly['GLACIER'].value_counts().sort_values(ascending=False)\n",
    "display(glacier_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. CH Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_FR = pd.read_csv(cfg.dataPath + path_PMB_GLACIOCLIM_csv + 'FR_wgms_dataset_all_oggm_with_blanc.csv')\n",
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "# Adjust dfs to match\n",
    "data_CH['GLACIER_ZONE'] = 'Placeholder'\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "data_CH = data_CH.drop(columns=['aspect_sgi', 'slope_sgi', 'topo_sgi'], errors='ignore')\n",
    "\n",
    "data_FR = data_FR[data_FR['PERIOD'] != 'summer']\n",
    "\n",
    "# Merge FR with CH\n",
    "data_FR_CH = pd.concat([data_FR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(len(data_FR_CH['GLACIER'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_GLACIOCLIM_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_Alps.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_Alps.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data_glacioclim(run_flag=RUN,\n",
    "                                     df=data_FR_CH,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'FR_CH_wgms_dataset_monthly_full_with_blanc.csv')\n",
    "data_monthly_FR_CH = dataloader_gl.data\n",
    "\n",
    "display(data_monthly_FR_CH.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_glaciers = list(data_FR['GLACIER'].unique())\n",
    "\n",
    "# 4% 2nd try\n",
    "#test_glaciers = ['FR4N01235A08 dArgentiere' ,'FR4N01146D09+E06 Gebroulaz' ,'FR4N01236A01 Mer de Glace/Geant' ,'FR4N01236A01 Leschaux' ,'FR4N01162B09+154D03 de Saint Sorlin']\n",
    "\n",
    "# 3rd try 5-10% or 653 IDs\n",
    "test_glaciers = ['FR4N01235A08 dArgentiere' ,'FR4N01146D09+E06 Gebroulaz' ,'FR4N01236A01 Mer de Glace/Geant','FR4N01162B09+154D03 de Saint Sorlin', 'FR4N01083B21 Blanc']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = list(data_CH['GLACIER'].unique())\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count POINT_IDs for the selected glaciers\n",
    "glaciers_50 = []\n",
    "\n",
    "glaciers_5_10 = ['FR4N01163A02 de Sarennes 1', 'FR4N01236A07 de Talefre', 'FR4N01236A02 des Grands Montets', 'FR4N01236A01 Leschaux']\n",
    "\n",
    "mask = data_FR['GLACIER'].isin(glaciers_5_10)\n",
    "\n",
    "# total number of rows (samples) from these glaciers\n",
    "total_samples = data_FR.loc[mask].shape[0]\n",
    "\n",
    "# number of unique POINT_IDs across those glaciers\n",
    "unique_points = data_FR.loc[mask, 'POINT_ID'].nunique()\n",
    "\n",
    "# per-glacier unique POINT_ID counts\n",
    "per_glacier_counts = data_FR.loc[mask].groupby('GLACIER')['POINT_ID'].nunique()\n",
    "\n",
    "print(\"Total samples from selected glaciers:\", total_samples)\n",
    "print(\"Unique POINT_IDs (all selected glaciers):\", unique_points)\n",
    "print(\"Unique POINT_IDs per glacier:\")\n",
    "print(per_glacier_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train/val split 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train/val split 80/20 but only target domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pool = CH + France subset\n",
    "data_train = train_set['df_X'].copy()\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "# France train_glaciers\n",
    "france_train_glacier = [\n",
    "    g for g in data_FR['GLACIER'].unique()\n",
    "    if g not in test_glaciers\n",
    "]\n",
    "display('train glaciers from target domain: ', france_train_glacier)\n",
    "\n",
    "# Find France subset within this pool\n",
    "france_mask = data_train['GLACIER'].isin(france_train_glacier)\n",
    "data_france = data_train.loc[france_mask]\n",
    "\n",
    "# Split only the France subset\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_france)\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "france_train_idx = list(train_itr)\n",
    "france_val_idx = list(val_itr)\n",
    "\n",
    "# Training set = CH + France train portion\n",
    "df_X_train = pd.concat([\n",
    "    data_train.loc[~france_mask],                           # all CH glaciers\n",
    "    data_france.iloc[france_train_idx]                    # France train glaciers\n",
    "])\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Validation set = France val portion only\n",
    "df_X_val = data_france.iloc[france_val_idx]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_period_indicator(df):\n",
    "    \"\"\"Create numerical PERIOD_INDICATOR feature\"\"\"\n",
    "    df = df.copy()\n",
    "    df['PERIOD_INDICATOR'] = df['PERIOD'].map({'annual': 0, 'winter': 1})\n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "df_X_train = create_period_indicator(df_X_train)\n",
    "df_X_val = create_period_indicator(df_X_val)\n",
    "test_set['df_X'] = create_period_indicator(test_set['df_X'])\n",
    "\n",
    "print(\"PERIOD_INDICATOR created:\")\n",
    "print(\"Annual (0):\", (df_X_train['PERIOD_INDICATOR'] == 0).sum())\n",
    "print(\"Winter (1):\", (df_X_train['PERIOD_INDICATOR'] == 1).sum())\n",
    "print(\"Original PERIOD column preserved:\", df_X_train['PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate) # + ['PERIOD_INDICATOR']\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,    # Increase from 10\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,   # Reduced more aggressively (was 0.5)\n",
    "                              patience=5,   # Reduced patience (was 5)\n",
    "                              threshold=0.01,  # Reduced threshold (was 0.01)\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "\n",
    "custom_params = {\n",
    "    'lr': 0.001,    # reduced from 0.001\n",
    "    'batch_size': 128,  # Increased from 128\n",
    "    'module__layer0': 128,\n",
    "    'module__layer1': 96,\n",
    "    'module__layer2': 64,\n",
    "    'module__layer3': 32,\n",
    "    'module__dropout': 0.2,\n",
    "    'optimizer':torch.optim.Adam\n",
    "}\n",
    "\n",
    "params = custom_params\n",
    "\n",
    "args = {\n",
    "    'module': PeriodSpecificNetBigger,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__hidden_dim_0': params['module__layer0'],\n",
    "    'module__hidden_dim_1': params['module__layer1'], \n",
    "    'module__hidden_dim_2': params['module__layer2'],\n",
    "    'module__hidden_dim_3': params['module__layer3'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 300,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "    \n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)\n",
    "\n",
    "else:\n",
    "    # Load model and set to CPU\n",
    "    model_filename = \"nn_model_2025-09-03_50%_regional.pt\"  # Replace with actual date if needed\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg,\n",
    "        model_filename,\n",
    "        **{\n",
    "            **args,\n",
    "            **param_init\n",
    "        },\n",
    "    )\n",
    "    loaded_model = loaded_model.set_params(device='cpu')\n",
    "    loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and set to CPU\n",
    "model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"  # Replace with actual date if needed\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by glacier for detailed analysis\n",
    "print(\"\\nDetailed Performance Summary by Glacier:\")\n",
    "glacier_stats = grouped_ids.groupby('GLACIER').agg({\n",
    "    'target': ['count', 'mean', 'std'],\n",
    "    'pred': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "# Calculate RMSE and MAE per glacier\n",
    "glacier_rmse = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: np.sqrt(np.mean((x['target'] - x['pred'])**2))\n",
    ").round(4)\n",
    "\n",
    "glacier_mae = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: np.mean(np.abs(x['target'] - x['pred']))\n",
    ").round(4)\n",
    "\n",
    "glacier_r2 = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: 1 - np.sum((x['target'] - x['pred'])**2) / np.sum((x['target'] - x['target'].mean())**2)\n",
    ").round(4)\n",
    "\n",
    "glacier_rho = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: x['target'].corr(x['pred'])\n",
    ").round(4)\n",
    "\n",
    "# Combine all metrics\n",
    "performance_summary = pd.DataFrame({\n",
    "    'N_samples': glacier_stats[('target', 'count')],\n",
    "    'RMSE': glacier_rmse,\n",
    "    'MAE': glacier_mae,\n",
    "    'R²': glacier_r2,\n",
    "    'Pearson': glacier_rho,\n",
    "    'Target_mean': glacier_stats[('target', 'mean')],\n",
    "    'Target_std': glacier_stats[('target', 'std')]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "print(\"Performance by glacier (sorted by RMSE):\")\n",
    "display(performance_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Filter the data for the specified glacier and year\n",
    "data_2023 = grouped_ids[(grouped_ids['GLACIER'] == 'FR4N01083B21 Blanc') & (grouped_ids['YEAR'] == 2023)]\n",
    "\n",
    "# Filter the data for the same glacier but all other years\n",
    "data_other_years = grouped_ids[(grouped_ids['GLACIER'] == 'FR4N01083B21 Blanc') & (grouped_ids['YEAR'] != 2023)]\n",
    "\n",
    "data_all_years = grouped_ids[grouped_ids['GLACIER'] == 'FR4N01083B21 Blanc']\n",
    "\n",
    "# Calculate RMSE for 2023\n",
    "rmse_2023 = np.sqrt(mean_squared_error(data_2023['target'], data_2023['pred']))\n",
    "\n",
    "# Calculate RMSE for all other years\n",
    "rmse_other_years = np.sqrt(mean_squared_error(data_other_years['target'], data_other_years['pred']))\n",
    "\n",
    "rmse_all_years = np.sqrt(mean_squared_error(data_all_years['target'], data_all_years['pred']))\n",
    "\n",
    "# Print the results\n",
    "print(f\"RMSE for 2023: {rmse_2023}\")\n",
    "print(f\"RMSE for all other years: {rmse_other_years}\")\n",
    "print(f\"RMSE for all years: {rmse_all_years}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all rows where GLACIER is 'FR4N01083B21 Blanc'\n",
    "display(grouped_ids[grouped_ids['GLACIER'] == 'FR4N01083B21 Blanc'])\n",
    "\n",
    "display(grouped_ids[(grouped_ids['GLACIER'] == 'FR4N01083B21 Blanc') & (grouped_ids['YEAR'] == '2023')])\n",
    "\n",
    "# Reset pandas display options to default if needed\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "display(grouped_ids[abs(grouped_ids['target'] - grouped_ids['pred']) > 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined_NN(grouped_ids, region_name='CH Train FR Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "predVSTruth_all(grouped_ids, mae, rmse, title='NN on test')\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN components: Gradient Reversal, network, regressor wrapper, and dataset bindings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skorch.utils import to_tensor\n",
    "import massbalancemachine as mbm\n",
    "from pathlib import Path\n",
    "\n",
    "# Gradient Reversal Layer\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class GradReverse(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class DANNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor identical to FlexibleNetwork's trunk, with two heads:\n",
    "    - Regressor head for SMB (label)\n",
    "    - Domain classifier head (binary)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout=0.2, use_batchnorm=False, domain_hidden=64, grl_lambda=1.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(hidden_layers)\n",
    "        for hidden_dim, drop_rate in zip(hidden_layers, dropout):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            current_dim = hidden_dim\n",
    "        # trunk outputs the last hidden representation\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        # heads\n",
    "        self.regressor = nn.Linear(current_dim, 1)\n",
    "        self.grl = GradReverse(lambda_=grl_lambda)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(current_dim, domain_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout[-1] if isinstance(dropout, list) else dropout),\n",
    "            nn.Linear(domain_hidden, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)\n",
    "        y_pred = self.regressor(h)\n",
    "        d_logits = self.domain_classifier(self.grl(h))\n",
    "        return y_pred, d_logits\n",
    "\n",
    "# Dataset that yields domain labels padded per ID to match monthly padding\n",
    "class DomainTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, aggregated_dataset):\n",
    "        self.base = aggregated_dataset\n",
    "        self.meta_has_domain = 'DOMAIN' in self.base.metadataColumns\n",
    "        if not self.meta_has_domain:\n",
    "            # fallback: try to read from features\n",
    "            assert 'DOMAIN' in self.base.cfg.featureColumns, \"DOMAIN must be in metadata or featureColumns\"\n",
    "            self.domain_feat_idx = self.base.cfg.featureColumns.index('DOMAIN')\n",
    "        else:\n",
    "            self.domain_idx = self.base.metadataColumns.index('DOMAIN')\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.base._getInd(index)\n",
    "        if self.meta_has_domain:\n",
    "            dval = self.base.metadata[ind[0]][self.domain_idx]\n",
    "        else:\n",
    "            dval = self.base.features[ind[0], self.domain_feat_idx]\n",
    "        dpad = np.empty(self.base.maxConcatNb, dtype=np.float32)\n",
    "        dpad.fill(np.nan)\n",
    "        dpad[:len(ind)] = dval\n",
    "        return dpad\n",
    "\n",
    "# Binding that returns (X, (y, d)) so y_true in get_loss can contain both\n",
    "class CombinedTargetBinding(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_slice, y_slice, d_dataset):\n",
    "        self.X = X_slice\n",
    "        self.y = y_slice\n",
    "        self.d = d_dataset\n",
    "        assert len(self.X) == len(self.y) == len(self.d)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], (self.y[idx], self.d[idx])\n",
    "\n",
    "# Skorch regressor that adds domain-adversarial loss on top of the SMB loss\n",
    "class CustomDANNRegressor(mbm.models.CustomNeuralNetRegressor):\n",
    "    def __init__(self, cfg, *args, dan_lambda=0.1, **kwargs):\n",
    "        super().__init__(cfg, *args, **kwargs)\n",
    "        self.dan_lambda = dan_lambda\n",
    "        self._last_domain_logits = None\n",
    "\n",
    "    def infer(self, x, **fit_params):\n",
    "        x = to_tensor(x, device=self.device)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        x, indNonNan = self._unpack_inp(x)\n",
    "        if self.modelDtype is not None:\n",
    "            x = x.type(self.modelDtype)\n",
    "        outputs = self.module_(x, **fit_params)\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_monthly, d_monthly = outputs\n",
    "            y_packed = self._pack_out(y_monthly, indNonNan)\n",
    "            d_packed = self._pack_out(d_monthly, indNonNan)\n",
    "            self._last_domain_logits = d_packed\n",
    "            return y_packed\n",
    "        else:\n",
    "            return self._pack_out(outputs, indNonNan)\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        # y_true can be (label_pad, domain_pad) from CombinedTargetBinding\n",
    "        if isinstance(y_true, (tuple, list)) and len(y_true) == 2:\n",
    "            y_true_labels, y_true_domain = y_true\n",
    "        else:\n",
    "            y_true_labels, y_true_domain = y_true, None\n",
    "\n",
    "        # Label loss (same as base implementation)\n",
    "        loss = 0.0\n",
    "        cnt = 0\n",
    "        for yi_pred, yi_true in zip(y_pred, y_true_labels):\n",
    "            valid = ~torch.isnan(yi_pred)\n",
    "            if valid.any():\n",
    "                pred_sum = yi_pred[valid].sum()\n",
    "                true_mean = yi_true[valid].mean()\n",
    "                loss = loss + (pred_sum - true_mean) ** 2\n",
    "                cnt += 1\n",
    "        label_loss = loss / max(cnt, 1)\n",
    "\n",
    "        # Domain loss (optional during training)\n",
    "        \"\"\"\"\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits\n",
    "            mask = ~torch.isnan(y_true_domain)\n",
    "            if mask.any():\n",
    "                domain_loss = F.binary_cross_entropy_with_logits(d_logits[mask], y_true_domain[mask].float())\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean, otherwise IDs with longer months have higher domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # shape: [batch, max_months] (or [batch, max_months, 1])\n",
    "            per_id_losses = []\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                # squeeze trailing dim if present\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                # mask valid months\n",
    "                mask = ~torch.isnan(d_true_row) # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    per_id_losses.append(loss_i)\n",
    "\n",
    "            if len(per_id_losses) > 0:\n",
    "                domain_loss = torch.stack(per_id_losses).mean()  # mean over IDs\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean for each domain\n",
    "        # otherwise IDs with longer months have higher domain loss and CH domain loss with more data is exaggerated\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # [batch_ids, max_months(,1)]\n",
    "            per_id_losses_ch, per_id_losses_nor = [], []\n",
    "\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                mask = ~torch.isnan(d_true_row)  # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    # domain for this ID (same across valid months)\n",
    "                    dom_i = int(d_true_row[mask][0].item())  # 0=CH, 1=NOR\n",
    "                    (per_id_losses_ch if dom_i == 0 else per_id_losses_nor).append(loss_i)\n",
    "\n",
    "            parts = []\n",
    "            if len(per_id_losses_ch) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_ch).mean())\n",
    "            if len(per_id_losses_nor) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_nor).mean())\n",
    "            if len(parts) > 0:\n",
    "                domain_loss = torch.stack(parts).mean()\n",
    "\n",
    "        return label_loss + self.dan_lambda * domain_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(cfg, fname: str, *args, **kwargs):\n",
    "        \"\"\"Loads a pre-trained DANN model from a file.\"\"\"\n",
    "        model = CustomDANNRegressor(cfg, *args, **kwargs)\n",
    "        model.initialize()\n",
    "        models_dir = Path(\"./models\")\n",
    "        model.load_params(f_params=models_dir / fname)\n",
    "        return model\n",
    "\n",
    "params_dann = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "    'module__domain_hidden': 64,\n",
    "    'module__grl_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# Use DANN network\n",
    "args_dann = {\n",
    "    'module': DANNNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params_dann['module__dropout'],\n",
    "    'module__hidden_layers': params_dann['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params_dann['module__use_batchnorm'],\n",
    "    'module__domain_hidden': params_dann['module__domain_hidden'],\n",
    "    'module__grl_lambda': params_dann['module__grl_lambda'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params_dann['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params_dann['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params_dann['optimizer'],\n",
    "    'optimizer__weight_decay': params_dann['optimizer__weight_decay'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_pmb_multi(models_grouped, title_prefix='Mean PMB'):\n",
    "    \"\"\"\n",
    "    models_grouped: dict[str -> DataFrame] where each DF has columns\n",
    "      ['target','ID','pred','PERIOD','GLACIER','YEAR'] as returned by loader above.\n",
    "    \"\"\"\n",
    "    # Use the first model’s DF as reference for ground-truth shading (same targets for all)\n",
    "    first_label = next(iter(models_grouped))\n",
    "    df_ref = models_grouped[first_label].copy()\n",
    "\n",
    "    # Prepare figure\n",
    "    fig, (ax_ann, ax_win) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax_ann.set_title('Mean annual PMB', fontsize=18)\n",
    "    ax_win.set_title('Mean winter PMB', fontsize=18)\n",
    "\n",
    "    # Colors for models\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=len(models_grouped))\n",
    "    label_to_color = {label: palette[i] for i, label in enumerate(models_grouped.keys())}\n",
    "\n",
    "    def _compute_metrics(df_y):\n",
    "        valid = df_y[['pred', 'target']].dropna()\n",
    "        if len(valid) == 0:\n",
    "            return np.nan, np.nan\n",
    "        y_pred = valid['pred'].values\n",
    "        y_true = valid['target'].values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "        rho = np.corrcoef(y_pred, y_true)[0, 1] if y_pred.size > 1 else np.nan\n",
    "        return rmse, rho\n",
    "\n",
    "    # Helper to plot one panel (annual or winter)\n",
    "    def _plot_panel(ax, period_name):\n",
    "        df_period = df_ref[df_ref['PERIOD'] == period_name].copy()\n",
    "        years = np.sort(df_period['YEAR'].unique())\n",
    "\n",
    "        # Ground-truth mean line (black)\n",
    "        tgt_mean = df_period.groupby('YEAR')['target'].mean().reindex(years)\n",
    "        ax.plot(years, tgt_mean, color=\"black\", label=\"target mean\", linewidth=2)\n",
    "        ax.scatter(years, tgt_mean, color=\"black\", marker='x', s=20)\n",
    "\n",
    "        # Plot each model’s mean prediction per year and compute metrics (per period)\n",
    "        for label, gdf in models_grouped.items():\n",
    "            gdf_p = gdf[gdf['PERIOD'] == period_name].copy()\n",
    "            pred_mean = gdf_p.groupby('YEAR')['pred'].mean().reindex(years)\n",
    "\n",
    "            # Compute RMSE and Pearson r for this period (use per-ID means)\n",
    "            metrics_df = pd.DataFrame({\n",
    "                'YEAR': years,\n",
    "                'pred': pred_mean.values,\n",
    "            }).merge(\n",
    "                df_period.groupby('YEAR')['target'].mean().reset_index(), on='YEAR', how='left'\n",
    "            ).rename(columns={'target': 'target'})\n",
    "            rmse, rho = _compute_metrics(metrics_df)\n",
    "\n",
    "            label_with_metrics = f\"{label} (RMSE={np.nan_to_num(rmse):.2f}, \\u03C1={np.nan_to_num(rho):.2f})\"\n",
    "            ax.plot(years, pred_mean, label=label_with_metrics,\n",
    "                    color=label_to_color.get(label, None), linestyle='--')\n",
    "            #ax.scatter(years, pred_mean, color=label_to_color.get(label, None), s=15)\n",
    "\n",
    "        ax.set_xlabel('YEAR')\n",
    "        ax.set_ylabel('PMB')\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.legend(fontsize=6, loc='best')\n",
    "\n",
    "    _plot_panel(ax_ann, 'annual')\n",
    "    _plot_panel(ax_win, 'winter')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "base = grouped_ids[['ID', 'target', 'PERIOD', 'GLACIER', 'YEAR']].copy()\n",
    "\n",
    "# List models (optionally mix NN and DANN). Provide up to 5.\n",
    "\n",
    "#5-10%\n",
    "models = [\n",
    "    #('CH Baseline',  'nn_model_2025-07-14_CH_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Regional Baseline',  'nn_model_2025-08-24_5-10%_3rd_try_regional_baseline.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('CH and Regional Baseline',  'nn_model_2025-08-24_5-10%_CH_and_5-10%_baseline_only_FR_val.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Fine-tuning',  'nn_model_finetuned_2025-08-24_5-10%__3rd_try_fine_tuning_lr0.0005_epoch_10.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Batchnorm Unfrozen',  'nn_model_finetuned_2025-08-24_5-10%_3rd_try_batchnorm_unfrozen_linear_frozen_lr0.05_epoch200.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('DANN',      'dann_model_2025-08-24_5-10%_lamba0_05_mean_domainloss_even_domainloss_only_FR_val_50_50epochsplit.pt', CustomDANNRegressor),\n",
    "]\n",
    "\n",
    "device = 'cpu'\n",
    "# Build loader args for plain NN (FlexibleNetwork) and DANN (DANNNetwork)\n",
    "nn_loader_args = {**args, **param_init}\n",
    "try:\n",
    "    _ = DANNNetwork  # ensure defined (from the DANN cell below)\n",
    "    dann_loader_args = {\n",
    "        **{\n",
    "            **args,\n",
    "            'module': DANNNetwork,\n",
    "            'module__domain_hidden': params.get('module__domain_hidden', 64),\n",
    "            'module__grl_lambda': params.get('module__grl_lambda', 1.0),\n",
    "        },\n",
    "        'dan_lambda': 0.05,  # inference-only, safe if different from train\n",
    "        **param_init,\n",
    "    }\n",
    "except NameError:\n",
    "    dann_loader_args = None  # will raise a clear error if DANN is requested\n",
    "\n",
    "models_grouped = {}\n",
    "\n",
    "for label, fname, model_cls in models:\n",
    "    print(f'Loading {label}: {fname}')\n",
    "    # Pick correct architecture for this checkpoint\n",
    "    if model_cls.__name__ == 'CustomDANNRegressor':\n",
    "        if dann_loader_args is None:\n",
    "            raise NameError(\"DANNNetwork not available. Run the DANN components cell before loading DANN models.\")\n",
    "        mdl = model_cls.load_model(cfg, fname, **dann_loader_args)\n",
    "    else:\n",
    "        mdl = model_cls.load_model(cfg, fname, **nn_loader_args)\n",
    "\n",
    "    mdl = mdl.set_params(device=device).to(device)\n",
    "\n",
    "    # Predict aggregated by measurement ID using the same dataset_test\n",
    "    y_pred_agg = mdl.aggrPredict(dataset_test[0])\n",
    "\n",
    "    assert len(y_pred_agg) == len(base), f'Length mismatch for {label}'\n",
    "    gdf = base.copy()\n",
    "    gdf['pred'] = y_pred_agg\n",
    "    models_grouped[label] = gdf\n",
    "\n",
    "# Plot mean annual and mean winter PMB for all models together\n",
    "plot_mean_pmb_multi(models_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_pmb_multi_fixed_lines(models_grouped, title_prefix='Mean PMB'):\n",
    "\n",
    "    # Use the first model's DF as reference for ground-truth shading (same targets for all)\n",
    "    first_label = next(iter(models_grouped))\n",
    "    df_ref = models_grouped[first_label].copy()\n",
    "\n",
    "    # Prepare figure\n",
    "    fig, (ax_ann, ax_win) = plt.subplots(2, 1, figsize=(12, 8))  # Increased height more\n",
    "\n",
    "    # Add titles to each subplot with more padding\n",
    "    ax_ann.set_title('Mean annual point mass balance', fontsize=14, fontweight='bold', pad=30)\n",
    "    ax_win.set_title('Mean winter point mass balance', fontsize=14, fontweight='bold', pad=30)\n",
    "\n",
    "    # Colors for models\n",
    "    palette = sns.color_palette(\"colorblind\", n_colors=len(models_grouped))\n",
    "    label_to_color = {label: palette[i] for i, label in enumerate(models_grouped.keys())}\n",
    "\n",
    "    # Define line styles for regional and transfer learning models\n",
    "    regional_models = ['Regional Baseline', 'CH and Regional Baseline']\n",
    "    transfer_models = ['Fine-tuning', 'Batchnorm Unfrozen', 'DANN']\n",
    "    line_styles = {model: '-' for model in regional_models}  # Solid lines for regional models\n",
    "    line_styles.update({model: '--' for model in transfer_models})  # Dashed lines for transfer models\n",
    "\n",
    "    def _compute_metrics(df_y):\n",
    "        valid = df_y[['pred', 'target']].dropna()\n",
    "        if len(valid) == 0:\n",
    "            return np.nan, np.nan\n",
    "        y_pred = valid['pred'].values\n",
    "        y_true = valid['target'].values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "        rho = np.corrcoef(y_pred, y_true)[0, 1] if y_pred.size > 1 else np.nan\n",
    "        return rmse, rho\n",
    "\n",
    "    # Helper to plot one panel (annual or winter)\n",
    "    def _plot_panel(ax, period_name):\n",
    "        df_period = df_ref[df_ref['PERIOD'] == period_name].copy()\n",
    "        years = np.sort(df_period['YEAR'].unique())\n",
    "\n",
    "        # Ground-truth mean line (black) - only connect consecutive years\n",
    "        tgt_mean = df_period.groupby('YEAR')['target'].mean()\n",
    "        available_years = tgt_mean.index.values\n",
    "        available_values = tgt_mean.values\n",
    "        \n",
    "        # Plot line segments only between consecutive years with x markers\n",
    "        target_line_plotted = False\n",
    "        for i in range(len(available_years) - 1):\n",
    "            if available_years[i+1] - available_years[i] == 1:  # consecutive years\n",
    "                if not target_line_plotted:\n",
    "                    # First line segment gets the label with line and x markers\n",
    "                    ax.plot([available_years[i], available_years[i+1]], \n",
    "                           [available_values[i], available_values[i+1]], \n",
    "                           color=\"black\", linewidth=2, marker='x', markersize=6,\n",
    "                           label=\"target mean\")\n",
    "                    target_line_plotted = True\n",
    "                else:\n",
    "                    # Subsequent segments without label but with x markers\n",
    "                    ax.plot([available_years[i], available_years[i+1]], \n",
    "                           [available_values[i], available_values[i+1]], \n",
    "                           color=\"black\", linewidth=2, marker='x', markersize=6)\n",
    "        \n",
    "        # If no line was plotted (all isolated points), plot isolated points with x markers\n",
    "        if not target_line_plotted:\n",
    "            ax.plot(available_years, available_values, color=\"black\", marker='x', markersize=6,\n",
    "                   linestyle='None', label=\"target mean\")\n",
    "\n",
    "        # Plot each model's mean prediction per year and compute metrics (per period)\n",
    "        for label, gdf in models_grouped.items():\n",
    "            gdf_p = gdf[gdf['PERIOD'] == period_name].copy()\n",
    "            pred_mean = gdf_p.groupby('YEAR')['pred'].mean()\n",
    "\n",
    "            # Compute RMSE and R² for this period (use per-ID means)\n",
    "            metrics_df = pd.DataFrame({\n",
    "                'YEAR': pred_mean.index,\n",
    "                'pred': pred_mean.values,\n",
    "            }).merge(\n",
    "                df_period.groupby('YEAR')['target'].mean().reset_index(), on='YEAR', how='left'\n",
    "            )\n",
    "            rmse, rho = _compute_metrics(metrics_df)\n",
    "\n",
    "            label_with_metrics = f\"{label} (RMSE={np.nan_to_num(rmse):.2f}, \\u03C1={np.nan_to_num(rho):.2f})\"\n",
    "            \n",
    "            # Plot line segments only between consecutive years\n",
    "            pred_years = pred_mean.index.values\n",
    "            pred_values = pred_mean.values\n",
    "            \n",
    "            label_added = False  # Track if we've added the label for this model\n",
    "            \n",
    "            for i in range(len(pred_years) - 1):\n",
    "                if pred_years[i+1] - pred_years[i] == 1:  # consecutive years\n",
    "                    ax.plot([pred_years[i], pred_years[i+1]], \n",
    "                           [pred_values[i], pred_values[i+1]], \n",
    "                           color=label_to_color.get(label, None), \n",
    "                           linestyle=line_styles.get(label, '-'),\n",
    "                           label=label_with_metrics if not label_added else \"\")\n",
    "                    label_added = True\n",
    "            \n",
    "            # If no consecutive years were found, add a dummy line for the legend\n",
    "            if not label_added and len(pred_years) > 0:\n",
    "                ax.plot([], [], \n",
    "                       color=label_to_color.get(label, None), \n",
    "                       linestyle=line_styles.get(label, '-'),\n",
    "                       label=label_with_metrics)\n",
    "\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    _plot_panel(ax_ann, 'annual')\n",
    "    _plot_panel(ax_win, 'winter')\n",
    "\n",
    "    # Add legends with much more spacing to avoid overlap\n",
    "    ax_ann.legend(fontsize=8, loc='upper center', ncol=3, frameon=True, bbox_to_anchor=(0.5, 1.22))\n",
    "    ax_win.legend(fontsize=8, loc='upper center', ncol=3, frameon=True, bbox_to_anchor=(0.5, 1.22))\n",
    "\n",
    "    # Add y-axis labels\n",
    "    ax_ann.set_ylabel('[m w.e.]')\n",
    "    ax_win.set_ylabel('[m w.e.]')\n",
    "\n",
    "    # Adjust layout to make space for the legends and titles\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.8))  # Much more space at top\n",
    "    plt.show()\n",
    "\n",
    "plot_mean_pmb_multi_fixed_lines(models_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_comparison_table(models_grouped, periods=['annual', 'winter', 'combined']):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    # Define the desired order of models\n",
    "    desired_order = [\n",
    "        'Regional Baseline',\n",
    "        'CH and Regional Baseline', \n",
    "        'Fine-tuning',\n",
    "        'Batchnorm Unfrozen',\n",
    "        'DANN'\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process models in the desired order\n",
    "    for model_name in desired_order:\n",
    "        if model_name not in models_grouped:\n",
    "            continue  # Skip if model not found\n",
    "            \n",
    "        df = models_grouped[model_name]\n",
    "        # Filter out summer data (keep only annual and winter)\n",
    "        df_filtered = df[df['PERIOD'].isin(['annual', 'winter'])].copy()\n",
    "        \n",
    "        # Calculate metrics for each period\n",
    "        for period in periods:\n",
    "            if period == 'combined':\n",
    "                # Use all data (annual + winter)\n",
    "                period_data = df_filtered\n",
    "                period_label = 'Combined'\n",
    "            else:\n",
    "                # Use specific period data\n",
    "                period_data = df_filtered[df_filtered['PERIOD'] == period]\n",
    "                period_label = period.capitalize()\n",
    "            \n",
    "            if len(period_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            y_true = period_data['target'].values\n",
    "            y_pred = period_data['pred'].values\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            rho = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "            \n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Period': period_label,\n",
    "                'N_samples': len(period_data),\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'ρ': rho,\n",
    "                'R²': r2,\n",
    "                \n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Pivot table for better display\n",
    "    metrics = ['N_samples', 'MAE', 'RMSE', 'ρ', 'R²']\n",
    "\n",
    "    # Create a multi-index table\n",
    "    pivot_tables = []\n",
    "    for metric in metrics:\n",
    "        pivot = results_df.pivot(index='Model', columns='Period', values=metric)\n",
    "        pivot.name = metric\n",
    "        pivot_tables.append(pivot)\n",
    "    \n",
    "    # Combine all metrics into one table with multi-level columns\n",
    "    combined_table = pd.concat(pivot_tables, axis=1, keys=metrics)\n",
    "    \n",
    "    # Reorder the index (models) to maintain desired order\n",
    "    combined_table = combined_table.reindex(desired_order)\n",
    "    \n",
    "    # Reorder columns to have Combined, Annual, Winter for each metric\n",
    "    if 'Combined' in combined_table.columns.get_level_values(1):\n",
    "        period_order = ['Combined', 'Annual', 'Winter']\n",
    "    else:\n",
    "        period_order = ['Annual', 'Winter']\n",
    "    \n",
    "    # Reorder columns\n",
    "    new_columns = []\n",
    "    for metric in metrics:\n",
    "        for period in period_order:\n",
    "            if (metric, period) in combined_table.columns:\n",
    "                new_columns.append((metric, period))\n",
    "    \n",
    "    combined_table = combined_table[new_columns]\n",
    "    \n",
    "    # Round numerical values for better display\n",
    "    numeric_metrics = ['MAE', 'RMSE', 'ρ', 'R²']\n",
    "    for metric in numeric_metrics:\n",
    "        for period in period_order:\n",
    "            if (metric, period) in combined_table.columns:\n",
    "                combined_table[(metric, period)] = combined_table[(metric, period)].round(3)\n",
    "    \n",
    "    return combined_table\n",
    "\n",
    "def display_model_comparison_table(models_grouped):\n",
    "    \"\"\"\n",
    "    Display a nicely formatted comparison table for all models.\n",
    "    \"\"\"\n",
    "    table = create_model_comparison_table(models_grouped)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    # Display the table\n",
    "    print(table.to_string())\n",
    "    \n",
    "    print()\n",
    "    print(\"Metrics explanation:\")\n",
    "    print(\"- N_samples: Number of data points\")\n",
    "    print(\"- MAE: Mean Absolute Error [m w.e.]\")\n",
    "    print(\"- RMSE: Root Mean Square Error [m w.e.]\") \n",
    "    print(\"- R²: Coefficient of determination\")\n",
    "    print(\"- ρ: Pearson correlation coefficient\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return table\n",
    "\n",
    "# Generate and display the comparison table\n",
    "comparison_table = display_model_comparison_table(models_grouped)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "comparison_table.to_csv('5-10%_model_comparison_fr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_obs_grid(models_grouped, ncols=3, point_size=45, region_name='CH->FR'):\n",
    "    \"\"\"\n",
    "    Grid of scatter plots (Observed vs Predicted) per model.\n",
    "    Summer excluded.\n",
    "    Global legend: Annual points, Winter points.\n",
    "    Per‑subplot legend: 1:1 line + Combined/Annual/Winter metrics.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    period_colors = {'annual': '#e31a1c', 'winter': '#1f78b4'}\n",
    "    labels = list(models_grouped.keys())\n",
    "    n_models = len(labels)\n",
    "    ncols_eff = min(ncols, n_models)\n",
    "    nrows = int(np.ceil(n_models / ncols_eff))\n",
    "\n",
    "    # Global axis limits\n",
    "    all_targets = np.concatenate([models_grouped[l].target.values for l in labels])\n",
    "    all_preds   = np.concatenate([models_grouped[l].pred.values   for l in labels])\n",
    "    gmin = np.nanmin([all_targets.min(), all_preds.min()])\n",
    "    gmax = np.nanmax([all_targets.max(), all_preds.max()])\n",
    "    pad = 0.05 * (gmax - gmin)\n",
    "    gmin -= pad\n",
    "    gmax += pad\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols_eff,\n",
    "                             figsize=(4.4 * ncols_eff, 4.6 * nrows),\n",
    "                             squeeze=False)\n",
    "    metrics_all = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        r, c = divmod(i, ncols_eff)\n",
    "        ax = axes[r][c]\n",
    "        df_plot = models_grouped[label]\n",
    "        df_plot = df_plot[df_plot.PERIOD.isin(['annual','winter'])].copy()\n",
    "\n",
    "        # Metrics per period\n",
    "        panel_metrics = {}\n",
    "        for period in ['annual','winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                y_t = sub.target.values\n",
    "                y_p = sub.pred.values\n",
    "                rmse = np.sqrt(np.mean((y_p - y_t)**2))\n",
    "                rho  = np.corrcoef(y_t, y_p)[0,1] if len(sub) > 1 else np.nan\n",
    "                panel_metrics[period] = (rmse, rho)\n",
    "\n",
    "        # Combined\n",
    "        y_t_all = df_plot.target.values\n",
    "        y_p_all = df_plot.pred.values\n",
    "        rmse_all = np.sqrt(np.mean((y_p_all - y_t_all)**2))\n",
    "        rho_all  = np.corrcoef(y_t_all, y_p_all)[0,1] if len(df_plot) > 1 else np.nan\n",
    "        panel_metrics['combined'] = (rmse_all, rho_all)\n",
    "        metrics_all[label] = panel_metrics\n",
    "\n",
    "        # Scatter (no per‑subplot point handles; handled globally)\n",
    "        for period in ['annual','winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                ax.scatter(sub.target, sub.pred,\n",
    "                           s=point_size, alpha=0.65, edgecolor='none',\n",
    "                           color=period_colors[period])\n",
    "\n",
    "        # 1:1 line + handle for subplot legend\n",
    "        ax.plot([gmin,gmax],[gmin,gmax],'k--',linewidth=1,alpha=0.55)\n",
    "        \n",
    "\n",
    "        # Metric legend entries\n",
    "        def metric_handle(name, key):\n",
    "            rmse, rho = panel_metrics[key]\n",
    "            return Line2D([],[], linestyle='',\n",
    "                          label=f\"{name}: RMSE {rmse:.2f} m w.e., rho:{rho:.2f}\",\n",
    "                          color='none')\n",
    "\n",
    "        metric_handles = [\n",
    "            metric_handle(\"Combined\", 'combined'),\n",
    "            *( [metric_handle(\"Annual\", 'annual')] if 'annual' in panel_metrics else [] ),\n",
    "            *( [metric_handle(\"Winter\", 'winter')] if 'winter' in panel_metrics else [] ),\n",
    "        ]\n",
    "\n",
    "        ax.set_xlim(gmin,gmax)\n",
    "        ax.set_ylim(gmin,gmax)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xlabel('Observed PMB', fontsize=9)\n",
    "        ax.set_ylabel('Predicted PMB', fontsize=9)\n",
    "\n",
    "        # Subplot legend: only 1:1 + metrics\n",
    "        ax.legend(handles=metric_handles,\n",
    "                  fontsize=7, loc='upper left',\n",
    "                  frameon=True, handlelength=1.2, borderpad=0.5)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(n_models, nrows * ncols_eff):\n",
    "        r, c = divmod(j, ncols_eff)\n",
    "        axes[r][c].axis('off')\n",
    "\n",
    "    # Global legend for point types (annual / winter)\n",
    "    global_point_handles = [\n",
    "        Line2D([0],[0], marker='o', linestyle='', color=period_colors['annual'],\n",
    "               label='Annual'),\n",
    "        Line2D([0],[0], marker='o', linestyle='', color=period_colors['winter'],\n",
    "               label='Winter')\n",
    "    ]\n",
    "    fig.legend(handles=global_point_handles,\n",
    "               loc='upper center',\n",
    "               ncol=2,\n",
    "               frameon=True,\n",
    "               fontsize=9,\n",
    "               bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    plt.tight_layout(rect=(0,0,1,0.95))\n",
    "    return metrics_all\n",
    "\n",
    "pred_grid_metrics = plot_pred_vs_obs_grid(models_grouped, ncols=3, region_name='CH->FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def PlotIndividualGlacierPredVsTruth_Multi(models_grouped, base_figsize=(20, 15), height_per_row=5, ncols_models=2):\n",
    "    \n",
    "    # Get unique glaciers from the first model (should be same for all)\n",
    "    first_model = next(iter(models_grouped.values()))\n",
    "    unique_glaciers = first_model['GLACIER'].unique()\n",
    "    n_glaciers = len(unique_glaciers)\n",
    "    \n",
    "    # Color palette for periods\n",
    "    color_palette_period = {\n",
    "        'annual': '#e31a1c',\n",
    "        'winter': '#1f78b4'\n",
    "    }\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_models = len(models_grouped)\n",
    "    ncols_total = min(ncols_models, n_models)\n",
    "    nrows_models = int(np.ceil(n_models / ncols_total))\n",
    "    \n",
    "    # Create figure with subplots for each glacier\n",
    "    fig = plt.figure(figsize=(base_figsize[0], n_glaciers * height_per_row))\n",
    "    \n",
    "    for glacier_idx, glacier_name in enumerate(unique_glaciers):\n",
    "        # Create a grid for this glacier's models\n",
    "        for model_idx, (model_name, model_df) in enumerate(models_grouped.items()):\n",
    "            \n",
    "            # Calculate subplot position\n",
    "            subplot_idx = glacier_idx * n_models + model_idx + 1\n",
    "            ax = plt.subplot(n_glaciers, n_models, subplot_idx)\n",
    "            \n",
    "            # Filter data for this glacier\n",
    "            df_gl = model_df[model_df['GLACIER'] == glacier_name].copy()\n",
    "            \n",
    "            if len(df_gl) == 0:\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(f'{glacier_name}\\n{model_name}', fontsize=12)\n",
    "                continue\n",
    "            \n",
    "            # Calculate metrics including R²\n",
    "            y_true = df_gl['target'].values\n",
    "            y_pred = df_gl['pred'].values\n",
    "            \n",
    "            scores = {\n",
    "                'mse': mean_squared_error(y_true, y_pred),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                'mae': mean_absolute_error(y_true, y_pred),\n",
    "                'r2': r2_score(y_true, y_pred),\n",
    "                'pearson_corr': np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 else np.nan\n",
    "            }\n",
    "            \n",
    "            # Create scatter plot colored by period\n",
    "            for period in df_gl['PERIOD'].unique():\n",
    "                if period in color_palette_period:\n",
    "                    period_data = df_gl[df_gl['PERIOD'] == period]\n",
    "                    ax.scatter(period_data['target'], period_data['pred'], \n",
    "                             c=color_palette_period[period], \n",
    "                             label=period, alpha=0.7, s=50)\n",
    "            \n",
    "            # Add 1:1 line\n",
    "            min_val = min(df_gl['target'].min(), df_gl['pred'].min())\n",
    "            max_val = max(df_gl['target'].max(), df_gl['pred'].max())\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=1)\n",
    "\n",
    "            ax.set_aspect('equal', adjustable='box')\n",
    "            \n",
    "            # Add metrics text\n",
    "            metrics_text = (f\"RMSE: {scores['rmse']:.2f}\\n\"\n",
    "                          f\"MAE: {scores['mae']:.2f}\\n\" \n",
    "                          f\"R²: {scores['r2']:.2f}\\n\"\n",
    "                          f\"ρ: {scores['pearson_corr']:.2f}\")\n",
    "            \n",
    "            ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n",
    "                   verticalalignment='top', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Set labels and title\n",
    "            ax.set_xlabel('Observed PMB [m w.e.]', fontsize=11)\n",
    "            ax.set_ylabel('Predicted PMB [m w.e.]', fontsize=11)\n",
    "            \n",
    "            # Title shows glacier name for first model in row, model name for all\n",
    "            if model_idx == 0:\n",
    "                title = f'{glacier_name}\\n{model_name}'\n",
    "            else:\n",
    "                title = model_name\n",
    "            ax.set_title(title, fontsize=12)\n",
    "            \n",
    "            # Add legend only for first subplot of each glacier\n",
    "            if model_idx == 0:\n",
    "                ax.legend(fontsize=9, loc='lower right')\n",
    "            \n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth_Multi(models_grouped, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importances_per_glacier(data, features, target_col='POINT_BALANCE', min_samples=30):\n",
    "    \"\"\"Compute feature importances for each glacier using RandomForestRegressor.\"\"\"\n",
    "    glaciers = data['GLACIER'].unique()\n",
    "    importances = []\n",
    "    for glacier in glaciers:\n",
    "        df = data[data['GLACIER'] == glacier][features + [target_col]].dropna()\n",
    "        if len(df) >= min_samples:\n",
    "            rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            X = df[features]\n",
    "            y = df[target_col]\n",
    "            rf.fit(X, y)\n",
    "            importances.append({'GLACIER': glacier, **dict(zip(features, rf.feature_importances_)), 'N_SAMPLES': len(df)})\n",
    "    return pd.DataFrame(importances)\n",
    "\n",
    "# Features to use\n",
    "features = ['t2m', 'tp', 'ssrd', 'ELEVATION_DIFFERENCE']\n",
    "\n",
    "# Compute importances for all glaciers\n",
    "importances_df = compute_feature_importances_per_glacier(data_monthly_FR_CH, features)\n",
    "\n",
    "# Get Blanc's importances\n",
    "blanc_row = importances_df[importances_df['GLACIER'] == 'FR4N01083B21 Blanc']\n",
    "if not blanc_row.empty:\n",
    "    blanc_importances = blanc_row[features].values[0]\n",
    "    # Compute L1 distance to all other glaciers\n",
    "    importances_df['similarity_score'] = importances_df[features].apply(\n",
    "        lambda row: np.sum(np.abs(row.values - blanc_importances)), axis=1\n",
    "    )\n",
    "    # Exclude Blanc itself\n",
    "    similar_df = importances_df[importances_df['GLACIER'] != 'FR4N01083B21 Blanc'].sort_values('similarity_score')\n",
    "    print(\"Top 10 glaciers with most similar feature importances to Blanc:\")\n",
    "    display(similar_df[['GLACIER', 'similarity_score', 'N_SAMPLES'] + features].head(10))\n",
    "else:\n",
    "    print(\"Blanc not found in importances_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations for EACH glacier individually\n",
    "def analyze_individual_glacier_correlations(data_monthly_FR_CH):\n",
    "    \"\"\"\n",
    "    Calculate correlations for each glacier individually to see if Blanc is truly unique\n",
    "    \"\"\"\n",
    "    \n",
    "    # Features to analyze\n",
    "    features = ['t2m', 'tp', 'ssrd', 'ELEVATION_DIFFERENCE']\n",
    "    \n",
    "    # Calculate correlations for each glacier\n",
    "    glacier_correlations = []\n",
    "    \n",
    "    for glacier in data_monthly_FR_CH['GLACIER'].unique():\n",
    "        glacier_data = data_monthly_FR_CH[data_monthly_FR_CH['GLACIER'] == glacier]\n",
    "        \n",
    "        if len(glacier_data) >= 10:  # Need minimum samples for reliable correlation\n",
    "            correlations = {}\n",
    "            correlations['GLACIER'] = glacier\n",
    "            correlations['N_SAMPLES'] = len(glacier_data)\n",
    "            \n",
    "            for feature in features:\n",
    "                corr = glacier_data[['POINT_BALANCE', feature]].corr().iloc[0,1]\n",
    "                correlations[f'{feature}_corr'] = corr\n",
    "            \n",
    "            glacier_correlations.append(correlations)\n",
    "    \n",
    "    corr_df = pd.DataFrame(glacier_correlations)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=== INDIVIDUAL GLACIER CORRELATIONS ===\")\n",
    "    print(f\"Total glaciers analyzed: {len(corr_df)}\")\n",
    "    \n",
    "    # Sort by different features to see patterns\n",
    "    for feature in features:\n",
    "        print(f\"\\n--- {feature} correlation with POINT_BALANCE ---\")\n",
    "        feature_col = f'{feature}_corr'\n",
    "        sorted_df = corr_df.sort_values(feature_col, ascending=False)\n",
    "        \n",
    "        print(\"Top 5 highest correlations:\")\n",
    "        display(sorted_df[['GLACIER', feature_col, 'N_SAMPLES']].head())\n",
    "        \n",
    "        print(\"Top 5 lowest correlations:\")\n",
    "        display(sorted_df[['GLACIER', feature_col, 'N_SAMPLES']].tail())\n",
    "        \n",
    "        \n",
    "        blanc_corr = sorted_df[sorted_df['GLACIER'] == 'FR4N01083B21 Blanc'][feature_col].values[0]\n",
    "        print(f\"Blanc correlation = {blanc_corr:.3f}\")\n",
    "\n",
    "    return corr_df\n",
    "\n",
    "# Run the individual analysis\n",
    "individual_corr_df = analyze_individual_glacier_correlations(data_monthly_FR_CH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find glaciers similar to Blanc\n",
    "def find_similar_glaciers(corr_df, target_glacier='FR4N01083B21 Blanc'):\n",
    "    \"\"\"\n",
    "    Find glaciers with correlation patterns similar to the target glacier\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_glacier not in corr_df['GLACIER'].values:\n",
    "        print(f\"{target_glacier} not found in correlation data\")\n",
    "        return\n",
    "    \n",
    "    # Get Blanc's correlations\n",
    "    blanc_row = corr_df[corr_df['GLACIER'] == target_glacier].iloc[0]\n",
    "    \n",
    "    features = ['t2m_corr', 'tp_corr', 'ssrd_corr', 'ELEVATION_DIFFERENCE_corr']\n",
    "    \n",
    "    print(f\"=== GLACIERS SIMILAR TO {target_glacier} ===\")\n",
    "    print(f\"{target_glacier} correlations:\")\n",
    "    for feature in features:\n",
    "        print(f\"  {feature}: {blanc_row[feature]:.3f}\")\n",
    "    \n",
    "    similar_glaciers = []\n",
    "    \n",
    "    for _, row in corr_df.iterrows():\n",
    "        if row['GLACIER'] == target_glacier:\n",
    "            continue\n",
    "            \n",
    "        # Check if this glacier has similar patterns\n",
    "        similar = True\n",
    "        similarity_score = 0\n",
    "        \n",
    "        for feature in features:\n",
    "            diff = abs(row[feature] - blanc_row[feature])\n",
    "            similarity_score += diff\n",
    "            \n",
    "            if feature == 'ELEVATION_DIFFERENCE_corr' and diff > 0.3: \n",
    "                similar = False\n",
    "            elif feature in ['t2m_corr', 'ssrd_corr'] and diff > 0.2: \n",
    "                similar = False\n",
    "            elif feature == 'tp_corr' and diff > 0.3:\n",
    "                similar = False\n",
    "        \n",
    "        if similar:\n",
    "            similar_glaciers.append({\n",
    "                'GLACIER': row['GLACIER'],\n",
    "                'similarity_score': similarity_score,\n",
    "                'N_SAMPLES': row['N_SAMPLES'],\n",
    "                **{f: row[f] for f in features}\n",
    "            })\n",
    "    \n",
    "    if similar_glaciers:\n",
    "        similar_df = pd.DataFrame(similar_glaciers).sort_values('similarity_score')\n",
    "        print(f\"\\nFound {len(similar_df)} glaciers with similar correlation patterns:\")\n",
    "        display(similar_df)\n",
    "        \n",
    "        train_glaciers_list = list(data_CH['GLACIER'].unique())\n",
    "        similar_in_train = similar_df[similar_df['GLACIER'].isin(train_glaciers_list)]\n",
    "        \n",
    "        if len(similar_in_train) > 0:\n",
    "            print(f\"\\n {len(similar_in_train)} similar glaciers found in CH training set!\")\n",
    "            display(similar_in_train[['GLACIER', 'similarity_score']])\n",
    "        else:\n",
    "            print(\"\\n No similar glaciers found in CH training set\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nNo glaciers found with similar correlation patterns - Blanc appears to be unique!\")\n",
    "    \n",
    "    return similar_glaciers\n",
    "\n",
    "# Find similar glaciers\n",
    "similar_to_blanc = find_similar_glaciers(individual_corr_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
