{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "from typing import Optional, Iterable, Dict, List\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from regions.TF_Europe.scripts.config_TF_Europe import *\n",
    "from regions.TF_Europe.scripts.dataset import *\n",
    "from regions.TF_Europe.scripts.plotting import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.EuropeTFConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-regional modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stakes datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of loading data:\n",
    "# Load Switzerland only\n",
    "df = load_stakes(cfg, \"CH\")\n",
    "\n",
    "# Load all Central Europe (FR+CH+IT+AT when you add them)\n",
    "df_ceu = load_stakes_for_rgi_region(cfg, \"11\")\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\"\"\"\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "dfs[\"11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "summarize_and_plot_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "plot_mb_distributions_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_Europe.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_Europe.nc\")\n",
    "}\n",
    "\n",
    "# Check that all these files exists\n",
    "for key, path in paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required file for {key} not found at {path}\")\n",
    "\n",
    "    vois_climate = [\n",
    "        \"t2m\",\n",
    "        \"tp\",\n",
    "        \"slhf\",\n",
    "        \"sshf\",\n",
    "        \"ssrd\",\n",
    "        \"fal\",\n",
    "        \"str\",\n",
    "    ]\n",
    "\n",
    "vois_topographical = [\"aspect\", \"slope\", \"svf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crossregional_df_ceu_with_ch(dfs: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate all stake dataframes in `dfs` into one Europe-wide dataframe.\n",
    "\n",
    "    Expects:\n",
    "      - Each df has at least columns: GLACIER, YEAR, ID, PERIOD, MONTHS, POINT_BALANCE\n",
    "      - Central Europe df includes SOURCE_CODE identifying CH/FR/IT_AT etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe (all rows across all RGI regions).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for rid, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            logging.warning(f\"RGI {rid}: empty, skipping in concat.\")\n",
    "            continue\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No non-empty dataframes in dfs.\")\n",
    "\n",
    "    d_all = pd.concat(frames, ignore_index=True)\n",
    "    return d_all\n",
    "\n",
    "\n",
    "def compute_crossregional_test_glaciers(\n",
    "    df_all: pd.DataFrame,\n",
    "    ch_code: str = \"CH\",\n",
    "    source_col: str = \"SOURCE_CODE\",\n",
    "    glacier_col: str = \"GLACIER\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train glaciers = all glaciers with SOURCE_CODE == CH\n",
    "    Test glaciers  = all glaciers with SOURCE_CODE != CH\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (train_glaciers, test_glaciers) : (list[str], list[str])\n",
    "    \"\"\"\n",
    "    if source_col not in df_all.columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing column {source_col}. Needed to separate CH vs others.\")\n",
    "    if glacier_col not in df_all.columns:\n",
    "        raise ValueError(f\"Missing column {glacier_col}.\")\n",
    "\n",
    "    ch_gl = sorted(df_all.loc[df_all[source_col] == ch_code,\n",
    "                              glacier_col].dropna().unique())\n",
    "    non_ch_gl = sorted(df_all.loc[df_all[source_col] != ch_code,\n",
    "                                  glacier_col].dropna().unique())\n",
    "\n",
    "    if not ch_gl:\n",
    "        raise ValueError(\"No CH glaciers found (SOURCE_CODE=='CH').\")\n",
    "    if not non_ch_gl:\n",
    "        raise ValueError(\"No non-CH glaciers found (SOURCE_CODE!='CH').\")\n",
    "\n",
    "    logging.info(\n",
    "        f\"Cross-regional split: CH train glaciers={len(ch_gl)}, non-CH test glaciers={len(non_ch_gl)}\"\n",
    "    )\n",
    "    return ch_gl, non_ch_gl\n",
    "\n",
    "\n",
    "def prepare_monthly_df_crossregional_CH_to_EU(\n",
    "    cfg,\n",
    "    dfs,\n",
    "    paths,\n",
    "    vois_climate,\n",
    "    vois_topographical,\n",
    "    run_flag=True,  # True recompute, False load\n",
    "    region_name=\"XREG_CH_TO_EU\",\n",
    "    region_id=11,  # arbitrary/int tag used by your pipeline; keep 11 or 0\n",
    "    csv_subfolder=\"CrossRegional/CH_to_Europe/csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ONE monthly-prepped dataset:\n",
    "      - data = concatenation of all Europe sources\n",
    "      - train = CH glaciers\n",
    "      - test  = all non-CH glaciers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : dict\n",
    "        Same output dict as prepare_monthly_dfs_with_padding (df_train/df_test/aug/etc.)\n",
    "    split_info : dict\n",
    "        {\"train_glaciers\": [...], \"test_glaciers\": [...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Concatenate all raw stake rows\n",
    "    df_all = build_crossregional_df_ceu_with_ch(dfs)\n",
    "\n",
    "    # 2) Define test glaciers: all non-CH\n",
    "    train_glaciers, test_glaciers = compute_crossregional_test_glaciers(\n",
    "        df_all, ch_code=\"CH\")\n",
    "\n",
    "    # 3) Choose an output folder for this experiment\n",
    "    paths_ = paths.copy()\n",
    "    paths_[\"csv_path\"] = os.path.join(cfg.dataPath, path_PMB_WGMS_csv,\n",
    "                                      csv_subfolder)\n",
    "    os.makedirs(paths_[\"csv_path\"], exist_ok=True)\n",
    "\n",
    "    logging.info(\n",
    "        f\"Preparing cross-regional monthlies: {region_name} \"\n",
    "        f\"(run_flag={run_flag}) | train(CH)={len(train_glaciers)} | test(non-CH)={len(test_glaciers)}\"\n",
    "    )\n",
    "\n",
    "    res = prepare_monthly_dfs_with_padding(\n",
    "        cfg=cfg,\n",
    "        df_region=df_all,\n",
    "        region_name=region_name,\n",
    "        region_id=int(region_id),\n",
    "        paths=paths_,\n",
    "        test_glaciers=test_glaciers,  # test = all non-CH glaciers\n",
    "        vois_climate=vois_climate,\n",
    "        vois_topographical=vois_topographical,\n",
    "        run_flag=run_flag,\n",
    "    )\n",
    "\n",
    "    return res, {\n",
    "        \"train_glaciers\": train_glaciers,\n",
    "        \"test_glaciers\": test_glaciers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all stake dfs\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\n",
    "# prepare monthlies (recompute or load)\n",
    "res_xreg, split_info = prepare_monthly_df_crossregional_CH_to_EU(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    run_flag=False,  # load if already computed\n",
    ")\n",
    "\n",
    "df_train = res_xreg[\"df_train\"]\n",
    "df_test = res_xreg[\"df_test\"]\n",
    "\n",
    "print(\"Train glaciers (CH):\", len(split_info[\"train_glaciers\"]))\n",
    "print(\"Test glaciers (non-CH):\", len(split_info[\"test_glaciers\"]))\n",
    "print(\"Train rows:\", len(df_train), \"Test rows:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pick_glaciers_by_row_fraction(\n",
    "    df_test: pd.DataFrame,\n",
    "    region_code: str,\n",
    "    target_frac: float,\n",
    "    source_col: str = \"SOURCE_CODE\",\n",
    "    glacier_col: str = \"GLACIER\",\n",
    "    seed: int = 42,\n",
    "    method: str = \"greedy_small_first\",\n",
    "    min_rows_per_glacier: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Select glaciers whose df_test row counts sum to ~target_frac of total rows for region_code.\n",
    "\n",
    "    method:\n",
    "      - \"greedy_small_first\": sorts glaciers by row count ascending, then accumulates\n",
    "        (often best for small targets like 5% because it can finely tune)\n",
    "      - \"greedy_large_first\": sorts descending, then accumulates (often fine for 50%)\n",
    "      - \"shuffle_then_greedy\": shuffle (seeded), then accumulate (stochastic but reproducible)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_glaciers : list[str]\n",
    "    summary : dict with totals and achieved fraction\n",
    "    per_glacier_counts : pd.Series of counts (for inspection)\n",
    "    \"\"\"\n",
    "    df_reg = df_test.loc[df_test[source_col] == region_code].copy()\n",
    "    if df_reg.empty:\n",
    "        raise ValueError(\n",
    "            f\"No rows in df_test for region '{region_code}' (source_col={source_col}).\"\n",
    "        )\n",
    "\n",
    "    counts = df_reg.groupby(glacier_col).size().sort_values(ascending=False)\n",
    "\n",
    "    # optional: remove tiny glaciers (if you want)\n",
    "    counts = counts[counts >= min_rows_per_glacier]\n",
    "    if counts.empty:\n",
    "        raise ValueError(\n",
    "            f\"After filtering min_rows_per_glacier={min_rows_per_glacier}, no glaciers remain for {region_code}.\"\n",
    "        )\n",
    "\n",
    "    total_rows = int(counts.sum())\n",
    "    target_rows = int(round(target_frac * total_rows))\n",
    "\n",
    "    # order glaciers for greedy\n",
    "    if method == \"greedy_small_first\":\n",
    "        ordered = counts.sort_values(ascending=True)\n",
    "    elif method == \"greedy_large_first\":\n",
    "        ordered = counts.sort_values(ascending=False)\n",
    "    elif method == \"shuffle_then_greedy\":\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = counts.index.to_numpy()\n",
    "        rng.shuffle(idx)\n",
    "        ordered = counts.loc[idx]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method='{method}'\")\n",
    "\n",
    "    selected = []\n",
    "    s = 0\n",
    "\n",
    "    # greedy accumulate\n",
    "    for gl, n in ordered.items():\n",
    "        # if we already hit/exceeded target, decide whether adding this glacier helps or hurts\n",
    "        if s >= target_rows:\n",
    "            # check if adding would improve closeness\n",
    "            cur_err = abs(s - target_rows)\n",
    "            new_err = abs((s + int(n)) - target_rows)\n",
    "            if new_err < cur_err:\n",
    "                selected.append(gl)\n",
    "                s += int(n)\n",
    "            break\n",
    "        else:\n",
    "            selected.append(gl)\n",
    "            s += int(n)\n",
    "\n",
    "    # small local improvement: try swapping one glacier if it improves error (optional, cheap)\n",
    "    # (helps especially near 50% targets)\n",
    "    selected_set = set(selected)\n",
    "    not_selected = [g for g in counts.index if g not in selected_set]\n",
    "\n",
    "    best_err = abs(s - target_rows)\n",
    "    best_swap = None\n",
    "\n",
    "    # limit search for speed (still usually enough)\n",
    "    cand_sel = selected[:min(len(selected), 40)]\n",
    "    cand_nsel = not_selected[:min(len(not_selected), 60)]\n",
    "\n",
    "    sel_counts = counts.loc[cand_sel]\n",
    "    nsel_counts = counts.loc[cand_nsel]\n",
    "\n",
    "    for g_out, n_out in sel_counts.items():\n",
    "        for g_in, n_in in nsel_counts.items():\n",
    "            s2 = s - int(n_out) + int(n_in)\n",
    "            err2 = abs(s2 - target_rows)\n",
    "            if err2 < best_err:\n",
    "                best_err = err2\n",
    "                best_swap = (g_out, g_in, s2)\n",
    "\n",
    "    if best_swap is not None:\n",
    "        g_out, g_in, s2 = best_swap\n",
    "        selected = [g for g in selected if g != g_out] + [g_in]\n",
    "        s = int(s2)\n",
    "\n",
    "    achieved_frac = s / total_rows if total_rows > 0 else np.nan\n",
    "\n",
    "    summary = {\n",
    "        \"region\": region_code,\n",
    "        \"target_frac\": float(target_frac),\n",
    "        \"total_rows_region\": total_rows,\n",
    "        \"target_rows\": target_rows,\n",
    "        \"selected_rows\": int(s),\n",
    "        \"achieved_frac\": float(achieved_frac),\n",
    "        \"achieved_pct\": float(100 * achieved_frac),\n",
    "        \"n_glaciers_total\": int(counts.shape[0]),\n",
    "        \"n_glaciers_selected\": int(len(selected)),\n",
    "        \"abs_row_error\": int(abs(s - target_rows)),\n",
    "    }\n",
    "\n",
    "    return selected, summary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SJM 5%: small-first greedy usually gives best control for a small fraction\n",
    "SJM_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"SJM\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# SJM 50%: large-first or small-first both work; I’d start with large-first\n",
    "SJM_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"SJM\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"SJM 5% summary:\", sjm5_info)\n",
    "print(\"SJM 50% summary:\", sjm50_info)\n",
    "\n",
    "print(\"\\nSJM_5pct glaciers:\", SJM_5pct)\n",
    "print(\"\\nSJM_50pct glaciers:\", SJM_50pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISL 5%: small-first greedy usually gives best control for a small fraction\n",
    "ISL_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"ISL\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# ISL 50%: large-first or small-first both work; I’d start with large-first\n",
    "ISL_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"ISL\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"ISL 5% summary:\", sjm5_info)\n",
    "print(\"ISL 50% summary:\", sjm50_info)\n",
    "\n",
    "print(\"\\nISL_5pct glaciers:\", ISL_5pct)\n",
    "print(\"\\nISL_50pct glaciers:\", ISL_50pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FR 5%: small-first greedy usually gives best control for a small fraction\n",
    "FR_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"FR\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# FR 50%: large-first or small-first both work; I’d start with large-first\n",
    "FR_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"FR\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"FR 5% summary:\", sjm5_info)\n",
    "print(\"FR 50% summary:\", sjm50_info)\n",
    "\n",
    "print(\"\\nFR_5pct glaciers:\", FR_5pct)\n",
    "print(\"\\nFR_50pct glaciers:\", FR_50pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT_AT 5%: small-first greedy usually gives best control for a small fraction\n",
    "IT_AT_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"IT_AT\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# IT_AT 50%: large-first or small-first both work; I’d start with large-first\n",
    "IT_AT_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"IT_AT\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"IT_AT 5% summary:\", sjm5_info)\n",
    "print(\"IT_AT 50% summary:\", sjm50_info)\n",
    "\n",
    "print(\"\\nIT_AT_5pct glaciers:\", IT_AT_5pct)\n",
    "print(\"\\nIT_AT_50pct glaciers:\", IT_AT_50pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOR 5%: small-first greedy usually gives best control for a small fraction\n",
    "NOR_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"NOR\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# NOR 50%: large-first or small-first both work; I’d start with large-first\n",
    "NOR_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"NOR\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"NOR 5% summary:\", sjm5_info)\n",
    "print(\"NOR 50% summary:\", sjm50_info)\n",
    "\n",
    "print(\"\\nNOR_5pct glaciers:\", NOR_5pct)\n",
    "print(\"\\nNOR_50pct glaciers:\", NOR_50pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norway\n",
    "# 50% split\n",
    "FT_50PCT_NOR = [\n",
    "    'Nigardsbreen', 'Aalfotbreen', 'Engabreen', 'Storsteinsfjellbreen',\n",
    "    'Cainhavarre'\n",
    "]\n",
    "\n",
    "# 5% split\n",
    "FT_5PCT_NOR = [\n",
    "    'Moesevassbrea', 'Vetlefjordbreen', 'Juvfonne', 'Graasubreen',\n",
    "    'Hellstugubreen', 'Storglombreen N', 'Blabreen', 'Ruklebreen',\n",
    "    'Vestre Memurubreen', 'Cainhavarre', 'Bondhusbrea'\n",
    "]\n",
    "\n",
    "# France\n",
    "# 5% split\n",
    "FT_5PCT_FR = ['Grands Montets', 'Sarennes', 'Talefre', 'Leschaux']\n",
    "\n",
    "# 50% split\n",
    "FT_50PCT_FR = ['Argentiere', 'Gebroulaz']\n",
    "\n",
    "# IT-AT\n",
    "FT_5PCT_IT_AT = [\n",
    "    'CIARDONEY', 'CARESER CENTRALE', 'CAMPO SETT.', 'ZETTALUNITZ/MULLWITZ K.',\n",
    "    'HALLSTAETTER G.', 'VENEDIGER K.', 'SURETTA MERIDIONALE', 'GOLDBERG K.',\n",
    "    'CARESER OCCIDENTALE', 'GRAND ETRET', 'LUPO'\n",
    "]\n",
    "\n",
    "FT_50PCT_IT_AT = [\n",
    "    'HINTEREIS F.', 'MALAVALLE (VEDR. DI) / UEBELTALF.',\n",
    "    'LUNGA (VEDRETTA) / LANGENF.', 'RIES OCC. (VEDR. DI) / RIESERF. WESTL.'\n",
    "]\n",
    "\n",
    "# Iceland\n",
    "# 5% split\n",
    "FT_5PCT_ISL = [\n",
    "    'RGI60-06.00306', 'RGI60-06.00296', 'RGI60-06.00479', 'RGI60-06.00425',\n",
    "    'RGI60-06.00445', 'RGI60-06.00474', 'RGI60-06.00542',\n",
    "    'Reykjafjardarjoekull', 'RGI60-06.00350', 'RGI60-06.00342',\n",
    "    'RGI60-06.00301', 'RGI60-06.00422', 'RGI60-06.00320', 'RGI60-06.00359',\n",
    "    'RGI60-06.00349', 'RGI60-06.00409', 'RGI60-06.00413', 'RGI60-06.00411',\n",
    "    'Oeldufellsjoekull', 'RGI60-06.00476', 'RGI60-06.00549', 'RGI60-06.00228',\n",
    "    'RGI60-06.00303', 'Kaldalonsjoekull', 'RGI60-06.00328', 'RGI60-06.00541',\n",
    "    'Slettjoekull West', 'RGI60-06.00232', 'RGI60-06.00305'\n",
    "]\n",
    "\n",
    "# 50% split\n",
    "FT_50PCT_ISL = [\n",
    "    'RGI60-06.00238', 'Bruarjoekull', 'Skeidararjoekull',\n",
    "    'Thjorsarjoekull (Hofsjoekull E)', 'Sidujoekull/Skaftarjoekull',\n",
    "    'Hagafellsjoekull West', 'RGI60-06.00305'\n",
    "]\n",
    "\n",
    "# Svalbard\n",
    "# 15% split\n",
    "FT_5PCT_SJM = ['WERENSKIOLDBREEN']\n",
    "\n",
    "# 5% split\n",
    "FT_50PCT_SJM = ['GROENFJORD E', 'WERENSKIOLDBREEN']\n",
    "\n",
    "FT_GLACIERS = {\n",
    "    \"FR\": {\n",
    "        \"5pct\": FT_5PCT_FR,\n",
    "        \"50pct\": FT_50PCT_FR\n",
    "    },\n",
    "    \"IT_AT\": {\n",
    "        \"5pct\": FT_5PCT_IT_AT,\n",
    "        \"50pct\": FT_50PCT_IT_AT\n",
    "    },\n",
    "    \"NOR\": {\n",
    "        \"5pct\": FT_5PCT_NOR,\n",
    "        \"50pct\": FT_50PCT_NOR\n",
    "    },\n",
    "    \"ISL\": {\n",
    "        \"5pct\": FT_5PCT_ISL,\n",
    "        \"50pct\": FT_50PCT_ISL\n",
    "    },\n",
    "    \"SJM\": {\n",
    "        \"5pct\": FT_5PCT_SJM,\n",
    "        \"50pct\": FT_50PCT_SJM\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_row_percentage(df_test,\n",
    "                          FT_GLACIERS,\n",
    "                          source_col=\"SOURCE_CODE\",\n",
    "                          glacier_col=\"GLACIER\"):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for region, splits in FT_GLACIERS.items():\n",
    "\n",
    "        df_reg = df_test[df_test[source_col] == region]\n",
    "\n",
    "        total_rows = len(df_reg)\n",
    "        if total_rows == 0:\n",
    "            print(f\"{region}: no rows in df_test\")\n",
    "            continue\n",
    "\n",
    "        for split_name, glacier_list in splits.items():\n",
    "\n",
    "            df_ft = df_reg[df_reg[glacier_col].isin(glacier_list)]\n",
    "            ft_rows = len(df_ft)\n",
    "\n",
    "            pct = 100 * ft_rows / total_rows\n",
    "\n",
    "            results.append({\n",
    "                \"region\": region,\n",
    "                \"split\": split_name,\n",
    "                \"rows_total_region\": total_rows,\n",
    "                \"rows_ft\": ft_rows,\n",
    "                \"pct_rows\": pct,\n",
    "            })\n",
    "\n",
    "            print(f\"{region} | {split_name}: \"\n",
    "                  f\"{ft_rows}/{total_rows} rows = {pct:.2f}%\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "df_row_check = verify_row_percentage(df_test, FT_GLACIERS)\n",
    "df_row_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reg in FT_GLACIERS.keys():\n",
    "    gls = sorted(df_test.loc[df_test[\"SOURCE_CODE\"] == reg,\n",
    "                             \"GLACIER\"].unique())\n",
    "    print(reg, \"unique glaciers in df_test:\", len(gls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot test/train glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    *,\n",
    "    rgi_region_id: str,\n",
    "    outline_shp_path: str,\n",
    "    ft_glaciers_by_split: dict,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    "    ft_label_ft=\"FT\",\n",
    "    ft_label_holdout=\"Hold-out\",\n",
    "    glacier_col=\"GLACIER\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    period_col=\"PERIOD\",\n",
    "    nmeas_col=\"Nb. measurements\",\n",
    "    source_col=\"SOURCE_CODE\",  # NEW\n",
    "    source_resolution=\"mode\",  # NEW: \"error\" | \"first\" | \"mode\" | \"list\"\n",
    "    load_stakes_fn=None,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic builder for per-glacier info tables (for maps / summaries), for any region + any splits.\n",
    "    Also carries SOURCE_CODE info into the final per-glacier dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_region : pd.DataFrame\n",
    "    glacier_outline_rgi : GeoDataFrame\n",
    "    glacier_info_by_split : dict[str, pd.DataFrame]\n",
    "        Indexed by GLACIER, with columns:\n",
    "          [POINT_LAT, POINT_LON, Nb. measurements, (period counts...), SOURCE_CODE, FT/Hold-out glacier]\n",
    "        SOURCE_CODE handling depends on source_resolution.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "\n",
    "    if load_stakes_fn is None:\n",
    "        load_stakes_fn = load_stakes_for_rgi_region  # noqa: F821\n",
    "\n",
    "    data_region = load_stakes_fn(cfg, rgi_region_id)\n",
    "    glacier_outline_rgi = gpd.read_file(outline_shp_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[{rgi_region_id}] stake rows: {len(data_region)} | \"\n",
    "              f\"glaciers: {data_region[glacier_col].nunique()}\")\n",
    "\n",
    "    # --- measurement counts ---\n",
    "    meas_counts = (data_region.groupby(glacier_col).size().sort_values(\n",
    "        ascending=False).rename(nmeas_col).to_frame())\n",
    "\n",
    "    # --- mean location ---\n",
    "    glacier_loc = data_region.groupby(glacier_col)[[lat_col, lon_col]].mean()\n",
    "\n",
    "    # --- counts per period (winter/annual) ---\n",
    "    glacier_period = (data_region.groupby(\n",
    "        [glacier_col, period_col]).size().unstack().fillna(0).astype(int))\n",
    "\n",
    "    # --- SOURCE_CODE per glacier (NEW) ---\n",
    "    if source_col in data_region.columns:\n",
    "        gsrc = data_region.groupby(glacier_col)[source_col].apply(\n",
    "            lambda s: s.dropna().astype(str).unique())\n",
    "\n",
    "        # detect mixed source glaciers\n",
    "        mixed = gsrc[gsrc.apply(len) > 1]\n",
    "        if len(mixed) > 0 and verbose:\n",
    "            print(\n",
    "                f\"Warning: {len(mixed)} glaciers have multiple {source_col} values \"\n",
    "                f\"(showing up to 5): {mixed.head(5).to_dict()}\")\n",
    "\n",
    "        if len(mixed) > 0 and source_resolution == \"error\":\n",
    "            raise ValueError(\n",
    "                f\"Found glaciers with multiple {source_col} values. \"\n",
    "                f\"Set source_resolution to 'first', 'mode', or 'list' to resolve.\"\n",
    "            )\n",
    "\n",
    "        if source_resolution == \"list\":\n",
    "            glacier_source = gsrc.apply(lambda arr: list(arr)).rename(\n",
    "                source_col).to_frame()\n",
    "        elif source_resolution == \"first\":\n",
    "            glacier_source = gsrc.apply(lambda arr: arr[0] if len(arr) else\n",
    "                                        None).rename(source_col).to_frame()\n",
    "        elif source_resolution == \"mode\":\n",
    "            # mode by frequency in raw rows (more stable than unique list)\n",
    "            def _mode(series):\n",
    "                s = series.dropna().astype(str)\n",
    "                if len(s) == 0:\n",
    "                    return None\n",
    "                vc = s.value_counts()\n",
    "                return vc.index[0]\n",
    "\n",
    "            glacier_source = (data_region.groupby(glacier_col)[source_col].\n",
    "                              apply(_mode).rename(source_col).to_frame())\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"source_resolution must be one of: 'error','first','mode','list'\"\n",
    "            )\n",
    "    else:\n",
    "        glacier_source = None\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Note: '{source_col}' not found in data_region; skipping SOURCE_CODE merge.\"\n",
    "            )\n",
    "\n",
    "    # --- merge base ---\n",
    "    base = glacier_loc.join(meas_counts, how=\"inner\").join(glacier_period,\n",
    "                                                           how=\"left\")\n",
    "    if glacier_source is not None:\n",
    "        base = base.join(glacier_source, how=\"left\")\n",
    "\n",
    "    glacier_info_by_split = {}\n",
    "\n",
    "    for split in split_names:\n",
    "        ft_set = set(ft_glaciers_by_split.get(split, []))\n",
    "\n",
    "        df = base.copy()\n",
    "        df[ft_label_col] = df.index.to_series().apply(\n",
    "            lambda g: ft_label_ft if g in ft_set else ft_label_holdout)\n",
    "        glacier_info_by_split[split] = df\n",
    "\n",
    "        if verbose:\n",
    "            n_ft = int((df[ft_label_col] == ft_label_ft).sum())\n",
    "            n_ho = int((df[ft_label_col] == ft_label_holdout).sum())\n",
    "            ft_rows = int(\n",
    "                data_region[data_region[glacier_col].isin(ft_set)].shape[0])\n",
    "            all_rows = int(data_region.shape[0])\n",
    "            frac = (ft_rows / all_rows) if all_rows else float(\"nan\")\n",
    "            print(\n",
    "                f\"  split={split}: FT glaciers={n_ft}, Hold-out glaciers={n_ho} | \"\n",
    "                f\"FT rows fraction ~ {frac:.3f}\")\n",
    "\n",
    "    return data_region, glacier_outline_rgi, glacier_info_by_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Central European Alps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_CEU_5pct = FT_GLACIERS[\"FR\"][\"5pct\"] + FT_GLACIERS[\"IT_AT\"][\"5pct\"]\n",
    "FT_GL_CEU_50pct = FT_GLACIERS[\"FR\"][\"50pct\"] + FT_GLACIERS[\"IT_AT\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_CEU_5pct,\n",
    "    \"50pct\": FT_GL_CEU_50pct,\n",
    "}\n",
    "\n",
    "data_CEU, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"11\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_11_CentralEurope/11_rgi60_CentralEurope.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_CEU_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_CEU_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "# remove CH glaciers\n",
    "glacier_df_CEU_5pct = glacier_df_CEU_5pct[~glacier_df_CEU_5pct[\"SOURCE_CODE\"].\n",
    "                                          isin([\"CH\"])]\n",
    "glacier_df_CEU_50pct = glacier_df_CEU_50pct[\n",
    "    ~glacier_df_CEU_50pct[\"SOURCE_CODE\"].isin([\"CH\"])]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_CEU_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier measurement locations Central European Alps (5pct)\",\n",
    "    extent=(5.8, 15, 44, 48),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000, 6000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_CEU_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier measurement locations Central European Alps (50pct)\",\n",
    "    extent=(5.8, 15, 44, 48),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000, 6000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_NOR_5pct = FT_GLACIERS[\"NOR\"][\"5pct\"]\n",
    "FT_GL_NOR_50pct = FT_GLACIERS[\"NOR\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_NOR_5pct,\n",
    "    \"50pct\": FT_GL_NOR_50pct,\n",
    "}\n",
    "\n",
    "data_NOR, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"08\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_08_Scandinavia/08_rgi60_Scandinavia.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_NOR_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_NOR_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_NOR_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Norway (5pct)\",\n",
    "    extent=(4, 24, 57, 71),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_NOR_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Norway (50pct)\",\n",
    "    extent=(4, 24, 57, 71),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Svalbard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_SJM_5pct = FT_GLACIERS[\"SJM\"][\"5pct\"]\n",
    "FT_GL_SJM_50pct = FT_GLACIERS[\"SJM\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_SJM_5pct,\n",
    "    \"50pct\": FT_GL_SJM_50pct,\n",
    "}\n",
    "\n",
    "data_SJM, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"07\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_07_Svalbard/07_rgi60_Svalbard.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_SJM_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_SJM_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_SJM_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Svalbard (5pct)\",\n",
    "    extent=(5, 30, 76, 80),\n",
    "    sizes=(100, 1000),\n",
    "    size_legend_values=(30, 100, 400),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_SJM_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Svalbard (50pct)\",\n",
    "    extent=(5, 30, 76, 80),\n",
    "    sizes=(100, 1000),\n",
    "    size_legend_values=(30, 100, 400),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iceland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_ISL_5pct = FT_GLACIERS[\"ISL\"][\"5pct\"]\n",
    "FT_GL_ISL_50pct = FT_GLACIERS[\"ISL\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_ISL_5pct,\n",
    "    \"50pct\": FT_GL_ISL_50pct,\n",
    "}\n",
    "\n",
    "data_ISL, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"06\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_06_Iceland/06_rgi60_Iceland.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_ISL_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_ISL_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_ISL_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Iceland (5pct)\",\n",
    "    extent=(-25, -11, 62, 68),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_ISL_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Iceland (50pct)\",\n",
    "    extent=(-25, -11, 62, 68),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_overlap_xreg_from_single_res(\n",
    "        res_xreg: dict,\n",
    "        cfg,\n",
    "        STATIC_COLS,\n",
    "        MONTHLY_COLS,\n",
    "        group_col: str = \"SOURCE_CODE\",\n",
    "        ch_code: str = \"CH\",\n",
    "        use_aug: bool = False,  # True -> df_train_aug/df_test_aug\n",
    "        n_iter: int = 1000,\n",
    "        only_codes=None,  # e.g. [\"IT_AT\", \"FR\"]\n",
    "        skip_codes=None,  # e.g. [\"CH\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    For XREG where train=CH and test=all non-CH inside ONE monthly result dict:\n",
    "\n",
    "      - df_ch = res_xreg[df_train*]\n",
    "      - df_test_all = res_xreg[df_test*]\n",
    "      - split df_test_all by SOURCE_CODE and plot CH vs each code\n",
    "\n",
    "    Returns dict: code -> figure\n",
    "    \"\"\"\n",
    "    only_codes = {c.upper() for c in (only_codes or [])} or None\n",
    "    skip_codes = {c.upper() for c in (skip_codes or [])}\n",
    "    skip_codes.add(ch_code.upper())\n",
    "\n",
    "    # pick which dfs\n",
    "    if use_aug:\n",
    "        df_ch = res_xreg.get(\"df_train_aug\")\n",
    "        df_test_all = res_xreg.get(\"df_test_aug\")\n",
    "        label_df = \"(*_aug)\"\n",
    "    else:\n",
    "        df_ch = res_xreg.get(\"df_train\")\n",
    "        df_test_all = res_xreg.get(\"df_test\")\n",
    "        label_df = \"\"\n",
    "\n",
    "    if df_ch is None or len(df_ch) == 0:\n",
    "        raise ValueError(f\"df_train{label_df} missing/empty in res_xreg.\")\n",
    "    if df_test_all is None or len(df_test_all) == 0:\n",
    "        raise ValueError(f\"df_test{label_df} missing/empty in res_xreg.\")\n",
    "\n",
    "    if group_col not in df_test_all.columns:\n",
    "        raise ValueError(\n",
    "            f\"'{group_col}' not found in df_test{label_df}. Needed to split by region.\"\n",
    "        )\n",
    "    if group_col not in df_ch.columns:\n",
    "        # not fatal, but helps sanity-check\n",
    "        print(\n",
    "            f\"[warn] '{group_col}' not in df_train{label_df}. That's OK for CH reference.\"\n",
    "        )\n",
    "\n",
    "    # palette\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    custom_palette = {\"Train\": color_dark_blue, \"Test\": \"#b2182b\"}\n",
    "\n",
    "    # codes present in test\n",
    "    codes_present = sorted(c for c in df_test_all[group_col].dropna().astype(\n",
    "        str).str.upper().unique() if c not in skip_codes)\n",
    "\n",
    "    if only_codes is not None:\n",
    "        codes_present = [c for c in codes_present if c in only_codes]\n",
    "\n",
    "    figs = {}\n",
    "    for code in codes_present:\n",
    "        df_other = df_test_all[df_test_all[group_col].astype(str).str.upper()\n",
    "                               == code].copy()\n",
    "        if len(df_other) == 0:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Plotting XREG t-SNE: CH(train n={len(df_ch)}) vs {code}(test n={len(df_other)})\"\n",
    "        )\n",
    "\n",
    "        fig = plot_tsne_overlap(\n",
    "            data_train=df_ch,\n",
    "            data_test=df_other,\n",
    "            STATIC_COLS=STATIC_COLS,\n",
    "            MONTHLY_COLS=MONTHLY_COLS,\n",
    "            sublabels=(\"a\", \"b\", \"c\"),\n",
    "            label_fmt=\"({})\",\n",
    "            label_xy=(0.02, 0.98),\n",
    "            label_fontsize=14,\n",
    "            n_iter=n_iter,\n",
    "            random_state=cfg.seed,\n",
    "            custom_palette=custom_palette,\n",
    "        )\n",
    "        fig.suptitle(f\"XREG overlap: CH vs {code}\", fontsize=14)\n",
    "        figs[code] = fig\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "# res_xreg is the ONE dict from your cross-regional monthly prep\n",
    "figs_by_code = plot_tsne_overlap_xreg_from_single_res(\n",
    "    res_xreg=res_xreg,\n",
    "    cfg=cfg,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    use_aug=False,  # or True if you want *_aug\n",
    "    n_iter=1000,\n",
    "    # only_codes=[\"IT_AT\"],  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def plot_feature_kde_overlap_xreg_ch_vs_codes(\n",
    "    res_xreg: dict,\n",
    "    cfg,\n",
    "    features,\n",
    "    group_col: str = \"SOURCE_CODE\",\n",
    "    ch_code: str = \"CH\",\n",
    "    use_aug: bool = False,  # True -> df_train_aug/df_test_aug\n",
    "    only_codes=None,  # e.g. [\"IT_AT\", \"FR\"]\n",
    "    skip_codes=None,  # e.g. [\"CH\"]\n",
    "    output_dir=None,  # e.g. \"figures/xreg_kde\"\n",
    "    include_ch_in_title: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot KDE-based feature overlap for XREG: CH vs each SOURCE_CODE subset.\n",
    "\n",
    "    Uses:\n",
    "      - CH reference: res_xreg[\"df_train\"] (or \"_aug\" if use_aug)\n",
    "      - Other region: subset of res_xreg[\"df_test\"] by SOURCE_CODE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    res_xreg : dict\n",
    "        Output dict from prepare_monthly_df_crossregional_CH_to_EU (or similar),\n",
    "        containing df_train/df_test and optionally df_train_aug/df_test_aug.\n",
    "        df_test must contain `group_col` (SOURCE_CODE).\n",
    "    cfg : object\n",
    "        Used only for consistent output naming if desired (optional).\n",
    "    features : list[str]\n",
    "        Feature columns to plot.\n",
    "    group_col : str\n",
    "        Column to split test set by (default: \"SOURCE_CODE\").\n",
    "    ch_code : str\n",
    "        Code identifying CH (default: \"CH\").\n",
    "    use_aug : bool\n",
    "        If True uses df_train_aug/df_test_aug.\n",
    "    only_codes : list[str] or None\n",
    "        If given, only plot these codes.\n",
    "    skip_codes : list[str] or None\n",
    "        Codes to skip (CH is always skipped by default).\n",
    "    output_dir : str or None\n",
    "        If set, saves one PNG per code into this directory.\n",
    "    include_ch_in_title : bool\n",
    "        Adds CH vs CODE title on each figure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        code -> matplotlib Figure\n",
    "    \"\"\"\n",
    "    # palette (reuse your consistent colors)\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    palette = {\n",
    "        \"Train\": color_dark_blue,\n",
    "        \"Test\": \"#b2182b\"\n",
    "    }  # Train=CH, Test=Other\n",
    "\n",
    "    ch_code = str(ch_code).upper()\n",
    "    only_set = {c.upper() for c in only_codes} if only_codes else None\n",
    "    skip_set = {c.upper() for c in (skip_codes or [])}\n",
    "    skip_set.add(ch_code)\n",
    "\n",
    "    if use_aug:\n",
    "        df_ch = res_xreg.get(\"df_train_aug\")\n",
    "        df_test_all = res_xreg.get(\"df_test_aug\")\n",
    "        suffix = \"_aug\"\n",
    "    else:\n",
    "        df_ch = res_xreg.get(\"df_train\")\n",
    "        df_test_all = res_xreg.get(\"df_test\")\n",
    "        suffix = \"\"\n",
    "\n",
    "    if df_ch is None or len(df_ch) == 0:\n",
    "        raise ValueError(f\"Missing/empty df_train{suffix} in res_xreg.\")\n",
    "    if df_test_all is None or len(df_test_all) == 0:\n",
    "        raise ValueError(f\"Missing/empty df_test{suffix} in res_xreg.\")\n",
    "    if group_col not in df_test_all.columns:\n",
    "        raise ValueError(f\"'{group_col}' not found in df_test{suffix}.\")\n",
    "\n",
    "    codes = sorted(\n",
    "        df_test_all[group_col].dropna().astype(str).str.upper().unique())\n",
    "    codes = [c for c in codes if c not in skip_set]\n",
    "    if only_set is not None:\n",
    "        codes = [c for c in codes if c in only_set]\n",
    "\n",
    "    if output_dir:\n",
    "        out_abs = os.path.join(cfg.dataPath, output_dir) if hasattr(\n",
    "            cfg, \"dataPath\") else output_dir\n",
    "        os.makedirs(out_abs, exist_ok=True)\n",
    "    else:\n",
    "        out_abs = None\n",
    "\n",
    "    figs = {}\n",
    "\n",
    "    for code in codes:\n",
    "        df_other = df_test_all[df_test_all[group_col].astype(str).str.upper()\n",
    "                               == code].copy()\n",
    "        if len(df_other) == 0:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Plotting XREG KDE: CH(train n={len(df_ch)}) vs {code}(test n={len(df_other)})\"\n",
    "        )\n",
    "\n",
    "        fig = plot_feature_kde_overlap(\n",
    "            df_train=df_ch,\n",
    "            df_test=df_other,\n",
    "            features=features,\n",
    "            palette=palette,\n",
    "            outfile=None,  # save here instead (so we control naming)\n",
    "        )\n",
    "\n",
    "        if include_ch_in_title:\n",
    "            fig.suptitle(f\"XREG feature overlap: CH vs {code}\", fontsize=14)\n",
    "            fig.tight_layout()\n",
    "\n",
    "        if out_abs:\n",
    "            out_png = os.path.join(\n",
    "                out_abs, f\"xreg_kde_overlap_CH_vs_{code}{suffix}.png\")\n",
    "            fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "        figs[code] = fig\n",
    "\n",
    "    return figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = MONTHLY_COLS + STATIC_COLS + [\"POINT_BALANCE\"]\n",
    "\n",
    "figs_kde = plot_feature_kde_overlap_xreg_ch_vs_codes(\n",
    "    res_xreg=res_xreg,\n",
    "    cfg=cfg,\n",
    "    features=FEATURES,\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    use_aug=True,  # usually best for feature overlap\n",
    "    # only_codes=[\"IT_AT\", \"FR\"],    # optional\n",
    "    output_dir=\"figures/xreg_kde\",  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "### LSTM datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_for_nans(key,\n",
    "                    df_loss,\n",
    "                    df_full,\n",
    "                    monthly_cols,\n",
    "                    static_cols,\n",
    "                    strict=True):\n",
    "    \"\"\"\n",
    "    Checks for NaNs/Infs in features and targets.\n",
    "    Raises ValueError if strict=True, otherwise prints warning.\n",
    "    \"\"\"\n",
    "    feat_cols = [\n",
    "        c for c in (monthly_cols + static_cols) if c in df_full.columns\n",
    "    ]\n",
    "\n",
    "    # --- feature NaNs ---\n",
    "    n_nan_feat = df_full[feat_cols].isna().sum().sum()\n",
    "    n_inf_feat = np.isinf(df_full[feat_cols].to_numpy(dtype=\"float64\",\n",
    "                                                      copy=False)).sum()\n",
    "\n",
    "    # --- target NaNs ---\n",
    "    n_nan_target = df_loss[\"POINT_BALANCE\"].isna().sum()\n",
    "    n_inf_target = np.isinf(df_loss[\"POINT_BALANCE\"].to_numpy(\n",
    "        dtype=\"float64\", copy=False)).sum()\n",
    "\n",
    "    if any([n_nan_feat, n_inf_feat, n_nan_target, n_inf_target]):\n",
    "\n",
    "        msg = (f\"[{key}] Data integrity issue:\\n\"\n",
    "               f\"  Feature NaNs: {n_nan_feat}\\n\"\n",
    "               f\"  Feature Infs: {n_inf_feat}\\n\"\n",
    "               f\"  Target  NaNs: {n_nan_target}\\n\"\n",
    "               f\"  Target  Infs: {n_inf_target}\")\n",
    "\n",
    "        if strict:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            warnings.warn(msg)\n",
    "\n",
    "\n",
    "def _lstm_cache_paths(cfg, key: str, cache_dir: str):\n",
    "    out_dir = os.path.join(cache_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_p = os.path.join(out_dir, f\"{key}_train.joblib\")\n",
    "    test_p = os.path.join(out_dir, f\"{key}_test.joblib\")\n",
    "    split_p = os.path.join(out_dir, f\"{key}_split.joblib\")\n",
    "    return train_p, test_p, split_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build/load a PRISTINE dataset only (no scalers inside)\n",
    "# ------------------------------------------------------------\n",
    "def build_or_load_lstm_dataset_only(\n",
    "        cfg,\n",
    "        key: str,\n",
    "        df_loss,\n",
    "        df_full,\n",
    "        months_head_pad,\n",
    "        months_tail_pad,\n",
    "        MONTHLY_COLS,\n",
    "        STATIC_COLS,\n",
    "        cache_dir=\"logs/LSTM_cache\",\n",
    "        force_recompute=False,\n",
    "        normalize_target=True,\n",
    "        expect_target=True,\n",
    "        strict_nan=True,\n",
    "        kind=\"dataset\",  # keep kind to avoid duplicate functions; default \"dataset\"\n",
    "):\n",
    "    out_dir = os.path.join(cache_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    p = os.path.join(out_dir, f\"{key}_{kind}.joblib\")\n",
    "\n",
    "    # ---- Load cached (must be pristine) ----\n",
    "    if (not force_recompute) and os.path.exists(p):\n",
    "        ds = joblib.load(p)\n",
    "        if (ds.month_mean is not None) or (ds.static_mean\n",
    "                                           is not None) or (ds.y_mean\n",
    "                                                            is not None):\n",
    "            raise ValueError(\n",
    "                f\"{key}_{kind}: cached dataset already has scalers set. \"\n",
    "                \"Cache should store pristine datasets only.\")\n",
    "        return ds\n",
    "\n",
    "    # ---- Build fresh ----\n",
    "    _check_for_nans(\n",
    "        key,\n",
    "        df_loss=df_loss,\n",
    "        df_full=df_full,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        strict=strict_nan,\n",
    "    )\n",
    "\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds = build_combined_LSTM_dataset(\n",
    "        df_loss=df_loss,\n",
    "        df_full=df_full,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    # sanity: ensure pristine before caching\n",
    "    if (ds.month_mean is not None) or (ds.static_mean\n",
    "                                       is not None) or (ds.y_mean is not None):\n",
    "        raise ValueError(\n",
    "            f\"{key}_{kind}: newly built dataset unexpectedly has scalers set.\")\n",
    "\n",
    "    joblib.dump(ds, p, compress=3)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Transfer-learning slicing (no scaling logic here)\n",
    "# ------------------------------------------------------------\n",
    "def make_res_transfer_learning(\n",
    "    res_xreg: dict,\n",
    "    target_code: str,\n",
    "    ft_glaciers: list,\n",
    "    source_col=\"SOURCE_CODE\",\n",
    "    glacier_col=\"GLACIER\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      res_pretrain: CH-only (df_train/df_train_aug + pads)\n",
    "      res_ft: target finetune subset (df_train/df_train_aug + pads)\n",
    "      res_test: target holdout (df_test/df_test_aug + pads)\n",
    "    \"\"\"\n",
    "    res_pretrain = {\n",
    "        \"df_train\": res_xreg[\"df_train\"],\n",
    "        \"df_train_aug\": res_xreg[\"df_train_aug\"],\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    df_t_all = res_xreg[\"df_test\"]\n",
    "    df_t_all_aug = res_xreg[\"df_test_aug\"]\n",
    "\n",
    "    df_target = df_t_all.loc[df_t_all[source_col] == target_code].copy()\n",
    "    df_target_aug = df_t_all_aug.loc[df_t_all_aug[source_col] ==\n",
    "                                     target_code].copy()\n",
    "\n",
    "    df_ft = df_target.loc[df_target[glacier_col].isin(ft_glaciers)].copy()\n",
    "    df_ft_aug = df_target_aug.loc[df_target_aug[glacier_col].isin(\n",
    "        ft_glaciers)].copy()\n",
    "\n",
    "    df_hold = df_target.loc[~df_target[glacier_col].isin(ft_glaciers)].copy()\n",
    "    df_hold_aug = df_target_aug.loc[~df_target_aug[glacier_col].\n",
    "                                    isin(ft_glaciers)].copy()\n",
    "\n",
    "    res_ft = {\n",
    "        \"df_train\": df_ft,\n",
    "        \"df_train_aug\": df_ft_aug,\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    res_test = {\n",
    "        \"df_test\": df_hold,\n",
    "        \"df_test_aug\": df_hold_aug,\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    return res_pretrain, res_ft, res_test\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Build/load CH train dataset + split + SCALER DONOR (Option 2)\n",
    "# ------------------------------------------------------------\n",
    "def build_or_load_lstm_train_only(\n",
    "    cfg,\n",
    "    key_train: str,\n",
    "    res_train: dict,  # must contain df_train, df_train_aug, pads\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    val_ratio=0.2,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    force_recompute=False,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "    strict_nan=True,\n",
    "):\n",
    "    train_p, _, split_p = _lstm_cache_paths(cfg,\n",
    "                                            key_train,\n",
    "                                            cache_dir=cache_dir)\n",
    "    scaler_p = os.path.join(cache_dir, f\"{key_train}_scalers.joblib\")\n",
    "\n",
    "    # ---- Load cached assets (train ds must be pristine; scalers ds must have scalers) ----\n",
    "    if (not force_recompute) and all(\n",
    "            os.path.exists(p) for p in [train_p, split_p, scaler_p]):\n",
    "        ds_train = joblib.load(train_p)\n",
    "        split = joblib.load(split_p)\n",
    "        ds_scalers = joblib.load(scaler_p)\n",
    "\n",
    "        # guards\n",
    "        if (ds_train.month_mean\n",
    "                is not None) or (ds_train.static_mean\n",
    "                                 is not None) or (ds_train.y_mean is not None):\n",
    "            raise ValueError(\n",
    "                f\"{key_train}: cached TRAIN dataset has scalers set. \"\n",
    "                \"train_p cache must store pristine dataset only.\")\n",
    "        if (ds_scalers.month_mean is None) or (ds_scalers.static_mean\n",
    "                                               is None) or (ds_scalers.y_mean\n",
    "                                                            is None):\n",
    "            raise ValueError(\n",
    "                f\"{key_train}: cached SCALER donor is missing scalers.\")\n",
    "\n",
    "        return ds_train, split[\"train_idx\"], split[\"val_idx\"], ds_scalers\n",
    "\n",
    "    # ---- Build fresh ----\n",
    "    df_train = res_train[\"df_train\"]\n",
    "    df_train_aug = res_train[\"df_train_aug\"]\n",
    "    months_head_pad = res_train[\"months_head_pad\"]\n",
    "    months_tail_pad = res_train[\"months_tail_pad\"]\n",
    "\n",
    "    _check_for_nans(\n",
    "        key_train,\n",
    "        df_loss=df_train,\n",
    "        df_full=df_train_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        strict=strict_nan,\n",
    "    )\n",
    "\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_train = build_combined_LSTM_dataset(\n",
    "        df_loss=df_train,\n",
    "        df_full=df_train_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    # split indices\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=val_ratio, seed=cfg.seed)\n",
    "\n",
    "    # ---- NEW: create scaler donor and fit scalers on CH TRAIN split only ----\n",
    "    ds_scalers = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        ds_train)\n",
    "    ds_scalers.fit_scalers(train_idx)\n",
    "\n",
    "    # ---- Cache ----\n",
    "    joblib.dump(ds_train, train_p, compress=3)\n",
    "    joblib.dump({\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx\n",
    "    },\n",
    "                split_p,\n",
    "                compress=3)\n",
    "    joblib.dump(ds_scalers, scaler_p, compress=3)\n",
    "\n",
    "    return ds_train, train_idx, val_idx, ds_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_source_codes_for_dataset(ds, df_monthly, source_col=\"SOURCE_CODE\"):\n",
    "    \"\"\"\n",
    "    Returns a list[str] of SOURCE_CODE aligned with ds.keys.\n",
    "    Assumes SOURCE_CODE is constant per (GLACIER, YEAR, ID, PERIOD).\n",
    "    \"\"\"\n",
    "    if source_col not in df_monthly.columns:\n",
    "        raise KeyError(f\"df_monthly is missing '{source_col}'\")\n",
    "\n",
    "    # mapping per sequence key\n",
    "    key_cols = [\"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\"]\n",
    "    miss = [c for c in key_cols if c not in df_monthly.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"df_monthly missing required key cols: {miss}\")\n",
    "\n",
    "    tmp = df_monthly[key_cols + [source_col]].copy()\n",
    "    tmp[\"PERIOD\"] = tmp[\"PERIOD\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # If a key appears with multiple source codes, that's a data issue\n",
    "    nun = tmp.groupby(key_cols)[source_col].nunique()\n",
    "    bad = nun[nun > 1]\n",
    "    if len(bad) > 0:\n",
    "        ex = bad.index[:5].tolist()\n",
    "        raise ValueError(\n",
    "            f\"Found keys with multiple SOURCE_CODE values (showing first 5): {ex}\"\n",
    "        )\n",
    "\n",
    "    key_to_sc = tmp.groupby(key_cols)[source_col].first().to_dict()\n",
    "\n",
    "    out = []\n",
    "    for (g, yr, mid, per) in ds.keys:\n",
    "        k = (g, int(yr), int(mid), str(per).strip().lower())\n",
    "        if k not in key_to_sc:\n",
    "            raise KeyError(f\"Missing SOURCE_CODE for ds key {k}\")\n",
    "        out.append(key_to_sc[k])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_assets(\n",
    "    cfg,\n",
    "    res_xreg,\n",
    "    FT_GLACIERS,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL\",\n",
    "    force_recompute=False,\n",
    "    val_ratio=0.2,\n",
    "):\n",
    "    logging.info(\"\\n\" + \"=\" * 70)\n",
    "    logging.info(\"TRANSFER LEARNING ASSET PREPARATION\")\n",
    "    logging.info(\"=\" * 70)\n",
    "    logging.info(f\"Cache directory: {cache_dir}\")\n",
    "    logging.info(f\"Regions in FT_GLACIERS: {list(FT_GLACIERS.keys())}\")\n",
    "\n",
    "    assets = {}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) CH PRETRAIN DATASET (shared across all TL experiments)\n",
    "    # ------------------------------------------------------------------\n",
    "    key_train = \"TL_CH_TRAIN\"\n",
    "\n",
    "    logging.info(\"\\n--- CH PRETRAIN DATASET ---\")\n",
    "    logging.info(f\"Cache key: {key_train}\")\n",
    "    logging.info(f\"Force recompute: {force_recompute}\")\n",
    "\n",
    "    res_train = {\n",
    "        \"df_train\": res_xreg[\"df_train\"],\n",
    "        \"df_train_aug\": res_xreg[\"df_train_aug\"],\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    logging.info(f\"CH train rows: {len(res_train['df_train'])} | \"\n",
    "                 f\"Aug rows: {len(res_train['df_train_aug'])}\")\n",
    "\n",
    "    # ---- Option 2: also returns ds_ch_scalers (cached) ----\n",
    "    ds_ch, train_idx, val_idx, ds_ch_scalers = build_or_load_lstm_train_only(\n",
    "        cfg=cfg,\n",
    "        key_train=key_train,\n",
    "        res_train=res_train,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        val_ratio=val_ratio,\n",
    "        cache_dir=cache_dir,\n",
    "        force_recompute=force_recompute,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: do NOT fit scalers on ds_ch here anymore\n",
    "    # ds_ch_scalers is the scaler donor; ds_ch stays pristine.\n",
    "\n",
    "    logging.info(f\"CH dataset size (sequences): {len(ds_ch)} | \"\n",
    "                 f\"Train split: {len(train_idx)} | Val split: {len(val_idx)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) PER REGION × SPLIT\n",
    "    # ------------------------------------------------------------------\n",
    "    for reg, splits in FT_GLACIERS.items():\n",
    "\n",
    "        logging.info(\"\\n\" + \"-\" * 60)\n",
    "        logging.info(f\"TARGET REGION: {reg}\")\n",
    "        logging.info(\"-\" * 60)\n",
    "\n",
    "        for split_name, ft_gls in splits.items():\n",
    "\n",
    "            exp_key = f\"TL_CH_to_{reg}_{split_name}\"\n",
    "\n",
    "            logging.info(\"\\n\" + \"-\" * 40)\n",
    "            logging.info(f\"Experiment: {exp_key}\")\n",
    "            logging.info(f\"Finetune glacier count: {len(ft_gls)}\")\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # Slice finetune + holdout\n",
    "            # ----------------------------------------------------------\n",
    "            res_pre, res_ft, res_test = make_res_transfer_learning(\n",
    "                res_xreg=res_xreg,\n",
    "                target_code=reg,\n",
    "                ft_glaciers=ft_gls,\n",
    "            )\n",
    "\n",
    "            logging.info(f\"FT rows: {len(res_ft['df_train'])} | \"\n",
    "                         f\"FT aug rows: {len(res_ft['df_train_aug'])}\")\n",
    "\n",
    "            logging.info(f\"Holdout rows: {len(res_test['df_test'])} | \"\n",
    "                         f\"Holdout aug rows: {len(res_test['df_test_aug'])}\")\n",
    "\n",
    "            if len(res_ft[\"df_train\"]) == 0:\n",
    "                logging.warning(f\"{exp_key}: EMPTY FINETUNE SET -> skipping.\")\n",
    "                continue\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # Finetune dataset (PRISTINE)\n",
    "            # ----------------------------------------------------------\n",
    "            ft_cache_key = f\"{exp_key}_FT\"\n",
    "            logging.info(f\"Finetune cache key: {ft_cache_key}\")\n",
    "\n",
    "            ds_ft = build_or_load_lstm_dataset_only(\n",
    "                cfg=cfg,\n",
    "                key=ft_cache_key,\n",
    "                df_loss=res_ft[\"df_train\"],\n",
    "                df_full=res_ft[\"df_train_aug\"],\n",
    "                months_head_pad=res_ft[\"months_head_pad\"],\n",
    "                months_tail_pad=res_ft[\"months_tail_pad\"],\n",
    "                MONTHLY_COLS=MONTHLY_COLS,\n",
    "                STATIC_COLS=STATIC_COLS,\n",
    "                cache_dir=cache_dir,\n",
    "                force_recompute=force_recompute,\n",
    "                kind=\"ft\",\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Finetune dataset size (sequences): {len(ds_ft)}\")\n",
    "\n",
    "            ft_train_idx, ft_val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "                len(ds_ft), val_ratio=val_ratio, seed=cfg.seed)\n",
    "\n",
    "            logging.info(f\"FT train split: {len(ft_train_idx)} | \"\n",
    "                         f\"FT val split: {len(ft_val_idx)}\")\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # Holdout test dataset (PRISTINE)\n",
    "            # ----------------------------------------------------------\n",
    "            ds_test = None\n",
    "            if len(res_test[\"df_test\"]) > 0:\n",
    "\n",
    "                test_cache_key = f\"{exp_key}_TEST\"\n",
    "                logging.info(f\"Holdout cache key: {test_cache_key}\")\n",
    "\n",
    "                ds_test = build_or_load_lstm_dataset_only(\n",
    "                    cfg=cfg,\n",
    "                    key=test_cache_key,\n",
    "                    df_loss=res_test[\"df_test\"],\n",
    "                    df_full=res_test[\"df_test_aug\"],\n",
    "                    months_head_pad=res_test[\"months_head_pad\"],\n",
    "                    months_tail_pad=res_test[\"months_tail_pad\"],\n",
    "                    MONTHLY_COLS=MONTHLY_COLS,\n",
    "                    STATIC_COLS=STATIC_COLS,\n",
    "                    cache_dir=cache_dir,\n",
    "                    force_recompute=force_recompute,\n",
    "                    kind=\"test\",\n",
    "                )\n",
    "\n",
    "                logging.info(\n",
    "                    f\"Holdout dataset size (sequences): {len(ds_test)}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"{exp_key}: No holdout test set available.\")\n",
    "\n",
    "            ft_source_codes = build_source_codes_for_dataset(\n",
    "                ds_ft, res_ft[\"df_train_aug\"], source_col=\"SOURCE_CODE\")\n",
    "\n",
    "            test_source_codes = None\n",
    "            if ds_test is not None:\n",
    "                test_source_codes = build_source_codes_for_dataset(\n",
    "                    ds_test, res_test[\"df_test_aug\"], source_col=\"SOURCE_CODE\")\n",
    "\n",
    "            # ----------------------------------------------------------\n",
    "            # Store assets (include ds_ch_scalers!)\n",
    "            # ----------------------------------------------------------\n",
    "            assets[exp_key] = {\n",
    "                \"ds_pretrain\": ds_ch,  # pristine CH dataset\n",
    "                \"ds_pretrain_scalers\":\n",
    "                ds_ch_scalers,  # <-- IMPORTANT: scaler donor\n",
    "                \"pretrain_train_idx\": train_idx,\n",
    "                \"pretrain_val_idx\": val_idx,\n",
    "                \"ds_finetune\": ds_ft,  # pristine FT dataset\n",
    "                \"finetune_train_idx\": ft_train_idx,\n",
    "                \"finetune_val_idx\": ft_val_idx,\n",
    "                \"ds_test\": ds_test,  # pristine test dataset\n",
    "                \"target_code\": reg,\n",
    "                \"split_name\": split_name,\n",
    "                \"ft_glaciers\": ft_gls,\n",
    "                \"cache_keys\": {\n",
    "                    \"pretrain\": key_train,\n",
    "                    \"finetune\": ft_cache_key,\n",
    "                    \"test\": f\"{exp_key}_TEST\",\n",
    "                },\n",
    "                \"ft_source_codes\": ft_source_codes,\n",
    "                \"test_source_codes\": test_source_codes,\n",
    "            }\n",
    "\n",
    "    logging.info(\"\\nFinished building transfer learning assets.\")\n",
    "    logging.info(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    return assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_assets = build_transfer_learning_assets(\n",
    "    cfg=cfg,\n",
    "    res_xreg=res_xreg,\n",
    "    FT_GLACIERS=FT_GLACIERS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL\",\n",
    "    force_recompute=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tl_assets.items():\n",
    "    print(\"\\n\", \"=\" * 60)\n",
    "    print(\"Experiment:\", k)\n",
    "    print(\"Available keys:\", list(v.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_key, assets in tl_assets.items():\n",
    "    ft_unique = set(assets[\"ft_source_codes\"])\n",
    "    test_unique = set(\n",
    "        assets[\"test_source_codes\"]) if assets[\"test_source_codes\"] else set()\n",
    "    print(f\"{exp_key} | FT domains: {ft_unique} | TEST domains: {test_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 1,\n",
    "    'static_hidden': 128,\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-05,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.1,\n",
    "    'loss_spec': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_param_groups_lstm_mb(model, lr_lstm, lr_static, lr_head,\n",
    "                              weight_decay):\n",
    "    groups = {\"lstm\": [], \"static\": [], \"head\": []}\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if name.startswith(\"lstm.\"):\n",
    "            groups[\"lstm\"].append(p)\n",
    "        elif name.startswith(\"static_mlp.\"):\n",
    "            groups[\"static\"].append(p)\n",
    "        else:\n",
    "            groups[\"head\"].append(p)\n",
    "\n",
    "    param_groups = []\n",
    "    if groups[\"head\"]:\n",
    "        param_groups.append({\n",
    "            \"params\": groups[\"head\"],\n",
    "            \"lr\": lr_head,\n",
    "            \"weight_decay\": weight_decay\n",
    "        })\n",
    "    if groups[\"static\"]:\n",
    "        param_groups.append({\n",
    "            \"params\": groups[\"static\"],\n",
    "            \"lr\": lr_static,\n",
    "            \"weight_decay\": weight_decay\n",
    "        })\n",
    "    if groups[\"lstm\"]:\n",
    "        param_groups.append({\n",
    "            \"params\": groups[\"lstm\"],\n",
    "            \"lr\": lr_lstm,\n",
    "            \"weight_decay\": weight_decay\n",
    "        })\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "def make_l2sp_loss(base_loss_fn,\n",
    "                   model,\n",
    "                   anchor_state_dict,\n",
    "                   lam=1e-4,\n",
    "                   include_prefixes=(\"lstm.\", \"static_mlp.\", \"head\")):\n",
    "    \"\"\"\n",
    "    base_loss_fn: callable(outputs, batch) -> torch scalar\n",
    "    anchor_state_dict: state dict tensors (CPU is fine; moved to device per param)\n",
    "    include_prefixes: which parameter name prefixes get anchored\n",
    "    \"\"\"\n",
    "    anchor = {k: v.detach().clone() for k, v in anchor_state_dict.items()}\n",
    "\n",
    "    def loss_fn(outputs, batch):\n",
    "        base = base_loss_fn(outputs, batch)\n",
    "\n",
    "        reg = 0.0\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if include_prefixes is not None and not any(\n",
    "                    name.startswith(pref) for pref in include_prefixes):\n",
    "                continue\n",
    "            if name in anchor:\n",
    "                reg = reg + torch.sum((p - anchor[name].to(p.device))**2)\n",
    "\n",
    "        return base + lam * reg\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_loaders_TL(\n",
    "    ds_tl,  # MBSequenceDatasetTL\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    *,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=42,\n",
    "    shuffle_train=True,\n",
    "    drop_last_train=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    use_weighted_sampler=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Like MBSequenceDataset.make_loaders, but works on MBSequenceDatasetTL wrapper.\n",
    "    Assumes base dataset is already scaled/transformed (fit_and_transform already done elsewhere).\n",
    "    \"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    # Ensure reproducible sampling\n",
    "    rd.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    def _seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        rd.seed(worker_seed)\n",
    "        torch.manual_seed(worker_seed)\n",
    "\n",
    "    train_ds = Subset(ds_tl, train_idx)\n",
    "    val_ds = Subset(ds_tl, val_idx)\n",
    "\n",
    "    if use_weighted_sampler:\n",
    "        # weights are based on winter/annual flags stored on the BASE dataset\n",
    "        base = ds_tl.base\n",
    "        iw = base.iw[train_idx].cpu().numpy().astype(bool)\n",
    "        ia = base.ia[train_idx].cpu().numpy().astype(bool)\n",
    "        n_w, n_a = int(iw.sum()), int(ia.sum())\n",
    "\n",
    "        if (n_w == 0) or (n_a == 0):\n",
    "            if verbose:\n",
    "                print(f\"Weighted sampler disabled (one class missing): \"\n",
    "                      f\"{n_w} winter | {n_a} annual. Using shuffle instead.\")\n",
    "            train_dl = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=batch_size_train,\n",
    "                shuffle=shuffle_train,\n",
    "                drop_last=drop_last_train,\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                worker_init_fn=_seed_worker,\n",
    "                generator=g,\n",
    "            )\n",
    "        else:\n",
    "            w_w = 1.0\n",
    "            w_a = n_w / n_a  # >0 since both >0\n",
    "            sample_weights = np.where(ia, w_a, w_w).astype(np.float32)\n",
    "\n",
    "            sw_sum = float(sample_weights.sum())\n",
    "            if (not np.isfinite(sw_sum)) or (sw_sum <= 0.0):\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        \"Weighted sampler disabled (invalid weights distribution). \"\n",
    "                        \"Using shuffle instead.\")\n",
    "                train_dl = DataLoader(\n",
    "                    train_ds,\n",
    "                    batch_size=batch_size_train,\n",
    "                    shuffle=shuffle_train,\n",
    "                    drop_last=drop_last_train,\n",
    "                    num_workers=num_workers,\n",
    "                    pin_memory=pin_memory,\n",
    "                    worker_init_fn=_seed_worker,\n",
    "                    generator=g,\n",
    "                )\n",
    "            else:\n",
    "                sample_weights = torch.from_numpy(sample_weights)\n",
    "                sampler = WeightedRandomSampler(\n",
    "                    sample_weights,\n",
    "                    num_samples=len(sample_weights),\n",
    "                    replacement=True,\n",
    "                    generator=g,\n",
    "                )\n",
    "                train_dl = DataLoader(\n",
    "                    train_ds,\n",
    "                    batch_size=batch_size_train,\n",
    "                    sampler=sampler,\n",
    "                    drop_last=drop_last_train,\n",
    "                    num_workers=num_workers,\n",
    "                    pin_memory=pin_memory,\n",
    "                    worker_init_fn=_seed_worker,\n",
    "                    generator=g,\n",
    "                )\n",
    "    else:\n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size_train,\n",
    "            shuffle=shuffle_train,\n",
    "            drop_last=drop_last_train,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            worker_init_fn=_seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "\n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        worker_init_fn=_seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        base = ds_tl.base\n",
    "        n_w_tr, n_a_tr = int(base.iw[train_idx].sum()), int(\n",
    "            base.ia[train_idx].sum())\n",
    "        n_w_va, n_a_va = int(base.iw[val_idx].sum()), int(\n",
    "            base.ia[val_idx].sum())\n",
    "        print(f\"Train counts: {n_w_tr} winter | {n_a_tr} annual\")\n",
    "        print(f\"Val   counts: {n_w_va} winter | {n_a_va} annual\")\n",
    "\n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def freeze_lstm_only(model):\n",
    "    for name, p in model.named_parameters():\n",
    "        if name.startswith(\"lstm.\"):\n",
    "            p.requires_grad = False\n",
    "        else:\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_finetune_loaders_for_exp(\n",
    "#     cfg,\n",
    "#     tl_assets_for_key,\n",
    "#     batch_size_train=64,\n",
    "#     batch_size_val=128,\n",
    "# ):\n",
    "#     ds_ft = tl_assets_for_key[\"ds_finetune\"]\n",
    "#     train_idx = tl_assets_for_key[\"finetune_train_idx\"]\n",
    "#     val_idx = tl_assets_for_key[\"finetune_val_idx\"]\n",
    "\n",
    "#     # ---- NEW: scaler donor from assets ----\n",
    "#     ds_ch_scalers = tl_assets_for_key[\"ds_pretrain_scalers\"]\n",
    "#     assert ds_ch_scalers.month_mean is not None, \"CH scaler donor has no fitted scalers!\"\n",
    "\n",
    "#     ds_ft_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#         ds_ft)\n",
    "\n",
    "#     # ---- apply CH scalers ----\n",
    "#     ds_ft_copy.set_scalers_from(ds_ch_scalers)\n",
    "#     ds_ft_copy.transform_inplace()\n",
    "\n",
    "#     # build loaders, no fitting\n",
    "#     ft_train_dl, ft_val_dl = ds_ft_copy.make_loaders(\n",
    "#         train_idx=train_idx,\n",
    "#         val_idx=val_idx,\n",
    "#         batch_size_train=batch_size_train,\n",
    "#         batch_size_val=batch_size_val,\n",
    "#         seed=cfg.seed,\n",
    "#         fit_and_transform=False,  # IMPORTANT\n",
    "#         shuffle_train=True,\n",
    "#         use_weighted_sampler=True,\n",
    "#     )\n",
    "#     return ds_ft_copy, ft_train_dl, ft_val_dl\n",
    "\n",
    "\n",
    "def make_finetune_loaders_for_exp(\n",
    "        cfg,\n",
    "        tl_assets_for_key,\n",
    "        batch_size_train=64,\n",
    "        batch_size_val=128,\n",
    "        domain_vocab=None,  # optional: {\"CH\":0,\"NOR\":1,...}\n",
    "):\n",
    "    ds_ft = tl_assets_for_key[\"ds_finetune\"]\n",
    "    train_idx = tl_assets_for_key[\"finetune_train_idx\"]\n",
    "    val_idx = tl_assets_for_key[\"finetune_val_idx\"]\n",
    "\n",
    "    # ---- scaler donor from assets ----\n",
    "    ds_ch_scalers = tl_assets_for_key[\"ds_pretrain_scalers\"]\n",
    "    assert ds_ch_scalers.month_mean is not None, \"CH scaler donor has no fitted scalers!\"\n",
    "\n",
    "    # ---- source codes aligned with ds_ft.keys ----\n",
    "    ft_source_codes = tl_assets_for_key.get(\"ft_source_codes\", None)\n",
    "    if ft_source_codes is None:\n",
    "        raise KeyError(\n",
    "            \"tl_assets_for_key missing 'ft_source_codes' (required for TL wrapper).\"\n",
    "        )\n",
    "    if len(ft_source_codes) != len(ds_ft):\n",
    "        raise ValueError(\n",
    "            f\"ft_source_codes length {len(ft_source_codes)} != len(ds_finetune) {len(ds_ft)}\"\n",
    "        )\n",
    "\n",
    "    # ---- clone pristine + apply CH scalers ----\n",
    "    ds_ft_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        ds_ft)\n",
    "    ds_ft_copy.set_scalers_from(ds_ch_scalers)\n",
    "    ds_ft_copy.transform_inplace()\n",
    "\n",
    "    # ---- wrap to inject domain labels (SOURCE_CODE / domain_id) ----\n",
    "    ds_ft_tl = mbm.data_processing.MBSequenceDatasetTL(\n",
    "        base_ds=ds_ft_copy,\n",
    "        source_codes=ft_source_codes,\n",
    "        domain_vocab=domain_vocab,\n",
    "    )\n",
    "\n",
    "    # ---- build loaders from wrapper (NOT ds_ft_copy.make_loaders) ----\n",
    "    ft_train_dl, ft_val_dl = make_loaders_TL(\n",
    "        ds_ft_tl,\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        batch_size_train=batch_size_train,\n",
    "        batch_size_val=batch_size_val,\n",
    "        seed=cfg.seed,\n",
    "        shuffle_train=True,\n",
    "        use_weighted_sampler=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return ds_ft_tl, ft_train_dl, ft_val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_or_load_one_TL(\n",
    "    cfg,\n",
    "    exp_key: str,  # e.g. \"TL_CH_to_ISL_5pct\"\n",
    "    tl_assets_for_key: dict,\n",
    "    best_params: dict,\n",
    "    device,\n",
    "    pretrained_ckpt_path: str,  # CH model checkpoint to start from\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_TL\",\n",
    "    strategy=\"safe\",  # existing: \"safe\" | \"full\" | \"two_stage\"\n",
    "    # new: \"disc_full\" | \"l2sp_safe\" | \"l2sp_full\" | \"disc_l2sp_full\"\n",
    "    force_retrain=False,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    epochs_safe=60,\n",
    "    epochs_full=80,\n",
    "    stage1_epochs=20,\n",
    "    stage2_epochs=60,\n",
    "    lr_safe=1e-4,\n",
    "    lr_full=1e-5,\n",
    "    lr_stage1=2e-4,\n",
    "    lr_stage2=1e-5,\n",
    "    # ---- NEW knobs (optional) ----\n",
    "    lr_head=5e-5,\n",
    "    lr_static=1e-5,\n",
    "    lr_lstm=5e-6,\n",
    "    l2sp_lambda=1e-4,\n",
    "    l2sp_include_prefixes=(\"lstm.\", \"static_mlp.\", \"head\", \"head_w\", \"head_a\"),\n",
    "):\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    out_name = f\"{prefix}_{exp_key}_{strategy}_{current_date}.pt\"\n",
    "    out_path = os.path.join(models_dir, out_name)\n",
    "\n",
    "    # load if exists\n",
    "    if (not force_retrain) and os.path.exists(out_path):\n",
    "        model = mbm.models.LSTM_MB.build_model_from_params(\n",
    "            cfg, best_params, device)\n",
    "        state = torch.load(out_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        return model, out_path, None\n",
    "\n",
    "    # build model + base loss\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params,\n",
    "                                                       device)\n",
    "    base_loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "    # load pretrained weights (CH)\n",
    "    state = torch.load(pretrained_ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    # anchor for L2-SP (snapshot right after loading pretrained)\n",
    "    anchor_state = {\n",
    "        k: v.detach().cpu().clone()\n",
    "        for k, v in model.state_dict().items()\n",
    "    }\n",
    "\n",
    "    # loaders\n",
    "    ds_ft_tl, ft_train_dl, ft_val_dl = make_finetune_loaders_for_exp(\n",
    "        cfg,\n",
    "        tl_assets_for_key,\n",
    "        batch_size_train=batch_size_train,\n",
    "        batch_size_val=batch_size_val,\n",
    "        domain_vocab=best_params.get(\"domain_vocab\", None),  # optional\n",
    "    )\n",
    "\n",
    "    # overwrite if retraining\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "        logging.info(f\"[{exp_key}] Deleted existing TL checkpoint: {out_path}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Existing strategies\n",
    "    # -------------------------\n",
    "    if strategy == \"safe\":\n",
    "        freeze_lstm_only(model)\n",
    "        opt = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr_safe,\n",
    "            weight_decay=best_params[\"weight_decay\"],\n",
    "        )\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=ft_train_dl,\n",
    "            val_dl=ft_val_dl,\n",
    "            epochs=epochs_safe,\n",
    "            optimizer=opt,\n",
    "            clip_val=1.0,\n",
    "            loss_fn=base_loss_fn,\n",
    "            es_patience=8,\n",
    "            save_best_path=out_path,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    elif strategy == \"full\":\n",
    "        unfreeze_all(model)\n",
    "        opt = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr_full,\n",
    "            weight_decay=best_params[\"weight_decay\"],\n",
    "        )\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=ft_train_dl,\n",
    "            val_dl=ft_val_dl,\n",
    "            epochs=epochs_full,\n",
    "            optimizer=opt,\n",
    "            clip_val=1.0,\n",
    "            loss_fn=base_loss_fn,\n",
    "            es_patience=10,\n",
    "            save_best_path=out_path,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    elif strategy == \"two_stage\":\n",
    "        tmp_stage1 = out_path.replace(\".pt\", \"_stage1_tmp.pt\")\n",
    "\n",
    "        freeze_lstm_only(model)\n",
    "        opt1 = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=lr_stage1,\n",
    "            weight_decay=best_params[\"weight_decay\"],\n",
    "        )\n",
    "        model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=ft_train_dl,\n",
    "            val_dl=ft_val_dl,\n",
    "            epochs=stage1_epochs,\n",
    "            optimizer=opt1,\n",
    "            clip_val=1.0,\n",
    "            loss_fn=base_loss_fn,\n",
    "            es_patience=5,\n",
    "            save_best_path=tmp_stage1,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        state = torch.load(tmp_stage1, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "        unfreeze_all(model)\n",
    "        opt2 = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr_stage2,\n",
    "            weight_decay=best_params[\"weight_decay\"],\n",
    "        )\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=ft_train_dl,\n",
    "            val_dl=ft_val_dl,\n",
    "            epochs=stage2_epochs,\n",
    "            optimizer=opt2,\n",
    "            clip_val=1.0,\n",
    "            loss_fn=base_loss_fn,\n",
    "            es_patience=10,\n",
    "            save_best_path=out_path,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            os.remove(tmp_stage1)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    elif strategy == \"disc_full\":\n",
    "        # all trainable, but different LRs per block\n",
    "        unfreeze_all(model)\n",
    "        opt = torch.optim.AdamW(\n",
    "            make_param_groups_lstm_mb(\n",
    "                model,\n",
    "                lr_lstm=float(best_params.get(\"lr_lstm\", lr_lstm)),\n",
    "                lr_static=float(best_params.get(\"lr_static\", lr_static)),\n",
    "                lr_head=float(best_params.get(\"lr_head\", lr_head)),\n",
    "                weight_decay=best_params[\"weight_decay\"],\n",
    "            ))\n",
    "        history, best_val, best_state = model.train_loop(\n",
    "            device=device,\n",
    "            train_dl=ft_train_dl,\n",
    "            val_dl=ft_val_dl,\n",
    "            epochs=epochs_full,\n",
    "            optimizer=opt,\n",
    "            clip_val=1.0,\n",
    "            loss_fn=base_loss_fn,\n",
    "            es_patience=10,\n",
    "            save_best_path=out_path,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    # load best\n",
    "    state = torch.load(out_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    return model, out_path, {\"history\": history, \"best_val\": best_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_TL_models_all(\n",
    "    cfg,\n",
    "    tl_assets_by_key: dict,  # e.g. tl_assets[\"TL_CH_to_ISL_5pct\"] -> {...}\n",
    "    best_params: dict,\n",
    "    device,\n",
    "    pretrained_ckpt_path: str,\n",
    "    strategies=(\"safe\", \"full\", \"two_stage\"),\n",
    "    train_keys=None,  # optional subset of exp_keys\n",
    "    force_retrain=False,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_TL\",\n",
    "):\n",
    "    models = {}\n",
    "    infos = {}\n",
    "\n",
    "    train_keys_set = set(train_keys) if train_keys else None\n",
    "\n",
    "    for exp_key in sorted(tl_assets_by_key.keys()):\n",
    "        if train_keys_set is not None and exp_key not in train_keys_set:\n",
    "            continue\n",
    "\n",
    "        assets = tl_assets_by_key[exp_key]\n",
    "        if assets is None or assets.get(\"ds_finetune\", None) is None:\n",
    "            logging.warning(f\"Skipping {exp_key}: missing finetune dataset.\")\n",
    "            continue\n",
    "\n",
    "        for strat in strategies:\n",
    "            run_key = f\"{exp_key}__{strat}\"\n",
    "            logging.info(f\"\\n=== FINETUNE {run_key} ===\")\n",
    "\n",
    "            model, path, info = finetune_or_load_one_TL(\n",
    "                cfg=cfg,\n",
    "                exp_key=exp_key,\n",
    "                tl_assets_for_key=assets,\n",
    "                best_params=best_params,\n",
    "                device=device,\n",
    "                pretrained_ckpt_path=pretrained_ckpt_path,\n",
    "                models_dir=models_dir,\n",
    "                prefix=prefix,\n",
    "                strategy=strat,\n",
    "                force_retrain=force_retrain,\n",
    "            )\n",
    "\n",
    "            models[run_key] = model\n",
    "            infos[run_key] = {\"model_path\": path, **(info or {})}\n",
    "\n",
    "    return models, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_load_CH_baseline(\n",
    "    cfg,\n",
    "    tl_assets: dict,  # the whole dict returned by build_transfer_learning_assets\n",
    "    default_params: dict,\n",
    "    device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_CH\",\n",
    "    key=\"BASELINE\",\n",
    "    train_flag=True,\n",
    "    force_retrain=False,\n",
    "    epochs=150,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a CH-only model on ds_pretrain using CH scalers from ds_pretrain_scalers.\n",
    "    Assumes all tl_assets share the same CH dataset + indices + scaler donor.\n",
    "    \"\"\"\n",
    "    any_key = next(iter(tl_assets.keys()))\n",
    "    assets0 = tl_assets[any_key]\n",
    "\n",
    "    ds_train_pristine = assets0[\"ds_pretrain\"]  # pristine CH dataset\n",
    "    ds_ch_scalers = assets0[\n",
    "        \"ds_pretrain_scalers\"]  # scaler donor (fitted on CH train split)\n",
    "    train_idx = assets0[\"pretrain_train_idx\"]\n",
    "    val_idx = assets0[\"pretrain_val_idx\"]\n",
    "\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    model_path = os.path.join(models_dir, f\"{prefix}_{key}_{current_date}.pt\")\n",
    "\n",
    "    # build model + loss\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg, default_params,\n",
    "                                                       device)\n",
    "    loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(default_params)\n",
    "\n",
    "    # load if exists\n",
    "    if (not train_flag) and os.path.exists(model_path):\n",
    "        state = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        return model, model_path, None\n",
    "\n",
    "    if train_flag and (not force_retrain) and os.path.exists(model_path):\n",
    "        state = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        return model, model_path, None\n",
    "\n",
    "    if (not train_flag) and (not os.path.exists(model_path)):\n",
    "        raise FileNotFoundError(f\"No CH checkpoint found: {model_path}\")\n",
    "\n",
    "    # loaders (DO NOT refit scalers; use ds_ch_scalers)\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_train_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "        ds_train_pristine)\n",
    "\n",
    "    # Apply CH scalers + transform once\n",
    "    ds_train_copy.set_scalers_from(ds_ch_scalers)\n",
    "    ds_train_copy.transform_inplace()\n",
    "\n",
    "    train_dl, val_dl = ds_train_copy.make_loaders(\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        batch_size_train=batch_size_train,\n",
    "        batch_size_val=batch_size_val,\n",
    "        seed=cfg.seed,\n",
    "        fit_and_transform=False,  # IMPORTANT: already transformed\n",
    "        shuffle_train=True,\n",
    "        use_weighted_sampler=True,\n",
    "    )\n",
    "\n",
    "    # fresh checkpoint\n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "        print(f\"Deleted existing CH model file: {model_path}\")\n",
    "\n",
    "    history, best_val, best_state = model.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl,\n",
    "        val_dl=val_dl,\n",
    "        epochs=epochs,\n",
    "        lr=default_params[\"lr\"],\n",
    "        weight_decay=default_params[\"weight_decay\"],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_path,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "    # load best\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    return model, model_path, {\"history\": history, \"best_val\": best_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ch, ch_path, ch_info = train_or_load_CH_baseline(\n",
    "    cfg=cfg,\n",
    "    tl_assets=tl_assets,\n",
    "    default_params=default_params,\n",
    "    device=device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_CH\",\n",
    "    key=\"defaultparams\",\n",
    "    train_flag=False,\n",
    "    force_retrain=False,  # set False after you have it once\n",
    "    epochs=150,\n",
    ")\n",
    "print(\"CH baseline saved at:\", ch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tl, infos_tl = finetune_TL_models_all(\n",
    "    cfg=cfg,\n",
    "    tl_assets_by_key=tl_assets,\n",
    "    best_params=default_params,\n",
    "    device=device,\n",
    "    pretrained_ckpt_path=ch_path,\n",
    "    strategies=(\"safe\", \"full\", \"two_stage\", \"disc_full\"),\n",
    "    force_retrain=True,\n",
    "    prefix=\"lstm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_test_loader_for_key_TL(cfg, tl_assets_for_key, batch_size=128):\n",
    "#     \"\"\"\n",
    "#     TL-only test loader builder.\n",
    "\n",
    "#     Uses CH scalers from tl_assets_for_key[\"ds_pretrain_scalers\"] and applies them to\n",
    "#     tl_assets_for_key[\"ds_test\"] (holdout target region).\n",
    "\n",
    "#     Returns (ds_scalers, ds_test_copy, test_dl) so the caller signature matches the old one.\n",
    "#     \"\"\"\n",
    "#     mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "#     ds_scalers = tl_assets_for_key[\n",
    "#         \"ds_pretrain_scalers\"]  # CH scaler donor (already fitted)\n",
    "#     ds_test = tl_assets_for_key[\"ds_test\"]  # pristine holdout dataset\n",
    "\n",
    "#     if ds_test is None:\n",
    "#         raise ValueError(\"TL assets have ds_test=None (no holdout set).\")\n",
    "\n",
    "#     # sanity: fitted scalers exist\n",
    "#     if (ds_scalers.month_mean is None) or (ds_scalers.static_mean\n",
    "#                                            is None) or (ds_scalers.y_mean\n",
    "#                                                         is None):\n",
    "#         raise ValueError(\n",
    "#             \"ds_pretrain_scalers is missing fitted scalers. Did Option-2 caching run?\"\n",
    "#         )\n",
    "\n",
    "#     # clone pristine test and transform using CH scalers\n",
    "#     ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "#         ds_test)\n",
    "\n",
    "#     test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "#         ds_test=ds_test_copy,\n",
    "#         ds_train=ds_scalers,\n",
    "#         seed=cfg.seed,\n",
    "#         batch_size=batch_size,\n",
    "#     )\n",
    "\n",
    "#     # return ds_scalers as first element to match old (ds_train_copy, ds_test_copy, test_dl)\n",
    "#     return ds_scalers, ds_test_copy, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_loader_for_key_TL(cfg, tl_assets_for_key, batch_size=128):\n",
    "    \"\"\"\n",
    "    TL-only test loader builder.\n",
    "\n",
    "    Uses CH scalers from tl_assets_for_key[\"ds_pretrain_scalers\"] and applies them to\n",
    "    tl_assets_for_key[\"ds_test\"] (holdout target region).\n",
    "\n",
    "    Also returns test_source_codes aligned with ds_test_copy.keys.\n",
    "    \"\"\"\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_scalers = tl_assets_for_key[\"ds_pretrain_scalers\"]  # fitted CH scaler donor\n",
    "    ds_test = tl_assets_for_key[\"ds_test\"]                 # pristine holdout dataset\n",
    "\n",
    "    if ds_test is None:\n",
    "        raise ValueError(\"TL assets have ds_test=None (no holdout set).\")\n",
    "\n",
    "    # sanity: scalers exist\n",
    "    if (ds_scalers.month_mean is None) or (ds_scalers.static_mean is None) or (ds_scalers.y_mean is None):\n",
    "        raise ValueError(\"ds_pretrain_scalers is missing fitted scalers.\")\n",
    "\n",
    "    # clone pristine test and transform using CH scalers\n",
    "    ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(ds_test)\n",
    "\n",
    "    test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "        ds_test=ds_test_copy,\n",
    "        ds_train=ds_scalers,\n",
    "        seed=cfg.seed,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # ---- NEW: aligned source codes for evaluation ----\n",
    "    test_source_codes = tl_assets_for_key.get(\"test_source_codes\", None)\n",
    "    if test_source_codes is None:\n",
    "        raise KeyError(\"tl_assets_for_key is missing 'test_source_codes'.\")\n",
    "\n",
    "    if len(test_source_codes) != len(ds_test_copy):\n",
    "        raise ValueError(\n",
    "            f\"test_source_codes length {len(test_source_codes)} != len(ds_test_copy) {len(ds_test_copy)}\"\n",
    "        )\n",
    "\n",
    "    return ds_scalers, ds_test_copy, test_dl, test_source_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_one_model_TL(\n",
    "#         cfg,\n",
    "#         model,\n",
    "#         device,\n",
    "#         tl_assets_for_key,\n",
    "#         ax=None,\n",
    "#         ax_xlim=(-16, 9),\n",
    "#         ax_ylim=(-16, 9),\n",
    "#         title=None,\n",
    "#         legend_fontsize=16,\n",
    "#         batch_size=128,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     TL-only evaluator (does not touch old within/xreg frameworks).\n",
    "\n",
    "#     - Builds a test loader with CH scalers via make_test_loader_for_key_TL\n",
    "#     - Uses model.evaluate_with_preds(device, test_dl, ds_test_copy) exactly like the original\n",
    "#     - Plots pred-vs-truth density exactly like the original\n",
    "#     \"\"\"\n",
    "#     _ds_scalers, ds_test_copy, test_dl = make_test_loader_for_key_TL(\n",
    "#         cfg, tl_assets_for_key, batch_size=batch_size)\n",
    "\n",
    "#     test_metrics, test_df_preds = model.evaluate_with_preds(\n",
    "#         device, test_dl, ds_test_copy)\n",
    "\n",
    "#     scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "#                                                            target_col=\"target\",\n",
    "#                                                            pred_col=\"pred\")\n",
    "\n",
    "#     out = {\n",
    "#         \"RMSE_annual\":\n",
    "#         float(test_metrics.get(\"RMSE_annual\", scores_annual[\"rmse\"])),\n",
    "#         \"RMSE_winter\":\n",
    "#         float(test_metrics.get(\"RMSE_winter\", scores_winter[\"rmse\"])),\n",
    "#         \"R2_annual\":\n",
    "#         float(scores_annual[\"R2\"]),\n",
    "#         \"R2_winter\":\n",
    "#         float(scores_winter[\"R2\"]),\n",
    "#         \"Bias_annual\":\n",
    "#         float(scores_annual[\"Bias\"]),\n",
    "#         \"Bias_winter\":\n",
    "#         float(scores_winter[\"Bias\"]),\n",
    "#         \"n_preds\":\n",
    "#         int(len(test_df_preds)),\n",
    "#         \"n_annual\": (int(scores_annual.get(\"n\", np.nan)) if isinstance(\n",
    "#             scores_annual, dict) else np.nan),\n",
    "#         \"n_winter\": (int(scores_winter.get(\"n\", np.nan)) if isinstance(\n",
    "#             scores_winter, dict) else np.nan),\n",
    "#     }\n",
    "\n",
    "#     # Plot\n",
    "#     created_fig = None\n",
    "#     if ax is None:\n",
    "#         created_fig = plt.figure(figsize=(15, 10))\n",
    "#         ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "#     #\n",
    "#     ax_xlim = (np.min((test_df_preds[[\"target\", \"pred\"]].min())) - 1,\n",
    "#                np.max((test_df_preds[[\"target\", \"pred\"]].max())) + 1)\n",
    "#     ax_ylim = ax_xlim\n",
    "#     pred_vs_truth_density(\n",
    "#         ax,\n",
    "#         test_df_preds,\n",
    "#         scores_annual,\n",
    "#         add_legend=False,\n",
    "#         palette=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "#         ax_xlim=ax_xlim,\n",
    "#         ax_ylim=ax_ylim,\n",
    "#     )\n",
    "\n",
    "#     def _fmt(x):\n",
    "#         return (\"NA\" if\n",
    "#                 (x is None or\n",
    "#                  (isinstance(x, float) and np.isnan(x))) else f\"{x:.2f}\")\n",
    "\n",
    "#     legend_NN = \"\\n\".join([\n",
    "#         rf\"$\\mathrm{{RMSE_a}}={_fmt(scores_annual['rmse'])},\\ \\mathrm{{RMSE_w}}={_fmt(scores_winter['rmse'])}$\",\n",
    "#         rf\"$\\mathrm{{R^2_a}}={_fmt(scores_annual['R2'])},\\ \\mathrm{{R^2_w}}={_fmt(scores_winter['R2'])}$\",\n",
    "#         rf\"$\\mathrm{{Bias_a}}={_fmt(scores_annual['Bias'])},\\ \\mathrm{{Bias_w}}={_fmt(scores_winter['Bias'])}$\",\n",
    "#     ])\n",
    "\n",
    "#     ax.text(\n",
    "#         0.02,\n",
    "#         0.98,\n",
    "#         legend_NN,\n",
    "#         transform=ax.transAxes,\n",
    "#         va=\"top\",\n",
    "#         fontsize=legend_fontsize,\n",
    "#         bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5),\n",
    "#     )\n",
    "\n",
    "#     if title:\n",
    "#         ax.set_title(title, fontsize=20)\n",
    "\n",
    "#     return out, test_df_preds, created_fig, ax\n",
    "\n",
    "\n",
    "def _pick_tl_exp_key_for_region(tl_assets_by_key, region, split_name=\"5pct\"):\n",
    "    k = f\"TL_CH_to_{region}_{split_name}\"\n",
    "    if k not in tl_assets_by_key:\n",
    "        raise KeyError(f\"Missing TL assets for {k}\")\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_model_TL(\n",
    "    cfg,\n",
    "    model,\n",
    "    device,\n",
    "    tl_assets_for_key,\n",
    "    ax=None,\n",
    "    ax_xlim=None,\n",
    "    ax_ylim=None,\n",
    "    title=None,\n",
    "    legend_fontsize=16,\n",
    "    batch_size=128,\n",
    "    domain_vocab=None,  # optional: {\"CH\":0,\"NOR\":1,...}\n",
    "):\n",
    "    \"\"\"\n",
    "    TL-only evaluator.\n",
    "\n",
    "    - Builds a test loader with CH scalers via make_test_loader_for_key_TL\n",
    "    - Runs model.evaluate_with_preds(...)\n",
    "    - Adds SOURCE_CODE (and domain_id if vocab provided) to df_preds\n",
    "    - Plots pred-vs-truth density like the original\n",
    "    \"\"\"\n",
    "    _ds_scalers, ds_test_copy, test_dl, test_source_codes = make_test_loader_for_key_TL(\n",
    "        cfg, tl_assets_for_key, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_metrics, test_df_preds = model.evaluate_with_preds(device, test_dl, ds_test_copy)\n",
    "\n",
    "    # ---- attach SOURCE_CODE/domain_id aligned by sequence keys ----\n",
    "    key_to_sc = {k: sc for k, sc in zip(ds_test_copy.keys, test_source_codes)}\n",
    "\n",
    "    def _row_key(r):\n",
    "        return (r[\"GLACIER\"], int(r[\"YEAR\"]), int(r[\"ID\"]), str(r[\"PERIOD\"]).strip().lower())\n",
    "\n",
    "    test_df_preds[\"SOURCE_CODE\"] = test_df_preds.apply(lambda r: key_to_sc.get(_row_key(r), None), axis=1)\n",
    "\n",
    "    if domain_vocab is not None:\n",
    "        test_df_preds[\"domain_id\"] = test_df_preds[\"SOURCE_CODE\"].map(domain_vocab)\n",
    "\n",
    "    # seasonal scores\n",
    "    scores_annual, scores_winter = compute_seasonal_scores(\n",
    "        test_df_preds, target_col=\"target\", pred_col=\"pred\"\n",
    "    )\n",
    "\n",
    "    out = {\n",
    "        \"RMSE_annual\": float(test_metrics.get(\"RMSE_annual\", scores_annual[\"rmse\"])),\n",
    "        \"RMSE_winter\": float(test_metrics.get(\"RMSE_winter\", scores_winter[\"rmse\"])),\n",
    "        \"R2_annual\": float(scores_annual[\"R2\"]),\n",
    "        \"R2_winter\": float(scores_winter[\"R2\"]),\n",
    "        \"Bias_annual\": float(scores_annual[\"Bias\"]),\n",
    "        \"Bias_winter\": float(scores_winter[\"Bias\"]),\n",
    "        \"n_preds\": int(len(test_df_preds)),\n",
    "        \"n_annual\": (int(scores_annual.get(\"n\", np.nan)) if isinstance(scores_annual, dict) else np.nan),\n",
    "        \"n_winter\": (int(scores_winter.get(\"n\", np.nan)) if isinstance(scores_winter, dict) else np.nan),\n",
    "    }\n",
    "\n",
    "    # Plot\n",
    "    created_fig = None\n",
    "    if ax is None:\n",
    "        created_fig = plt.figure(figsize=(15, 10))\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    # auto-lims if not provided\n",
    "    if ax_xlim is None or ax_ylim is None:\n",
    "        lo = float(np.min(test_df_preds[[\"target\", \"pred\"]].min())) - 1\n",
    "        hi = float(np.max(test_df_preds[[\"target\", \"pred\"]].max())) + 1\n",
    "        if ax_xlim is None:\n",
    "            ax_xlim = (lo, hi)\n",
    "        if ax_ylim is None:\n",
    "            ax_ylim = (lo, hi)\n",
    "\n",
    "    pred_vs_truth_density(\n",
    "        ax,\n",
    "        test_df_preds,\n",
    "        scores_annual,\n",
    "        add_legend=False,\n",
    "        palette=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "        ax_xlim=ax_xlim,\n",
    "        ax_ylim=ax_ylim,\n",
    "    )\n",
    "\n",
    "    def _fmt(x):\n",
    "        return \"NA\" if (x is None or (isinstance(x, float) and np.isnan(x))) else f\"{x:.2f}\"\n",
    "\n",
    "    legend_NN = \"\\n\".join(\n",
    "        [\n",
    "            rf\"$\\mathrm{{RMSE_a}}={_fmt(scores_annual['rmse'])},\\ \\mathrm{{RMSE_w}}={_fmt(scores_winter['rmse'])}$\",\n",
    "            rf\"$\\mathrm{{R^2_a}}={_fmt(scores_annual['R2'])},\\ \\mathrm{{R^2_w}}={_fmt(scores_winter['R2'])}$\",\n",
    "            rf\"$\\mathrm{{Bias_a}}={_fmt(scores_annual['Bias'])},\\ \\mathrm{{Bias_w}}={_fmt(scores_winter['Bias'])}$\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        legend_NN,\n",
    "        transform=ax.transAxes,\n",
    "        va=\"top\",\n",
    "        fontsize=legend_fontsize,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=20)\n",
    "\n",
    "    return out, test_df_preds, created_fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transfer_learning_grid(\n",
    "        cfg,\n",
    "        regions,\n",
    "        models_xreg_by_region: dict,\n",
    "        models_tl_by_key: dict,\n",
    "        tl_assets_by_key: dict,\n",
    "        device,\n",
    "        *,\n",
    "        split_name=\"5pct\",\n",
    "        strategies=None,  # NEW: which columns to plot\n",
    "        strategy_labels=None,  # NEW: pretty names for column headers\n",
    "        include_region_in_titles=True,  # NEW: convenience\n",
    "        save_dir=None,\n",
    "        fig_size_per_cell=(5.2, 5.2),\n",
    "        ax_xlim=None,\n",
    "        ax_ylim=None,\n",
    "        legend_fontsize=11,\n",
    "        batch_size_eval=128,\n",
    "        domain_vocab=None):\n",
    "    \"\"\"\n",
    "    Flexible TL grid evaluator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strategies : list[str] or None\n",
    "        Strategies to plot as columns. Examples:\n",
    "          [\"no_ft\", \"safe\", \"two_stage\"]\n",
    "          [\"no_ft\", \"safe\", \"l2sp_safe\", \"disc_full\", \"disc_l2sp_full\"]\n",
    "        If None, defaults to [\"no_ft\",\"safe\",\"full\",\"two_stage\"].\n",
    "\n",
    "    strategy_labels : dict[str,str] or None\n",
    "        Mapping strategy -> column label. If None, uses defaults and falls back\n",
    "        to the strategy string.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses assets_row[\"ds_test\"] as holdout set for that region.\n",
    "    - Uses CH scalers via assets_row[\"ds_pretrain_scalers\"] through evaluate_one_model_TL.\n",
    "    \"\"\"\n",
    "    if strategies is None:\n",
    "        strategies = [\"no_ft\", \"safe\", \"full\", \"two_stage\"]\n",
    "    strategies = list(strategies)\n",
    "\n",
    "    default_labels = {\n",
    "        \"no_ft\": \"No fine-tuning (xreg CH)\",\n",
    "        \"safe\": \"Heads-only FT\",\n",
    "        \"full\": \"Full FT\",\n",
    "        \"two_stage\": \"Two-stage FT\",\n",
    "        \"disc_full\": \"Disc-LR FT\",\n",
    "        \"l2sp_safe\": \"L2SP + Heads-only\",\n",
    "        \"l2sp_full\": \"L2SP + Full\",\n",
    "        \"disc_l2sp_full\": \"Disc-LR + L2SP\",\n",
    "    }\n",
    "    if strategy_labels is None:\n",
    "        strategy_labels = {}\n",
    "    col_labels = {**default_labels, **strategy_labels}\n",
    "\n",
    "    nrows = len(regions)\n",
    "    ncols = len(strategies)\n",
    "\n",
    "    if save_dir:\n",
    "        save_abs = os.path.join(save_dir)\n",
    "        os.makedirs(save_abs, exist_ok=True)\n",
    "    else:\n",
    "        save_abs = None\n",
    "\n",
    "    figsize = (fig_size_per_cell[0] * ncols, fig_size_per_cell[1] * nrows)\n",
    "    fig, axes = plt.subplots(nrows,\n",
    "                             ncols,\n",
    "                             figsize=figsize,\n",
    "                             sharex=False,\n",
    "                             sharey=False)\n",
    "    axes = np.array(axes)\n",
    "    if nrows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    if ncols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "\n",
    "    rows = []\n",
    "    preds = {}\n",
    "\n",
    "    for r, region in enumerate(regions):\n",
    "        exp_key = _pick_tl_exp_key_for_region(tl_assets_by_key,\n",
    "                                              region,\n",
    "                                              split_name=split_name)\n",
    "        assets_row = tl_assets_by_key.get(exp_key, None)\n",
    "\n",
    "        # validate assets\n",
    "        if assets_row is None or assets_row.get(\"ds_test\", None) is None:\n",
    "            logging.warning(\n",
    "                f\"Skipping region {region}: no ds_test in {exp_key}\")\n",
    "            for c in range(ncols):\n",
    "                axes[r, c].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        if assets_row.get(\"ds_pretrain_scalers\", None) is None:\n",
    "            logging.warning(\n",
    "                f\"Skipping region {region}: missing ds_pretrain_scalers in {exp_key}\"\n",
    "            )\n",
    "            for c in range(ncols):\n",
    "                axes[r, c].axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        for c, strat in enumerate(strategies):\n",
    "            ax = axes[r, c]\n",
    "\n",
    "            # pick model\n",
    "            if strat == \"no_ft\":\n",
    "                model = models_xreg_by_region.get(region, None)\n",
    "            else:\n",
    "                model_key = f\"{exp_key}__{strat}\"\n",
    "                model = models_tl_by_key.get(model_key, None)\n",
    "\n",
    "            if model is None:\n",
    "                ax.axis(\"off\")\n",
    "                logging.warning(\n",
    "                    f\"Missing model for region={region}, strategy={strat}\")\n",
    "                continue\n",
    "\n",
    "            # title inside each cell (optional; you also set titles later)\n",
    "            cell_title = f\"{region}\\n{strat}\" if include_region_in_titles else strat\n",
    "\n",
    "            metrics, df_preds, _fig_ind, _ = evaluate_one_model_TL(\n",
    "                cfg=cfg,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                tl_assets_for_key=assets_row,\n",
    "                ax=ax,\n",
    "                ax_xlim=ax_xlim,\n",
    "                ax_ylim=ax_ylim,\n",
    "                title=cell_title if include_region_in_titles else None,\n",
    "                legend_fontsize=legend_fontsize,\n",
    "                batch_size=batch_size_eval,\n",
    "                domain_vocab=domain_vocab,  # <-- NEW\n",
    "            )\n",
    "\n",
    "            metrics.update({\n",
    "                \"region\": region,\n",
    "                \"strategy\": strat,\n",
    "                \"exp_key\": exp_key,\n",
    "                \"split_name\": split_name,\n",
    "            })\n",
    "            rows.append(metrics)\n",
    "            preds[(region, strat)] = df_preds\n",
    "\n",
    "            # remove legend if present\n",
    "            leg = ax.get_legend()\n",
    "            if leg is not None:\n",
    "                leg.remove()\n",
    "\n",
    "    # ---- nicer axis titles: region as row + strategy label as column ----\n",
    "    for rr in range(nrows):\n",
    "        for cc in range(ncols):\n",
    "            strat = strategies[cc]\n",
    "            col_name = col_labels.get(strat, strat)\n",
    "            axes[rr, cc].set_title(f\"{regions[rr]} - {col_name}\", fontsize=14)\n",
    "\n",
    "            if cc == 0:\n",
    "                axes[rr, cc].set_ylabel(\"Modeled PMB [m w.e.]\", fontsize=12)\n",
    "            else:\n",
    "                axes[rr, cc].set_ylabel(\"\")\n",
    "\n",
    "            if rr == nrows - 1:\n",
    "                axes[rr, cc].set_xlabel(\"Observed PMB [m w.e.]\", fontsize=12)\n",
    "            else:\n",
    "                axes[rr, cc].set_xlabel(\"\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Transfer learning evaluation (holdout test) — split={split_name}\",\n",
    "        fontsize=18,\n",
    "        y=0.995,\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "\n",
    "    if save_abs:\n",
    "        # include strategies in filename to avoid overwriting\n",
    "        tag = \"_\".join(strategies)\n",
    "        out_png = os.path.join(save_abs, f\"TL_grid_{split_name}_{tag}.png\")\n",
    "        fig.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "    df_metrics = pd.DataFrame(rows)\n",
    "    if len(df_metrics) > 0:\n",
    "        df_metrics = df_metrics.set_index([\"region\", \"strategy\"]).sort_index()\n",
    "\n",
    "    return df_metrics, preds, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_xreg_model(\n",
    "        cfg,\n",
    "        region,\n",
    "        best_params,\n",
    "        device,\n",
    "        models_dir=\"models\",\n",
    "        prefix=\"lstm_xreg_CH_to\",\n",
    "        date=None,  # if None → auto-detect latest\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads one cross-regional CH→region model.\n",
    "    \"\"\"\n",
    "\n",
    "    if date is None:\n",
    "        # find latest file matching pattern\n",
    "        pattern = f\"{prefix}_{region}_\"\n",
    "        candidates = [\n",
    "            f for f in os.listdir(models_dir)\n",
    "            if f.startswith(pattern) and f.endswith(\".pt\")\n",
    "        ]\n",
    "        if len(candidates) == 0:\n",
    "            raise FileNotFoundError(f\"No checkpoint found for region {region}\")\n",
    "\n",
    "        candidates = sorted(candidates)  # last = latest by name\n",
    "        filename = candidates[-1]\n",
    "    else:\n",
    "        filename = f\"{prefix}_{region}_{date}.pt\"\n",
    "\n",
    "    path = os.path.join(models_dir, filename)\n",
    "\n",
    "    # rebuild model\n",
    "    model = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params,\n",
    "                                                       device)\n",
    "\n",
    "    state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    return model, path\n",
    "\n",
    "\n",
    "def load_xreg_models_all(\n",
    "    cfg,\n",
    "    regions,\n",
    "    best_params,\n",
    "    device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_xreg_CH_to\",\n",
    "    date=None,\n",
    "):\n",
    "    models = {}\n",
    "    paths = {}\n",
    "\n",
    "    for region in regions:\n",
    "        try:\n",
    "            model, path = load_one_xreg_model(\n",
    "                cfg=cfg,\n",
    "                region=region,\n",
    "                best_params=best_params,\n",
    "                device=device,\n",
    "                models_dir=models_dir,\n",
    "                prefix=prefix,\n",
    "                date=date,\n",
    "            )\n",
    "            models[region] = model\n",
    "            paths[region] = path\n",
    "            print(f\"Loaded CH→{region} from {path}\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Skipping {region}: {e}\")\n",
    "            models[region] = None\n",
    "            paths[region] = None\n",
    "\n",
    "    return models, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"FR\", \"IT_AT\", \"NOR\", \"ISL\",\n",
    "           \"SJM\"]  # pick any 4 you have models for\n",
    "\n",
    "models_xreg, paths_xreg = load_xreg_models_all(\n",
    "    cfg=cfg,\n",
    "    regions=regions,\n",
    "    best_params=default_params,\n",
    "    device=device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_xreg_CH_to\",\n",
    "    date=None,  # auto-detect latest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 percent split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "#     cfg=cfg,\n",
    "#     regions=regions,\n",
    "#     models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "#     models_tl_by_key=models_tl,  # TL models keyed by \"exp__strategy\"\n",
    "#     tl_assets_by_key=tl_assets,  # TL assets\n",
    "#     device=device,\n",
    "#     split_name=\"5pct\",  # or \"50pct\"\n",
    "#     save_dir=\"figures/eval_TL\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    regions=regions,\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_tl,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"5pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\"no_ft\", \"safe\", \"full\", \"disc_full\"],\n",
    "    # strategy_labels={\n",
    "    #     \"safe\": \"Freeze LSTM\",\n",
    "    #     \"disc_full\": \"Disc LR\",\n",
    "    #     \"disc_l2sp_full\": \"Disc LR + L2SP\",\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 percent split (moderate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    # regions=regions,\n",
    "    regions=[\"NOR\", \"ISL\", \"SJM\"],  # only regions with 50pct splits\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_tl,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"50pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    # regions=regions,\n",
    "    regions=[\"NOR\", \"SJM\", \"ISL\"],  # only regions with 50pct splits\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_tl,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"50pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\n",
    "        \"no_ft\",\n",
    "        \"safe\",\n",
    "        \"full\",\n",
    "        \"disc_full\",\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
