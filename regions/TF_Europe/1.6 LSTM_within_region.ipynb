{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "from typing import Optional, Iterable, Dict, List\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# from regions.Svalbard.scripts.config_SVA import *\n",
    "# from regions.Svalbard.scripts.dataset import get_stakes_data_SVA\n",
    "# from regions.Svalbard.scripts.utils import *\n",
    "\n",
    "# from regions.Switzerland.scripts.dataset import process_or_load_data, get_CV_splits\n",
    "# from regions.Switzerland.scripts.plotting import plot_predictions_summary, plot_individual_glacier_pred, plot_history_lstm, get_cmap_hex,plot_tsne_overlap, plot_feature_kde_overlap, alpha_labels, pred_vs_truth_density\n",
    "# from regions.Switzerland.scripts.dataset import get_stakes_data, build_combined_LSTM_dataset, inspect_LSTM_sample, prepare_monthly_dfs_with_padding\n",
    "# from regions.Switzerland.scripts.models import compute_seasonal_scores, get_best_params_for_lstm\n",
    "\n",
    "from regions.TF_Europe.scripts.config_TF_Europe import *\n",
    "from regions.TF_Europe.scripts.dataset import *\n",
    "from regions.TF_Europe.scripts.plotting import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.EuropeConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within region modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stakes datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of loading data:\n",
    "# Load Switzerland only\n",
    "df = load_stakes(cfg, \"CH\")\n",
    "\n",
    "# Load all Central Europe (FR+CH+IT+AT when you add them)\n",
    "df_ceu = load_stakes_for_rgi_region(cfg, \"11\")\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "dfs[\"11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def summarize_and_plot_all_regions(dfs):\n",
    "    for rid, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\"\\n=== RGI {rid}: empty ===\")\n",
    "            continue\n",
    "\n",
    "        d = df.copy()\n",
    "\n",
    "        # keep only annual+winter\n",
    "        if \"PERIOD\" in d.columns:\n",
    "            d = d[d[\"PERIOD\"].isin([\"annual\", \"winter\"])].copy()\n",
    "\n",
    "        print(f\"\\n========== RGI {rid} ==========\")\n",
    "\n",
    "        # --- glaciers per subregion (SOURCE_CODE) ---\n",
    "        if \"SOURCE_CODE\" in d.columns and \"GLACIER\" in d.columns:\n",
    "            glaciers_per_sub = (\n",
    "                d.groupby(\"SOURCE_CODE\")[\"GLACIER\"].nunique().sort_values(\n",
    "                    ascending=False))\n",
    "            print(\"Unique glaciers per subregion (SOURCE_CODE):\")\n",
    "            print(glaciers_per_sub)\n",
    "        else:\n",
    "            print(\n",
    "                \"[warn] Missing SOURCE_CODE and/or GLACIER columns; skipping glacier counts.\"\n",
    "            )\n",
    "\n",
    "        # --- stacked bars per year for each subregion ---\n",
    "        if not {\"YEAR\", \"PERIOD\"}.issubset(d.columns):\n",
    "            print(\"[warn] Missing YEAR/PERIOD columns; skipping plots.\")\n",
    "            continue\n",
    "\n",
    "        group_key = \"SOURCE_CODE\" if \"SOURCE_CODE\" in d.columns else None\n",
    "        if group_key is None:\n",
    "            # no subregions: treat everything as one group\n",
    "            groups = [(\"ALL\", d)]\n",
    "        else:\n",
    "            groups = list(d.groupby(group_key))\n",
    "\n",
    "        for code, dsub in groups:\n",
    "            counts = (dsub.groupby([\"YEAR\", \"PERIOD\"\n",
    "                                    ]).size().unstack(fill_value=0).reindex(\n",
    "                                        columns=[\"annual\", \"winter\"],\n",
    "                                        fill_value=0).sort_index())\n",
    "\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(counts.index,\n",
    "                    counts[\"annual\"].values,\n",
    "                    label=\"annual\",\n",
    "                    color=mbm.plots.COLOR_ANNUAL)\n",
    "            plt.bar(counts.index,\n",
    "                    counts[\"winter\"].values,\n",
    "                    bottom=counts[\"annual\"].values,\n",
    "                    label=\"winter\",\n",
    "                    color=mbm.plots.COLOR_WINTER)\n",
    "            plt.title(\n",
    "                f\"RGI {rid} – {code}: measurements per year (annual + winter)\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(\"Number of measurements\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# run it\n",
    "summarize_and_plot_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mb_distributions_all_regions(\n",
    "    dfs,\n",
    "    periods=(\"annual\", \"winter\"),\n",
    "    value_col=\"POINT_BALANCE\",\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    bins_n=21,\n",
    "):\n",
    "    for rid, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            print(f\"\\n=== RGI {rid}: empty ===\")\n",
    "            continue\n",
    "\n",
    "        if not {\"PERIOD\", value_col}.issubset(df.columns):\n",
    "            print(\n",
    "                f\"\\n=== RGI {rid}: missing PERIOD or {value_col}, skipping ===\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # keep only the periods we want\n",
    "        d = df[df[\"PERIOD\"].isin(periods)].copy()\n",
    "\n",
    "        # choose grouping\n",
    "        if group_col in d.columns:\n",
    "            groups = list(d[group_col].dropna().unique())\n",
    "            groups = sorted(groups)\n",
    "        else:\n",
    "            groups = [\"ALL\"]\n",
    "            d[group_col] = \"ALL\"\n",
    "\n",
    "        # build plot\n",
    "        fig, axes = plt.subplots(1, len(periods), figsize=(14, 5), sharey=True)\n",
    "        if len(periods) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, period in zip(axes, periods):\n",
    "            # Collect all values across groups to define common bins\n",
    "            vals_all = []\n",
    "            for g in groups:\n",
    "                vals = d.loc[(d[\"PERIOD\"] == period) & (d[group_col] == g),\n",
    "                             value_col].dropna().values\n",
    "                if vals.size:\n",
    "                    vals_all.append(vals)\n",
    "\n",
    "            if not vals_all:\n",
    "                ax.set_title(f\"{period.capitalize()} Mass Balance (no data)\")\n",
    "                ax.set_xlabel(\"Mass balance [m w.e.]\")\n",
    "                continue\n",
    "\n",
    "            vals_all = np.concatenate(vals_all)\n",
    "            vmin, vmax = float(vals_all.min()), float(vals_all.max())\n",
    "            if np.isclose(vmin, vmax):\n",
    "                # degenerate case: all values identical\n",
    "                bins = np.linspace(vmin - 1e-6, vmax + 1e-6, bins_n)\n",
    "            else:\n",
    "                bins = np.linspace(vmin, vmax, bins_n)\n",
    "\n",
    "            # Plot each group\n",
    "            for g in groups:\n",
    "                vals = d.loc[(d[\"PERIOD\"] == period) & (d[group_col] == g),\n",
    "                             value_col].dropna().values\n",
    "                if not vals.size:\n",
    "                    continue\n",
    "                ax.hist(vals, bins=bins, alpha=0.5, label=str(g))\n",
    "                ax.axvline(vals.mean(), linestyle=\"--\")\n",
    "\n",
    "            ax.set_title(f\"{period.capitalize()} Mass Balance\")\n",
    "            ax.set_xlabel(\"Mass balance [m w.e.]\")\n",
    "            ax.legend()\n",
    "\n",
    "        axes[0].set_ylabel(\"Number of measurements\")\n",
    "        plt.suptitle(f\"RGI {rid} – Seasonal Point Mass Balance Distribution\",\n",
    "                     fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# run it\n",
    "plot_mb_distributions_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test glaciers\n",
    "TEST_GLACIERS_SJM = ['WERENSKIOLDBREEN']\n",
    "\n",
    "TEST_GLACIERS_CH = [\n",
    "    \"tortin\",\n",
    "    \"plattalva\",\n",
    "    \"schwarzberg\",\n",
    "    \"hohlaub\",\n",
    "    \"sanktanna\",\n",
    "    \"corvatsch\",\n",
    "    \"tsanfleuron\",\n",
    "    \"forno\",\n",
    "]\n",
    "\n",
    "TEST_GLACIERS_NOR = [\n",
    "    'Cainhavarre', 'Rundvassbreen', 'Svartisheibreen', 'Trollbergdalsbreen',\n",
    "    'Hansebreen', 'Tunsbergdalsbreen', 'Austdalsbreen', 'Hellstugubreen',\n",
    "    'Austre Memurubreen', 'Bondhusbrea', 'Svelgjabreen', 'Moesevassbrea',\n",
    "    'Blomstoelskardsbreen'\n",
    "]\n",
    "\n",
    "TEST_GLACIERS_IT_AT = [\n",
    "    'GOLDBERG K.', 'HALLSTAETTER G.', 'HINTEREIS F.', 'JAMTAL F.',\n",
    "    'KESSELWAND F.', 'KLEINFLEISS K.', 'OE. WURTEN K.', 'VENEDIGER K.',\n",
    "    'VERNAGT F.', 'ZETTALUNITZ/MULLWITZ K.'\n",
    "]\n",
    "\n",
    "TEST_GLACIERS_ISL = [\n",
    "    'RGI60-06.00311', 'RGI60-06.00305', 'Thjorsarjoekull (Hofsjoekull E)',\n",
    "    'RGI60-06.00445', 'RGI60-06.00474', 'RGI60-06.00425', 'RGI60-06.00480',\n",
    "    'Dyngjujoekull', 'RGI60-06.00478', 'Koeldukvislarjoekull',\n",
    "    'Oeldufellsjoekull', 'RGI60-06.00350', 'RGI60-06.00340'\n",
    "]\n",
    "\n",
    "TEST_GLACIERS_FR = ['Talefre', 'Argentiere', 'Gebroulaz']\n",
    "\n",
    "TEST_GLACIERS_BY_CODE = {\n",
    "    \"SJM\": TEST_GLACIERS_SJM,\n",
    "    \"ISL\": TEST_GLACIERS_ISL,\n",
    "    \"NOR\": TEST_GLACIERS_NOR,\n",
    "    \"FR\": TEST_GLACIERS_FR,\n",
    "    \"CH\": TEST_GLACIERS_CH,\n",
    "    \"IT_AT\": TEST_GLACIERS_IT_AT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_Europe.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_Europe.nc\")\n",
    "}\n",
    "\n",
    "# Check that all these files exists\n",
    "for key, path in paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required file for {key} not found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_for_rgi_region(rid: str) -> list[str]:\n",
    "    rid = str(rid).zfill(2)\n",
    "    spec = RGI_REGIONS[rid]\n",
    "    # If subregions exist, run per subregion code\n",
    "    if spec.get(\"subregions_codes\"):\n",
    "        return [c.upper() for c in spec[\"subregions_codes\"]]\n",
    "    # Otherwise run the region code\n",
    "    return [spec[\"code\"].upper()]\n",
    "\n",
    "\n",
    "def country_folder_for_code(rid: str, code: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the WGMS country folder name for a given RGI region id + code.\n",
    "    \"\"\"\n",
    "    rid = str(rid).zfill(2)\n",
    "    spec = RGI_REGIONS[rid]\n",
    "\n",
    "    # Case 1: region has subregions\n",
    "    if spec.get(\"subregions_codes\"):\n",
    "        codes = spec[\"subregions_codes\"]\n",
    "        names = spec[\"subregions\"]\n",
    "        mapping = dict(zip(codes, names))\n",
    "        return mapping.get(code)\n",
    "\n",
    "    # Case 2: single-region (no subregions)\n",
    "    return spec[\"name\"]\n",
    "\n",
    "\n",
    "def prepare_monthlies_for_all_regions(\n",
    "    cfg,\n",
    "    dfs,\n",
    "    paths,\n",
    "    vois_climate,\n",
    "    vois_topographical,\n",
    "    run_flag=True,\n",
    "    only_rids=None,  # e.g. [\"08\"]\n",
    "    only_codes=None,  # e.g. [\"NOR\"]\n",
    "    test_glaciers_override=None,\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    only_rids_set = {str(r).zfill(2) for r in only_rids} if only_rids else None\n",
    "    only_codes_set = {c.upper() for c in only_codes} if only_codes else None\n",
    "    test_glaciers_override = test_glaciers_override or {}\n",
    "\n",
    "    for rid, df_region in dfs.items():\n",
    "        rid2 = str(rid).zfill(2)\n",
    "\n",
    "        if df_region is None or len(df_region) == 0:\n",
    "            print(f\"Skipping RGI {rid2}: empty dataframe\")\n",
    "            continue\n",
    "\n",
    "        region_id_int = int(rid2)\n",
    "        codes = [c.upper() for c in codes_for_rgi_region(rid2)]\n",
    "        has_source = \"SOURCE_CODE\" in df_region.columns\n",
    "\n",
    "        for code in codes:\n",
    "\n",
    "            # Decide whether this one should recompute\n",
    "            should_run = run_flag\n",
    "            if only_rids_set or only_codes_set:\n",
    "                match_rid = (only_rids_set is None or rid2 in only_rids_set)\n",
    "                match_code = (only_codes_set is None or code in only_codes_set)\n",
    "                should_run = match_rid and match_code\n",
    "\n",
    "            # Slice df\n",
    "            if has_source:\n",
    "                df_sub = df_region[df_region[\"SOURCE_CODE\"] == code].copy()\n",
    "            else:\n",
    "                df_sub = df_region.copy()\n",
    "\n",
    "            if len(df_sub) == 0:\n",
    "                print(f\"[RGI {rid2}] No rows for code={code}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Override test glaciers if requested\n",
    "            test_glaciers = test_glaciers_override.get(\n",
    "                code, TEST_GLACIERS_BY_CODE.get(code, []))\n",
    "\n",
    "            # Get country folder\n",
    "            country = country_folder_for_code(rid2, code)\n",
    "\n",
    "            # Build csv path\n",
    "            paths_ = paths.copy()\n",
    "            paths_[\"csv_path\"] = os.path.join(cfg.dataPath, path_PMB_WGMS_csv,\n",
    "                                              country, \"csv\")\n",
    "\n",
    "            print(f\"\\nProcessing RGI {rid2} / {code} \"\n",
    "                  f\"(country={country}, run_flag={should_run})\")\n",
    "\n",
    "            res = prepare_monthly_dfs_with_padding(\n",
    "                cfg=cfg,\n",
    "                df_region=df_sub,\n",
    "                region_name=code,\n",
    "                region_id=region_id_int,\n",
    "                paths=paths_,\n",
    "                test_glaciers=test_glaciers,\n",
    "                vois_climate=vois_climate,\n",
    "                vois_topographical=vois_topographical,\n",
    "                run_flag=should_run,\n",
    "            )\n",
    "\n",
    "            results[f\"{rid2}_{code}\"] = res\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all = prepare_monthlies_for_all_regions(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    ")\n",
    "\n",
    "# Optional: rerun parts only\n",
    "\"\"\"\n",
    "Example: Only recompute Norway\n",
    "res_all = prepare_monthlies_for_all_regions(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=True,\n",
    "    only_codes=[\"NOR\"],   # only Norway recomputes\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlap_for_all_results(\n",
    "    results_dict,\n",
    "    cfg,\n",
    "    STATIC_COLS,\n",
    "    MONTHLY_COLS,\n",
    "    n_iter=1000,\n",
    "):\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    custom_palette = {'Train': color_dark_blue, 'Test': '#b2182b'}\n",
    "\n",
    "    figs = {}\n",
    "\n",
    "    for key, res in results_dict.items():\n",
    "        if res is None:\n",
    "            continue\n",
    "\n",
    "        df_train = res.get(\"df_train\")\n",
    "        df_test = res.get(\"df_test\")\n",
    "\n",
    "        if df_train is None or df_test is None or len(df_train) == 0 or len(\n",
    "                df_test) == 0:\n",
    "            print(f\"[{key}] Missing/empty df_train or df_test, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Plotting t-SNE overlap for {key}: train={len(df_train)}, test={len(df_test)}\"\n",
    "        )\n",
    "\n",
    "        fig = plot_tsne_overlap(\n",
    "            df_train,\n",
    "            df_test,\n",
    "            STATIC_COLS,\n",
    "            MONTHLY_COLS,\n",
    "            sublabels=(\"a\", \"b\", \"c\"),\n",
    "            label_fmt=\"({})\",\n",
    "            label_xy=(0.02, 0.98),\n",
    "            label_fontsize=14,\n",
    "            n_iter=n_iter,\n",
    "            random_state=cfg.seed,\n",
    "            custom_palette=custom_palette,\n",
    "        )\n",
    "\n",
    "        figs[key] = fig\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# res_all is what you got from prepare_monthlies_for_all_regions(...)\n",
    "figs = plot_overlap_for_all_results(\n",
    "    results_dict=res_all,\n",
    "    cfg=cfg,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    n_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def plot_feature_overlap_all_regions(\n",
    "    results_dict,\n",
    "    STATIC_COLS,\n",
    "    MONTHLY_COLS,\n",
    "    output_dir=\"figures\",\n",
    "    include_target=True,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    palette = {'Train': color_dark_blue, 'Test': '#b2182b'}\n",
    "\n",
    "    features = STATIC_COLS + MONTHLY_COLS\n",
    "    if include_target:\n",
    "        features = features + [\"POINT_BALANCE\"]\n",
    "\n",
    "    figs = {}\n",
    "\n",
    "    for key, res in results_dict.items():\n",
    "        if res is None:\n",
    "            continue\n",
    "\n",
    "        df_train = res.get(\"df_train\")\n",
    "        df_test = res.get(\"df_test\")\n",
    "\n",
    "        if df_train is None or df_test is None:\n",
    "            print(f\"[{key}] Missing df_train/df_test, skipping.\")\n",
    "            continue\n",
    "\n",
    "        if len(df_train) == 0 or len(df_test) == 0:\n",
    "            print(f\"[{key}] Empty train/test, skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Plotting KDE overlap for {key}\")\n",
    "\n",
    "        fig = plot_feature_kde_overlap(df_train,\n",
    "                                       df_test,\n",
    "                                       features,\n",
    "                                       palette,\n",
    "                                       outfile=None)\n",
    "\n",
    "        figs[key] = fig\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "figs_kde = plot_feature_overlap_all_regions(res_all, STATIC_COLS, MONTHLY_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "def _lstm_cache_paths(cfg, key: str, cache_dir: str):\n",
    "    out_dir = os.path.join(cache_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_p = os.path.join(out_dir, f\"{key}_train.joblib\")\n",
    "    test_p = os.path.join(out_dir, f\"{key}_test.joblib\")\n",
    "    split_p = os.path.join(out_dir, f\"{key}_split.joblib\")\n",
    "    return train_p, test_p, split_p\n",
    "\n",
    "\n",
    "def build_or_load_lstm_for_key(\n",
    "    cfg,\n",
    "    key: str,\n",
    "    res: dict,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    val_ratio=0.2,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    force_recompute=False,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "):\n",
    "    train_p, test_p, split_p = _lstm_cache_paths(cfg, key, cache_dir=cache_dir)\n",
    "\n",
    "    if (not force_recompute) and all(\n",
    "            os.path.exists(p) for p in [train_p, test_p, split_p]):\n",
    "        ds_train = joblib.load(train_p)\n",
    "        ds_test = joblib.load(test_p)\n",
    "        split = joblib.load(split_p)\n",
    "        return ds_train, ds_test, split[\"train_idx\"], split[\"val_idx\"]\n",
    "\n",
    "    # required pieces from your monthly prep\n",
    "    df_train = res[\"df_train\"]\n",
    "    df_test = res[\"df_test\"]\n",
    "    df_train_aug = res[\"df_train_aug\"]\n",
    "    df_test_aug = res[\"df_test_aug\"]\n",
    "    months_head_pad = res[\"months_head_pad\"]\n",
    "    months_tail_pad = res[\"months_tail_pad\"]\n",
    "\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_train = build_combined_LSTM_dataset(\n",
    "        df_loss=df_train,\n",
    "        df_full=df_train_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    ds_test = build_combined_LSTM_dataset(\n",
    "        df_loss=df_test,\n",
    "        df_full=df_test_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=val_ratio, seed=cfg.seed)\n",
    "\n",
    "    # save\n",
    "    joblib.dump(ds_train, train_p, compress=3)\n",
    "    joblib.dump(ds_test, test_p, compress=3)\n",
    "    joblib.dump({\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx\n",
    "    },\n",
    "                split_p,\n",
    "                compress=3)\n",
    "\n",
    "    return ds_train, ds_test, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_lstm_all(\n",
    "    cfg,\n",
    "    res_all: dict,  # e.g. {\"07_SJM\": res, \"08_NOR\": res, ...}\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    only_keys=None,  # e.g. [\"08_NOR\"] to recompute only Norway\n",
    "    force_recompute=False,  # global default\n",
    "    val_ratio=0.2,\n",
    "):\n",
    "    outputs = {}\n",
    "    only_keys_set = set(only_keys) if only_keys else None\n",
    "\n",
    "    for key, res in res_all.items():\n",
    "        if res is None:\n",
    "            continue\n",
    "\n",
    "        # recompute only some keys; others load if possible\n",
    "        fr = force_recompute\n",
    "        if only_keys_set is not None:\n",
    "            fr = key in only_keys_set\n",
    "\n",
    "        print(f\"\\nLSTM prep: {key} (force_recompute={fr})\")\n",
    "\n",
    "        ds_train, ds_test, train_idx, val_idx = build_or_load_lstm_for_key(\n",
    "            cfg=cfg,\n",
    "            key=key,\n",
    "            res=res,\n",
    "            MONTHLY_COLS=MONTHLY_COLS,\n",
    "            STATIC_COLS=STATIC_COLS,\n",
    "            val_ratio=val_ratio,\n",
    "            cache_dir=cache_dir,\n",
    "            force_recompute=fr,\n",
    "        )\n",
    "\n",
    "        outputs[key] = {\n",
    "            \"ds_train\": ds_train,\n",
    "            \"ds_test\": ds_test,\n",
    "            \"train_idx\": train_idx,\n",
    "            \"val_idx\": val_idx,\n",
    "        }\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_assets = build_or_load_lstm_all(\n",
    "    cfg=cfg,\n",
    "    res_all=res_all,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    ")\n",
    "\"\"\"Example: \n",
    "# only recompute Norway, load others if cached\n",
    "lstm_assets = build_or_load_lstm_all(\n",
    "    cfg=cfg,\n",
    "    res_all=res_all,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    only_keys=[\"08_NOR\"],\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "def all_codes_from_config(RGI_REGIONS: dict) -> list[str]:\n",
    "    codes = set()\n",
    "\n",
    "    for rid, spec in RGI_REGIONS.items():\n",
    "        sub_codes = spec.get(\"subregions_codes\", []) or []\n",
    "\n",
    "        if sub_codes:\n",
    "            # If subregions exist → only add those\n",
    "            codes.update(c.upper() for c in sub_codes)\n",
    "        else:\n",
    "            # Otherwise add the region-level code\n",
    "            codes.add(spec[\"code\"].upper())\n",
    "\n",
    "    return sorted(codes)\n",
    "\n",
    "\n",
    "def build_lstm_params_by_code(\n",
    "    default_params: dict,\n",
    "    log_path_gs_results: dict,\n",
    "    RGI_REGIONS: dict,\n",
    "    select_by: str = \"avg_test_loss\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict: code -> params\n",
    "\n",
    "    For every code in RGI_REGIONS (region codes + subregion codes):\n",
    "      - if a grid-search log exists: load best params and override defaults\n",
    "      - else: use defaults\n",
    "    \"\"\"\n",
    "    params_by_code = {}\n",
    "    all_codes = all_codes_from_config(RGI_REGIONS)\n",
    "\n",
    "    for code in all_codes:\n",
    "        params = copy.deepcopy(default_params)\n",
    "\n",
    "        log_path = log_path_gs_results.get(code)\n",
    "        if log_path and os.path.exists(log_path):\n",
    "            print(f\"Loading tuned params for {code} from {log_path}\")\n",
    "            best_params = get_best_params_for_lstm(log_path,\n",
    "                                                   select_by=select_by)\n",
    "            params.update(best_params)\n",
    "        else:\n",
    "            print(f\"No grid-search log for {code}. Using default params.\")\n",
    "\n",
    "        params_by_code[code] = params\n",
    "\n",
    "    return params_by_code\n",
    "\n",
    "\n",
    "log_path_gs_results = {\n",
    "    \"ISL\": 'logs/GS_results/lstm_param_search_progress_OOS_ISL_2026-02-11.csv',\n",
    "    \"NOR\": 'logs/GS_results/lstm_param_search_progress_OOS_NOR_2026-02-09.csv',\n",
    "    \"FR\": 'logs/GS_results/lstm_param_search_progress_OOS_FR_2026-02-06.csv',\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 1,\n",
    "    'static_hidden': 128,\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-05,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.1,\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "params_by_code = build_lstm_params_by_code(\n",
    "    default_params=default_params,\n",
    "    log_path_gs_results=log_path_gs_results,\n",
    "    RGI_REGIONS=RGI_REGIONS,\n",
    ")\n",
    "\n",
    "params_by_code.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
