{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "\n",
    "from regions.TF_Europe.scripts.config_TF_Europe import *\n",
    "from regions.TF_Europe.scripts.dataset import *\n",
    "from regions.TF_Europe.scripts.plotting import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "from regions.TF_Europe.scripts.utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.EuropeTFConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read stakes datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of loading data:\n",
    "# Load Switzerland only\n",
    "df = load_stakes(cfg, \"CH\")\n",
    "\n",
    "# Load all Central Europe (FR+CH+IT+AT when you add them)\n",
    "df_ceu = load_stakes_for_rgi_region(cfg, \"11\")\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\"\"\"\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "dfs[\"11\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_Europe.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_Europe.nc\")\n",
    "}\n",
    "\n",
    "# Check that all these files exists\n",
    "for key, path in paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required file for {key} not found at {path}\")\n",
    "\n",
    "vois_climate = [\n",
    "    \"t2m\",\n",
    "    \"tp\",\n",
    "    \"slhf\",\n",
    "    \"sshf\",\n",
    "    \"ssrd\",\n",
    "    \"fal\",\n",
    "    \"str\",\n",
    "]\n",
    "\n",
    "vois_topographical = [\"aspect\", \"slope\", \"svf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all stake dfs\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\n",
    "# prepare monthlies (recompute or load)\n",
    "res_xreg, split_info = prepare_monthly_df_xreg_CH_to_EU(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    run_flag=False,  # load if already computed\n",
    ")\n",
    "\n",
    "df_train = res_xreg[\"df_train\"]\n",
    "df_test = res_xreg[\"df_test\"]\n",
    "\n",
    "print(\"Train glaciers (CH):\", len(split_info[\"train_glaciers\"]))\n",
    "print(\"Test glaciers (non-CH):\", len(split_info[\"test_glaciers\"]))\n",
    "print(\"Train rows:\", len(df_train), \"Test rows:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISL_RID = \"ISL\"  # or maybe \"06\" depending on your setup\n",
    "\n",
    "df_isl = df_test[df_test[\"SOURCE_CODE\"] == ISL_RID].copy()\n",
    "\n",
    "print(\"ISL rows:\", len(df_isl))\n",
    "print(\"ISL glaciers:\", df_isl[\"GLACIER\"].nunique())\n",
    "print(\"ISL year range:\", df_isl[\"YEAR\"].min(), \"-\", df_isl[\"YEAR\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed glacier hold-out split (spatial generalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def glacier_level_features(\n",
    "    df_isl: pd.DataFrame,\n",
    "    glacier_col=\"GLACIER\",\n",
    "    year_col=\"YEAR\",\n",
    "):\n",
    "    d = df_isl.copy()\n",
    "\n",
    "    # --- handle circular aspect properly ---\n",
    "    aspect_deg = d[\"aspect\"].astype(float) % 360.0\n",
    "    aspect_rad = np.deg2rad(aspect_deg)\n",
    "\n",
    "    d[\"_asp_sin\"] = np.sin(aspect_rad)\n",
    "    d[\"_asp_cos\"] = np.cos(aspect_rad)\n",
    "\n",
    "    # glacier-level summaries\n",
    "    g = d.groupby(glacier_col).agg(\n",
    "        nrows=(glacier_col, \"size\"),\n",
    "        nyears=(year_col, pd.Series.nunique),\n",
    "        slope_mean=(\"slope\", \"mean\"),\n",
    "        slope_std=(\"slope\", \"std\"),\n",
    "        svf_mean=(\"svf\", \"mean\"),\n",
    "        asp_sin_mean=(\"_asp_sin\", \"mean\"),\n",
    "        asp_cos_mean=(\"_asp_cos\", \"mean\"),\n",
    "    )\n",
    "\n",
    "    return g.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfeat = glacier_level_features(df_isl)\n",
    "print(\"Number of glaciers:\", len(gfeat))\n",
    "gfeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def holdout_split_cluster_stratified(\n",
    "        df_isl,\n",
    "        holdout_frac=0.30,\n",
    "        seed=cfg.seed,\n",
    "        n_clusters=6,  # start conservative; increase if many glaciers\n",
    "):\n",
    "    gfeat = glacier_level_features(df_isl)\n",
    "\n",
    "    # feature space for clustering\n",
    "    feat_cols = [\n",
    "        \"slope_mean\",\n",
    "        \"slope_std\",\n",
    "        \"svf_mean\",\n",
    "        \"asp_sin_mean\",\n",
    "        \"asp_cos_mean\",\n",
    "        \"nyears\",  # optional but stabilizes split\n",
    "        \"nrows\",  # optional but stabilizes split\n",
    "    ]\n",
    "\n",
    "    X = gfeat[feat_cols].astype(float).fillna(gfeat[feat_cols].median())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=seed, n_init=10)\n",
    "    gfeat[\"cluster\"] = km.fit_predict(Xs)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    holdout = []\n",
    "\n",
    "    # stratified sampling per cluster\n",
    "    for c, sub in gfeat.groupby(\"cluster\"):\n",
    "        gls = sub[\"GLACIER\"].to_numpy()\n",
    "        rng.shuffle(gls)\n",
    "        k = int(np.ceil(holdout_frac * len(gls)))\n",
    "        holdout.extend(gls[:k])\n",
    "\n",
    "    holdout_glaciers = set(holdout)\n",
    "    pool_glaciers = set(gfeat[\"GLACIER\"]) - holdout_glaciers\n",
    "\n",
    "    df_holdout = df_isl[df_isl[\"GLACIER\"].isin(holdout_glaciers)].copy()\n",
    "    df_pool = df_isl[df_isl[\"GLACIER\"].isin(pool_glaciers)].copy()\n",
    "\n",
    "    summary = {\n",
    "        \"holdout_frac\": holdout_frac,\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"n_glaciers_total\": int(gfeat.shape[0]),\n",
    "        \"n_glaciers_holdout\": int(len(holdout_glaciers)),\n",
    "        \"n_glaciers_pool\": int(len(pool_glaciers)),\n",
    "        \"rows_holdout\": int(len(df_holdout)),\n",
    "        \"rows_pool\": int(len(df_pool)),\n",
    "    }\n",
    "\n",
    "    return df_pool, df_holdout, holdout_glaciers, pool_glaciers, gfeat, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isl_pool, df_isl_holdout, holdout_glaciers, pool_glaciers, gfeat, split_summary = (\n",
    "    holdout_split_cluster_stratified(df_isl,\n",
    "                                     holdout_frac=0.30,\n",
    "                                     seed=cfg.seed,\n",
    "                                     n_clusters=6))\n",
    "\n",
    "print(split_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hold = gfeat[gfeat[\"GLACIER\"].isin(holdout_glaciers)]\n",
    "g_pool = gfeat[gfeat[\"GLACIER\"].isin(pool_glaciers)]\n",
    "\n",
    "for col in [\"slope_mean\", \"slope_std\", \"svf_mean\", \"nyears\", \"nrows\"]:\n",
    "    print(col)\n",
    "    print(\"  pool   :\", g_pool[col].quantile([0.1, 0.5, 0.9]).to_dict())\n",
    "    print(\"  holdout:\", g_hold[col].quantile([0.1, 0.5, 0.9]).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster counts - pool\")\n",
    "print(g_pool[\"cluster\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCluster counts - holdout\")\n",
    "print(g_hold[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_glaciers_by_split = {\n",
    "    \"spatial\": holdout_glaciers,\n",
    "}\n",
    "\n",
    "data_ISL, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"06\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_06_Iceland/06_rgi60_Iceland.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=[\"spatial\"],\n",
    "    ft_label_col=\"Pool/Hold-out glacier\",\n",
    "    ft_label_ft=\"Pool\",\n",
    "    ft_label_holdout=\"Hold-out\",\n",
    ")\n",
    "\n",
    "glacier_df_ISL_5pct = glacier_info_by_split[\"spatial\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"Pool\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_ISL_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Iceland\",\n",
    "    extent=(-25, -11, 62, 68),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"Pool/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring subsamples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_xreg is the ONE dict from your cross-regional monthly prep\n",
    "figs_by_code = plot_tsne_overlap_xreg_from_single_res(\n",
    "    res_xreg=res_xreg,\n",
    "    cfg=cfg,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    use_aug=False,  # or True if you want *_aug\n",
    "    n_iter=1000,\n",
    "    only_codes=[\"ISL\"],  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_monitoring_subset_from_pool(\n",
    "    df_pool: pd.DataFrame,\n",
    "    G: int,\n",
    "    Y: int,\n",
    "    M: int,\n",
    "    seed: int = 0,\n",
    "    glacier_pick_method:\n",
    "    str = \"random\",  # \"random\" / \"small_first\" / \"large_first\" / \"shuffle\"\n",
    "    min_rows_per_glacier: int = 1,\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    counts = df_pool.groupby(\"GLACIER\").size().sort_values(ascending=False)\n",
    "    counts = counts[counts >= min_rows_per_glacier]\n",
    "    glaciers = counts.index.to_numpy()\n",
    "    if G > len(glaciers):\n",
    "        raise ValueError(f\"G={G} > available pool glaciers ({len(glaciers)})\")\n",
    "\n",
    "    if glacier_pick_method == \"random\":\n",
    "        chosen = rng.choice(glaciers, size=G, replace=False)\n",
    "    elif glacier_pick_method == \"small_first\":\n",
    "        chosen = counts.sort_values(ascending=True).index[:G].to_numpy()\n",
    "    elif glacier_pick_method == \"large_first\":\n",
    "        chosen = counts.sort_values(ascending=False).index[:G].to_numpy()\n",
    "    elif glacier_pick_method == \"shuffle\":\n",
    "        idx = glaciers.copy()\n",
    "        rng.shuffle(idx)\n",
    "        chosen = idx[:G]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unknown glacier_pick_method='{glacier_pick_method}'\")\n",
    "\n",
    "    chosen = set(chosen)\n",
    "    df_g = df_pool[df_pool[\"GLACIER\"].isin(chosen)].copy()\n",
    "\n",
    "    # earliest contiguous Y years per glacier\n",
    "    keep_parts = []\n",
    "    for gid, dfgid in df_g.groupby(\"GLACIER\"):\n",
    "        years = np.array(sorted(dfgid[\"YEAR\"].unique()))\n",
    "        y_keep = years[:min(Y, len(years))]\n",
    "        keep_parts.append(dfgid[dfgid[\"YEAR\"].isin(y_keep)])\n",
    "    df_y = pd.concat(keep_parts, ignore_index=True)\n",
    "\n",
    "    # sample up to M rows per glacier-year\n",
    "    df_y[\"GLACIER_YEAR\"] = df_y[\"GLACIER\"].astype(\n",
    "        str) + \"_\" + df_y[\"YEAR\"].astype(int).astype(str)\n",
    "\n",
    "    sampled = []\n",
    "    for gy, dfgy in df_y.groupby(\"GLACIER_YEAR\"):\n",
    "        if len(dfgy) <= M:\n",
    "            sampled.append(dfgy)\n",
    "        else:\n",
    "            rs = int(rng.integers(1, 1_000_000))\n",
    "            sampled.append(dfgy.sample(n=M, random_state=rs))\n",
    "\n",
    "    df_ft = pd.concat(sampled, ignore_index=True)\n",
    "    return df_ft, chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_res_transfer_learning_custom(\n",
    "    res_xreg: dict,\n",
    "    target_code: str,\n",
    "    df_ft: pd.DataFrame,\n",
    "    holdout_glaciers: set,\n",
    "    source_col=\"SOURCE_CODE\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom TL slicing:\n",
    "      - pretrain: CH from res_xreg\n",
    "      - finetune: provided df_ft (+ its _aug subset)\n",
    "      - test: fixed holdout glaciers (+ its _aug subset)\n",
    "    \"\"\"\n",
    "    res_pretrain = {\n",
    "        \"df_train\": res_xreg[\"df_train\"],\n",
    "        \"df_train_aug\": res_xreg[\"df_train_aug\"],\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    df_t_all = res_xreg[\"df_test\"]\n",
    "    df_t_all_aug = res_xreg[\"df_test_aug\"]\n",
    "\n",
    "    df_target = df_t_all.loc[df_t_all[source_col] == target_code].copy()\n",
    "    df_target_aug = df_t_all_aug.loc[df_t_all_aug[source_col] ==\n",
    "                                     target_code].copy()\n",
    "\n",
    "    # finetune aug: match the same keys as df_ft (GLACIER,YEAR,ID,PERIOD)\n",
    "    key_cols = [\"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\"]\n",
    "    ft_keys = df_ft[key_cols].copy()\n",
    "    ft_keys[\"PERIOD\"] = ft_keys[\"PERIOD\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    df_target_aug2 = df_target_aug.copy()\n",
    "    df_target_aug2[\"PERIOD\"] = df_target_aug2[\"PERIOD\"].astype(\n",
    "        str).str.strip().str.lower()\n",
    "\n",
    "    df_ft_aug = df_target_aug2.merge(ft_keys.drop_duplicates(),\n",
    "                                     on=key_cols,\n",
    "                                     how=\"inner\")\n",
    "\n",
    "    # holdout = fixed glaciers\n",
    "    df_hold = df_target[df_target[\"GLACIER\"].isin(holdout_glaciers)].copy()\n",
    "    df_hold_aug = df_target_aug2[df_target_aug2[\"GLACIER\"].isin(\n",
    "        holdout_glaciers)].copy()\n",
    "\n",
    "    res_ft = {\n",
    "        \"df_train\": df_ft,\n",
    "        \"df_train_aug\": df_ft_aug,\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "    res_test = {\n",
    "        \"df_test\": df_hold,\n",
    "        \"df_test_aug\": df_hold_aug,\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "    return res_pretrain, res_ft, res_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_static_tl_assets_CH_and_holdout(\n",
    "    cfg,\n",
    "    res_xreg,\n",
    "    target_code: str,  # \"ISL\"\n",
    "    holdout_glaciers: set,  # fixed glacier IDs\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL_budget\",\n",
    "    force_recompute=False,\n",
    "    val_ratio=0.2,\n",
    "    key_train=\"TL_CH_TRAIN\",\n",
    "    key_holdout=None,  # if None -> auto name\n",
    "    show_progress=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds (or loads) assets that are constant across all (G,Y,M,seed) experiments:\n",
    "      - CH pretrain dataset + split + scaler donor\n",
    "      - fixed target holdout dataset (evaluation-only)\n",
    "    \"\"\"\n",
    "    if key_holdout is None:\n",
    "        key_holdout = f\"TL_CH_to_{target_code}_HOLDOUT_FIXED\"\n",
    "\n",
    "    # ---- CH pretrain datasets + scaler donor\n",
    "    res_train = {\n",
    "        \"df_train\": res_xreg[\"df_train\"],\n",
    "        \"df_train_aug\": res_xreg[\"df_train_aug\"],\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    ds_ch, train_idx, val_idx, ds_ch_scalers = build_or_load_lstm_train_only(\n",
    "        cfg=cfg,\n",
    "        key_train=key_train,\n",
    "        res_train=res_train,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        val_ratio=val_ratio,\n",
    "        cache_dir=cache_dir,\n",
    "        force_recompute=force_recompute,\n",
    "        show_progress=show_progress)\n",
    "\n",
    "    ch_source_codes = build_source_codes_for_dataset(ds_ch,\n",
    "                                                     res_xreg[\"df_train_aug\"],\n",
    "                                                     source_col=\"SOURCE_CODE\")\n",
    "\n",
    "    # ---- fixed holdout df (target region)\n",
    "    df_target = res_xreg[\"df_test\"].loc[res_xreg[\"df_test\"][\"SOURCE_CODE\"] ==\n",
    "                                        target_code].copy()\n",
    "    df_target_aug = res_xreg[\"df_test_aug\"].loc[\n",
    "        res_xreg[\"df_test_aug\"][\"SOURCE_CODE\"] == target_code].copy()\n",
    "\n",
    "    df_hold = df_target[df_target[\"GLACIER\"].isin(holdout_glaciers)].copy()\n",
    "    df_hold_aug = df_target_aug[df_target_aug[\"GLACIER\"].isin(\n",
    "        holdout_glaciers)].copy()\n",
    "\n",
    "    if len(df_hold) == 0:\n",
    "        raise ValueError(\n",
    "            f\"{target_code}: fixed holdout is empty. Check holdout_glaciers.\")\n",
    "\n",
    "    ds_holdout = build_or_load_lstm_dataset_only(\n",
    "        cfg=cfg,\n",
    "        key=key_holdout,\n",
    "        df_loss=df_hold,\n",
    "        df_full=df_hold_aug,\n",
    "        months_head_pad=res_xreg[\"months_head_pad\"],\n",
    "        months_tail_pad=res_xreg[\"months_tail_pad\"],\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        cache_dir=cache_dir,\n",
    "        force_recompute=force_recompute,\n",
    "        kind=\"test\",\n",
    "        show_progress=show_progress)\n",
    "\n",
    "    holdout_source_codes = build_source_codes_for_dataset(\n",
    "        ds_holdout, df_hold_aug, source_col=\"SOURCE_CODE\")\n",
    "\n",
    "    static_assets = {\n",
    "        \"ds_pretrain\": ds_ch,\n",
    "        \"ds_pretrain_scalers\": ds_ch_scalers,\n",
    "        \"pretrain_train_idx\": train_idx,\n",
    "        \"pretrain_val_idx\": val_idx,\n",
    "        \"pretrain_source_codes\": ch_source_codes,\n",
    "        \"ds_test\": ds_holdout,\n",
    "        \"test_source_codes\": holdout_source_codes,\n",
    "        \"target_code\": target_code,\n",
    "        \"cache_keys\": {\n",
    "            \"pretrain\": key_train,\n",
    "            \"test\": key_holdout,\n",
    "        },\n",
    "    }\n",
    "    return static_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_budget_assets_finetune_only(\n",
    "    cfg,\n",
    "    res_xreg,\n",
    "    static_assets: dict,\n",
    "    df_ft: pd.DataFrame,\n",
    "    exp_key: str,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL_budget\",\n",
    "    force_recompute=False,\n",
    "    val_ratio=0.2,\n",
    "    show_progress=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the only thing that varies per experiment: the finetune dataset + split.\n",
    "    Then combines with static_assets into the final assets[exp_key] dict.\n",
    "    \"\"\"\n",
    "    target_code = static_assets[\"target_code\"]\n",
    "\n",
    "    # target aug for extracting df_ft_aug\n",
    "    df_target_aug = res_xreg[\"df_test_aug\"].loc[\n",
    "        res_xreg[\"df_test_aug\"][\"SOURCE_CODE\"] == target_code].copy()\n",
    "    df_target_aug[\"PERIOD\"] = df_target_aug[\"PERIOD\"].astype(\n",
    "        str).str.strip().str.lower()\n",
    "\n",
    "    # match aug rows to df_ft keys\n",
    "    key_cols = [\"GLACIER\", \"YEAR\", \"ID\", \"PERIOD\"]\n",
    "    ft_keys = df_ft[key_cols].copy()\n",
    "    ft_keys[\"PERIOD\"] = ft_keys[\"PERIOD\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    df_ft_aug = df_target_aug.merge(ft_keys.drop_duplicates(),\n",
    "                                    on=key_cols,\n",
    "                                    how=\"inner\")\n",
    "\n",
    "    if len(df_ft) == 0 or len(df_ft_aug) == 0:\n",
    "        raise ValueError(f\"{exp_key}: finetune df or aug df is empty.\")\n",
    "\n",
    "    # build finetune dataset (pristine)\n",
    "    ft_cache_key = f\"{exp_key}_FT\"\n",
    "    ds_ft = build_or_load_lstm_dataset_only(\n",
    "        cfg=cfg,\n",
    "        key=ft_cache_key,\n",
    "        df_loss=df_ft,\n",
    "        df_full=df_ft_aug,\n",
    "        months_head_pad=res_xreg[\"months_head_pad\"],\n",
    "        months_tail_pad=res_xreg[\"months_tail_pad\"],\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        cache_dir=cache_dir,\n",
    "        force_recompute=force_recompute,\n",
    "        kind=\"ft\",\n",
    "        show_progress=show_progress)\n",
    "\n",
    "    ft_train_idx, ft_val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_ft), val_ratio=val_ratio, seed=cfg.seed)\n",
    "\n",
    "    ft_source_codes = build_source_codes_for_dataset(ds_ft,\n",
    "                                                     df_ft_aug,\n",
    "                                                     source_col=\"SOURCE_CODE\")\n",
    "\n",
    "    # domain vocab: CH + FT + HOLDOUT\n",
    "    domain_vocab = sorted(\n",
    "        set(static_assets[\"pretrain_source_codes\"])\n",
    "        | set(ft_source_codes)\n",
    "        | set(static_assets[\"test_source_codes\"]))\n",
    "\n",
    "    # assemble final experiment assets (same shape as before)\n",
    "    assets = {\n",
    "        exp_key: {\n",
    "            **static_assets,\n",
    "            \"ds_finetune\": ds_ft,\n",
    "            \"finetune_train_idx\": ft_train_idx,\n",
    "            \"finetune_val_idx\": ft_val_idx,\n",
    "            \"ft_source_codes\": ft_source_codes,\n",
    "            \"domain_vocab\": domain_vocab,\n",
    "            \"split_name\": exp_key,  # optional\n",
    "            \"cache_keys\": {\n",
    "                **static_assets[\"cache_keys\"],\n",
    "                \"finetune\": ft_cache_key,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    return assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build static assets once (CH + fixed ISL holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_assets = build_static_tl_assets_CH_and_holdout(\n",
    "    cfg=cfg,\n",
    "    res_xreg=res_xreg,\n",
    "    target_code=\"ISL\",\n",
    "    holdout_glaciers=holdout_glaciers,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL_budget\",\n",
    "    force_recompute=True,\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the pools for experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isl_pool_rows = df_isl[df_isl[\"GLACIER\"].isin(pool_glaciers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pool = df_isl_pool_rows.copy()\n",
    "\n",
    "G_max = df_pool[\"GLACIER\"].nunique()\n",
    "Y_max = df_pool.groupby(\n",
    "    \"GLACIER\")[\"YEAR\"].nunique().max()  # max years available on any glacier\n",
    "\n",
    "# How many rows per glacier-year exist? (this sets an upper bound for M)\n",
    "rows_per_gy = df_pool.groupby([\"GLACIER\", \"YEAR\"]).size()\n",
    "M_p50 = int(rows_per_gy.median())\n",
    "M_p90 = int(rows_per_gy.quantile(0.90))\n",
    "M_max = int(rows_per_gy.max())\n",
    "\n",
    "print(\"POOL CAPACITY\")\n",
    "print(\"G_max:\", G_max)\n",
    "print(\"Y_max (max years on a glacier):\", Y_max)\n",
    "print(\"Rows per glacier-year: median\", M_p50, \"| p90\", M_p90, \"| max\", M_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_set = [1, 2, 3, 5, 8, 13, 21, 35]\n",
    "Y_set = [1, 2, 3, 5, 8, 13, 21, 37]\n",
    "M_set = [4, 8, 16, 32, 64, 128, 200, 300]\n",
    "\n",
    "G0, Y0, M0 = 8, 8, 64\n",
    "\n",
    "# Tier A: sweeps\n",
    "tierA = ([dict(G=g, Y=Y0, M=M0)\n",
    "          for g in G_set] + [dict(G=G0, Y=y, M=M0) for y in Y_set] +\n",
    "         [dict(G=G0, Y=Y0, M=m) for m in M_set])\n",
    "\n",
    "# Tier B: corners/near-corners\n",
    "tierB = [\n",
    "    dict(G=1, Y=1, M=4),\n",
    "    dict(G=35, Y=37, M=300),\n",
    "    dict(G=35, Y=37, M=4),\n",
    "    dict(G=35, Y=1, M=300),\n",
    "    dict(G=1, Y=37, M=300),\n",
    "    dict(G=35, Y=1, M=4),\n",
    "    dict(G=1, Y=37, M=4),\n",
    "    dict(G=1, Y=1, M=300),\n",
    "]\n",
    "\n",
    "# Tier C: interaction sampling\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample_tierC(n=20, seed=cfg.seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    budgets = []\n",
    "    for _ in range(n):\n",
    "        budgets.append(\n",
    "            dict(\n",
    "                G=int(rng.choice(G_set)),\n",
    "                Y=int(rng.choice(Y_set)),\n",
    "                M=int(rng.choice(M_set)),\n",
    "            ))\n",
    "    # dedupe but keep order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for b in budgets:\n",
    "        t = (b[\"G\"], b[\"Y\"], b[\"M\"])\n",
    "        if t not in seen:\n",
    "            seen.add(t)\n",
    "            out.append(b)\n",
    "    return out\n",
    "\n",
    "\n",
    "tierC = sample_tierC(n=20, seed=cfg.seed)\n",
    "\n",
    "\n",
    "# final list (dedupe)\n",
    "def dedupe_budgets(lst):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for b in lst:\n",
    "        t = (b[\"G\"], b[\"Y\"], b[\"M\"])\n",
    "        if t not in seen:\n",
    "            out.append(b)\n",
    "            seen.add(t)\n",
    "    return out\n",
    "\n",
    "\n",
    "BUDGETS = dedupe_budgets(tierA + tierB + tierC)\n",
    "print(\"Total budget points:\", len(BUDGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEEDS = [10, 20, 30, 40, 50]  # R=5; later: add more\n",
    "# Build list of tasks first\n",
    "TASKS = [(b, seed) for b in BUDGETS for seed in SEEDS]\n",
    "\n",
    "print(\"Total experiments to build:\", len(TASKS))\n",
    "\n",
    "assets_all = {}\n",
    "\n",
    "for b, seed in tqdm(TASKS, desc=\"Building LSTM assets\"):\n",
    "\n",
    "    df_ft, chosen = sample_monitoring_subset_from_pool(\n",
    "        df_pool=df_isl_pool_rows,\n",
    "        G=b[\"G\"],\n",
    "        Y=b[\"Y\"],\n",
    "        M=b[\"M\"],\n",
    "        seed=seed,\n",
    "        glacier_pick_method=\"random\",\n",
    "    )\n",
    "\n",
    "    exp_key = f\"TL_CH_to_ISL_G{b['G']}_Y{b['Y']}_M{b['M']}_seed{seed}\"\n",
    "\n",
    "    assets_one = build_budget_assets_finetune_only(\n",
    "        cfg=cfg,\n",
    "        res_xreg=res_xreg,\n",
    "        static_assets=static_assets,\n",
    "        df_ft=df_ft,\n",
    "        exp_key=exp_key,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        cache_dir=\"logs/LSTM_cache_TL_budget\",\n",
    "        force_recompute=False,\n",
    "        val_ratio=0.2,\n",
    "        show_progress=False)\n",
    "\n",
    "    assets_all.update(assets_one)\n",
    "\n",
    "print(\"Total experiments built:\", len(assets_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check:\n",
    "# for k, v in assets_all.items():\n",
    "#     print(\"\\n\", \"=\" * 60)\n",
    "#     print(\"Experiment:\", k)\n",
    "#     print(\"Available keys:\", list(v.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check:\n",
    "# for exp_key, assets in assets_all.items():\n",
    "#     ft_unique = set(assets[\"ft_source_codes\"])\n",
    "#     test_unique = set(\n",
    "#         assets[\"test_source_codes\"]) if assets[\"test_source_codes\"] else set()\n",
    "#     print(f\"{exp_key} | FT domains: {ft_unique} | TEST domains: {test_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM CH Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_gs_results = {\n",
    "    \"ISL\": 'logs/GS_results/lstm_param_search_progress_OOS_ISL_2026-02-11.csv',\n",
    "    \"NOR\": 'logs/GS_results/lstm_param_search_progress_OOS_NOR_2026-02-09.csv',\n",
    "    \"FR\": 'logs/GS_results/lstm_param_search_progress_OOS_FR_2026-02-06.csv',\n",
    "    \"CH\": 'logs/GS_results/lstm_param_search_progress_CH_2026-02-18.csv',\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 1,\n",
    "    'static_hidden': 128,\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-05,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.1,\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "params_by_key = build_lstm_params_by_key(\n",
    "    default_params=default_params,\n",
    "    log_path_gs_results=log_path_gs_results,\n",
    "    RGI_REGIONS=RGI_REGIONS,\n",
    ")\n",
    "\n",
    "tl_assets_static = {\"STATIC\": static_assets}\n",
    "model_ch, ch_path, ch_info = train_or_load_CH_baseline(\n",
    "    cfg=cfg,\n",
    "    tl_assets=tl_assets_static,\n",
    "    default_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    models_dir=\"models/ISL_experiment\",\n",
    "    prefix=\"lstm_CH\",\n",
    "    key=\"BASELINE\",\n",
    "    train_flag=True,  # or False to only load\n",
    "    force_retrain=False,\n",
    "    epochs=150,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute E_ZERO:\n",
    "E_ZERO = error of the CH baseline model evaluated on the fixed ISL holdout dataset (unseen glaciers), with no finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tl_assets dict with one key (because your codebase often expects dict-of-keys)\n",
    "tl_assets_zero = {\"TL_CH_to_ISL_ZERO\": static_assets}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "metrics_zero, df_preds_zero, _, _ = evaluate_one_model_TL(\n",
    "    cfg=cfg,\n",
    "    model=model_ch,  # <-- CH baseline model\n",
    "    device=device,\n",
    "    tl_assets_for_key=tl_assets_zero[\"TL_CH_to_ISL_ZERO\"],\n",
    "    ax=ax,\n",
    "    title=\"E_ZERO: CH baseline on ISL holdout\",\n",
    "    batch_size=128,\n",
    "    domain_vocab=tl_assets_zero[\"TL_CH_to_ISL_ZERO\"].get(\"domain_vocab\", None),\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "E_ZERO = metrics_zero[\"RMSE_annual\"]\n",
    "print(\"E_ZERO (RMSE_annual):\", E_ZERO)\n",
    "print(metrics_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E_TL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train adapter-only models for experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tl, infos_tl = finetune_TL_models_all(\n",
    "    cfg=cfg,\n",
    "    tl_assets_by_key=assets_all,\n",
    "    best_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    pretrained_ckpt_path=ch_path,\n",
    "    strategies=(\"adapter\", ),\n",
    "    force_retrain=False,\n",
    "    models_dir=\"models/ISL_experiment/\",\n",
    "    prefix=\"lstm_TL\",\n",
    "    verbose=False,\n",
    "    best_by_region=None,\n",
    "    date=None,  # optional: to load old dates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate E_TL:\n",
    "For each budget point on the same holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for exp_key in tqdm(sorted(assets_all.keys()), desc=\"Evaluating TL models\"):\n",
    "    run_key = f\"{exp_key}__adapter\"\n",
    "    model = models_tl.get(run_key, None)\n",
    "    if model is None:\n",
    "        # checkpoint might not exist / training skipped\n",
    "        continue\n",
    "\n",
    "    assets = assets_all[exp_key]\n",
    "\n",
    "    metrics, df_preds, _, _ = evaluate_one_model_TL(\n",
    "        cfg=cfg,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        tl_assets_for_key=assets,\n",
    "        ax=None,\n",
    "        title=None,\n",
    "        batch_size=128,\n",
    "        domain_vocab=assets.get(\"domain_vocab\", None),\n",
    "    )\n",
    "\n",
    "    metrics[\"exp_key\"] = exp_key\n",
    "    rows.append(metrics)\n",
    "\n",
    "df_etl = pd.DataFrame(rows).set_index(\"exp_key\").sort_index()\n",
    "df_etl[\"E_ZERO_RMSE_annual\"] = E_ZERO\n",
    "df_etl[\"Delta_vs_ZERO\"] = df_etl[\"RMSE_annual\"] - E_ZERO\n",
    "\n",
    "display(df_etl[[\n",
    "    \"RMSE_annual\", \"R2_annual\", \"Bias_annual\", \"n_annual\",\n",
    "    \"E_ZERO_RMSE_annual\", \"Delta_vs_ZERO\"\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_budget(exp_key: str):\n",
    "    m = re.search(r\"_G(\\d+)_Y(\\d+)_M(\\d+)_seed(\\d+)\", exp_key)\n",
    "    if not m:\n",
    "        return None\n",
    "    return dict(G=int(m.group(1)),\n",
    "                Y=int(m.group(2)),\n",
    "                M=int(m.group(3)),\n",
    "                seed=int(m.group(4)))\n",
    "\n",
    "\n",
    "meta = df_etl.index.to_series().apply(parse_budget).apply(pd.Series)\n",
    "df_etl2 = pd.concat([meta, df_etl.reset_index(drop=False)], axis=1)\n",
    "\n",
    "# aggregate across seeds per (G,Y,M)\n",
    "agg = df_etl2.groupby([\"G\", \"Y\", \"M\"]).agg(\n",
    "    RMSE_med=(\"RMSE_annual\", \"median\"),\n",
    "    RMSE_p10=(\"RMSE_annual\", lambda x: np.quantile(x, 0.10)),\n",
    "    RMSE_p90=(\"RMSE_annual\", lambda x: np.quantile(x, 0.90)),\n",
    "    n=(\"RMSE_annual\", \"size\"),\n",
    ").reset_index().sort_values([\"G\", \"Y\", \"M\"])\n",
    "\n",
    "display(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute E_FULL: \n",
    "Adapter fine-tuned on all ISL pool data (everything that is not in the fixed holdout glaciers), evaluated on the same fixed holdout (ds_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_key_full = \"TL_CH_to_ISL_FULLPOOL\"\n",
    "\n",
    "assets_full = build_budget_assets_finetune_only(\n",
    "    cfg=cfg,\n",
    "    res_xreg=res_xreg,\n",
    "    static_assets=static_assets,\n",
    "    df_ft=df_isl_pool_rows,  # <-- ALL pool data\n",
    "    exp_key=exp_key_full,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL_budget\",\n",
    "    force_recompute=False,\n",
    "    val_ratio=0.2,\n",
    ")\n",
    "\n",
    "# merge into your experiment dict (optional but convenient)\n",
    "assets_all_plus = dict(assets_all)\n",
    "assets_all_plus.update(assets_full)\n",
    "\n",
    "print(\"FULL asset built:\", exp_key_full)\n",
    "print(\"Full finetune sequences:\",\n",
    "      len(assets_all_plus[exp_key_full][\"ds_finetune\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_full, infos_full = finetune_TL_models_all(\n",
    "    cfg=cfg,\n",
    "    tl_assets_by_key={exp_key_full:\n",
    "                      assets_all_plus[exp_key_full]},  # only this one\n",
    "    best_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    pretrained_ckpt_path=ch_path,\n",
    "    strategies=(\"adapter\", ),\n",
    "    force_retrain=False,\n",
    "    models_dir=\"models/ISL_experiment/\",\n",
    "    prefix=\"lstm_TL\",\n",
    "    verbose=False,\n",
    "    best_by_region=None,\n",
    "    date=\"fixed\",\n",
    ")\n",
    "\n",
    "run_key_full = f\"{exp_key_full}__adapter\"\n",
    "model_full = models_full[run_key_full]  # <-- this is the model object\n",
    "\n",
    "metrics_full, df_preds_full, _, _ = evaluate_one_model_TL(\n",
    "    cfg=cfg,\n",
    "    model=model_full,\n",
    "    device=device,\n",
    "    tl_assets_for_key=assets_all_plus[exp_key_full],\n",
    "    ax=None,\n",
    "    title=None,\n",
    "    batch_size=128,\n",
    "    domain_vocab=assets_all_plus[exp_key_full].get(\"domain_vocab\", None),\n",
    ")\n",
    "\n",
    "E_FULL = metrics_full[\"RMSE_annual\"]\n",
    "print(\"E_FULL (RMSE_annual):\", E_FULL)\n",
    "print(metrics_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute E_SCRATCH:\n",
    "“no-transfer” (from-scratch) baseline trained on the same small ISL monitoring subset and evaluated on the same fixed ISL holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_budget(exp_key: str):\n",
    "    m = re.search(r\"_G(\\d+)_Y(\\d+)_M(\\d+)_seed(\\d+)\", exp_key)\n",
    "    if not m:\n",
    "        return {}\n",
    "    return {\n",
    "        \"G\": int(m.group(1)),\n",
    "        \"Y\": int(m.group(2)),\n",
    "        \"M\": int(m.group(3)),\n",
    "        \"seed\": int(m.group(4))\n",
    "    }\n",
    "\n",
    "\n",
    "def within_assets_from_tl_assets(tl_assets_for_key: dict):\n",
    "    return {\n",
    "        \"ds_train\": tl_assets_for_key[\"ds_finetune\"],  # finetune subset\n",
    "        \"ds_test\": tl_assets_for_key[\"ds_test\"],  # fixed holdout\n",
    "        \"train_idx\": tl_assets_for_key[\"finetune_train_idx\"],\n",
    "        \"val_idx\": tl_assets_for_key[\"finetune_val_idx\"],\n",
    "    }\n",
    "\n",
    "\n",
    "models_within = {}\n",
    "infos_within = {}\n",
    "E_SCRATCH_by_key = {}\n",
    "rows = []\n",
    "\n",
    "exp_keys = sorted(assets_all.keys())\n",
    "\n",
    "pbar = tqdm(exp_keys, desc=\"Training+Evaluating E_SCRATCH\", dynamic_ncols=True)\n",
    "\n",
    "for exp_key in pbar:\n",
    "    meta = parse_budget(exp_key)\n",
    "    if meta:\n",
    "        pbar.set_postfix(meta)\n",
    "\n",
    "    w_assets = within_assets_from_tl_assets(assets_all[exp_key])\n",
    "\n",
    "    model_w, path_w, info_w = train_or_load_one_within_region(\n",
    "        cfg=cfg,\n",
    "        key=exp_key,\n",
    "        lstm_assets=w_assets,\n",
    "        best_params=params_by_key[\"06_ISL\"],\n",
    "        device=device,\n",
    "        models_dir=\"models/ISL_experiment/\",\n",
    "        prefix=\"lstm_within_ISL\",\n",
    "        train_flag=True,\n",
    "        force_retrain=True,\n",
    "        epochs=150,\n",
    "        batch_size_train=64,\n",
    "        batch_size_val=128,\n",
    "        batch_size_test=128,\n",
    "    )\n",
    "\n",
    "    models_within[exp_key] = model_w\n",
    "    infos_within[exp_key] = {\"model_path\": path_w, **(info_w or {})}\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    met_w, df_w = model_w.evaluate_with_preds(\n",
    "        device,\n",
    "        info_w[\"test_dl\"],\n",
    "        info_w[\"ds_test\"],\n",
    "    )\n",
    "\n",
    "    E_SCRATCH = float(met_w[\"RMSE_annual\"])\n",
    "    E_SCRATCH_by_key[exp_key] = E_SCRATCH\n",
    "\n",
    "    rows.append({\n",
    "        \"exp_key\": exp_key,\n",
    "        \"RMSE_SCRATCH\": E_SCRATCH,\n",
    "        **meta,\n",
    "    })\n",
    "\n",
    "df_scratch = pd.DataFrame(rows).set_index(\"exp_key\").sort_index()\n",
    "display(df_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Recovery for your 3 budgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = df_etl.copy()\n",
    "df_rec[\"E_FULL_RMSE_annual\"] = E_FULL\n",
    "\n",
    "den = (E_ZERO - E_FULL)\n",
    "df_rec[\"Recovery\"] = np.where(\n",
    "    np.abs(den) < 1e-12, np.nan, (E_ZERO - df_rec[\"RMSE_annual\"]) / den)\n",
    "\n",
    "display(df_rec[[\n",
    "    \"RMSE_annual\", \"E_ZERO_RMSE_annual\", \"E_FULL_RMSE_annual\", \"Recovery\",\n",
    "    \"R2_annual\", \"Bias_annual\"\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
