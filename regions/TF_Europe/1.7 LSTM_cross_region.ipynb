{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "from typing import Optional, Iterable, Dict, List\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from regions.TF_Europe.scripts.config_TF_Europe import *\n",
    "from regions.TF_Europe.scripts.dataset import *\n",
    "from regions.TF_Europe.scripts.plotting import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.EuropeTFConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-regional modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stakes datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of loading data:\n",
    "# Load Switzerland only\n",
    "df = load_stakes(cfg, \"CH\")\n",
    "\n",
    "# Load all Central Europe (FR+CH+IT+AT when you add them)\n",
    "df_ceu = load_stakes_for_rgi_region(cfg, \"11\")\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\"\"\"\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "dfs[\"11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "summarize_and_plot_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it\n",
    "plot_mb_distributions_all_regions(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test glaciers are all their glaciers:\n",
    "TEST_GLACIERS_SJM = load_stakes(cfg, \"SJM\").GLACIER.unique().tolist()\n",
    "TEST_GLACIERS_ISL = load_stakes(cfg, \"ISL\").GLACIER.unique().tolist()\n",
    "TEST_GLACIERS_NOR = load_stakes(cfg, \"NOR\").GLACIER.unique().tolist()\n",
    "TEST_GLACIERS_FR = load_stakes(cfg, \"FR\").GLACIER.unique().tolist()\n",
    "TEST_GLACIERS_IT_AT = load_stakes(cfg, \"IT_AT\").GLACIER.unique().tolist()\n",
    "\n",
    "TEST_GLACIERS_BY_CODE = {\n",
    "    \"SJM\": TEST_GLACIERS_SJM,\n",
    "    \"ISL\": TEST_GLACIERS_ISL,\n",
    "    \"NOR\": TEST_GLACIERS_NOR,\n",
    "    \"FR\": TEST_GLACIERS_FR,\n",
    "    # \"CH\": TEST_GLACIERS_CH,\n",
    "    \"IT_AT\": TEST_GLACIERS_IT_AT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_Europe.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_Europe.nc\")\n",
    "}\n",
    "\n",
    "# Check that all these files exists\n",
    "for key, path in paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required file for {key} not found at {path}\")\n",
    "\n",
    "    vois_climate = [\n",
    "        \"t2m\",\n",
    "        \"tp\",\n",
    "        \"slhf\",\n",
    "        \"sshf\",\n",
    "        \"ssrd\",\n",
    "        \"fal\",\n",
    "        \"str\",\n",
    "    ]\n",
    "\n",
    "vois_topographical = [\"aspect\", \"slope\", \"svf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crossregional_df_ceu_with_ch(dfs: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate all stake dataframes in `dfs` into one Europe-wide dataframe.\n",
    "\n",
    "    Expects:\n",
    "      - Each df has at least columns: GLACIER, YEAR, ID, PERIOD, MONTHS, POINT_BALANCE\n",
    "      - Central Europe df includes SOURCE_CODE identifying CH/FR/IT_AT etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe (all rows across all RGI regions).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for rid, df in dfs.items():\n",
    "        if df is None or len(df) == 0:\n",
    "            logging.warning(f\"RGI {rid}: empty, skipping in concat.\")\n",
    "            continue\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No non-empty dataframes in dfs.\")\n",
    "\n",
    "    d_all = pd.concat(frames, ignore_index=True)\n",
    "    return d_all\n",
    "\n",
    "\n",
    "def compute_crossregional_test_glaciers(\n",
    "    df_all: pd.DataFrame,\n",
    "    ch_code: str = \"CH\",\n",
    "    source_col: str = \"SOURCE_CODE\",\n",
    "    glacier_col: str = \"GLACIER\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train glaciers = all glaciers with SOURCE_CODE == CH\n",
    "    Test glaciers  = all glaciers with SOURCE_CODE != CH\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (train_glaciers, test_glaciers) : (list[str], list[str])\n",
    "    \"\"\"\n",
    "    if source_col not in df_all.columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing column {source_col}. Needed to separate CH vs others.\")\n",
    "    if glacier_col not in df_all.columns:\n",
    "        raise ValueError(f\"Missing column {glacier_col}.\")\n",
    "\n",
    "    ch_gl = sorted(df_all.loc[df_all[source_col] == ch_code,\n",
    "                              glacier_col].dropna().unique())\n",
    "    non_ch_gl = sorted(df_all.loc[df_all[source_col] != ch_code,\n",
    "                                  glacier_col].dropna().unique())\n",
    "\n",
    "    if not ch_gl:\n",
    "        raise ValueError(\"No CH glaciers found (SOURCE_CODE=='CH').\")\n",
    "    if not non_ch_gl:\n",
    "        raise ValueError(\"No non-CH glaciers found (SOURCE_CODE!='CH').\")\n",
    "\n",
    "    logging.info(\n",
    "        f\"Cross-regional split: CH train glaciers={len(ch_gl)}, non-CH test glaciers={len(non_ch_gl)}\"\n",
    "    )\n",
    "    return ch_gl, non_ch_gl\n",
    "\n",
    "\n",
    "def prepare_monthly_df_crossregional_CH_to_EU(\n",
    "    cfg,\n",
    "    dfs,\n",
    "    paths,\n",
    "    vois_climate,\n",
    "    vois_topographical,\n",
    "    run_flag=True,  # True recompute, False load\n",
    "    region_name=\"XREG_CH_TO_EU\",\n",
    "    region_id=11,  # arbitrary/int tag used by your pipeline; keep 11 or 0\n",
    "    csv_subfolder=\"CrossRegional/CH_to_Europe/csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build ONE monthly-prepped dataset:\n",
    "      - data = concatenation of all Europe sources\n",
    "      - train = CH glaciers\n",
    "      - test  = all non-CH glaciers\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : dict\n",
    "        Same output dict as prepare_monthly_dfs_with_padding (df_train/df_test/aug/etc.)\n",
    "    split_info : dict\n",
    "        {\"train_glaciers\": [...], \"test_glaciers\": [...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Concatenate all raw stake rows\n",
    "    df_all = build_crossregional_df_ceu_with_ch(dfs)\n",
    "\n",
    "    # 2) Define test glaciers: all non-CH\n",
    "    train_glaciers, test_glaciers = compute_crossregional_test_glaciers(\n",
    "        df_all, ch_code=\"CH\")\n",
    "\n",
    "    # 3) Choose an output folder for this experiment\n",
    "    paths_ = paths.copy()\n",
    "    paths_[\"csv_path\"] = os.path.join(cfg.dataPath, path_PMB_WGMS_csv,\n",
    "                                      csv_subfolder)\n",
    "    os.makedirs(paths_[\"csv_path\"], exist_ok=True)\n",
    "\n",
    "    logging.info(\n",
    "        f\"Preparing cross-regional monthlies: {region_name} \"\n",
    "        f\"(run_flag={run_flag}) | train(CH)={len(train_glaciers)} | test(non-CH)={len(test_glaciers)}\"\n",
    "    )\n",
    "\n",
    "    res = prepare_monthly_dfs_with_padding(\n",
    "        cfg=cfg,\n",
    "        df_region=df_all,\n",
    "        region_name=region_name,\n",
    "        region_id=int(region_id),\n",
    "        paths=paths_,\n",
    "        test_glaciers=test_glaciers,  # test = all non-CH glaciers\n",
    "        vois_climate=vois_climate,\n",
    "        vois_topographical=vois_topographical,\n",
    "        run_flag=run_flag,\n",
    "    )\n",
    "\n",
    "    return res, {\n",
    "        \"train_glaciers\": train_glaciers,\n",
    "        \"test_glaciers\": test_glaciers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all stake dfs\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\n",
    "# prepare monthlies (recompute or load)\n",
    "res_xreg, split_info = prepare_monthly_df_crossregional_CH_to_EU(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    run_flag=False,  # load if already computed\n",
    ")\n",
    "\n",
    "df_train = res_xreg[\"df_train\"]\n",
    "df_test = res_xreg[\"df_test\"]\n",
    "\n",
    "print(\"Train glaciers (CH):\", len(split_info[\"train_glaciers\"]))\n",
    "print(\"Test glaciers (non-CH):\", len(split_info[\"test_glaciers\"]))\n",
    "print(\"Train rows:\", len(df_train), \"Test rows:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_overlap_xreg_from_single_res(\n",
    "        res_xreg: dict,\n",
    "        cfg,\n",
    "        STATIC_COLS,\n",
    "        MONTHLY_COLS,\n",
    "        group_col: str = \"SOURCE_CODE\",\n",
    "        ch_code: str = \"CH\",\n",
    "        use_aug: bool = False,  # True -> df_train_aug/df_test_aug\n",
    "        n_iter: int = 1000,\n",
    "        only_codes=None,  # e.g. [\"IT_AT\", \"FR\"]\n",
    "        skip_codes=None,  # e.g. [\"CH\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    For XREG where train=CH and test=all non-CH inside ONE monthly result dict:\n",
    "\n",
    "      - df_ch = res_xreg[df_train*]\n",
    "      - df_test_all = res_xreg[df_test*]\n",
    "      - split df_test_all by SOURCE_CODE and plot CH vs each code\n",
    "\n",
    "    Returns dict: code -> figure\n",
    "    \"\"\"\n",
    "    only_codes = {c.upper() for c in (only_codes or [])} or None\n",
    "    skip_codes = {c.upper() for c in (skip_codes or [])}\n",
    "    skip_codes.add(ch_code.upper())\n",
    "\n",
    "    # pick which dfs\n",
    "    if use_aug:\n",
    "        df_ch = res_xreg.get(\"df_train_aug\")\n",
    "        df_test_all = res_xreg.get(\"df_test_aug\")\n",
    "        label_df = \"(*_aug)\"\n",
    "    else:\n",
    "        df_ch = res_xreg.get(\"df_train\")\n",
    "        df_test_all = res_xreg.get(\"df_test\")\n",
    "        label_df = \"\"\n",
    "\n",
    "    if df_ch is None or len(df_ch) == 0:\n",
    "        raise ValueError(f\"df_train{label_df} missing/empty in res_xreg.\")\n",
    "    if df_test_all is None or len(df_test_all) == 0:\n",
    "        raise ValueError(f\"df_test{label_df} missing/empty in res_xreg.\")\n",
    "\n",
    "    if group_col not in df_test_all.columns:\n",
    "        raise ValueError(\n",
    "            f\"'{group_col}' not found in df_test{label_df}. Needed to split by region.\"\n",
    "        )\n",
    "    if group_col not in df_ch.columns:\n",
    "        # not fatal, but helps sanity-check\n",
    "        print(\n",
    "            f\"[warn] '{group_col}' not in df_train{label_df}. That's OK for CH reference.\"\n",
    "        )\n",
    "\n",
    "    # palette\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    custom_palette = {\"Train\": color_dark_blue, \"Test\": \"#b2182b\"}\n",
    "\n",
    "    # codes present in test\n",
    "    codes_present = sorted(c for c in df_test_all[group_col].dropna().astype(\n",
    "        str).str.upper().unique() if c not in skip_codes)\n",
    "\n",
    "    if only_codes is not None:\n",
    "        codes_present = [c for c in codes_present if c in only_codes]\n",
    "\n",
    "    figs = {}\n",
    "    for code in codes_present:\n",
    "        df_other = df_test_all[df_test_all[group_col].astype(str).str.upper()\n",
    "                               == code].copy()\n",
    "        if len(df_other) == 0:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Plotting XREG t-SNE: CH(train n={len(df_ch)}) vs {code}(test n={len(df_other)})\"\n",
    "        )\n",
    "\n",
    "        fig = plot_tsne_overlap(\n",
    "            data_train=df_ch,\n",
    "            data_test=df_other,\n",
    "            STATIC_COLS=STATIC_COLS,\n",
    "            MONTHLY_COLS=MONTHLY_COLS,\n",
    "            sublabels=(\"a\", \"b\", \"c\"),\n",
    "            label_fmt=\"({})\",\n",
    "            label_xy=(0.02, 0.98),\n",
    "            label_fontsize=14,\n",
    "            n_iter=n_iter,\n",
    "            random_state=cfg.seed,\n",
    "            custom_palette=custom_palette,\n",
    "        )\n",
    "        fig.suptitle(f\"XREG overlap: CH vs {code}\", fontsize=14)\n",
    "        figs[code] = fig\n",
    "\n",
    "    return figs\n",
    "\n",
    "\n",
    "# res_xreg is the ONE dict from your cross-regional monthly prep\n",
    "figs_by_code = plot_tsne_overlap_xreg_from_single_res(\n",
    "    res_xreg=res_xreg,\n",
    "    cfg=cfg,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    use_aug=False,  # or True if you want *_aug\n",
    "    n_iter=1000,\n",
    "    # only_codes=[\"IT_AT\"],  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def plot_feature_kde_overlap_xreg_ch_vs_codes(\n",
    "    res_xreg: dict,\n",
    "    cfg,\n",
    "    features,\n",
    "    group_col: str = \"SOURCE_CODE\",\n",
    "    ch_code: str = \"CH\",\n",
    "    use_aug: bool = False,  # True -> df_train_aug/df_test_aug\n",
    "    only_codes=None,  # e.g. [\"IT_AT\", \"FR\"]\n",
    "    skip_codes=None,  # e.g. [\"CH\"]\n",
    "    output_dir=None,  # e.g. \"figures/xreg_kde\"\n",
    "    include_ch_in_title: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot KDE-based feature overlap for XREG: CH vs each SOURCE_CODE subset.\n",
    "\n",
    "    Uses:\n",
    "      - CH reference: res_xreg[\"df_train\"] (or \"_aug\" if use_aug)\n",
    "      - Other region: subset of res_xreg[\"df_test\"] by SOURCE_CODE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    res_xreg : dict\n",
    "        Output dict from prepare_monthly_df_crossregional_CH_to_EU (or similar),\n",
    "        containing df_train/df_test and optionally df_train_aug/df_test_aug.\n",
    "        df_test must contain `group_col` (SOURCE_CODE).\n",
    "    cfg : object\n",
    "        Used only for consistent output naming if desired (optional).\n",
    "    features : list[str]\n",
    "        Feature columns to plot.\n",
    "    group_col : str\n",
    "        Column to split test set by (default: \"SOURCE_CODE\").\n",
    "    ch_code : str\n",
    "        Code identifying CH (default: \"CH\").\n",
    "    use_aug : bool\n",
    "        If True uses df_train_aug/df_test_aug.\n",
    "    only_codes : list[str] or None\n",
    "        If given, only plot these codes.\n",
    "    skip_codes : list[str] or None\n",
    "        Codes to skip (CH is always skipped by default).\n",
    "    output_dir : str or None\n",
    "        If set, saves one PNG per code into this directory.\n",
    "    include_ch_in_title : bool\n",
    "        Adds CH vs CODE title on each figure.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        code -> matplotlib Figure\n",
    "    \"\"\"\n",
    "    # palette (reuse your consistent colors)\n",
    "    colors = get_cmap_hex(cm.batlow, 10)\n",
    "    color_dark_blue = colors[0]\n",
    "    palette = {\n",
    "        \"Train\": color_dark_blue,\n",
    "        \"Test\": \"#b2182b\"\n",
    "    }  # Train=CH, Test=Other\n",
    "\n",
    "    ch_code = str(ch_code).upper()\n",
    "    only_set = {c.upper() for c in only_codes} if only_codes else None\n",
    "    skip_set = {c.upper() for c in (skip_codes or [])}\n",
    "    skip_set.add(ch_code)\n",
    "\n",
    "    if use_aug:\n",
    "        df_ch = res_xreg.get(\"df_train_aug\")\n",
    "        df_test_all = res_xreg.get(\"df_test_aug\")\n",
    "        suffix = \"_aug\"\n",
    "    else:\n",
    "        df_ch = res_xreg.get(\"df_train\")\n",
    "        df_test_all = res_xreg.get(\"df_test\")\n",
    "        suffix = \"\"\n",
    "\n",
    "    if df_ch is None or len(df_ch) == 0:\n",
    "        raise ValueError(f\"Missing/empty df_train{suffix} in res_xreg.\")\n",
    "    if df_test_all is None or len(df_test_all) == 0:\n",
    "        raise ValueError(f\"Missing/empty df_test{suffix} in res_xreg.\")\n",
    "    if group_col not in df_test_all.columns:\n",
    "        raise ValueError(f\"'{group_col}' not found in df_test{suffix}.\")\n",
    "\n",
    "    codes = sorted(\n",
    "        df_test_all[group_col].dropna().astype(str).str.upper().unique())\n",
    "    codes = [c for c in codes if c not in skip_set]\n",
    "    if only_set is not None:\n",
    "        codes = [c for c in codes if c in only_set]\n",
    "\n",
    "    if output_dir:\n",
    "        out_abs = os.path.join(cfg.dataPath, output_dir) if hasattr(\n",
    "            cfg, \"dataPath\") else output_dir\n",
    "        os.makedirs(out_abs, exist_ok=True)\n",
    "    else:\n",
    "        out_abs = None\n",
    "\n",
    "    figs = {}\n",
    "\n",
    "    for code in codes:\n",
    "        df_other = df_test_all[df_test_all[group_col].astype(str).str.upper()\n",
    "                               == code].copy()\n",
    "        if len(df_other) == 0:\n",
    "            continue\n",
    "\n",
    "        print(\n",
    "            f\"Plotting XREG KDE: CH(train n={len(df_ch)}) vs {code}(test n={len(df_other)})\"\n",
    "        )\n",
    "\n",
    "        fig = plot_feature_kde_overlap(\n",
    "            df_train=df_ch,\n",
    "            df_test=df_other,\n",
    "            features=features,\n",
    "            palette=palette,\n",
    "            outfile=None,  # save here instead (so we control naming)\n",
    "        )\n",
    "\n",
    "        if include_ch_in_title:\n",
    "            fig.suptitle(f\"XREG feature overlap: CH vs {code}\", fontsize=14)\n",
    "            fig.tight_layout()\n",
    "\n",
    "        if out_abs:\n",
    "            out_png = os.path.join(\n",
    "                out_abs, f\"xreg_kde_overlap_CH_vs_{code}{suffix}.png\")\n",
    "            fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "        figs[code] = fig\n",
    "\n",
    "    return figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = MONTHLY_COLS + STATIC_COLS + [\"POINT_BALANCE\"]\n",
    "\n",
    "figs_kde = plot_feature_kde_overlap_xreg_ch_vs_codes(\n",
    "    res_xreg=res_xreg,\n",
    "    cfg=cfg,\n",
    "    features=FEATURES,\n",
    "    group_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    use_aug=True,  # usually best for feature overlap\n",
    "    # only_codes=[\"IT_AT\", \"FR\"],    # optional\n",
    "    output_dir=\"figures/xreg_kde\",  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "### LSTM datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crossregional_res_all(\n",
    "    res_xreg: dict,\n",
    "    target_source_codes=None,\n",
    "    source_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    key_prefix=\"XREG_CH_TO\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns res_all dict: {key: res_like_dict}\n",
    "    where each res contains df_train (CH), df_test (only that target region),\n",
    "    plus *_aug and pads.\n",
    "    \"\"\"\n",
    "    df_test = res_xreg[\"df_test\"]\n",
    "    if source_col not in df_test.columns:\n",
    "        raise ValueError(f\"Missing {source_col} in res_xreg['df_test'].\")\n",
    "\n",
    "    if target_source_codes is None:\n",
    "        target_source_codes = sorted(\n",
    "            set(df_test[source_col].dropna().unique()) - {ch_code})\n",
    "\n",
    "    res_all = {}\n",
    "    for sc in target_source_codes:\n",
    "        key = f\"{key_prefix}_{sc}\"\n",
    "\n",
    "        res_sc = {\n",
    "            \"df_train\":\n",
    "            res_xreg[\"df_train\"],\n",
    "            \"df_train_aug\":\n",
    "            res_xreg[\"df_train_aug\"],\n",
    "            \"df_test\":\n",
    "            res_xreg[\"df_test\"].loc[res_xreg[\"df_test\"][source_col] ==\n",
    "                                    sc].copy(),\n",
    "            \"df_test_aug\":\n",
    "            res_xreg[\"df_test_aug\"].loc[res_xreg[\"df_test_aug\"][source_col] ==\n",
    "                                        sc].copy(),\n",
    "            \"months_head_pad\":\n",
    "            res_xreg[\"months_head_pad\"],\n",
    "            \"months_tail_pad\":\n",
    "            res_xreg[\"months_tail_pad\"],\n",
    "        }\n",
    "\n",
    "        res_all[key] = res_sc\n",
    "\n",
    "    return res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all_xreg = build_crossregional_res_all(\n",
    "    res_xreg=res_xreg,\n",
    "    target_source_codes=None,  # auto-discover from df_test\n",
    "    source_col=\"SOURCE_CODE\",\n",
    "    key_prefix=\"XREG_CH_TO\",\n",
    ")\n",
    "res_all_xreg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs_xreg = build_or_load_lstm_all(\n",
    "#     cfg=cfg,\n",
    "#     res_all=res_all_xreg,\n",
    "#     MONTHLY_COLS=MONTHLY_COLS,\n",
    "#     STATIC_COLS=STATIC_COLS,\n",
    "#     cache_dir=\"logs/LSTM_cache\",\n",
    "#     only_keys=None,  # e.g. [\"XREG_CH_TO_NO\"] to recompute only Norway\n",
    "#     force_recompute=False,  # global default: load if cached\n",
    "#     val_ratio=0.2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_for_nans(key,\n",
    "                    df_loss,\n",
    "                    df_full,\n",
    "                    monthly_cols,\n",
    "                    static_cols,\n",
    "                    strict=True):\n",
    "    \"\"\"\n",
    "    Checks for NaNs/Infs in features and targets.\n",
    "    Raises ValueError if strict=True, otherwise prints warning.\n",
    "    \"\"\"\n",
    "    feat_cols = [\n",
    "        c for c in (monthly_cols + static_cols) if c in df_full.columns\n",
    "    ]\n",
    "\n",
    "    # --- feature NaNs ---\n",
    "    n_nan_feat = df_full[feat_cols].isna().sum().sum()\n",
    "    n_inf_feat = np.isinf(df_full[feat_cols].to_numpy(dtype=\"float64\",\n",
    "                                                      copy=False)).sum()\n",
    "\n",
    "    # --- target NaNs ---\n",
    "    n_nan_target = df_loss[\"POINT_BALANCE\"].isna().sum()\n",
    "    n_inf_target = np.isinf(df_loss[\"POINT_BALANCE\"].to_numpy(\n",
    "        dtype=\"float64\", copy=False)).sum()\n",
    "\n",
    "    if any([n_nan_feat, n_inf_feat, n_nan_target, n_inf_target]):\n",
    "\n",
    "        msg = (f\"[{key}] Data integrity issue:\\n\"\n",
    "               f\"  Feature NaNs: {n_nan_feat}\\n\"\n",
    "               f\"  Feature Infs: {n_inf_feat}\\n\"\n",
    "               f\"  Target  NaNs: {n_nan_target}\\n\"\n",
    "               f\"  Target  Infs: {n_inf_target}\")\n",
    "\n",
    "        if strict:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            warnings.warn(msg)\n",
    "\n",
    "\n",
    "def _lstm_cache_paths(cfg, key: str, cache_dir: str):\n",
    "    out_dir = os.path.join(cache_dir)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_p = os.path.join(out_dir, f\"{key}_train.joblib\")\n",
    "    test_p = os.path.join(out_dir, f\"{key}_test.joblib\")\n",
    "    split_p = os.path.join(out_dir, f\"{key}_split.joblib\")\n",
    "    return train_p, test_p, split_p\n",
    "\n",
    "\n",
    "def build_or_load_lstm_train_only(\n",
    "    cfg,\n",
    "    key_train: str,\n",
    "    res_train: dict,  # must contain df_train, df_train_aug, pads\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    val_ratio=0.2,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    force_recompute=False,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "    strict_nan=True,\n",
    "):\n",
    "    train_p, _, split_p = _lstm_cache_paths(cfg,\n",
    "                                            key_train,\n",
    "                                            cache_dir=cache_dir)\n",
    "\n",
    "    if (not force_recompute) and all(\n",
    "            os.path.exists(p) for p in [train_p, split_p]):\n",
    "        ds_train = joblib.load(train_p)\n",
    "        split = joblib.load(split_p)\n",
    "        return ds_train, split[\"train_idx\"], split[\"val_idx\"]\n",
    "\n",
    "    df_train = res_train[\"df_train\"]\n",
    "    df_train_aug = res_train[\"df_train_aug\"]\n",
    "    months_head_pad = res_train[\"months_head_pad\"]\n",
    "    months_tail_pad = res_train[\"months_tail_pad\"]\n",
    "\n",
    "    _check_for_nans(\n",
    "        key_train,\n",
    "        df_loss=df_train,\n",
    "        df_full=df_train_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        strict=strict_nan,\n",
    "    )\n",
    "\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_train = build_combined_LSTM_dataset(\n",
    "        df_loss=df_train,\n",
    "        df_full=df_train_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    train_idx, val_idx = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "        len(ds_train), val_ratio=val_ratio, seed=cfg.seed)\n",
    "\n",
    "    joblib.dump(ds_train, train_p, compress=3)\n",
    "    joblib.dump({\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx\n",
    "    },\n",
    "                split_p,\n",
    "                compress=3)\n",
    "\n",
    "    return ds_train, train_idx, val_idx\n",
    "\n",
    "\n",
    "def build_or_load_lstm_test_only(\n",
    "    cfg,\n",
    "    key_test: str,\n",
    "    res_test: dict,  # must contain df_test, df_test_aug, pads\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    force_recompute=False,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "    strict_nan=True,\n",
    "):\n",
    "    _, test_p, _ = _lstm_cache_paths(cfg, key_test, cache_dir=cache_dir)\n",
    "\n",
    "    if (not force_recompute) and os.path.exists(test_p):\n",
    "        return joblib.load(test_p)\n",
    "\n",
    "    df_test = res_test[\"df_test\"]\n",
    "    df_test_aug = res_test[\"df_test_aug\"]\n",
    "    months_head_pad = res_test[\"months_head_pad\"]\n",
    "    months_tail_pad = res_test[\"months_tail_pad\"]\n",
    "\n",
    "    _check_for_nans(\n",
    "        key_test,\n",
    "        df_loss=df_test,\n",
    "        df_full=df_test_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        strict=strict_nan,\n",
    "    )\n",
    "\n",
    "    mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "    ds_test = build_combined_LSTM_dataset(\n",
    "        df_loss=df_test,\n",
    "        df_full=df_test_aug,\n",
    "        monthly_cols=MONTHLY_COLS,\n",
    "        static_cols=STATIC_COLS,\n",
    "        months_head_pad=months_head_pad,\n",
    "        months_tail_pad=months_tail_pad,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "    )\n",
    "\n",
    "    joblib.dump(ds_test, test_p, compress=3)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "def build_or_load_lstm_all_crossregional(\n",
    "    cfg,\n",
    "    res_xreg: dict,\n",
    "    MONTHLY_COLS,\n",
    "    STATIC_COLS,\n",
    "    target_source_codes=None,\n",
    "    source_col=\"SOURCE_CODE\",\n",
    "    ch_code=\"CH\",\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    "    force_recompute_train=False,\n",
    "    force_recompute_tests=False,\n",
    "    only_test_keys=None,\n",
    "    val_ratio=0.2,\n",
    "    normalize_target=True,\n",
    "    expect_target=True,\n",
    "    strict_nan=True,\n",
    "):\n",
    "\n",
    "    logging.info(\"\\n\" + \"=\" * 60)\n",
    "    logging.info(\"CROSS-REGIONAL LSTM DATASET PREPARATION (CH → EU)\")\n",
    "    logging.info(\"=\" * 60)\n",
    "\n",
    "    # ---- discover target codes ----\n",
    "    df_test_all = res_xreg[\"df_test\"]\n",
    "\n",
    "    if target_source_codes is None:\n",
    "        target_source_codes = sorted(\n",
    "            set(df_test_all[source_col].dropna().unique()) - {ch_code})\n",
    "        logging.info(\n",
    "            f\"Auto-detected target SOURCE_CODEs (excluding {ch_code}): \"\n",
    "            f\"{target_source_codes}\")\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Using provided target SOURCE_CODEs: {target_source_codes}\")\n",
    "\n",
    "    logging.info(f\"Total target regions: {len(target_source_codes)}\")\n",
    "    logging.info(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "    # ---- train (CH) cached once ----\n",
    "    key_train = \"XREG_CH_TRAIN\"\n",
    "\n",
    "    logging.info(\"\\n--- CH TRAIN DATASET ---\")\n",
    "    logging.info(f\"Cache key: {key_train}\")\n",
    "    logging.info(f\"Force recompute train: {force_recompute_train}\")\n",
    "\n",
    "    res_train = {\n",
    "        \"df_train\": res_xreg[\"df_train\"],\n",
    "        \"df_train_aug\": res_xreg[\"df_train_aug\"],\n",
    "        \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "        \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "    }\n",
    "\n",
    "    logging.info(f\"CH train rows: {len(res_train['df_train'])} | \"\n",
    "                 f\"Aug rows: {len(res_train['df_train_aug'])}\")\n",
    "\n",
    "    ds_train, train_idx, val_idx = build_or_load_lstm_train_only(\n",
    "        cfg=cfg,\n",
    "        key_train=key_train,\n",
    "        res_train=res_train,\n",
    "        MONTHLY_COLS=MONTHLY_COLS,\n",
    "        STATIC_COLS=STATIC_COLS,\n",
    "        val_ratio=val_ratio,\n",
    "        cache_dir=cache_dir,\n",
    "        force_recompute=force_recompute_train,\n",
    "        normalize_target=normalize_target,\n",
    "        expect_target=expect_target,\n",
    "        strict_nan=strict_nan,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"CH train dataset size: {len(ds_train)} | \"\n",
    "                 f\"Train split: {len(train_idx)} | Val split: {len(val_idx)}\")\n",
    "\n",
    "    # ---- tests cached per target ----\n",
    "    logging.info(\"\\n--- TARGET REGION TEST DATASETS ---\")\n",
    "\n",
    "    outputs = {}\n",
    "    only_set = set(only_test_keys) if only_test_keys else None\n",
    "\n",
    "    for sc in target_source_codes:\n",
    "\n",
    "        fr_test = force_recompute_tests\n",
    "        if only_set is not None:\n",
    "            fr_test = (sc in only_set) or (f\"XREG_CH_TO_{sc}\" in only_set)\n",
    "\n",
    "        logging.info(\"\\n\" + \"-\" * 50)\n",
    "        logging.info(f\"Target region: {sc}\")\n",
    "        logging.info(f\"Force recompute test: {fr_test}\")\n",
    "\n",
    "        df_test_sc = res_xreg[\"df_test\"].loc[res_xreg[\"df_test\"][source_col] ==\n",
    "                                             sc].copy()\n",
    "\n",
    "        df_test_aug_sc = res_xreg[\"df_test_aug\"].loc[\n",
    "            res_xreg[\"df_test_aug\"][source_col] == sc].copy()\n",
    "\n",
    "        logging.info(f\"Test rows: {len(df_test_sc)} | \"\n",
    "                     f\"Aug rows: {len(df_test_aug_sc)}\")\n",
    "\n",
    "        if len(df_test_sc) == 0 or len(df_test_aug_sc) == 0:\n",
    "            logging.warning(f\"Skipping {sc}: no usable test rows.\")\n",
    "            outputs[sc] = {\n",
    "                \"ds_train\": ds_train,\n",
    "                \"ds_test\": None,\n",
    "                \"train_idx\": train_idx,\n",
    "                \"val_idx\": val_idx,\n",
    "                \"note\": f\"No test rows for SOURCE_CODE={sc}\",\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        res_sc = {\n",
    "            \"df_test\": df_test_sc,\n",
    "            \"df_test_aug\": df_test_aug_sc,\n",
    "            \"months_head_pad\": res_xreg[\"months_head_pad\"],\n",
    "            \"months_tail_pad\": res_xreg[\"months_tail_pad\"],\n",
    "        }\n",
    "\n",
    "        key_test = f\"XREG_CH_TO_{sc}\"\n",
    "        logging.info(f\"Cache key (test): {key_test}\")\n",
    "\n",
    "        ds_test = build_or_load_lstm_test_only(\n",
    "            cfg=cfg,\n",
    "            key_test=key_test,\n",
    "            res_test=res_sc,\n",
    "            MONTHLY_COLS=MONTHLY_COLS,\n",
    "            STATIC_COLS=STATIC_COLS,\n",
    "            cache_dir=cache_dir,\n",
    "            force_recompute=fr_test,\n",
    "            normalize_target=normalize_target,\n",
    "            expect_target=expect_target,\n",
    "            strict_nan=strict_nan,\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Test dataset size (sequences): {len(ds_test)}\")\n",
    "\n",
    "        outputs[sc] = {\n",
    "            \"ds_train\": ds_train,\n",
    "            \"ds_test\": ds_test,\n",
    "            \"train_idx\": train_idx,\n",
    "            \"val_idx\": val_idx,\n",
    "            \"cache_keys\": {\n",
    "                \"train\": key_train,\n",
    "                \"test\": key_test,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    logging.info(\"\\nFinished cross-regional LSTM dataset preparation.\")\n",
    "    logging.info(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_xreg = build_or_load_lstm_all_crossregional(\n",
    "    cfg=cfg,\n",
    "    res_xreg=res_xreg,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 1,\n",
    "    'static_hidden': 128,\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-05,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.1,\n",
    "    'loss_spec': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_xreg, infos_xreg = train_crossregional_models_all(\n",
    "    cfg=cfg,\n",
    "    lstm_assets_by_key=outputs_xreg,  # from build_or_load_lstm_all_crossregional\n",
    "    default_params=default_params,\n",
    "    device=device,\n",
    "    train_keys=None,  # train/load all targets\n",
    "    force_retrain=True,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_xreg_CH_to\",\n",
    "    epochs=150,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models_crossregional(\n",
    "        cfg,\n",
    "        models_by_key: dict,\n",
    "        lstm_assets_by_key: dict,\n",
    "        device,\n",
    "        save_dir=None,\n",
    "        grid_shape=(2, 3),\n",
    "        grid_figsize=(20, 12),\n",
    "        ax_xlim=(-16, 9),\n",
    "        ax_ylim=(-16, 9),\n",
    "        title_prefix=\"CH → \",\n",
    "        file_prefix=\"CH_to_\",\n",
    "        order=None,  # e.g. [\"FR\",\"IT\",\"AT\",\"NO\",\"SE\",\"IS\"]\n",
    "):\n",
    "    # ---- collect valid keys ----\n",
    "    valid_keys = []\n",
    "    for k in models_by_key.keys():\n",
    "        m = models_by_key.get(k, None)\n",
    "        a = lstm_assets_by_key.get(k, None)\n",
    "        if m is None:\n",
    "            continue\n",
    "        if a is None or a.get(\"ds_test\", None) is None:\n",
    "            continue\n",
    "        valid_keys.append(k)\n",
    "\n",
    "    valid_set = set(valid_keys)\n",
    "\n",
    "    # ---- apply custom order (if provided) ----\n",
    "    if order is None:\n",
    "        keys = sorted(valid_keys)\n",
    "    else:\n",
    "        # keep only valid keys in the requested order\n",
    "        ordered = [k for k in order if k in valid_set]\n",
    "        # append any remaining valid keys not mentioned in order, deterministically\n",
    "        remainder = sorted(valid_set - set(ordered))\n",
    "        keys = ordered + remainder\n",
    "\n",
    "        missing_in_outputs = [k for k in order if k not in valid_set]\n",
    "        if missing_in_outputs:\n",
    "            logging.warning(\n",
    "                \"Some requested order keys were not evaluated (missing model or ds_test): \"\n",
    "                + \", \".join(missing_in_outputs))\n",
    "\n",
    "    if len(keys) == 0:\n",
    "        raise ValueError(\n",
    "            \"No valid (model, ds_test) pairs found for evaluation.\")\n",
    "\n",
    "    # directory\n",
    "    if save_dir:\n",
    "        save_abs = os.path.join(save_dir)\n",
    "        os.makedirs(save_abs, exist_ok=True)\n",
    "    else:\n",
    "        save_abs = None\n",
    "\n",
    "    nrows, ncols = grid_shape\n",
    "    fig_grid, axes = plt.subplots(nrows,\n",
    "                                  ncols,\n",
    "                                  figsize=grid_figsize,\n",
    "                                  sharex=True,\n",
    "                                  sharey=True)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    n_slots = len(axes)\n",
    "\n",
    "    rows = []\n",
    "    preds_by_key = {}\n",
    "    figs_by_key = {}\n",
    "\n",
    "    for i, key in enumerate(keys):\n",
    "        model = models_by_key[key]\n",
    "        print(f\"\\nEvaluating CH -> {key} ...\")\n",
    "\n",
    "        # --- individual ---\n",
    "        metrics, df_preds, fig_ind, _ax_ind = evaluate_one_model(\n",
    "            cfg=cfg,\n",
    "            model=model,\n",
    "            device=device,\n",
    "            lstm_assets_for_key=lstm_assets_by_key[key],\n",
    "            ax=None,\n",
    "            ax_xlim=ax_xlim,\n",
    "            ax_ylim=ax_ylim,\n",
    "            title=f\"{title_prefix}{key} – Pred vs Truth (Test)\",\n",
    "            legend_fontsize=14,\n",
    "        )\n",
    "\n",
    "        metrics[\"key\"] = key\n",
    "        rows.append(metrics)\n",
    "        preds_by_key[key] = df_preds\n",
    "        figs_by_key[key] = fig_ind\n",
    "\n",
    "        if save_abs:\n",
    "            out_png = os.path.join(save_abs,\n",
    "                                   f\"pred_vs_truth_{file_prefix}{key}.png\")\n",
    "            fig_ind.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "        plt.close(fig_ind)\n",
    "\n",
    "        # --- grid subplot ---\n",
    "        if i < n_slots:\n",
    "            ax_grid = axes[i]\n",
    "            evaluate_one_model(\n",
    "                cfg=cfg,\n",
    "                model=model,\n",
    "                device=device,\n",
    "                lstm_assets_for_key=lstm_assets_by_key[key],\n",
    "                ax=ax_grid,\n",
    "                ax_xlim=ax_xlim,\n",
    "                ax_ylim=ax_ylim,\n",
    "                title=f\"{title_prefix}{key}\",\n",
    "                legend_fontsize=15,\n",
    "            )\n",
    "\n",
    "    # turn off unused axes\n",
    "    for j in range(len(keys), n_slots):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig_grid.suptitle(\"Pred vs Truth (Test) — Cross-regional (train CH)\",\n",
    "                      fontsize=20)\n",
    "    fig_grid.tight_layout()\n",
    "\n",
    "    if save_abs:\n",
    "        out_grid = os.path.join(\n",
    "            save_abs, f\"pred_vs_truth_{file_prefix}ALL_targets_grid.png\")\n",
    "        fig_grid.savefig(out_grid, dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "    df_metrics = pd.DataFrame(rows).set_index(\"key\").sort_index()\n",
    "\n",
    "    return df_metrics, preds_by_key, figs_by_key, fig_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_order = [\"FR\", \"IT_AT\", \"NOR\", \"SJM\", \"ISL\"]  # whatever you want\n",
    "df_metrics_xreg, preds_xreg, figs_xreg, fig_grid_xreg = evaluate_all_models_crossregional(\n",
    "    cfg=cfg,\n",
    "    models_by_key=models_xreg,\n",
    "    lstm_assets_by_key=outputs_xreg,\n",
    "    device=device,\n",
    "    save_dir=\"figures/eval_xreg\",\n",
    "    grid_shape=(2, 3),\n",
    "    order=custom_order,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
