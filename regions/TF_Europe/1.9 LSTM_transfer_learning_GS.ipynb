{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "\n",
    "from regions.TF_Europe.scripts.config_TF_Europe import *\n",
    "from regions.TF_Europe.scripts.dataset import *\n",
    "from regions.TF_Europe.scripts.plotting import *\n",
    "from regions.TF_Europe.scripts.models import *\n",
    "from regions.TF_Europe.scripts.utils import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.EuropeTFConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-regional modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stakes datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examples of loading data:\n",
    "# Load Switzerland only\n",
    "df = load_stakes(cfg, \"CH\")\n",
    "\n",
    "# Load all Central Europe (FR+CH+IT+AT when you add them)\n",
    "df_ceu = load_stakes_for_rgi_region(cfg, \"11\")\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\"\"\"\n",
    "\n",
    "# Load all Europe regions configured\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "dfs[\"11\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly datasets:\n",
    "Build monthly datasets for LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_Europe.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_Europe.nc\")\n",
    "}\n",
    "\n",
    "# Check that all these files exists\n",
    "for key, path in paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Required file for {key} not found at {path}\")\n",
    "\n",
    "vois_climate = [\n",
    "    \"t2m\",\n",
    "    \"tp\",\n",
    "    \"slhf\",\n",
    "    \"sshf\",\n",
    "    \"ssrd\",\n",
    "    \"fal\",\n",
    "    \"str\",\n",
    "]\n",
    "\n",
    "vois_topographical = [\"aspect\", \"slope\", \"svf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all stake dfs\n",
    "dfs = {rid: load_stakes_for_rgi_region(cfg, rid) for rid in RGI_REGIONS.keys()}\n",
    "\n",
    "# prepare monthlies (recompute or load)\n",
    "res_xreg, split_info = prepare_monthly_df_xreg_CH_to_EU(\n",
    "    cfg=cfg,\n",
    "    dfs=dfs,\n",
    "    paths=paths,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    run_flag=False,  # load if already computed\n",
    ")\n",
    "\n",
    "df_train = res_xreg[\"df_train\"]\n",
    "df_test = res_xreg[\"df_test\"]\n",
    "\n",
    "print(\"Train glaciers (CH):\", len(split_info[\"train_glaciers\"]))\n",
    "print(\"Test glaciers (non-CH):\", len(split_info[\"test_glaciers\"]))\n",
    "print(\"Train rows:\", len(df_train), \"Test rows:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic picking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SJM 5%: small-first greedy usually gives best control for a small fraction\n",
    "SJM_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"SJM\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# SJM 50%: large-first or small-first both work; I’d start with large-first\n",
    "SJM_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"SJM\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"\\nSJM_5pct glaciers:\", SJM_5pct)\n",
    "print(\"\\nSJM_50pct glaciers:\", SJM_50pct)\n",
    "\n",
    "# ISL 5%: small-first greedy usually gives best control for a small fraction\n",
    "ISL_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"ISL\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# ISL 50%: large-first or small-first both work; I’d start with large-first\n",
    "ISL_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"ISL\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"\\nISL_5pct glaciers:\", ISL_5pct)\n",
    "print(\"\\nISL_50pct glaciers:\", ISL_50pct)\n",
    "\n",
    "# FR 5%: small-first greedy usually gives best control for a small fraction\n",
    "FR_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"FR\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# FR 50%: large-first or small-first both work; I’d start with large-first\n",
    "FR_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"FR\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"\\nFR_5pct glaciers:\", FR_5pct)\n",
    "print(\"\\nFR_50pct glaciers:\", FR_50pct)\n",
    "\n",
    "# IT_AT 5%: small-first greedy usually gives best control for a small fraction\n",
    "IT_AT_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"IT_AT\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# IT_AT 50%: large-first or small-first both work; I’d start with large-first\n",
    "IT_AT_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"IT_AT\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"\\nIT_AT_5pct glaciers:\", IT_AT_5pct)\n",
    "print(\"\\nIT_AT_50pct glaciers:\", IT_AT_50pct)\n",
    "\n",
    "# NOR 5%: small-first greedy usually gives best control for a small fraction\n",
    "NOR_5pct, sjm5_info, sjm_counts = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"NOR\",\n",
    "    target_frac=0.05,\n",
    "    method=\"greedy_small_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "# NOR 50%: large-first or small-first both work; I’d start with large-first\n",
    "NOR_50pct, sjm50_info, _ = pick_glaciers_by_row_fraction(\n",
    "    df_test=df_test,\n",
    "    region_code=\"NOR\",\n",
    "    target_frac=0.50,\n",
    "    method=\"greedy_large_first\",\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "print(\"\\nNOR_5pct glaciers:\", NOR_5pct)\n",
    "print(\"\\nNOR_50pct glaciers:\", NOR_50pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final ft and hold-out glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norway\n",
    "# 5% split\n",
    "FT_5PCT_NOR = [\n",
    "    'Moesevassbrea', 'Vetlefjordbreen', 'Juvfonne', 'Graasubreen',\n",
    "    'Hellstugubreen', 'Storglombreen N', 'Blabreen', 'Ruklebreen',\n",
    "    'Vestre Memurubreen', 'Cainhavarre', 'Bondhusbrea'\n",
    "]\n",
    "\n",
    "# 50% split\n",
    "FT_50PCT_NOR = [\n",
    "    'Nigardsbreen', 'Aalfotbreen', 'Engabreen', 'Storsteinsfjellbreen',\n",
    "    'Cainhavarre'\n",
    "]\n",
    "\n",
    "# France\n",
    "# 5% split\n",
    "FT_5PCT_FR = ['Grands Montets', 'Sarennes', 'Talefre', 'Leschaux']\n",
    "\n",
    "# 50% split\n",
    "FT_50PCT_FR = ['Argentiere', 'Gebroulaz']\n",
    "\n",
    "# IT-AT\n",
    "FT_5PCT_IT_AT = [\n",
    "    'CIARDONEY', 'CARESER CENTRALE', 'CAMPO SETT.', 'ZETTALUNITZ/MULLWITZ K.',\n",
    "    'HALLSTAETTER G.', 'VENEDIGER K.', 'SURETTA MERIDIONALE', 'GOLDBERG K.',\n",
    "    'CARESER OCCIDENTALE', 'GRAND ETRET', 'LUPO'\n",
    "]\n",
    "\n",
    "FT_50PCT_IT_AT = [\n",
    "    'HINTEREIS F.', 'MALAVALLE (VEDR. DI) / UEBELTALF.',\n",
    "    'LUNGA (VEDRETTA) / LANGENF.', 'RIES OCC. (VEDR. DI) / RIESERF. WESTL.'\n",
    "]\n",
    "\n",
    "# Iceland\n",
    "# 5% split\n",
    "FT_5PCT_ISL = [\n",
    "    \"Tungnaarjoekull\", \"Slettjoekull West\",\n",
    "    \"Hagafellsjoekull East (Langjoekull S Dome)\", \"RGI60-06.00478\",\n",
    "    \"Mulajoekull\"\n",
    "]\n",
    "\n",
    "# 50% split\n",
    "FT_50PCT_ISL = [\n",
    "    'RGI60-06.00238', 'Bruarjoekull', 'Skeidararjoekull',\n",
    "    'Thjorsarjoekull (Hofsjoekull E)', 'Sidujoekull/Skaftarjoekull',\n",
    "    'Hagafellsjoekull West', 'RGI60-06.00305'\n",
    "]\n",
    "\n",
    "# Svalbard\n",
    "# 15% split\n",
    "FT_5PCT_SJM = ['WERENSKIOLDBREEN']\n",
    "\n",
    "# 5% split\n",
    "FT_50PCT_SJM = ['GROENFJORD E', 'WERENSKIOLDBREEN']\n",
    "\n",
    "# Summary of splits for all regions\n",
    "FT_GLACIERS = {\n",
    "    \"FR\": {\n",
    "        \"5pct\": FT_5PCT_FR,\n",
    "        \"50pct\": FT_50PCT_FR\n",
    "    },\n",
    "    \"IT_AT\": {\n",
    "        \"5pct\": FT_5PCT_IT_AT,\n",
    "        \"50pct\": FT_50PCT_IT_AT\n",
    "    },\n",
    "    \"NOR\": {\n",
    "        \"5pct\": FT_5PCT_NOR,\n",
    "        \"50pct\": FT_50PCT_NOR\n",
    "    },\n",
    "    \"ISL\": {\n",
    "        \"5pct\": FT_5PCT_ISL,\n",
    "        \"50pct\": FT_50PCT_ISL\n",
    "    },\n",
    "    \"SJM\": {\n",
    "        \"5pct\": FT_5PCT_SJM,\n",
    "        \"50pct\": FT_50PCT_SJM\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row_check = verify_row_percentage(df_test, FT_GLACIERS)\n",
    "df_row_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for reg in FT_GLACIERS.keys():\n",
    "    gls = sorted(df_test.loc[df_test[\"SOURCE_CODE\"] == reg,\n",
    "                             \"GLACIER\"].unique())\n",
    "    print(reg, \"unique glaciers in df_test:\", len(gls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot test/train glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Central European Alps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_CEU_5pct = FT_GLACIERS[\"FR\"][\"5pct\"] + FT_GLACIERS[\"IT_AT\"][\"5pct\"]\n",
    "FT_GL_CEU_50pct = FT_GLACIERS[\"FR\"][\"50pct\"] + FT_GLACIERS[\"IT_AT\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_CEU_5pct,\n",
    "    \"50pct\": FT_GL_CEU_50pct,\n",
    "}\n",
    "\n",
    "data_CEU, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"11\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_11_CentralEurope/11_rgi60_CentralEurope.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_CEU_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_CEU_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "# remove CH glaciers\n",
    "glacier_df_CEU_5pct = glacier_df_CEU_5pct[~glacier_df_CEU_5pct[\"SOURCE_CODE\"].\n",
    "                                          isin([\"CH\"])]\n",
    "glacier_df_CEU_50pct = glacier_df_CEU_50pct[\n",
    "    ~glacier_df_CEU_50pct[\"SOURCE_CODE\"].isin([\"CH\"])]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_CEU_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier measurement locations Central European Alps (5pct)\",\n",
    "    extent=(5.8, 15, 44, 48),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_CEU_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier measurement locations Central European Alps (50pct)\",\n",
    "    extent=(5.8, 15, 44, 48),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_NOR_5pct = FT_GLACIERS[\"NOR\"][\"5pct\"]\n",
    "FT_GL_NOR_50pct = FT_GLACIERS[\"NOR\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_NOR_5pct,\n",
    "    \"50pct\": FT_GL_NOR_50pct,\n",
    "}\n",
    "\n",
    "data_NOR, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"08\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_08_Scandinavia/08_rgi60_Scandinavia.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_NOR_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_NOR_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_NOR_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Norway (5pct)\",\n",
    "    extent=(4, 24, 57, 71),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\",\n",
    "    legend_ncol=2)\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_NOR_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Norway (50pct)\",\n",
    "    extent=(4, 24, 57, 71),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\",\n",
    "    legend_ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Svalbard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_SJM_5pct = FT_GLACIERS[\"SJM\"][\"5pct\"]\n",
    "FT_GL_SJM_50pct = FT_GLACIERS[\"SJM\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_SJM_5pct,\n",
    "    \"50pct\": FT_GL_SJM_50pct,\n",
    "}\n",
    "\n",
    "data_SJM, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"07\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_07_Svalbard/07_rgi60_Svalbard.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_SJM_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_SJM_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_SJM_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Svalbard (5pct)\",\n",
    "    extent=(5, 30, 76, 80),\n",
    "    sizes=(100, 1000),\n",
    "    size_legend_values=(30, 100, 400),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_SJM_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Svalbard (50pct)\",\n",
    "    extent=(5, 30, 76, 80),\n",
    "    sizes=(100, 1000),\n",
    "    size_legend_values=(30, 100, 400),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iceland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_GL_ISL_5pct = FT_GLACIERS[\"ISL\"][\"5pct\"]\n",
    "FT_GL_ISL_50pct = FT_GLACIERS[\"ISL\"][\"50pct\"]\n",
    "\n",
    "ft_glaciers_by_split = {\n",
    "    \"5pct\": FT_GL_ISL_5pct,\n",
    "    \"50pct\": FT_GL_ISL_50pct,\n",
    "}\n",
    "\n",
    "data_ISL, glacier_outline_rgi, glacier_info_by_split = build_region_glacier_info_for_splits(\n",
    "    cfg,\n",
    "    rgi_region_id=\"06\",\n",
    "    outline_shp_path=cfg.dataPath +\n",
    "    \"RGI_v6/RGI_06_Iceland/06_rgi60_Iceland.shp\",\n",
    "    ft_glaciers_by_split=ft_glaciers_by_split,\n",
    "    split_names=(\"5pct\", \"50pct\"),\n",
    "    ft_label_col=\"FT/Hold-out glacier\",\n",
    ")\n",
    "\n",
    "glacier_df_ISL_5pct = glacier_info_by_split[\"5pct\"]\n",
    "glacier_df_ISL_50pct = glacier_info_by_split[\"50pct\"]\n",
    "\n",
    "cmap_for_train = cm.batlow\n",
    "train_color = \"#1f4e79\"\n",
    "# requires your helper\n",
    "colors = get_cmap_hex(cmap_for_train, 10)  # noqa: F821\n",
    "train_color = colors[0]\n",
    "\n",
    "palette = {\"Hold-out\": train_color, \"FT\": \"#b2182b\"}\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_ISL_5pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Iceland (5pct)\",\n",
    "    extent=(-25, -11, 62, 68),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")\n",
    "\n",
    "fig, ax, glacier_info_plot, scaled_size_fn = plot_glacier_measurements_map(\n",
    "    glacier_info=glacier_df_ISL_50pct,\n",
    "    glacier_outline_rgi=glacier_outline_rgi,\n",
    "    title=\"Glacier PMB location Iceland (50pct)\",\n",
    "    extent=(-25, -11, 62, 68),\n",
    "    sizes=(100, 1500),\n",
    "    size_legend_values=(30, 100, 1000),\n",
    "    palette=palette,\n",
    "    cmap_for_train=cm.batlow,  # optional, uses your get_cmap_hex if available\n",
    "    split_col=\"FT/Hold-out glacier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot feature overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # res_xreg is the ONE dict from your cross-regional monthly prep\n",
    "# figs_by_code = plot_tsne_overlap_xreg_from_single_res(\n",
    "#     res_xreg=res_xreg,\n",
    "#     cfg=cfg,\n",
    "#     STATIC_COLS=STATIC_COLS,\n",
    "#     MONTHLY_COLS=MONTHLY_COLS,\n",
    "#     group_col=\"SOURCE_CODE\",\n",
    "#     ch_code=\"CH\",\n",
    "#     use_aug=False,  # or True if you want *_aug\n",
    "#     n_iter=1000,\n",
    "#     # only_codes=[\"IT_AT\"],  # optional\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = MONTHLY_COLS + STATIC_COLS + [\"POINT_BALANCE\"]\n",
    "\n",
    "# figs_kde = plot_feature_kde_overlap_xreg_ch_vs_codes(\n",
    "#     res_xreg=res_xreg,\n",
    "#     cfg=cfg,\n",
    "#     features=FEATURES,\n",
    "#     group_col=\"SOURCE_CODE\",\n",
    "#     ch_code=\"CH\",\n",
    "#     use_aug=True,  # usually best for feature overlap\n",
    "#     # only_codes=[\"IT_AT\", \"FR\"],    # optional\n",
    "#     output_dir=\"figures/xreg_kde\",  # optional\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model\n",
    "### LSTM datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_assets = build_transfer_learning_assets(\n",
    "    cfg=cfg,\n",
    "    res_xreg=res_xreg,\n",
    "    FT_GLACIERS=FT_GLACIERS,\n",
    "    MONTHLY_COLS=MONTHLY_COLS,\n",
    "    STATIC_COLS=STATIC_COLS,\n",
    "    cache_dir=\"logs/LSTM_cache_TL\",\n",
    "    force_recompute=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "for k, v in tl_assets.items():\n",
    "    print(\"\\n\", \"=\" * 60)\n",
    "    print(\"Experiment:\", k)\n",
    "    print(\"Available keys:\", list(v.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "for exp_key, assets in tl_assets.items():\n",
    "    ft_unique = set(assets[\"ft_source_codes\"])\n",
    "    test_unique = set(\n",
    "        assets[\"test_source_codes\"]) if assets[\"test_source_codes\"] else set()\n",
    "    print(f\"{exp_key} | FT domains: {ft_unique} | TEST domains: {test_unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_gs_results = {\n",
    "    \"ISL\": 'logs/GS_results/lstm_param_search_progress_OOS_ISL_2026-02-11.csv',\n",
    "    \"NOR\": 'logs/GS_results/lstm_param_search_progress_OOS_NOR_2026-02-09.csv',\n",
    "    \"FR\": 'logs/GS_results/lstm_param_search_progress_OOS_FR_2026-02-06.csv',\n",
    "    \"CH\": 'logs/GS_results/lstm_param_search_progress_CH_2026-02-18.csv',\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'Fm': 8,\n",
    "    'Fs': 3,\n",
    "    'hidden_size': 128,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'dropout': 0.0,\n",
    "    'static_layers': 1,\n",
    "    'static_hidden': 128,\n",
    "    'static_dropout': 0.1,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-05,\n",
    "    'loss_name': 'neutral',\n",
    "    'two_heads': False,\n",
    "    'head_dropout': 0.1,\n",
    "    'loss_spec': None\n",
    "}\n",
    "\n",
    "params_by_key = build_lstm_params_by_key(\n",
    "    default_params=default_params,\n",
    "    log_path_gs_results=log_path_gs_results,\n",
    "    RGI_REGIONS=RGI_REGIONS,\n",
    ")\n",
    "\n",
    "params_by_key.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train or load CH baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ch, ch_path, ch_info = train_or_load_CH_baseline(\n",
    "    cfg=cfg,\n",
    "    tl_assets=tl_assets,\n",
    "    default_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_CH\",\n",
    "    key=\"defaultparams\",\n",
    "    train_flag=False,\n",
    "    force_retrain=False,  # set False after you have it once\n",
    "    epochs=150,\n",
    ")\n",
    "print(\"CH baseline saved at:\", ch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_REGIONS = {\"NOR\", \"ISL\", \"SVAL\"}  # adjust codes to match SOURCE_CODE\n",
    "SPLIT = \"5pct\"\n",
    "\n",
    "ADAPTER_GRID = [\n",
    "    {\n",
    "        \"adapter_bottleneck\": 16,\n",
    "        \"lr_adapter\": 3e-5,\n",
    "        \"adapter_dropout\": 0.0\n",
    "    },\n",
    "    {\n",
    "        \"adapter_bottleneck\": 32,\n",
    "        \"lr_adapter\": 1e-4,\n",
    "        \"adapter_dropout\": 0.0\n",
    "    },\n",
    "    {\n",
    "        \"adapter_bottleneck\": 32,\n",
    "        \"lr_adapter\": 1e-4,\n",
    "        \"adapter_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"adapter_bottleneck\": 64,\n",
    "        \"lr_adapter\": 1e-4,\n",
    "        \"adapter_dropout\": 0.1\n",
    "    },\n",
    "]\n",
    "\n",
    "DAN_GRID = [\n",
    "    {\n",
    "        \"dan_alpha\": 0.05,\n",
    "        \"mix_ratio_ft\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"dan_alpha\": 0.10,\n",
    "        \"mix_ratio_ft\": 1.0\n",
    "    },\n",
    "    {\n",
    "        \"dan_alpha\": 0.10,\n",
    "        \"mix_ratio_ft\": 2.0\n",
    "    },\n",
    "    {\n",
    "        \"dan_alpha\": 0.20,\n",
    "        \"mix_ratio_ft\": 2.0\n",
    "    },\n",
    "    {\n",
    "        \"dan_alpha\": 0.20,\n",
    "        \"mix_ratio_ft\": 4.0\n",
    "    },\n",
    "]\n",
    "\n",
    "# Keep grl_lambda=1.0, disc_hidden=128, disc_dropout=0.1 initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tune_special_regions(\n",
    "    cfg,\n",
    "    tl_assets,\n",
    "    base_params,  # your default_params / best_params\n",
    "    device,\n",
    "    ch_path,\n",
    "    regions=(\"NOR\", \"ISL\", \"SVAL\"),\n",
    "    split_name=\"5pct\",\n",
    "    *,\n",
    "    tune_adapter=True,\n",
    "    tune_dan=True,\n",
    "    adapter_grid=None,\n",
    "    dan_grid=None,\n",
    "    # training knobs\n",
    "    adapter_epochs=60,\n",
    "    dan_epochs=60,\n",
    "    force_retrain=True,\n",
    "):\n",
    "    if adapter_grid is None:\n",
    "        adapter_grid = ADAPTER_GRID\n",
    "    if dan_grid is None:\n",
    "        dan_grid = DAN_GRID\n",
    "\n",
    "    results = []\n",
    "    best_by_region = {}\n",
    "\n",
    "    for region in regions:\n",
    "        exp_key = f\"TL_CH_to_{region}_{split_name}\"\n",
    "        if exp_key not in tl_assets:\n",
    "            print(f\"[WARN] Missing assets for {exp_key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        assets = tl_assets[exp_key]\n",
    "        if assets.get(\"ds_finetune\", None) is None:\n",
    "            print(f\"[WARN] No finetune dataset for {exp_key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # --------- ADAPTER tuning ----------\n",
    "        best_adapter = None\n",
    "        if tune_adapter:\n",
    "            for cand in adapter_grid:\n",
    "                params = copy.deepcopy(base_params)\n",
    "                # model-building knobs\n",
    "                params[\"use_adapter\"] = True\n",
    "                params[\"adapter_bottleneck\"] = cand[\"adapter_bottleneck\"]\n",
    "                params[\"adapter_dropout\"] = cand[\"adapter_dropout\"]\n",
    "                # keep adapter_domainwise as you like; if True, set n_domains from vocab\n",
    "                params[\"adapter_domainwise\"] = bool(\n",
    "                    params.get(\"adapter_domainwise\", True))\n",
    "                if params[\"adapter_domainwise\"]:\n",
    "                    dv = assets.get(\"domain_vocab\", None)\n",
    "                    params[\"n_domains\"] = len(dv) if dv is not None else 1\n",
    "\n",
    "                model, path, info = finetune_or_load_one_TL(\n",
    "                    cfg=cfg,\n",
    "                    exp_key=exp_key,\n",
    "                    tl_assets_for_key=assets,\n",
    "                    best_params=params,  # NOTE: pass modified params\n",
    "                    device=device,\n",
    "                    pretrained_ckpt_path=ch_path,\n",
    "                    strategy=\"adapter\",\n",
    "                    force_retrain=force_retrain,\n",
    "                    epochs_safe=adapter_epochs,\n",
    "                    lr_adapter=cand[\"lr_adapter\"],\n",
    "                    models_dir = \"models/GS/\"\n",
    "                )\n",
    "\n",
    "                best_val = info[\"best_val\"] if info else float(\"inf\")\n",
    "                row = {\n",
    "                    \"region\": region,\n",
    "                    \"split\": split_name,\n",
    "                    \"method\": \"adapter\",\n",
    "                    **cand,\n",
    "                    \"best_val\": best_val,\n",
    "                    \"ckpt\": path,\n",
    "                }\n",
    "                results.append(row)\n",
    "\n",
    "                if (best_adapter is None) or (best_val\n",
    "                                              < best_adapter[\"best_val\"]):\n",
    "                    best_adapter = row\n",
    "\n",
    "        # --------- DAN tuning ----------\n",
    "        best_dan = None\n",
    "        if tune_dan:\n",
    "            for cand in dan_grid:\n",
    "                model, path, info = train_dan_one_TL(\n",
    "                    cfg=cfg,\n",
    "                    exp_key=exp_key,\n",
    "                    tl_assets_for_key=assets,\n",
    "                    best_params=base_params,\n",
    "                    device=device,\n",
    "                    pretrained_ckpt_path=ch_path,\n",
    "                    force_retrain=force_retrain,\n",
    "                    epochs=dan_epochs,\n",
    "                    dan_alpha=cand[\"dan_alpha\"],\n",
    "                    mix_ratio_ft=cand[\"mix_ratio_ft\"],\n",
    "                    models_dir = \"models/GS/\"\n",
    "                )\n",
    "\n",
    "                best_val = info[\"best_val\"] if info else float(\"inf\")\n",
    "                row = {\n",
    "                    \"region\": region,\n",
    "                    \"split\": split_name,\n",
    "                    \"method\": \"dan\",\n",
    "                    **cand,\n",
    "                    \"best_val\": best_val,\n",
    "                    \"ckpt\": path,\n",
    "                }\n",
    "                results.append(row)\n",
    "\n",
    "                if (best_dan is None) or (best_val < best_dan[\"best_val\"]):\n",
    "                    best_dan = row\n",
    "\n",
    "        best_by_region[region] = {\n",
    "            \"best_adapter\": best_adapter,\n",
    "            \"best_dan\": best_dan\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values([\"region\", \"method\", \"best_val\"])\n",
    "    return best_by_region, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "best_by_region, df_tuning = tune_special_regions(\n",
    "    cfg=cfg,\n",
    "    tl_assets=tl_assets,\n",
    "    base_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    ch_path=ch_path,\n",
    "    regions=(\"NOR\", \"ISL\", \"SVAL\"),\n",
    "    split_name=\"5pct\",\n",
    "    force_retrain=True,\n",
    ")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = \"results/tuning_adapter\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(save_dir, f\"adapter_tuning_{timestamp}.csv\")\n",
    "df_tuning.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved tuning results to: {csv_path}\")\n",
    "\n",
    "json_path = os.path.join(save_dir, f\"adapter_best_by_region_{timestamp}.json\")\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(best_by_region, f, indent=2)\n",
    "\n",
    "print(f\"Saved best params to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "best_by_region, df_tuning = tune_special_regions(\n",
    "    cfg=cfg,\n",
    "    tl_assets=tl_assets,\n",
    "    base_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    ch_path=ch_path,\n",
    "    regions=[\"SJM\"],\n",
    "    split_name=\"5pct\",\n",
    "    force_retrain=True,\n",
    ")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = \"results/tuning_adapter\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(save_dir, f\"adapter_tuning_{timestamp}.csv\")\n",
    "df_tuning.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved tuning results to: {csv_path}\")\n",
    "\n",
    "json_path = os.path.join(save_dir, f\"adapter_best_by_region_{timestamp}.json\")\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(best_by_region, f, indent=2)\n",
    "\n",
    "print(f\"Saved best params to: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_tl_xreg, infos_tl_xreg = finetune_TL_models_all(\n",
    "    cfg=cfg,\n",
    "    tl_assets_by_key=tl_assets,\n",
    "    best_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    pretrained_ckpt_path=ch_path,\n",
    "    strategies=(\"safe\", \"full\", \"disc_full\", \"adapter\"),\n",
    "    force_retrain=False,\n",
    "    prefix=\"lstm\",\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that domain-wise adapter is activated\n",
    "m = models_tl_xreg[\"TL_CH_to_ISL_5pct__adapter\"]  # pick any adapter run\n",
    "\n",
    "print(\"use_adapter:\", getattr(m, \"use_adapter\", None))\n",
    "print(\"adapter_domainwise:\", getattr(m, \"adapter_domainwise\", None))\n",
    "print(\"has adapters:\", hasattr(m, \"adapters\"))\n",
    "print(\"has single adapter:\", hasattr(m, \"adapter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dan, infos_dan = finetune_TL_models_all(\n",
    "    cfg=cfg,\n",
    "    tl_assets_by_key=tl_assets,\n",
    "    best_params=params_by_key[\"11_CH\"],\n",
    "    device=device,\n",
    "    pretrained_ckpt_path=ch_path,\n",
    "    strategies=[\"dan\"],\n",
    "    force_retrain=False,\n",
    "    prefix=\"lstm\",\n",
    "    verbose=True,\n",
    "    regions_only=[\"ISL\", \"NOR\", \"SJM\"],  # only run DAN on these regions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"FR\", \"IT_AT\", \"NOR\", \"ISL\", \"SJM\"]  # pick any you have models for\n",
    "\n",
    "models_xreg, paths_xreg = load_xreg_models_all(\n",
    "    cfg=cfg,\n",
    "    regions=regions,\n",
    "    best_params=default_params,\n",
    "    device=device,\n",
    "    models_dir=\"models\",\n",
    "    prefix=\"lstm_xreg_CH_to\",\n",
    "    date=None,  # auto-detect latest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 percent split (sparse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    regions=[\"FR\", \"IT_AT\"],\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_tl_xreg,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"5pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\"no_ft\", \"safe\", \"full\", \"disc_full\", \"adapter\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_total = {**models_tl_xreg, **models_dan}\n",
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    regions=[\"NOR\", \"ISL\", \"SJM\"],\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_total,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"5pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\"no_ft\", \"safe\", \"full\", \"adapter\", \"dan\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 percent split (moderate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    regions=[\"FR\", \"IT_AT\"],\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_tl_xreg,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"50pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\"no_ft\", \"safe\", \"full\", \"disc_full\", \"adapter\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_total = {**models_tl_xreg, **models_dan}\n",
    "df_tl_grid, preds_tl_grid, fig_tl_grid = evaluate_transfer_learning_grid(\n",
    "    cfg=cfg,\n",
    "    regions=[\"NOR\", \"ISL\", \"SJM\"],\n",
    "    models_xreg_by_region=models_xreg,  # baseline CH→Region models\n",
    "    models_tl_by_key=models_total,  # TL models keyed by \"exp__strategy\"\n",
    "    tl_assets_by_key=tl_assets,  # TL assets\n",
    "    device=device,\n",
    "    split_name=\"50pct\",  # or \"50pct\"\n",
    "    save_dir=\"figures/eval_TL\",\n",
    "    strategies=[\"no_ft\", \"safe\", \"full\", \"adapter\", \"dan\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
