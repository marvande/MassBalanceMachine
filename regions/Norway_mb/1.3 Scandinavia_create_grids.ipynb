{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glacier grids from RGI:\n",
    "\n",
    "Creates monthly grid files for the MBM to make PMB predictions over the whole glacier grid. The files come from the RGI grid with OGGM topography. Computing takes a long time because of the conversion to monthly format.\n",
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System & utilities ---\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import ast\n",
    "import math\n",
    "import traceback\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Add repo root for MBM imports\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../\"))\n",
    "\n",
    "# --- Data science stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from cmcrameri import cm\n",
    "\n",
    "# --- Machine learning / DL ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# --- Cartography / plotting ---\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "# --- Custom MBM modules ---\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# --- Warnings & autoreload (notebook) ---\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from regions.Switzerland.scripts.geo_data import *\n",
    "from regions.Switzerland.scripts.oggm import *\n",
    "\n",
    "# --- Configuration ---\n",
    "cfg = mbm.NorwayConfig()\n",
    "\n",
    "from regions.Norway_mb.scripts.config_NOR import *\n",
    "\n",
    "# Plot styles:\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "print(\"Using seed:\", cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    mbm.utils.free_up_cuda()\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"08\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "# Save OGGM xr for all needed glaciers in RGI region 11.6:\n",
    "df_missing = export_oggm_grids(cfg, gdirs, rgi_region=\"08\")\n",
    "\n",
    "# load RGI shapefile\n",
    "gdf = gpd.read_file(cfg.dataPath + path_rgi_outlines_NOR)\n",
    "# reproject to a local equal-area projection (example: EPSG:3035 for Europe)\n",
    "gdf_proj = gdf.to_crs(3035)\n",
    "gdf_proj.rename(columns={\"RGIId\": \"rgi_id\"}, inplace=True)\n",
    "# gdf_proj.set_index('rgi_id', inplace=True)\n",
    "gdf_proj[\"area_m2\"] = gdf_proj.geometry.area\n",
    "gdf_proj[\"area_km2\"] = gdf_proj[\"area_m2\"] / 1e6\n",
    "\n",
    "df_missing = df_missing.merge(gdf_proj[['area_km2', 'rgi_id']], on=\"rgi_id\")\n",
    "\n",
    "# total glacier area\n",
    "total_area = gdf_proj[\"area_km2\"].sum()\n",
    "\n",
    "# explode the list of missing vars into rows (one var per row)\n",
    "df_exploded = df_missing.explode(\"missing_vars\")\n",
    "\n",
    "# 1) COUNT: number of glaciers missing each variable\n",
    "counts_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"rgi_id\"].nunique().sort_values(\n",
    "        ascending=False))\n",
    "\n",
    "# 2) TOTAL % AREA with ANY missing var\n",
    "total_missing_area_km2 = df_missing[\"area_km2\"].sum()\n",
    "total_missing_area_pct = (total_missing_area_km2 / total_area) * 100\n",
    "\n",
    "print(f\"Total glacier area with ANY missing variable: \"\n",
    "      f\"{total_missing_area_km2:,.2f} km² \"\n",
    "      f\"({total_missing_area_pct:.2f}%)\")\n",
    "\n",
    "# Optional: also show % area per variable (kept from your earlier logic)\n",
    "area_missing_per_var = (\n",
    "    df_exploded.groupby(\"missing_vars\")[\"area_km2\"].sum().sort_values(\n",
    "        ascending=False))\n",
    "perc_missing_per_var = (area_missing_per_var / total_area) * 100\n",
    "\n",
    "print(\"\\n% of total glacier area missing per variable:\")\n",
    "for var, pct in perc_missing_per_var.items():\n",
    "    print(f\"  - {var}: {pct:.2f}%\")\n",
    "\n",
    "# ---- barplot: number of glaciers missing each variable ----\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(counts_missing_per_var.index, counts_missing_per_var.values)\n",
    "plt.xlabel(\"Missing variable\")\n",
    "plt.ylabel(\"Number of glaciers\")\n",
    "plt.title(\"Count of glaciers missing each variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export geotifs of DEMs (needed for svf in separate notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_RGIs = os.path.join(cfg.dataPath, path_OGGM_NOR, \"xr_grids/\")\n",
    "path_geotiff = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                            \"geotiff/\")\n",
    "\n",
    "glaciers = os.listdir(path_RGIs)\n",
    "print(f\"Found {len(glaciers)} glaciers in RGI region 08\")\n",
    "\n",
    "RUN = False\n",
    "if RUN:\n",
    "    emptyfolder(path_geotiff)\n",
    "\n",
    "    for gdir in tqdm(gdirs):\n",
    "        rgi_gl = gdir.rgi_id\n",
    "\n",
    "        try:\n",
    "            # Export DEMs to GeoTIFF\n",
    "            out_tif = export_glacier_dems_to_geotiff(path_RGIs, rgi_gl,\n",
    "                                                     path_geotiff)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {rgi_gl}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RGI grids for all glaciers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create masked xarray grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def process_one_glacier(\n",
    "    rgi_gl: str,\n",
    "    path_RGIs: str,\n",
    "    path_xr_svf: str,\n",
    "    path_xr_grids: str,\n",
    "    target_res_m: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Worker: load OGGM grid, mask, optional coarsen, reproject to lat/lon,\n",
    "    merge SVF, write per-glacier zarr. Returns a small status tuple.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) Masked OGGM grid in projected coords\n",
    "        ds, _ = create_masked_glacier_grid(path_RGIs, rgi_gl)\n",
    "\n",
    "        # 2) Optional coarsen in projected space\n",
    "        dx_m, dy_m = get_res_from_projected(ds)\n",
    "        if 20 < dx_m < target_res_m:\n",
    "            ds = coarsenDS_mercator(ds, target_res_m=target_res_m)\n",
    "\n",
    "        # 3) Reproject to WGS84 lat/lon\n",
    "        original_proj = ds.pyproj_srs\n",
    "        ds = ds.rio.write_crs(original_proj)\n",
    "        ds_latlon = ds.rio.reproject(\"EPSG:4326\").rename({\n",
    "            \"x\": \"lon\",\n",
    "            \"y\": \"lat\"\n",
    "        })\n",
    "\n",
    "        # 4) Load SVF + merge (if exists)\n",
    "        svf_path = os.path.join(path_xr_svf, f\"{rgi_gl}_svf_latlon.nc\")\n",
    "        if os.path.exists(svf_path):\n",
    "            ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "            # Normalize coord names\n",
    "            if \"x\" in ds_svf.dims or \"y\" in ds_svf.dims:\n",
    "                ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "            if \"longitude\" in ds_svf.dims or \"latitude\" in ds_svf.dims:\n",
    "                ds_svf = ds_svf.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "\n",
    "            # Sort ascending for interp stability\n",
    "            if ds_latlon.lon[0] > ds_latlon.lon[-1]:\n",
    "                ds_latlon = ds_latlon.sortby(\"lon\")\n",
    "            if ds_latlon.lat[0] > ds_latlon.lat[-1]:\n",
    "                ds_latlon = ds_latlon.sortby(\"lat\")\n",
    "            if ds_svf.lon[0] > ds_svf.lon[-1]:\n",
    "                ds_svf = ds_svf.sortby(\"lon\")\n",
    "            if ds_svf.lat[0] > ds_svf.lat[-1]:\n",
    "                ds_svf = ds_svf.sortby(\"lat\")\n",
    "\n",
    "            svf_vars = [\n",
    "                v for v in (\"svf\", \"asvf\", \"opns\") if v in ds_svf.data_vars\n",
    "            ]\n",
    "\n",
    "            if svf_vars:\n",
    "                # Merge directly if grids match; else interpolate\n",
    "                if (np.array_equal(ds_latlon.lon.values, ds_svf.lon.values)\n",
    "                        and np.array_equal(ds_latlon.lat.values,\n",
    "                                           ds_svf.lat.values)):\n",
    "                    ds_latlon = xr.merge([ds_latlon, ds_svf[svf_vars]])\n",
    "                else:\n",
    "                    svf_on_grid = ds_svf[svf_vars].interp(lon=ds_latlon.lon,\n",
    "                                                          lat=ds_latlon.lat,\n",
    "                                                          method=\"linear\")\n",
    "                    for v in svf_vars:\n",
    "                        svf_on_grid[v] = svf_on_grid[v].astype(\"float32\")\n",
    "                    ds_latlon = ds_latlon.assign(\n",
    "                        **{v: svf_on_grid[v]\n",
    "                           for v in svf_vars})\n",
    "\n",
    "                # Masked SVF versions using glacier_mask (if present)\n",
    "                if \"glacier_mask\" in ds_latlon:\n",
    "                    gmask = xr.where(ds_latlon[\"glacier_mask\"] == 1, 1.0,\n",
    "                                     np.nan)\n",
    "                    for v in svf_vars:\n",
    "                        ds_latlon[f\"masked_{v}\"] = gmask * ds_latlon[v]\n",
    "\n",
    "        # 5) Save final lat/lon grid\n",
    "        os.makedirs(path_xr_grids, exist_ok=True)\n",
    "        save_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        ds_latlon.to_zarr(save_path, mode=\"w\")\n",
    "\n",
    "        return (rgi_gl, \"ok\", \"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (rgi_gl, \"error\", f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def run_parallel_processing(\n",
    "    gdirs,\n",
    "    path_RGIs,\n",
    "    path_xr_svf,\n",
    "    path_xr_grids,\n",
    "    n_workers=None,\n",
    "    clear_out=False,\n",
    "    target_res_m=50,\n",
    "):\n",
    "    rgi_ids = [g.rgi_id for g in gdirs]\n",
    "\n",
    "    if clear_out:\n",
    "        emptyfolder(path_xr_grids)\n",
    "    else:\n",
    "        os.makedirs(path_xr_grids, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "        futures = {\n",
    "            ex.submit(\n",
    "                process_one_glacier,\n",
    "                rgi_id,\n",
    "                path_RGIs,\n",
    "                path_xr_svf,\n",
    "                path_xr_grids,\n",
    "                target_res_m,\n",
    "            ):\n",
    "            rgi_id\n",
    "            for rgi_id in rgi_ids\n",
    "        }\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(fut.result())\n",
    "\n",
    "    # quick summary\n",
    "    n_ok = sum(r[1] == \"ok\" for r in results)\n",
    "    n_err = sum(r[1] == \"error\" for r in results)\n",
    "    print(f\"Done. ok={n_ok}, error={n_err}\")\n",
    "\n",
    "    if n_err:\n",
    "        for rgi_id, status, msg in results:\n",
    "            if status == \"error\":\n",
    "                print(f\"[{rgi_id}] {msg}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xr_grids = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                             \"xr_masked_grids/\")\n",
    "path_xr_svf = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                           \"svf_nc_latlon/\")\n",
    "\n",
    "RUN = True\n",
    "if RUN:\n",
    "    results = run_parallel_processing(\n",
    "        gdirs=gdirs,\n",
    "        path_RGIs=path_RGIs,\n",
    "        path_xr_svf=path_xr_svf,\n",
    "        path_xr_grids=path_xr_grids,\n",
    "        n_workers=6,  # start modest (4–8 is usually good)\n",
    "        clear_out=True,  # or False if you want to keep existing zarrs\n",
    "        target_res_m=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id = \"RGI60-08.01126\"\n",
    "# --- Paths ---\n",
    "dem_path = os.path.join(path_geotiff, f\"{rgi_id}.tif\")\n",
    "zarr_path = os.path.join(path_xr_grids, f\"{rgi_id}.zarr\")\n",
    "svf_path = os.path.join(path_xr_svf, f\"{rgi_id}_svf_latlon.nc\")\n",
    "\n",
    "# --- Load data ---\n",
    "dem = rioxarray.open_rasterio(dem_path).squeeze()\n",
    "ds = xr.open_zarr(zarr_path)\n",
    "ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "# Handle coord naming for SVF\n",
    "if \"lon\" not in ds_svf.coords:\n",
    "    ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "\n",
    "# --- Figure layout ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# 1️⃣ DEM (projected)\n",
    "dem.plot(ax=axes[0], cmap=\"terrain\")\n",
    "axes[0].set_title(\"DEM (projected meters)\")\n",
    "axes[0].set_xlabel(\"Easting [m]\")\n",
    "axes[0].set_ylabel(\"Northing [m]\")\n",
    "\n",
    "# 2️⃣ Masked aspect (projected OGGM grid)\n",
    "ds[\"masked_aspect\"].plot(ax=axes[1])\n",
    "axes[1].set_title(\"Masked Aspect (°)\")\n",
    "axes[1].set_xlabel(\"Longitude (°)\")\n",
    "axes[1].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "# 3️⃣ SVF (lat/lon)\n",
    "ds[\"svf\"].plot(ax=axes[2])\n",
    "\n",
    "axes[2].set_title(\"Sky View Factor (lat/lon)\")\n",
    "axes[2].set_xlabel(\"Longitude (°)\")\n",
    "axes[2].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "plt.suptitle(f\"{rgi_id}\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgi_id = \"RGI60-08.01258\"\n",
    "\n",
    "# --- Paths ---\n",
    "dem_path = os.path.join(path_geotiff, f\"{rgi_id}.tif\")\n",
    "zarr_path = os.path.join(path_xr_grids, f\"{rgi_id}.zarr\")\n",
    "svf_path = os.path.join(path_xr_svf, f\"{rgi_id}_svf_latlon.nc\")\n",
    "\n",
    "# --- Load data ---\n",
    "dem = rioxarray.open_rasterio(dem_path).squeeze()\n",
    "ds = xr.open_zarr(zarr_path)\n",
    "ds_svf = xr.open_dataset(svf_path)\n",
    "\n",
    "# Handle coord naming for SVF\n",
    "if \"lon\" not in ds_svf.coords:\n",
    "    ds_svf = ds_svf.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "\n",
    "# --- Figure layout ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n",
    "\n",
    "# 1️⃣ DEM (projected)\n",
    "dem.plot(ax=axes[0], cmap=\"terrain\")\n",
    "axes[0].set_title(\"DEM (projected meters)\")\n",
    "axes[0].set_xlabel(\"Easting [m]\")\n",
    "axes[0].set_ylabel(\"Northing [m]\")\n",
    "\n",
    "# 2️⃣ Masked aspect (projected OGGM grid)\n",
    "ds[\"masked_aspect\"].plot(ax=axes[1])\n",
    "axes[1].set_title(\"Masked Aspect (°)\")\n",
    "axes[1].set_xlabel(\"Longitude (°)\")\n",
    "axes[1].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "# 3️⃣ SVF (lat/lon)\n",
    "ds[\"svf\"].plot(ax=axes[2])\n",
    "\n",
    "axes[2].set_title(\"Sky View Factor (lat/lon)\")\n",
    "axes[2].set_xlabel(\"Longitude (°)\")\n",
    "axes[2].set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "plt.suptitle(f\"{rgi_id}\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate columns\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "# Topographical columns\n",
    "vois_topographical = [\n",
    "    \"aspect\", \"slope\", \"hugonnet_dhdt\", \"consensus_ice_thickness\", \"millan_v\",\n",
    "    \"topo\", \"svf\"\n",
    "]\n",
    "\n",
    "RUN = True\n",
    "path_rgi_alps = os.path.join(cfg.dataPath,\n",
    "                             'GLAMOS/topo/gridded_topo_inputs/RGI_v6_11_svf/')\n",
    "# #emptyfolder(path_rgi_alps)\n",
    "\n",
    "# Avoid BLAS/OpenMP oversubscription inside each worker\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# ------------ config ------------\n",
    "years = range(2000, 2024)  # inclusive\n",
    "#max_workers = max(1, min(os.cpu_count() or 4, 8))  # be gentle with I/O\n",
    "#max_workers = min(os.cpu_count(), 16)   # or 20 if SSD & RAM are strong\n",
    "max_workers = min(os.cpu_count(), 12)  # or 20 if SSD & RAM are strong\n",
    "\n",
    "\n",
    "# ------------ helpers (unchanged) ------------\n",
    "def expected_fname(rgi_gl: str, year: int) -> str:\n",
    "    return f\"{rgi_gl}_grid_{year}.parquet\"\n",
    "\n",
    "\n",
    "def years_present_for_glacier(folder_path: str, rgi_gl: str) -> set:\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return set()\n",
    "    rx = re.compile(rf\"^{re.escape(rgi_gl)}_grid_(\\d{{4}})\\.parquet$\")\n",
    "    years_found = set()\n",
    "    for f in os.listdir(folder_path):\n",
    "        m = rx.match(f)\n",
    "        if m:\n",
    "            years_found.add(int(m.group(1)))\n",
    "    return years_found\n",
    "\n",
    "\n",
    "def glacier_is_complete(rgi_gl: str, years: range) -> bool:\n",
    "    folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "    found = years_present_for_glacier(folder_path, rgi_gl)\n",
    "    return set(years).issubset(found)\n",
    "\n",
    "\n",
    "# ------------ per-glacier worker ------------\n",
    "def process_one_glacier(rgi_gl: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (rgi_gl, 'ok') or (rgi_gl, 'skip:<reason>'/'error:<message>')\n",
    "    Runs in a separate process: DO NOT capture big globals except paths & configs safely.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input files\n",
    "        file_path = os.path.join(path_xr_grids, f\"{rgi_gl}.zarr\")\n",
    "        if not os.path.exists(file_path):\n",
    "            return (rgi_gl, f\"skip:missing_zarr {file_path}\")\n",
    "\n",
    "        # Skip if already fully complete\n",
    "        folder_path = os.path.join(path_rgi_alps, rgi_gl)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        existing_years = years_present_for_glacier(folder_path, rgi_gl)\n",
    "        missing_years = [y for y in years if y not in existing_years]\n",
    "        if not missing_years:\n",
    "            return (rgi_gl, \"skip:complete\")\n",
    "\n",
    "        # Open Zarr *inside* the worker\n",
    "        try:\n",
    "            ds = xr.open_zarr(file_path, consolidated=True)\n",
    "        except Exception:\n",
    "            ds = xr.open_zarr(file_path)\n",
    "\n",
    "        # Build grid (once) for all years\n",
    "        try:\n",
    "            df_grid = create_glacier_grid_RGI(ds, years, rgi_gl)\n",
    "        except Exception as e:\n",
    "            return (rgi_gl, f\"error:create_grid {e}\")\n",
    "\n",
    "        df_grid = df_grid.reset_index(drop=True)\n",
    "\n",
    "        # GLWD_ID & GLACIER\n",
    "        df_grid['GLWD_ID'] = [\n",
    "            mbm.data_processing.utils.get_hash(f\"{r}_{y}\") for r, y in zip(\n",
    "                df_grid['RGIId'].astype(str), df_grid['YEAR'].astype(str))\n",
    "        ]\n",
    "        df_grid['GLWD_ID'] = df_grid['GLWD_ID'].astype(str)\n",
    "        df_grid['GLACIER'] = df_grid['RGIId']\n",
    "\n",
    "        # Process only the missing years\n",
    "        for year in missing_years:\n",
    "            try:\n",
    "                df_grid_y = df_grid[df_grid.YEAR == year].copy()\n",
    "                if df_grid_y.empty:\n",
    "                    continue\n",
    "\n",
    "                # Build dataset & add climate features\n",
    "                dataset_grid_yearly = mbm.data_processing.Dataset(\n",
    "                    cfg=cfg,\n",
    "                    data=df_grid_y,\n",
    "                    region_name='CH',\n",
    "                    region_id=11,\n",
    "                    data_path=os.path.join(cfg.dataPath, path_PMB_GLAMOS_csv))\n",
    "\n",
    "                era5_climate_data = os.path.join(\n",
    "                    cfg.dataPath, path_ERA5_raw,\n",
    "                    'era5_monthly_averaged_data_Alps.nc')\n",
    "                geopotential_data = os.path.join(\n",
    "                    cfg.dataPath, path_ERA5_raw,\n",
    "                    'era5_geopotential_pressure_Alps.nc')\n",
    "\n",
    "                dataset_grid_yearly.get_climate_features(\n",
    "                    climate_data=era5_climate_data,\n",
    "                    geopotential_data=geopotential_data,\n",
    "                    change_units=True,\n",
    "                    smoothing_vois={\n",
    "                        'vois_climate': vois_climate,\n",
    "                        'vois_other': ['ALTITUDE_CLIMATE']\n",
    "                    })\n",
    "\n",
    "                vois_topographical_sub = [\n",
    "                    v for v in vois_topographical if v in df_grid_y.columns\n",
    "                ]\n",
    "\n",
    "                dataset_grid_yearly.convert_to_monthly(\n",
    "                    meta_data_columns=cfg.metaData,\n",
    "                    vois_climate=vois_climate,\n",
    "                    vois_topographical=vois_topographical_sub)\n",
    "\n",
    "                save_path = os.path.join(folder_path,\n",
    "                                         expected_fname(rgi_gl, year))\n",
    "                dataset_grid_yearly.data.to_parquet(save_path,\n",
    "                                                    engine=\"pyarrow\",\n",
    "                                                    compression=\"snappy\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # keep going with other years but record error\n",
    "                return (rgi_gl, f\"error:year_{year} {e}\")\n",
    "\n",
    "        return (rgi_gl, \"ok\")\n",
    "\n",
    "    except Exception as e:\n",
    "        return (rgi_gl, f\"error:{e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "\n",
    "# ------------ main parallel driver ------------\n",
    "if RUN:\n",
    "    os.makedirs(path_rgi_alps, exist_ok=True)\n",
    "    #emptyfolder(path_rgi_alps)\n",
    "\n",
    "    valid_rgis = [\n",
    "        f.replace('.zarr', '') for f in os.listdir(path_xr_grids)\n",
    "        if f.endswith('.zarr')\n",
    "    ]\n",
    "\n",
    "    # Filter to those not fully complete\n",
    "    targets = [r for r in valid_rgis if not glacier_is_complete(r, years)]\n",
    "    print(\n",
    "        f\"Total valid glaciers: {len(valid_rgis)} | Remaining to process: {len(targets)}\"\n",
    "    )\n",
    "\n",
    "    results = {\"ok\": [], \"skip\": [], \"error\": []}\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(process_one_glacier, rgi): rgi for rgi in targets}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=\"Glaciers\"):\n",
    "            rgi = futs[fut]\n",
    "            try:\n",
    "                rid, status = fut.result()\n",
    "            except Exception as e:\n",
    "                rid, status = rgi, f\"error:{e}\"\n",
    "            if status.startswith(\"ok\"):\n",
    "                results[\"ok\"].append(rgi)\n",
    "            elif status.startswith(\"skip\"):\n",
    "                results[\"skip\"].append((rgi, status))\n",
    "            else:\n",
    "                results[\"error\"].append((rgi, status))\n",
    "\n",
    "    print(\n",
    "        f\"\\nFinished. ok={len(results['ok'])}, skip={len(results['skip'])}, error={len(results['error'])}\"\n",
    "    )\n",
    "    if results[\"error\"]:\n",
    "        for rgi, msg in results[\"error\"][:10]:\n",
    "            print(\"  \", rgi, \"→\", msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.00001':\n",
    "        gdir_nigardsbreen = gdir\n",
    "\n",
    "rgi_gl = gdir_nigardsbreen.rgi_id\n",
    "\n",
    "year = 2000\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())\n",
    "\n",
    "year = 2004\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "print(df['t2m'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one example\n",
    "for gdir in gdirs:\n",
    "    if gdir.rgi_id == 'RGI60-11.01238':\n",
    "        gdir_nigardsbreen = gdir\n",
    "\n",
    "year = 2000\n",
    "rgi_gl = gdir_nigardsbreen.rgi_id\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    os.path.join(path_rgi_alps, rgi_gl, f\"{rgi_gl}_grid_{year}.parquet\"))\n",
    "df = df[df.MONTHS == 'sep']\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "voi = [\n",
    "    't2m', 'tp', 'ALTITUDE_CLIMATE', 'ELEVATION_DIFFERENCE', 'hugonnet_dhdt',\n",
    "    'consensus_ice_thickness'\n",
    "]\n",
    "axs = axs.flatten()\n",
    "for i, var in enumerate(voi):\n",
    "    sns.scatterplot(df,\n",
    "                    x='POINT_LON',\n",
    "                    y='POINT_LAT',\n",
    "                    hue=var,\n",
    "                    s=5,\n",
    "                    alpha=0.5,\n",
    "                    palette='twilight_shifted',\n",
    "                    ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
