{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.norway_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_NOR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Norway/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. CH Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "data_NOR = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'Nor_dataset_all_oggm_with_millanv.csv')\n",
    "\n",
    "# Drop Nan entries in millan_v of Norway dataset\n",
    "data_NOR = data_NOR.dropna(subset=data_NOR.columns.drop('DATA_MODIFICATION'))\n",
    "display(data_NOR)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "display(data_NOR.columns)\n",
    "\n",
    "data_CH = data_CH.drop(['aspect_sgi', 'slope_sgi', 'topo_sgi'], axis=1)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "# Merge CH with NOR\n",
    "data_NOR_CH = pd.concat([data_NOR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(data_NOR_CH.head(2))\n",
    "\n",
    "display(len(data_NOR_CH['GLACIER'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH_NOR_test = data_NOR_CH.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_NOR_CH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_NOR_CH.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_CH_NOR_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'CH_NOR_wgms_dataset_monthly_full_with_millanv_v2.csv')\n",
    "data_monthly_CH_NOR = dataloader_gl.data\n",
    "\n",
    "# Add DOMAIN column back using RGI ID patterns\n",
    "def assign_domain(rgi_id):\n",
    "    if rgi_id.startswith('RGI60-11'):\n",
    "        return 0  # Switzerland\n",
    "    elif rgi_id.startswith('RGI60-08'):\n",
    "        return 1  # Norway\n",
    "    else:\n",
    "        return -1  # Unknown\n",
    "\n",
    "data_monthly_CH_NOR['DOMAIN'] = data_monthly_CH_NOR['RGIId'].apply(assign_domain)\n",
    "\n",
    "# Verify domain assignment\n",
    "print(\"Domain distribution after assignment:\")\n",
    "print(data_monthly_CH_NOR['DOMAIN'].value_counts())\n",
    "print(f\"Unknown domains (-1): {(data_monthly_CH_NOR['DOMAIN'] == -1).sum()}\")\n",
    "\n",
    "# Update the dataloader with the new data including DOMAIN\n",
    "dataloader_gl.data = data_monthly_CH_NOR\n",
    "\n",
    "display(data_monthly_CH_NOR.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DOMAIN assignment by checking some specific glaciers\n",
    "print(\"Sample of DOMAIN assignments:\")\n",
    "sample_glaciers = data_monthly_CH_NOR[['GLACIER', 'RGIId', 'DOMAIN']].drop_duplicates().head(10)\n",
    "display(sample_glaciers)\n",
    "\n",
    "# Check if we have both domains represented\n",
    "ch_glaciers = data_monthly_CH_NOR[data_monthly_CH_NOR['DOMAIN'] == 0]['GLACIER'].nunique()\n",
    "nor_glaciers = data_monthly_CH_NOR[data_monthly_CH_NOR['DOMAIN'] == 1]['GLACIER'].nunique()\n",
    "print(f\"\\nNumber of Swiss glaciers (DOMAIN=0): {ch_glaciers}\")\n",
    "print(f\"Number of Norwegian glaciers (DOMAIN=1): {nor_glaciers}\")\n",
    "\n",
    "# Verify some known glaciers\n",
    "if 'Engabreen' in data_monthly_CH_NOR['GLACIER'].values:\n",
    "    engabreen_domain = data_monthly_CH_NOR[data_monthly_CH_NOR['GLACIER'] == 'Engabreen']['DOMAIN'].iloc[0]\n",
    "    print(f\"Engabreen (should be 1 for Norway): {engabreen_domain}\")\n",
    "\n",
    "# Check for any Swiss glacier examples\n",
    "swiss_example = data_monthly_CH_NOR[data_monthly_CH_NOR['DOMAIN'] == 0]['GLACIER'].unique()[:3]\n",
    "if len(swiss_example) > 0:\n",
    "    print(f\"Example Swiss glaciers: {swiss_example}\")\n",
    "    \n",
    "print(\"\\nDOMAIN column successfully added to monthly dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'DOMAIN' not in cfg.metaData:\n",
    "    cfg.metaData = cfg.metaData + ['DOMAIN']\n",
    "print('Metadata columns now:', cfg.metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate source and target\n",
    "data_source_full = data_monthly_CH_NOR.query('DOMAIN == 0').copy()\n",
    "data_target_full = data_monthly_CH_NOR.query('DOMAIN == 1').copy()\n",
    "print(f\"Source (CH) rows: {len(data_source_full)} | Target (NOR) rows: {len(data_target_full)}\")\n",
    "\n",
    "# Build a temporary dataloader with only CH for reproducible val split\n",
    "_ch_only_loader = mbm.dataloader.DataLoader(cfg, data=data_source_full.copy())\n",
    "ch_train_itr, ch_val_itr = _ch_only_loader.set_train_test_split(test_size=0.2)\n",
    "ch_train_idx, ch_val_idx = list(ch_train_itr), list(ch_val_itr)\n",
    "ch_train_df = data_source_full.iloc[ch_train_idx]\n",
    "ch_val_df = data_source_full.iloc[ch_val_idx]\n",
    "print(f\"CH split -> train IDs: {len(ch_train_df.ID.unique())} | val IDs: {len(ch_val_df.ID.unique())}\")\n",
    "\n",
    "# Create unlabeled target adaptation copy (labels hidden)\n",
    "adapt_target_df = data_target_full.copy()\n",
    "adapt_target_df['POINT_BALANCE'] = np.nan\n",
    "\n",
    "# Pools\n",
    "train_pool = pd.concat([ch_train_df, adapt_target_df], axis=0).reset_index(drop=True)\n",
    "val_pool = ch_val_df.reset_index(drop=True)\n",
    "eval_pool_target = data_target_full.reset_index(drop=True)  # full NOR with labels for post-training eval\n",
    "\n",
    "# Assign canonical variables\n",
    "df_X_train = train_pool; y_train = df_X_train['POINT_BALANCE'].values\n",
    "df_X_val = val_pool; y_val = df_X_val['POINT_BALANCE'].values\n",
    "df_X_eval_target = eval_pool_target \n",
    "y_eval_target = df_X_eval_target['POINT_BALANCE'].values\n",
    "\n",
    "print('\\DANN pools created:')\n",
    "print('  Train total:', len(df_X_train), '| labeled CH rows:', np.isfinite(y_train).sum(), '| unlabeled NOR rows:', np.isnan(y_train).sum())\n",
    "print('  Val (CH only):', len(df_X_val), '| labeled rows:', np.isfinite(y_val).sum())\n",
    "print('  Eval NOR (with labels):', len(df_X_eval_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = ['ELEVATION_DIFFERENCE'] + list(vois_topographical)\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "cfg.setFeatures(feature_columns)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Subset train/val (target eval later)\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "\n",
    "print('Shape train:', df_X_train_subset.shape)\n",
    "print('Shape val:', df_X_val_subset.shape)\n",
    "print('Running with features:', feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN components: Gradient Reversal, network, regressor wrapper, and dataset bindings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skorch.utils import to_tensor\n",
    "import massbalancemachine as mbm\n",
    "from pathlib import Path\n",
    "\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class GradReverse(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class DANNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout=0.2, use_batchnorm=False, domain_hidden=64, grl_lambda=1.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(hidden_layers)\n",
    "        for hidden_dim, drop_rate in zip(hidden_layers, dropout):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            current_dim = hidden_dim\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.regressor = nn.Linear(current_dim, 1)\n",
    "        self.grl = GradReverse(lambda_=grl_lambda)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(current_dim, domain_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout[-1] if isinstance(dropout, list) else dropout),\n",
    "            nn.Linear(domain_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)\n",
    "        y_pred = self.regressor(h)\n",
    "        d_logits = self.domain_classifier(self.grl(h))\n",
    "        return y_pred, d_logits\n",
    "\n",
    "# Dataset that yields domain labels padded per ID to match monthly padding\n",
    "class DomainTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, aggregated_dataset):\n",
    "        self.base = aggregated_dataset\n",
    "        self.meta_has_domain = 'DOMAIN' in self.base.metadataColumns\n",
    "        if not self.meta_has_domain:\n",
    "            # fallback: try to read from features\n",
    "            assert 'DOMAIN' in self.base.cfg.featureColumns, \"DOMAIN required\"\n",
    "            self.domain_feat_idx = self.base.cfg.featureColumns.index('DOMAIN')\n",
    "        else:\n",
    "            self.domain_idx = self.base.metadataColumns.index('DOMAIN')\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.base._getInd(index)\n",
    "        if self.meta_has_domain:\n",
    "            dval = self.base.metadata[ind[0]][self.domain_idx]\n",
    "        else:\n",
    "            dval = self.base.features[ind[0], self.domain_feat_idx]\n",
    "        dpad = np.empty(self.base.maxConcatNb, dtype=np.float32)\n",
    "        dpad.fill(np.nan)\n",
    "        dpad[:len(ind)] = dval\n",
    "        return dpad\n",
    "\n",
    "# Binding that returns (X, (y, d)) so y_true in get_loss can contain both\n",
    "class CombinedTargetBinding(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_slice, y_slice, d_dataset):\n",
    "        self.X = X_slice\n",
    "        self.y = y_slice\n",
    "        self.d = d_dataset\n",
    "        assert len(self.X) == len(self.y) == len(self.d)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], (self.y[idx], self.d[idx])\n",
    "    \n",
    "# Skorch regressor that adds domain-adversarial loss on top of the SMB loss\n",
    "class CustomDANNRegressor(mbm.models.CustomNeuralNetRegressor):\n",
    "    def __init__(self, cfg, *args, dan_lambda=0.1, **kwargs):\n",
    "        super().__init__(cfg, *args, **kwargs)\n",
    "        self.dan_lambda = dan_lambda\n",
    "        self._last_domain_logits = None\n",
    "\n",
    "    def infer(self, x, **fit_params):\n",
    "        x = to_tensor(x, device=self.device)\n",
    "        if len(x.shape) == 1: x = x[None]\n",
    "        x, indNonNan = self._unpack_inp(x)\n",
    "        if self.modelDtype is not None: x = x.type(self.modelDtype)\n",
    "        outputs = self.module_(x, **fit_params)\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_monthly, d_monthly = outputs\n",
    "            y_packed = self._pack_out(y_monthly, indNonNan)\n",
    "            d_packed = self._pack_out(d_monthly, indNonNan)\n",
    "            self._last_domain_logits = d_packed\n",
    "            return y_packed\n",
    "        return self._pack_out(outputs, indNonNan)\n",
    "    \n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        if isinstance(y_true, (tuple, list)) and len(y_true) == 2:\n",
    "            y_true_labels, y_true_domain = y_true\n",
    "        else:\n",
    "            y_true_labels, y_true_domain = y_true, None\n",
    "\n",
    "        # Label loss (same as base implementation)  \n",
    "        loss = 0.0; cnt = 0\n",
    "        for yi_pred, yi_true in zip(y_pred, y_true_labels):\n",
    "            valid = ~torch.isnan(yi_true) # Changed from yi_pred to yi_true to not only maks padded month but target domain NaN model predictions\n",
    "            if valid.any():\n",
    "                pred_sum = yi_pred[valid].sum()\n",
    "                true_mean = yi_true[valid].mean()\n",
    "                loss += (pred_sum - true_mean) ** 2\n",
    "                cnt += 1\n",
    "        label_loss = loss / max(cnt, 1)\n",
    "\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean for each domain\n",
    "        # otherwise IDs with longer months have higher domain loss and CH domain loss with more data is exaggerated\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits\n",
    "            per_id_losses_ch, per_id_losses_nor = [], []    # track losses separately, so we can average them individually later\n",
    "\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                if d_log_row.ndim > 1: \n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                mask = ~torch.isnan(d_true_row) # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    # domain for this ID (same across valid months)\n",
    "                    dom_i = int(d_true_row[mask][0].item())\n",
    "                    (per_id_losses_ch if dom_i == 0 else per_id_losses_nor).append(loss_i)\n",
    "            parts = []\n",
    "            if len(per_id_losses_ch) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_ch).mean())\n",
    "            if len(per_id_losses_nor) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_nor).mean())\n",
    "            if len(parts) > 0:\n",
    "                domain_loss = torch.stack(parts).mean()\n",
    "\n",
    "        return label_loss + self.dan_lambda * domain_loss\n",
    "    @staticmethod\n",
    "    def load_model(cfg, fname: str, *args, **kwargs):\n",
    "        model = CustomDANNRegressor(cfg, *args, **kwargs)\n",
    "        model.initialize(); models_dir = Path('./models')\n",
    "        model.load_params(f_params=models_dir / fname)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current Norway feature order\n",
    "print(\"Current Norway feature order:\")\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(f\"{i}: {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "    'module__domain_hidden': 64,\n",
    "    'module__grl_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# Use DANN network\n",
    "args = {\n",
    "    'module': DANNNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'module__domain_hidden': params['module__domain_hidden'],\n",
    "    'module__grl_lambda': params['module__grl_lambda'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# DANN loss weight\n",
    "DAN_LAMBDA = 0.1\n",
    "custom_nn = CustomDANNRegressor(cfg, dan_lambda=DAN_LAMBDA, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the aggregated datasets\n",
    "agg_train = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                  features=features,\n",
    "                                                  metadata=metadata,\n",
    "                                                  targets=y_train)\n",
    "agg_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features_val,\n",
    "                                                metadata=metadata_val,\n",
    "                                                targets=y_val)\n",
    "\n",
    "# Build domain targets per ID\n",
    "domain_train_ds = DomainTargetDataset(agg_train)\n",
    "domain_val_ds = DomainTargetDataset(agg_val)\n",
    "\n",
    "# Slice features/labels\n",
    "X_train_slice = SliceDataset(agg_train, idx=0)\n",
    "y_train_slice = SliceDataset(agg_train, idx=1)\n",
    "X_val_slice = SliceDataset(agg_val, idx=0)\n",
    "y_val_slice = SliceDataset(agg_val, idx=1)\n",
    "\n",
    "# Bind (X, (y, d)) so CustomDANNRegressor receives both labels and domains\n",
    "dataset = CombinedTargetBinding(X_train_slice, y_train_slice, domain_train_ds)\n",
    "dataset_val = CombinedTargetBinding(X_val_slice, y_val_slice, domain_val_ds)\n",
    "\n",
    "# For info\n",
    "print(\"train:\", len(dataset))\n",
    "print(\"validation:\", len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-balanced sampling (~50/50 CH:ICE) using WeightedRandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Map each training aggregated item (ID) to its DOMAIN\n",
    "# 1) get the ID per aggregated sample in the training SliceDataset\n",
    "train_ids = X_train_slice.dataset.indexToId(list(range(len(X_train_slice))))\n",
    "\n",
    "# 2) build an ID -> DOMAIN dict from the training dataframe\n",
    "id_to_domain = (\n",
    "    df_X_train_subset.groupby('ID')['DOMAIN'].first().to_dict()\n",
    ")\n",
    "\n",
    "# 3) lookup domains per training aggregated item\n",
    "sample_domains = np.array([id_to_domain.get(i, -1) for i in train_ids], dtype=int)\n",
    "\n",
    "# Safety: ensure only CH(0) and NOR(1) are present; any other values get zero weight\n",
    "n_ch = int((sample_domains == 0).sum())\n",
    "n_nor = int((sample_domains == 1).sum())\n",
    "if (n_ch + n_nor) == 0:\n",
    "    raise ValueError(\"No CH/NOR samples found in training set for sampler.\")\n",
    "\n",
    "# Compute weights so the expected sampling is 50% CH and 50% NOR per epoch\n",
    "w = np.zeros_like(sample_domains, dtype=float)\n",
    "if n_ch > 0:\n",
    "    w[sample_domains == 0] = 0.5 / n_ch\n",
    "if n_nor > 0:\n",
    "    w[sample_domains == 1] = 0.5 / n_nor\n",
    "# Any unknown domains remain with weight 0 (won't be sampled)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.as_tensor(w, dtype=torch.double),\n",
    "    num_samples=len(w),   # one epoch worth of samples, drawn with replacement\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "# Sanity Check\n",
    "# Single simulated epoch\n",
    "sim_idx = list(iter(WeightedRandomSampler(torch.as_tensor(w, dtype=torch.double),\n",
    "                                         num_samples=len(w),\n",
    "                                         replacement=True)))\n",
    "sim_ratio = (sample_domains[sim_idx] == 1).mean()  # NOR share\n",
    "print(f\"Simulated epoch: NOR share={sim_ratio:.3f}, unique IDs drawn={len(set(sim_idx))}/{len(w)}\")\n",
    "\n",
    "# Optional: a few repeats to see variance\n",
    "for t in range(3):\n",
    "    idx_t = list(iter(WeightedRandomSampler(torch.as_tensor(w, dtype=torch.double),\n",
    "                                           num_samples=len(w),\n",
    "                                           replacement=True)))\n",
    "    print(f\" trial {t+1}: NOR={(sample_domains[idx_t] == 1).mean():.3f}, unique={len(set(idx_t))}\")\n",
    "\n",
    "\n",
    "# Plug into skorch (disable shuffle when using a sampler)\n",
    "custom_nn = custom_nn.set_params(\n",
    "    iterator_train__sampler=sampler,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__batch_sampler=None,  # ensure no clash if set elsewhere\n",
    ")\n",
    "\n",
    "print(f\"Sampler configured: CH IDs={n_ch}, NOR IDs={n_nor}, total train items={len(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the DANN model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        if key == 'module':\n",
    "            print(f\"{key}: DANNNetwork\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(f\"dan_lambda: {DAN_LAMBDA}\")\n",
    "    custom_nn.fit(dataset, None)\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"dann_model_crossregional_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "    \n",
    "    # save params dic\n",
    "    params_filename = f\"dann_params_crossregional_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump({**args, 'dan_lambda': DAN_LAMBDA}, f)\n",
    "\n",
    "else:\n",
    "    # Load model and set to CPU\n",
    "    model_filename = \"dann_model_2025-08-19_2nd_try_4_glaciers_50%_lamba0_05_mean_domainloss_even_domainloss_only_NOR_val.pt\"  # Replace if needed\n",
    "    loaded_model = CustomDANNRegressor.load_model(\n",
    "        cfg,\n",
    "        model_filename,\n",
    "        **{\n",
    "            **args,\n",
    "            **param_init\n",
    "        },\n",
    "    )\n",
    "    loaded_model = loaded_model.set_params(device='cpu')\n",
    "    loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"dann_model_crossregional_2025-08-25_lambda0_1_mean_domainloss_even_domainloss_50_50_epochsplit_grllambdascheduling.pt\"\n",
    "loaded_model = CustomDANNRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on NOR (post-adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_eval_target_subset = df_X_eval_target[all_columns]\n",
    "\n",
    "feat_target, meta_target = loaded_model._create_features_metadata(df_X_eval_target_subset)\n",
    "agg_target = mbm.data_processing.AggregatedDataset(cfg, features=feat_target, metadata=meta_target, targets=y_eval_target)\n",
    "X_target = SliceDataset(agg_target, idx=0)\n",
    "y_target = SliceDataset(agg_target, idx=1)\n",
    "\n",
    "y_pred_monthly = loaded_model.predict(X_target)\n",
    "y_pred_aggr = loaded_model.aggrPredict(X_target)\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_aggr))\n",
    "y_true_aggr = np.array([e for e in y_target[batchIndex]])\n",
    "\n",
    "mse_t, rmse_t, mae_t, pearson_t = loaded_model.evalMetrics(y_pred_monthly, y_true_aggr)\n",
    "print(f\"NOR Evaluation (simple) -> RMSE: {rmse_t:.4f} | MAE: {mae_t:.4f} | Pearson: {pearson_t:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build features/metadata for target evaluation set (Norway full labeled)\n",
    "features_eval, metadata_eval = loaded_model._create_features_metadata(df_X_eval_target_subset)\n",
    "\n",
    "# Move to CPU if needed\n",
    "if hasattr(features_eval, 'cpu'):\n",
    "    features_eval = features_eval.cpu()\n",
    "\n",
    "targets_eval = y_eval_target\n",
    "if hasattr(targets_eval, 'cpu'):\n",
    "    targets_eval = targets_eval.cpu()\n",
    "\n",
    "\n",
    "# Aggregated dataset and sliced view (X, y)\n",
    "agg_eval = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                 features=features_eval,\n",
    "                                                 metadata=metadata_eval,\n",
    "                                                 targets=targets_eval)\n",
    "X_eval_slice = SliceDataset(agg_eval, idx=0)\n",
    "y_eval_slice = SliceDataset(agg_eval, idx=1)\n",
    "\n",
    "y_pred_monthly_full = loaded_model.predict(X_eval_slice)\n",
    "y_pred_agg_full = loaded_model.aggrPredict(X_eval_slice)\n",
    "\n",
    "batchIndex_full = np.arange(len(y_pred_agg_full))\n",
    "y_true_agg_full = np.array([e for e in y_eval_slice[batchIndex_full]])\n",
    "\n",
    "# Metrics (score uses model.score which may aggregate differently; keep both)\n",
    "score_full = loaded_model.score(X_eval_slice, y_eval_slice)\n",
    "mse_full, rmse_full, mae_full, pearson_full = loaded_model.evalMetrics(y_pred_monthly_full, y_true_agg_full)\n",
    "print(f\"NOR (legacy style) -> RMSE: {rmse_full:.4f} | MAE: {mae_full:.4f} | Pearson: {pearson_full:.4f} | Score (skorch): {score_full:.4f}\")\n",
    "\n",
    "# Aggregate predictions into DataFrame per ID\n",
    "eval_ids = X_eval_slice.dataset.indexToId(batchIndex_full)\n",
    "data_rows = {\n",
    "    'target': [e[0] for e in y_eval_slice],  # original summed/mean target per ID (depends on base class semantics)\n",
    "    'ID': eval_ids,\n",
    "    'pred': y_pred_agg_full,\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data_rows)\n",
    "\n",
    "# Merge ancillary columns (PERIOD, GLACIER, YEAR) from evaluation subset dataframe\n",
    "periods_per_ids = df_X_eval_target_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID', how='left')\n",
    "\n",
    "glaciers_per_ids = df_X_eval_target_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glaciers_per_ids, on='ID', how='left')\n",
    "\n",
    "years_per_ids = df_X_eval_target_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID', how='left')\n",
    "\n",
    "PlotPredictionsCombined_NN(grouped_ids, region_name='CH -> NOR', include_summer=False)\n",
    "PlotPredictions_NN(grouped_ids)\n",
    "predVSTruth_all(grouped_ids, mae_full, rmse_full, title='DANN on NOR (legacy style)')\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain confusion analysis - Check if DANN learned domain-invariant features\n",
    "print(\"Running domain confusion analysis...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    loaded_model.module_.eval()\n",
    "    \n",
    "    # Get domain predictions for both CH and NOR\n",
    "    ch_domain_preds = []\n",
    "    nor_domain_preds = []\n",
    "    \n",
    "    # Use CH validation data\n",
    "    for i, (x, (y, d)) in enumerate(dataset_val):\n",
    "        if i > 20: break\n",
    "        \n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        x_processed, _ = loaded_model._unpack_inp(x_tensor)\n",
    "        y_pred, d_logits = loaded_model.module_(x_processed)\n",
    "        d_prob = torch.sigmoid(d_logits.squeeze()).mean().item()\n",
    "        \n",
    "        domain_label = d[0].item() if not np.isnan(d[0].item()) else None\n",
    "        if domain_label == 0:  # CH\n",
    "            ch_domain_preds.append(d_prob)\n",
    "    \n",
    "    # Use NOR evaluation data for NOR samples\n",
    "    for i, x in enumerate(X_eval_slice):\n",
    "        if i > 20: break\n",
    "        \n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        x_processed, _ = loaded_model._unpack_inp(x_tensor)\n",
    "        y_pred, d_logits = loaded_model.module_(x_processed)\n",
    "        d_prob = torch.sigmoid(d_logits.squeeze()).mean().item()\n",
    "        nor_domain_preds.append(d_prob)\n",
    "    \n",
    "    if len(ch_domain_preds) > 0 and len(nor_domain_preds) > 0:\n",
    "        print(f\"CH domain predictions (should be ~0.5): {np.mean(ch_domain_preds):.3f} ± {np.std(ch_domain_preds):.3f}\")\n",
    "        print(f\"NOR domain predictions (should be ~0.5): {np.mean(nor_domain_preds):.3f} ± {np.std(nor_domain_preds):.3f}\")\n",
    "        \n",
    "        ch_mean = np.mean(ch_domain_preds)\n",
    "        nor_mean = np.mean(nor_domain_preds)\n",
    "        domain_separation = abs(ch_mean - nor_mean)\n",
    "        \n",
    "        if abs(ch_mean - 0.5) < 0.15 and abs(nor_mean - 0.5) < 0.15:\n",
    "            print(\"Domain confusion SUCCESS: Model learned domain-invariant features!\")\n",
    "        else:\n",
    "            print(\"Domain confusion FAILED: Model can still distinguish domains\")\n",
    "        \n",
    "        print(f\"   Domain separation: {domain_separation:.3f} (closer to 0 is better)\")\n",
    "        print(f\"   CH mean distance from 0.5: {abs(ch_mean - 0.5):.3f}\")\n",
    "        print(f\"   NOR mean distance from 0.5: {abs(nor_mean - 0.5):.3f}\")\n",
    "    else:\n",
    "        print(f\"Could not analyze domain confusion - CH samples: {len(ch_domain_preds)}, NOR samples: {len(nor_domain_preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
