{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Glacier Mass Balance Prediction\n",
    "\n",
    "1. **Load a pre-trained neural network** trained on Swiss glacier data\n",
    "2. **Fine-tune it on subset of Norwegian glacier data** using various strategies\n",
    "3. **Evaluate performance** on unseen Norwegian glaciers\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Pre-trained MLP model on all Swiss data from ../regions/Switzerland/3.2.2 Train-ML-model-NN.ipynb e.g. `nn_model_2025-07-14_CH_flexible.pt`\n",
    "- Norwegian glacier dataset from ../regions/Norway_mb/1.1. Norway-prepro.ipynb\n",
    "- ERA5 climate data of Norway from ../regions/Norway_mb/1.2. ERA5Land-prepro.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root of repo to import MBM\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "#import pickle # for displaying saved model parameters etc.\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "\n",
    "# Scientific computing\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Skorch (scikit-learn compatible PyTorch)\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# MassBalanceMachine (custom package)\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# Local helper modules\n",
    "# from regions.Norway_mb.scripts.norway_preprocess import *\n",
    "# from regions.Norway_mb.scripts.plots import *\n",
    "# from regions.Norway_mb.scripts.config_NOR import *\n",
    "# from regions.Norway_mb.scripts.nn_helpers import *\n",
    "# from regions.Norway_mb.scripts.xgb_helpers import *\n",
    "# from regions.Norway_mb.scripts.NN_networks import *\n",
    "\n",
    "from regions.Norway_mb.scripts.config_NOR import *\n",
    "from regions.Norway_mb.scripts.dataset import get_stakes_data_NOR\n",
    "from regions.Norway_mb.scripts.utils import *\n",
    "\n",
    "from regions.Switzerland.scripts.dataset import process_or_load_data, get_CV_splits\n",
    "from regions.Switzerland.scripts.plotting import plot_predictions_summary, plot_individual_glacier_pred, plot_history_lstm, get_cmap_hex,plot_tsne_overlap, plot_feature_kde_overlap\n",
    "from regions.Switzerland.scripts.dataset import get_stakes_data, build_combined_LSTM_dataset, inspect_LSTM_sample, prepare_monthly_dfs_with_padding\n",
    "from regions.Switzerland.scripts.models import compute_seasonal_scores\n",
    "\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging for tracking progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "### Create Norwegian Glacier Dataset\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Norwegian glacier dataset with topographical features\n",
    "data_wgms = get_stakes_data_NOR(cfg)\n",
    "\n",
    "# Remove entries with missing data\n",
    "data_wgms = data_wgms.dropna(\n",
    "    subset=data_wgms.columns.drop('DATA_MODIFICATION'))\n",
    "\n",
    "# Dataset summary\n",
    "print('Dataset Overview:')\n",
    "print(f'   Number of glaciers: {len(data_wgms[\"GLACIER\"].unique())}')\n",
    "print(f'   Total measurements: {len(data_wgms)}')\n",
    "print('   Measurement breakdown:')\n",
    "print(f'     Annual samples: {len(data_wgms[data_wgms.PERIOD == \"annual\"])}')\n",
    "print(f'     Winter samples: {len(data_wgms[data_wgms.PERIOD == \"winter\"])}')\n",
    "print(f'     Summer samples: {len(data_wgms[data_wgms.PERIOD == \"summer\"])}')\n",
    "\n",
    "print(f'\\nAvailable columns: {list(data_wgms.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vois_climate = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "vois_topographical = ['aspect', 'slope', 'svf']\n",
    "\n",
    "# Copy dataset for processing\n",
    "data_NOR_test = data_wgms.copy()\n",
    "\n",
    "# Define file paths for climate data integration\n",
    "paths = {\n",
    "    'csv_path':\n",
    "    os.path.join(cfg.dataPath, path_PMB_GLACIOCLIM_csv),\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_NOR_Alps.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_NOR_Alps.nc\")\n",
    "}\n",
    "\n",
    "# Process or load preprocessed data\n",
    "# Set RUN=True to reprocess data, False to load existing preprocessed file\n",
    "RUN = False\n",
    "print(f\"{'Processing' if RUN else 'Loading'} monthly climate data...\")\n",
    "\n",
    "# Transform point measurements to monthly format and merge with ERA5 climate data\n",
    "data_monthly_NOR = process_or_load_data(\n",
    "    run_flag=RUN,\n",
    "    df=data_NOR_test,\n",
    "    region_id=8,\n",
    "    region_name='NOR',\n",
    "    paths=paths,\n",
    "    cfg=cfg,\n",
    "    vois_climate=vois_climate,\n",
    "    vois_topographical=vois_topographical,\n",
    "    output_file='NOR_ft_wgms_dataset_monthly.csv')\n",
    "\n",
    "dataloader_gl = mbm.dataloader.DataLoader(cfg,\n",
    "                                            data=data_monthly_NOR,\n",
    "                                            random_seed=cfg.seed,\n",
    "                                            meta_data_columns=cfg.metaData)\n",
    "display(data_monthly_NOR.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split Strategies\n",
    "\n",
    "Implement three different strategies for splitting the Norway data to test various transfer learning scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: 50% Split\n",
    "**Data**: About 50 % of the available data is used as fine-tuning set.\n",
    "\n",
    "**Use case**: Balanced representation with good data availability for transfer learning adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50%\n",
    "train_glaciers = [\n",
    "    'Engabreen', 'Storglombreen N', 'Moesevassbrea', 'Blaaisen', 'Blabreen',\n",
    "    'Harbardsbreen', 'Graasubreen', 'Svelgjabreen', 'Aalfotbreen',\n",
    "    'Rundvassbreen', 'Juvfonne', 'Storsteinsfjellbreen', 'Hansebreen',\n",
    "    'Vesledalsbreen', 'Vetlefjordbreen', 'Blomstoelskardsbreen',\n",
    "    'Vestre Memurubreen', 'Austre Memurubreen'\n",
    "]\n",
    "\n",
    "# Test glaciers (all remaining Norway glaciers)\n",
    "all_norway_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_norway_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Validate glacier names exist in dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print(f'\\nSplit Summary:')\n",
    "print(\n",
    "    f'   Train glaciers: ({len(train_set[\"splits_vals\"])}) {train_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(\n",
    "    f'   Test glaciers: ({len(test_set[\"splits_vals\"])}) {test_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(f'   Train samples: {len(train_set[\"df_X\"])}')\n",
    "print(f'   Test samples: {len(test_set[\"df_X\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: North/South Geographic Split\n",
    "\n",
    "**Data:** Southern glaciers are used as fine-tuning set, northern as test set. Split is based on 63 degree latitude.\n",
    "\n",
    "**Use case**: Test geographic generalization across latitudinal gradients and climatic zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate geographic split based on glacier latitudes\n",
    "print(\"Strategy 2: North/South Geographic Split\")\n",
    "\n",
    "# Get glacier latitudes for geographic analysis\n",
    "glacier_lat = data_wgms.groupby('GLACIER')['POINT_LAT'].first()\n",
    "\n",
    "# Use the median latitude as the split threshold\n",
    "#lat_threshold = glacier_lat.median()\n",
    "\n",
    "# Use manually set cut off\n",
    "lat_threshold = 63\n",
    "\n",
    "# Split glaciers into northern and southern groups\n",
    "north_glaciers = glacier_lat[glacier_lat >= lat_threshold].index.tolist()\n",
    "south_glaciers = glacier_lat[glacier_lat < lat_threshold].index.tolist()\n",
    "\n",
    "print(f\"Latitude threshold: {lat_threshold:.2f}°\")\n",
    "print(f\"North glaciers ({len(north_glaciers)}): {north_glaciers}\")\n",
    "print(f\"South glaciers ({len(south_glaciers)}): {south_glaciers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63 lat split\n",
    "train_glaciers = [\n",
    "    'Aalfotbreen', 'Austdalsbreen', 'Austre Memurubreen', 'Blabreen',\n",
    "    'Blomstoelskardsbreen', 'Bondhusbrea', 'Breidablikkbrea', 'Graafjellsbrea',\n",
    "    'Graasubreen', 'Hansebreen', 'Harbardsbreen', 'Hellstugubreen', 'Juvfonne',\n",
    "    'Moesevassbrea', 'Nigardsbreen', 'Rembesdalskaaka', 'Ruklebreen',\n",
    "    'Svelgjabreen', 'Tunsbergdalsbreen', 'Vesledalsbreen',\n",
    "    'Vestre Memurubreen', 'Vetlefjordbreen'\n",
    "]\n",
    "\n",
    "# Test glaciers (northern glaciers with different climate characteristics)\n",
    "all_norway_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_norway_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Validate glacier names exist in dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print(f'\\nSplit Summary:')\n",
    "print(\n",
    "    f'   Train glaciers: ({len(train_set[\"splits_vals\"])}) {train_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(\n",
    "    f'   Test glaciers: ({len(test_set[\"splits_vals\"])}) {test_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(f'   Train samples: {len(train_set[\"df_X\"])}')\n",
    "print(f'   Test samples: {len(test_set[\"df_X\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Limited Data Split (5-10% for fine-tuning)\n",
    "\n",
    "**Data**: About 5-10 % of the available data is used as fine tuning set.\n",
    "\n",
    "**Use case**: Test performance with minimal fine-tuning data (~500 measurements) to simulate data-scarce scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glaciers = [\n",
    "    'Tunsbergdalsbreen', 'Austre Memurubreen', 'Svartisheibreen',\n",
    "    'Bondhusbrea', 'Harbardsbreen', 'Moesevassbrea', 'Graasubreen'\n",
    "]\n",
    "\n",
    "# Test glaciers (majority of Norwegian glaciers)\n",
    "all_norway_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_norway_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(\n",
    "    f\"Limited fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Validate glacier names exist in dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "\n",
    "print(f'\\nSplit Summary:')\n",
    "print(\n",
    "    f'   Train glaciers: ({len(train_set[\"splits_vals\"])}) {train_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(\n",
    "    f'   Test glaciers: ({len(test_set[\"splits_vals\"])}) {test_set[\"splits_vals\"]}'\n",
    ")\n",
    "print(f'   Train samples: {len(train_set[\"df_X\"])}')\n",
    "print(f'   Test samples: {len(test_set[\"df_X\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split Options\n",
    "\n",
    "Use the same option here as was used for the source (Swiss) model.\n",
    "\n",
    "### Option 1: Random 80/20 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random 80/20 train/validation split within the fine-tuning set\n",
    "print(\"Creating random 80/20 train/validation split...\")\n",
    "\n",
    "# Combine training features with targets\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "# Create random train/validation split\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valdating dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "# Create training subset\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Create validation subset\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "print(\"Split Results:\")\n",
    "print(\n",
    "    f\"   Train data glacier distribution: {df_X_train['GLACIER'].value_counts().head()}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Val data glacier distribution: {df_X_val['GLACIER'].value_counts().head()}\"\n",
    ")\n",
    "print(f\"   Train data shape: {df_X_train.shape}\")\n",
    "print(f\"   Val data shape: {df_X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Glacier-wise Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "val_glacier = ['Engabreen']\n",
    "train_glaciers_subset = [g for g in train_glaciers if g not in val_glacier]\n",
    "\n",
    "# Create training subset (excluding validation glacier)\n",
    "df_X_train = data_train[data_train['GLACIER'].isin(\n",
    "    train_glaciers_subset)].copy()\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Create validation subset (only validation glacier)\n",
    "df_X_val = data_train[data_train['GLACIER'].isin(val_glacier)].copy()\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "print(\"Glacier-wise Split Results:\")\n",
    "print(f\"   Training on: {train_glaciers_subset}\")\n",
    "print(f\"   Validating on: {val_glacier}\")\n",
    "print(\n",
    "    f\"   Train data glacier distribution: {df_X_train['GLACIER'].value_counts().head()}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Val data glacier distribution: {df_X_val['GLACIER'].value_counts().head()}\"\n",
    ")\n",
    "print(f\"   Train data shape: {df_X_train.shape}\")\n",
    "print(f\"   Val data shape: {df_X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Configuration\n",
    "\n",
    "### Feature Engineering and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define complete feature set for model training\n",
    "features_topo = ['ELEVATION_DIFFERENCE'] + list(vois_topographical)\n",
    "\n",
    "# Combine topographical and climate features\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "# Set features in config\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "# Include all necessary columns (features + metadata)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Create feature subsets (handle potential extra columns from Swiss data)\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Feature Configuration:')\n",
    "print(f'   Training dataset shape: {df_X_train_subset.shape}')\n",
    "print(f'   Validation dataset shape: {df_X_val_subset.shape}')\n",
    "print(f'   Testing dataset shape: {df_X_test_subset.shape}')\n",
    "print(f'   Features ({len(feature_columns)}): {feature_columns}')\n",
    "\n",
    "# Sanity check: ensure targets match features\n",
    "assert all(train_set['df_X'].POINT_BALANCE ==\n",
    "           train_set['y']), \"Target mismatch detected!\"\n",
    "print('Feature-target alignment verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks and Training Configuration\n",
    "Set up training callbacks and configuration for optimal performance and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regions.Norway_mb.scripts.nn_helpers import SaveBestAtEpochs\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',  # Monitor validation loss\n",
    "    patience=15,  # Stop after 15 epochs without improvement\n",
    "    threshold=1e-4,  # Minimum change threshold\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for adaptive training\n",
    "lr_scheduler_cb = LRScheduler(\n",
    "    policy=ReduceLROnPlateau,\n",
    "    monitor='valid_loss',\n",
    "    mode='min',\n",
    "    factor=0.5,  # Reduce LR by half\n",
    "    patience=5,  # Wait 5 epochs before reducing\n",
    "    threshold=0.01,\n",
    "    threshold_mode='rel',\n",
    "    verbose=True)\n",
    "\n",
    "# Global variables for dataset management\n",
    "dataset = dataset_val = None\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    \"\"\"Custom train/validation split function for skorch.\"\"\"\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# Model configuration parameters\n",
    "param_init = {'device': 'cpu'}\n",
    "nInp = len(feature_columns)  # Number of input features\n",
    "\n",
    "# Model checkpointing to save best model during training\n",
    "checkpoint_cb = Checkpoint(\n",
    "    monitor='valid_loss_best',\n",
    "    f_params='best_model.pt',\n",
    "    f_optimizer=None,  # Don't save optimizer state\n",
    "    f_history=None,  # Don't save training history\n",
    "    f_criterion=None,  # Don't save criterion state\n",
    "    load_best=True,  # Load best model after training\n",
    ")\n",
    "\n",
    "# Custom callback to save models at specific epochs for analysis\n",
    "save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\n",
    "\n",
    "print('Callbacks and configuration ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "Datasets will be created in the training loop after loading the pre-trained Swiss model to ensure compatible preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset variables as None\n",
    "features = features_val = None\n",
    "metadata = metadata_val = None\n",
    "dataset = dataset_val = None\n",
    "\n",
    "print(\"Dataset creation deferred until Swiss model is loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Execution\n",
    "\n",
    "### Loading Pre-trained Swiss Model and Fine-tuning on Norwegian Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Standard Fine-tuning with Selective Layer Freezing\n",
    "\n",
    "After loading the model, all layers will be frozen by default, to unfreeze a layer you have to include it in \"if name not in [...]\" in Step 3.\n",
    "\n",
    " The SaveBestAtEpochs callback automatically saves the current best model at epochs [10, 15, 20, 30, 50, 100], which can then be evaluated in the Epoch-wise model evalution section. Comment out the callback if you don't want this feature. If you do and you continuously want to retrain models at different learning rates, you have to reexecute the \"save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regions.Switzerland.scripts.models import FlexibleNetwork\n",
    "\n",
    "TRAIN = True  # Set to True to actually train, False to skip training\n",
    "\n",
    "if TRAIN:\n",
    "    print(\"Starting Method 1: Standard Fine-tuning with Layer Freezing\")\n",
    "\n",
    "    # STEP 1: Load the pre-trained Swiss model\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "\n",
    "    # Define Swiss model architecture and parameters\n",
    "    swiss_args = {\n",
    "        'module':\n",
    "        FlexibleNetwork,\n",
    "        'nbFeatures':\n",
    "        nInp,\n",
    "        'module__input_dim':\n",
    "        nInp,\n",
    "        'module__dropout':\n",
    "        0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm':\n",
    "        True,\n",
    "        'warm_start':\n",
    "        True,  # CRITICAL: preserve pretrained weights\n",
    "        'train_split':\n",
    "        my_train_split,\n",
    "        'batch_size':\n",
    "        128,\n",
    "        'verbose':\n",
    "        1,\n",
    "        'iterator_train__shuffle':\n",
    "        True,\n",
    "        'lr':\n",
    "        0.001,\n",
    "        'max_epochs':\n",
    "        200,\n",
    "        'optimizer':\n",
    "        torch.optim.Adam,\n",
    "        'optimizer__weight_decay':\n",
    "        1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "            #('save_best_at_epochs', save_best_epochs_cb)  # Save models at specific epochs\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{\n",
    "            **swiss_args,\n",
    "            **param_init\n",
    "        })\n",
    "    print(\"Swiss model loaded successfully!\")\n",
    "\n",
    "    # STEP 2: Create datasets using Swiss model preprocessing\n",
    "    print(\"Creating datasets with Swiss model preprocessing...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(\n",
    "        df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(\n",
    "        df_X_val_subset)\n",
    "\n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset, idx=0), SliceDataset(dataset, idx=1))\n",
    "\n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "\n",
    "    print(\n",
    "        f\"Dataset shapes - Train: {dataset.X.shape}, Val: {dataset_val.X.shape}\"\n",
    "    )\n",
    "\n",
    "    # STEP 3: Apply selective layer freezing\n",
    "    print(\"Applying selective layer freezing...\")\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        if name not in [  #'model.0.weight', 'model.0.bias',\n",
    "                'model.1.weight',\n",
    "                'model.1.bias',\n",
    "                #'model.4.weight', 'model.4.bias',\n",
    "                'model.5.weight',\n",
    "                'model.5.bias',\n",
    "                #'model.8.weight', 'model.8.bias',\n",
    "                'model.9.weight',\n",
    "                'model.9.bias',\n",
    "                #'model.12.weight', 'model.12.bias',\n",
    "                'model.13.weight',\n",
    "                'model.13.bias',\n",
    "                #'model.16.weight', 'model.16.bias'\n",
    "        ]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(\n",
    "        \"Layer freezing applied - batch norm and later layers remain trainable\"\n",
    "    )\n",
    "\n",
    "    # STEP 4: Configure for fine-tuning\n",
    "    print(\"Configuring fine-tuning parameters...\")\n",
    "    loaded_model = loaded_model.set_params(\n",
    "        lr=0.1,\n",
    "        max_epochs=1,\n",
    "    )\n",
    "\n",
    "    # STEP 5: Execute fine-tuning\n",
    "    print(\"Starting fine-tuning process...\")\n",
    "    loaded_model.fit(features, y_train)\n",
    "\n",
    "    # STEP 6: Save fine-tuned model\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"Fine-tuned model saved as: {finetuned_model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Progressive Layer Unfreezing\n",
    "This approach gradually unfreezes layers during training for more controlled adaptation to the Norwegian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False  # Set to True to execute progressive unfreezing\n",
    "\n",
    "if TRAIN:\n",
    "    print(\"Starting Method 2: Progressive Layer Unfreezing\")\n",
    "\n",
    "    # STEP 1: Load the pre-trained Swiss model\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "\n",
    "    # Define Swiss model architecture (same as Method 1)\n",
    "    swiss_args = {\n",
    "        'module':\n",
    "        FlexibleNetwork,\n",
    "        'nbFeatures':\n",
    "        nInp,\n",
    "        'module__input_dim':\n",
    "        nInp,\n",
    "        'module__dropout':\n",
    "        0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm':\n",
    "        True,\n",
    "        'warm_start':\n",
    "        True,  # CRITICAL: preserve pretrained weights\n",
    "        'train_split':\n",
    "        my_train_split,\n",
    "        'batch_size':\n",
    "        128,\n",
    "        'verbose':\n",
    "        1,\n",
    "        'iterator_train__shuffle':\n",
    "        True,\n",
    "        'lr':\n",
    "        0.001,\n",
    "        'max_epochs':\n",
    "        200,\n",
    "        'optimizer':\n",
    "        torch.optim.Adam,\n",
    "        'optimizer__weight_decay':\n",
    "        1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{\n",
    "            **swiss_args,\n",
    "            **param_init\n",
    "        })\n",
    "    print(\"Swiss model loaded successfully!\")\n",
    "\n",
    "    # STEP 2: Create datasets using Swiss model preprocessing\n",
    "    print(\"Creating datasets with Swiss model preprocessing...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(\n",
    "        df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(\n",
    "        df_X_val_subset)\n",
    "\n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset, idx=0), SliceDataset(dataset, idx=1))\n",
    "\n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "\n",
    "    print(\n",
    "        f\"Dataset shapes - Train: {dataset.X.shape}, Val: {dataset_val.X.shape}\"\n",
    "    )\n",
    "\n",
    "    # STEP 3: Define progressive unfreezing strategy\n",
    "    # Helper to freeze/unfreeze layers\n",
    "    def set_requires_grad(layer_names, requires_grad=True):\n",
    "        \"\"\"Helper function to freeze/unfreeze specific layers\"\"\"\n",
    "        for name, param in loaded_model.module_.named_parameters():\n",
    "            if name in layer_names:\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    # Define layer groups for progressive unfreezing\n",
    "    # Format: (layers_to_unfreeze, epochs, learning_rate)\n",
    "    layer_groups = [\n",
    "        # Stage 1: Unfreeze batch normalization layers first\n",
    "        ([\n",
    "            'model.1.weight', 'model.1.bias', 'model.5.weight', 'model.5.bias',\n",
    "            'model.9.weight', 'model.9.bias', 'model.13.weight',\n",
    "            'model.13.bias'\n",
    "        ], 200, 0.1),\n",
    "        # Stage 2: Unfreeze output layer\n",
    "        (['model.16.weight', 'model.16.bias'], 200, 0.001),\n",
    "        # Stage 3: Unfreeze final hidden layer\n",
    "        (['model.12.weight', 'model.12.bias'], 200, 0.001),\n",
    "        # Stage 4: Unfreeze middle layer\n",
    "        (['model.8.weight', 'model.8.bias'], 200, 0.001)\n",
    "    ]\n",
    "\n",
    "    # Start with all layers frozen\n",
    "    print(\"Freezing all layers initially...\")\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Progressive unfreezing loop\n",
    "    for i, (layers, epochs, lr) in enumerate(layer_groups, 1):\n",
    "        print(\n",
    "            f\"Stage {i}: Unfreezing {len(layers)//2} layer(s) for {epochs} epochs (lr={lr})...\"\n",
    "        )\n",
    "\n",
    "        # Unfreeze current layer group\n",
    "        set_requires_grad(layers, True)\n",
    "\n",
    "        # Update model parameters\n",
    "        loaded_model = loaded_model.set_params(lr=lr, max_epochs=epochs)\n",
    "\n",
    "        # Train current stage\n",
    "        loaded_model.fit(features, y_train)\n",
    "\n",
    "        # Evaluate current stage\n",
    "        val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "        print(f\"   Stage {i} validation score: {val_score:.4f}\")\n",
    "\n",
    "    # STEP 4: Save progressively fine-tuned model\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_progressive_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(\n",
    "        f\"Progressively fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"Progressive training skipped (TRAIN=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Performance Evaluation\n",
    "Get immediate performance metrics on the test set using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comprehensive evaluation of the fine-tuned model\n",
    "print(\"Evaluating fine-tuned model performance...\")\n",
    "\n",
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "print(\"Test Set Performance Metrics:\")\n",
    "display(scores_NN)\n",
    "\n",
    "# Validation score for confirmation that model with the best val_loss is used\n",
    "val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "print(f\"Validation score (for reference): {val_score:.4f}\")\n",
    "\n",
    "# Calculate additional performance metrics by glacier\n",
    "print(\"\\nPerformance by glacier:\")\n",
    "glacier_performance = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_samples':\n",
    "        len(x),\n",
    "        'rmse':\n",
    "        np.sqrt(np.mean((x['target'] - x['pred'])**2)),\n",
    "        'mae':\n",
    "        np.mean(np.abs(x['target'] - x['pred'])),\n",
    "        'r2':\n",
    "        1 - np.sum((x['target'] - x['pred'])**2) / np.sum(\n",
    "            (x['target'] - x['target'].mean())**2),\n",
    "        'Bias':\n",
    "        np.mean(x['pred'].values - x['target'].values)\n",
    "    })).round(4)\n",
    "\n",
    "display(glacier_performance.sort_values('rmse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch-wise Model Evaluation\n",
    "Evaluate models saved at different training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models saved at different epochs to analyze training dynamics\n",
    "print(\"Evaluating models saved at different training epochs...\")\n",
    "\n",
    "epochs_to_evaluate = [10, 15, 20, 30, 50, 100]\n",
    "model_prefix = \"nn_model_best_epoch\"\n",
    "\n",
    "epoch_results = {}\n",
    "\n",
    "for epoch in epochs_to_evaluate:\n",
    "    model_name = f\"{model_prefix}_{epoch}.pt\"\n",
    "\n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_name):\n",
    "        print(f\"Model for epoch {epoch} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model at epoch {epoch}...\")\n",
    "\n",
    "    # Load model with same architecture as Swiss model\n",
    "    epoch_model = mbm.models.CustomNeuralNetRegressor(cfg, **swiss_args,\n",
    "                                                      **param_init)\n",
    "    epoch_model = epoch_model.set_params(device='cpu').to('cpu')\n",
    "    epoch_model.initialize()\n",
    "\n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(model_name, map_location='cpu')\n",
    "    epoch_model.module_.load_state_dict(state_dict)\n",
    "\n",
    "    # Evaluate the model\n",
    "    grouped_ids_epoch, scores_NN_epoch, ids_NN_epoch, y_pred_NN_epoch = evaluate_model_and_group_predictions(\n",
    "        epoch_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "\n",
    "    # Store results\n",
    "    epoch_results[epoch] = scores_NN_epoch\n",
    "\n",
    "    print(f\"Epoch {epoch} performance:\")\n",
    "    display(scores_NN_epoch)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Epoch-wise evaluation completed!\")\n",
    "\n",
    "# Compare performance across epochs\n",
    "if epoch_results:\n",
    "    print(\"\\nPerformance comparison across epochs:\")\n",
    "    comparison_df = pd.DataFrame(epoch_results).T\n",
    "    comparison_df.index.name = 'Epoch'\n",
    "    display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Visualization and Analysis\n",
    "Generate detailed visualizations to understand model performance across different glaciers and time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive visualization data\n",
    "print(\"Preparing data for comprehensive visualizations...\")\n",
    "\n",
    "# Create features and metadata for final evaluation\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU for visualization\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "if hasattr(test_set['y'], 'cpu'):\n",
    "    targets_test = test_set['y'].cpu()\n",
    "else:\n",
    "    targets_test = test_set['y']\n",
    "\n",
    "# Create final test dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),  # Features\n",
    "    SliceDataset(dataset_test, idx=1)  # Targets\n",
    "]\n",
    "\n",
    "# Generate final predictions\n",
    "print(\"Generating final predictions for visualization...\")\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "# Prepare evaluation metrics\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate comprehensive performance metrics\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "print(f\"Final Model Performance Summary:\")\n",
    "print(f\"   R² Score: {score:.4f}\")\n",
    "print(f\"   RMSE: {rmse:.4f} mm w.e.\")\n",
    "print(f\"   MAE: {mae:.4f} mm w.e.\")\n",
    "print(f\"   Pearson r: {pearson:.4f}\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "grouped_ids = pd.DataFrame({\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "})\n",
    "\n",
    "# Add comprehensive metadata\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')\n",
    "\n",
    "print(\"Visualization data prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "\n",
    "PlotPredictionsCombined_NN_additional(grouped_ids)\n",
    "\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of all model strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN components: Gradient Reversal, network, regressor wrapper, and dataset bindings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skorch.utils import to_tensor\n",
    "import massbalancemachine as mbm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Gradient Reversal Layer\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "class GradReverse(nn.Module):\n",
    "\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "\n",
    "class DANNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor identical to FlexibleNetwork's trunk, with two heads:\n",
    "    - Regressor head for SMB (label)\n",
    "    - Domain classifier head (binary)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_layers,\n",
    "                 dropout=0.2,\n",
    "                 use_batchnorm=False,\n",
    "                 domain_hidden=64,\n",
    "                 grl_lambda=1.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(hidden_layers)\n",
    "        for hidden_dim, drop_rate in zip(hidden_layers, dropout):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            current_dim = hidden_dim\n",
    "        # trunk outputs the last hidden representation\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        # heads\n",
    "        self.regressor = nn.Linear(current_dim, 1)\n",
    "        self.grl = GradReverse(lambda_=grl_lambda)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(current_dim, domain_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout[-1] if isinstance(dropout, list) else dropout),\n",
    "            nn.Linear(domain_hidden, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)\n",
    "        y_pred = self.regressor(h)\n",
    "        d_logits = self.domain_classifier(self.grl(h))\n",
    "        return y_pred, d_logits\n",
    "\n",
    "\n",
    "# Dataset that yields domain labels padded per ID to match monthly padding\n",
    "class DomainTargetDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, aggregated_dataset):\n",
    "        self.base = aggregated_dataset\n",
    "        self.meta_has_domain = 'DOMAIN' in self.base.metadataColumns\n",
    "        if not self.meta_has_domain:\n",
    "            # fallback: try to read from features\n",
    "            assert 'DOMAIN' in self.base.cfg.featureColumns, \"DOMAIN must be in metadata or featureColumns\"\n",
    "            self.domain_feat_idx = self.base.cfg.featureColumns.index('DOMAIN')\n",
    "        else:\n",
    "            self.domain_idx = self.base.metadataColumns.index('DOMAIN')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.base._getInd(index)\n",
    "        if self.meta_has_domain:\n",
    "            dval = self.base.metadata[ind[0]][self.domain_idx]\n",
    "        else:\n",
    "            dval = self.base.features[ind[0], self.domain_feat_idx]\n",
    "        dpad = np.empty(self.base.maxConcatNb, dtype=np.float32)\n",
    "        dpad.fill(np.nan)\n",
    "        dpad[:len(ind)] = dval\n",
    "        return dpad\n",
    "\n",
    "\n",
    "# Binding that returns (X, (y, d)) so y_true in get_loss can contain both\n",
    "class CombinedTargetBinding(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X_slice, y_slice, d_dataset):\n",
    "        self.X = X_slice\n",
    "        self.y = y_slice\n",
    "        self.d = d_dataset\n",
    "        assert len(self.X) == len(self.y) == len(self.d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], (self.y[idx], self.d[idx])\n",
    "\n",
    "\n",
    "# Skorch regressor that adds domain-adversarial loss on top of the SMB loss\n",
    "class CustomDANNRegressor(mbm.models.CustomNeuralNetRegressor):\n",
    "\n",
    "    def __init__(self, cfg, *args, dan_lambda=0.1, **kwargs):\n",
    "        super().__init__(cfg, *args, **kwargs)\n",
    "        self.dan_lambda = dan_lambda\n",
    "        self._last_domain_logits = None\n",
    "\n",
    "    def infer(self, x, **fit_params):\n",
    "        x = to_tensor(x, device=self.device)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        x, indNonNan = self._unpack_inp(x)\n",
    "        if self.modelDtype is not None:\n",
    "            x = x.type(self.modelDtype)\n",
    "        outputs = self.module_(x, **fit_params)\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_monthly, d_monthly = outputs\n",
    "            y_packed = self._pack_out(y_monthly, indNonNan)\n",
    "            d_packed = self._pack_out(d_monthly, indNonNan)\n",
    "            self._last_domain_logits = d_packed\n",
    "            return y_packed\n",
    "        else:\n",
    "            return self._pack_out(outputs, indNonNan)\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        # y_true can be (label_pad, domain_pad) from CombinedTargetBinding\n",
    "        if isinstance(y_true, (tuple, list)) and len(y_true) == 2:\n",
    "            y_true_labels, y_true_domain = y_true\n",
    "        else:\n",
    "            y_true_labels, y_true_domain = y_true, None\n",
    "\n",
    "        # Label loss (same as base implementation)\n",
    "        loss = 0.0\n",
    "        cnt = 0\n",
    "        for yi_pred, yi_true in zip(y_pred, y_true_labels):\n",
    "            valid = ~torch.isnan(yi_pred)\n",
    "            if valid.any():\n",
    "                pred_sum = yi_pred[valid].sum()\n",
    "                true_mean = yi_true[valid].mean()\n",
    "                loss = loss + (pred_sum - true_mean)**2\n",
    "                cnt += 1\n",
    "        label_loss = loss / max(cnt, 1)\n",
    "\n",
    "        # Domain loss (optional during training)\n",
    "        \"\"\"\n",
    "        # Simple domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits\n",
    "            mask = ~torch.isnan(y_true_domain)\n",
    "            if mask.any():\n",
    "                domain_loss = F.binary_cross_entropy_with_logits(d_logits[mask], y_true_domain[mask].float())\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean, otherwise IDs with longer months have higher domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # shape: [batch, max_months] (or [batch, max_months, 1])\n",
    "            per_id_losses = []\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                # squeeze trailing dim if present\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                # mask valid months\n",
    "                mask = ~torch.isnan(d_true_row) # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    per_id_losses.append(loss_i)\n",
    "\n",
    "            if len(per_id_losses) > 0:\n",
    "                domain_loss = torch.stack(per_id_losses).mean()  # mean over IDs\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean for each domain\n",
    "        # otherwise IDs with longer months have higher domain loss and CH domain loss with more data is exaggerated\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain\n",
    "                         is not None) and (self._last_domain_logits\n",
    "                                           is not None):\n",
    "            d_logits = self._last_domain_logits  # [batch_ids, max_months(,1)]\n",
    "            per_id_losses_ch, per_id_losses_nor = [], []\n",
    "\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                mask = ~torch.isnan(\n",
    "                    d_true_row\n",
    "                )  # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    # domain for this ID (same across valid months)\n",
    "                    dom_i = int(d_true_row[mask][0].item())  # 0=CH, 1=NOR\n",
    "                    (per_id_losses_ch\n",
    "                     if dom_i == 0 else per_id_losses_nor).append(loss_i)\n",
    "\n",
    "            parts = []\n",
    "            if len(per_id_losses_ch) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_ch).mean())\n",
    "            if len(per_id_losses_nor) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_nor).mean())\n",
    "            if len(parts) > 0:\n",
    "                domain_loss = torch.stack(parts).mean()\n",
    "\n",
    "        return label_loss + self.dan_lambda * domain_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(cfg, fname: str, *args, **kwargs):\n",
    "        \"\"\"Loads a pre-trained DANN model from a file.\"\"\"\n",
    "        model = CustomDANNRegressor(cfg, *args, **kwargs)\n",
    "        model.initialize()\n",
    "        models_dir = Path(\"./models\")\n",
    "        model.load_params(f_params=models_dir / fname)\n",
    "        return model\n",
    "\n",
    "\n",
    "params_dann = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "    'module__domain_hidden': 64,\n",
    "    'module__grl_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# Use DANN network\n",
    "args_dann = {\n",
    "    'module': DANNNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params_dann['module__dropout'],\n",
    "    'module__hidden_layers': params_dann['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params_dann['module__use_batchnorm'],\n",
    "    'module__domain_hidden': params_dann['module__domain_hidden'],\n",
    "    'module__grl_lambda': params_dann['module__grl_lambda'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params_dann['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params_dann['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params_dann['optimizer'],\n",
    "    'optimizer__weight_decay': params_dann['optimizer__weight_decay'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_pmb_multi_fixed_lines(models_grouped, title_prefix='Mean PMB'):\n",
    "\n",
    "    first_label = next(iter(models_grouped))\n",
    "    df_ref = models_grouped[first_label].copy()\n",
    "\n",
    "    fig, (ax_ann, ax_win) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    ax_ann.set_title('Mean annual point mass balance',\n",
    "                     fontsize=14,\n",
    "                     fontweight='bold',\n",
    "                     pad=30)\n",
    "    ax_win.set_title('Mean winter point mass balance',\n",
    "                     fontsize=14,\n",
    "                     fontweight='bold',\n",
    "                     pad=30)\n",
    "\n",
    "    palette = sns.color_palette(\"colorblind\", n_colors=len(models_grouped))\n",
    "    label_to_color = {\n",
    "        label: palette[i]\n",
    "        for i, label in enumerate(models_grouped.keys())\n",
    "    }\n",
    "\n",
    "    regional_models = ['Regional Baseline', 'CH and Regional Baseline']\n",
    "    transfer_models = ['Fine-tuning', 'Batchnorm Unfrozen', 'DANN']\n",
    "    line_styles = {model: '-' for model in regional_models}\n",
    "    line_styles.update({model: '--' for model in transfer_models})\n",
    "\n",
    "    def _compute_metrics(df_y):\n",
    "        valid = df_y[['pred', 'target']].dropna()\n",
    "        if len(valid) == 0:\n",
    "            return np.nan, np.nan\n",
    "        y_pred = valid['pred'].values\n",
    "        y_true = valid['target'].values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "        rho = np.corrcoef(y_pred, y_true)[0, 1] if y_pred.size > 1 else np.nan\n",
    "        return rmse, rho\n",
    "\n",
    "    def _plot_panel(ax, period_name):\n",
    "        df_period = df_ref[df_ref['PERIOD'] == period_name].copy()\n",
    "        years = np.sort(df_period['YEAR'].unique())\n",
    "\n",
    "        tgt_mean = df_period.groupby('YEAR')['target'].mean()\n",
    "        available_years = tgt_mean.index.values\n",
    "        available_values = tgt_mean.values\n",
    "\n",
    "        target_line_plotted = False\n",
    "        for i in range(len(available_years) - 1):\n",
    "            if available_years[i + 1] - available_years[i] == 1:\n",
    "                if not target_line_plotted:\n",
    "\n",
    "                    ax.plot([available_years[i], available_years[i + 1]],\n",
    "                            [available_values[i], available_values[i + 1]],\n",
    "                            color=\"black\",\n",
    "                            linewidth=2,\n",
    "                            marker='x',\n",
    "                            markersize=6,\n",
    "                            label=\"target mean\")\n",
    "                    target_line_plotted = True\n",
    "                else:\n",
    "\n",
    "                    ax.plot([available_years[i], available_years[i + 1]],\n",
    "                            [available_values[i], available_values[i + 1]],\n",
    "                            color=\"black\",\n",
    "                            linewidth=2,\n",
    "                            marker='x',\n",
    "                            markersize=6)\n",
    "\n",
    "        if not target_line_plotted:\n",
    "            ax.plot(available_years,\n",
    "                    available_values,\n",
    "                    color=\"black\",\n",
    "                    marker='x',\n",
    "                    markersize=6,\n",
    "                    linestyle='None',\n",
    "                    label=\"target mean\")\n",
    "\n",
    "        for label, gdf in models_grouped.items():\n",
    "            gdf_p = gdf[gdf['PERIOD'] == period_name].copy()\n",
    "            pred_mean = gdf_p.groupby('YEAR')['pred'].mean()\n",
    "\n",
    "            metrics_df = pd.DataFrame({\n",
    "                'YEAR': pred_mean.index,\n",
    "                'pred': pred_mean.values,\n",
    "            }).merge(df_period.groupby('YEAR')['target'].mean().reset_index(),\n",
    "                     on='YEAR',\n",
    "                     how='left')\n",
    "            rmse, rho = _compute_metrics(metrics_df)\n",
    "\n",
    "            label_with_metrics = f\"{label} (RMSE={np.nan_to_num(rmse):.2f}, \\u03C1={np.nan_to_num(rho):.2f})\"\n",
    "\n",
    "            pred_years = pred_mean.index.values\n",
    "            pred_values = pred_mean.values\n",
    "\n",
    "            label_added = False\n",
    "\n",
    "            for i in range(len(pred_years) - 1):\n",
    "                if pred_years[i + 1] - pred_years[i] == 1:\n",
    "                    ax.plot(\n",
    "                        [pred_years[i], pred_years[i + 1]],\n",
    "                        [pred_values[i], pred_values[i + 1]],\n",
    "                        color=label_to_color.get(label, None),\n",
    "                        linestyle=line_styles.get(label, '-'),\n",
    "                        label=label_with_metrics if not label_added else \"\")\n",
    "                    label_added = True\n",
    "\n",
    "            if not label_added and len(pred_years) > 0:\n",
    "                ax.plot([], [],\n",
    "                        color=label_to_color.get(label, None),\n",
    "                        linestyle=line_styles.get(label, '-'),\n",
    "                        label=label_with_metrics)\n",
    "\n",
    "        ax.grid(True, linestyle='--', alpha=0.4)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    _plot_panel(ax_ann, 'annual')\n",
    "    _plot_panel(ax_win, 'winter')\n",
    "\n",
    "    ax_ann.legend(fontsize=8,\n",
    "                  loc='upper center',\n",
    "                  ncol=3,\n",
    "                  frameon=True,\n",
    "                  bbox_to_anchor=(0.5, 1.22))\n",
    "    ax_win.legend(fontsize=8,\n",
    "                  loc='upper center',\n",
    "                  ncol=3,\n",
    "                  frameon=True,\n",
    "                  bbox_to_anchor=(0.5, 1.22))\n",
    "\n",
    "    ax_ann.set_ylabel('[m w.e.]')\n",
    "    ax_win.set_ylabel('[m w.e.]')\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.8))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "base = grouped_ids[['ID', 'target', 'PERIOD', 'GLACIER', 'YEAR']].copy()\n",
    "\n",
    "# List models (optionally mix NN and DANN). Provide up to 5.\n",
    "\"\"\"\n",
    "# 50% kMeans\n",
    "models = [\n",
    "    #('CH baseline',  'nn_model_2025-07-14_CH_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Norwegian baseline',  'nn_model_2025-09-07_50%_kmeans_regional_baseline.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Swiss and Norwegian baseline',  'nn_model_2025-09-07_50%_kmeans_CH_and_regional_baseline_only_NOR_val.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Fine-tuning',  'nn_model_finetuned_2025-09-07_50%_kMeans_fine_tuning_lr0.00005_epoch_10.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('BatchNorm unfrozen',  'nn_model_finetuned_2025-09-07_50%_kMeans_batchnorm_unfrozen_linear_frozen_lr0.1_epoch200.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('DANN',      'dann_model_2025-09-07_50%_kMeans_lambda0_05_mean_domainloss_even_domainloss_only_NOR_val_50_50_epochsplit_epoch60.pt', CustomDANNRegressor),\n",
    "]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# 5-10%\n",
    "models = [\n",
    "    #('CH baseline',  'nn_model_2025-07-14_CH_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Norwegian baseline',  'nn_model_2025-07-15_7_glaciers_regional_baseline_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Swiss and Norwegian baseline',  'nn_model_2025-08-14_CH_and_7_glaciers_regional_baseline_flexible_only_NOR_val.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Fine-tuning',  'nn_model_finetuned_2025-08-21_5-10%_glaciers_fine_tuning_lr0.0001_epoch_100.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('BatchNorm unfrozen',  'nn_model_finetuned_2025-08-21_5-10%_batchnorm_unfrozen_linear_frozen_lr0.05_epoch200.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('DANN',      'dann_model_2025-09-12_5-10%_lambda0_05_mean_domainloss_even_domainloss_only_NOR_val_50_50_epochsplit.pt', CustomDANNRegressor),\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# 63lat\n",
    "models = [\n",
    "    #('CH baseline',  'nn_model_2025-07-14_CH_flexible.pt', mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Norwegian baseline',\n",
    "     'nn_model_2025-08-26_63lat_regional_baseline_flexible.pt',\n",
    "     mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Swiss and Norwegian baseline',\n",
    "     'nn_model_2025-08-26_63lat_CH_and_regional_baseline_flexible_only_NOR_val.pt',\n",
    "     mbm.models.CustomNeuralNetRegressor),\n",
    "    ('Fine-tuning',\n",
    "     'nn_model_finetuned_2025-08-26_63lat_fine_tuning_lr0.005_epoch_30.pt',\n",
    "     mbm.models.CustomNeuralNetRegressor),\n",
    "    ('BatchNorm unfrozen',\n",
    "     'nn_model_finetuned_2025-08-26_63lat_batchnorm_unfrozen_linear_frozen_lr0.05_epoch200.pt',\n",
    "     mbm.models.CustomNeuralNetRegressor),\n",
    "    ('DANN',\n",
    "     'dann_model_2025-08-26_63lat_lambda0_05_mean_domainloss_even_domainloss_only_NOR_val_50_50_epochsplit.pt',\n",
    "     CustomDANNRegressor),\n",
    "]\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "nn_loader_args = {**swiss_args, **param_init}\n",
    "try:\n",
    "    _ = DANNNetwork\n",
    "    dann_loader_args = {\n",
    "        **{\n",
    "            **args_dann,\n",
    "            'module': DANNNetwork,\n",
    "            'module__domain_hidden': params_dann.get('module__domain_hidden', 64),\n",
    "            'module__grl_lambda': params_dann.get('module__grl_lambda', 1.0),\n",
    "        },\n",
    "        'dan_lambda': 0.05,  # inference-only, safe if different from train\n",
    "        **param_init,\n",
    "    }\n",
    "except NameError:\n",
    "    dann_loader_args = None\n",
    "\n",
    "models_grouped = {}\n",
    "\n",
    "for label, fname, model_cls in models:\n",
    "    print(f'Loading {label}: {fname}')\n",
    "\n",
    "    if model_cls.__name__ == 'CustomDANNRegressor':\n",
    "        if dann_loader_args is None:\n",
    "            raise NameError(\n",
    "                \"DANNNetwork not available. Run the DANN components cell before loading DANN models.\"\n",
    "            )\n",
    "        mdl = model_cls.load_model(cfg, fname, **dann_loader_args)\n",
    "    else:\n",
    "        mdl = model_cls.load_model(cfg, fname, **nn_loader_args)\n",
    "\n",
    "    mdl = mdl.set_params(device=device).to(device)\n",
    "\n",
    "    # Predict aggregated by measurement ID using the same dataset_test\n",
    "    y_pred_agg = mdl.aggrPredict(dataset_test[0])\n",
    "\n",
    "    assert len(y_pred_agg) == len(base), f'Length mismatch for {label}'\n",
    "    gdf = base.copy()\n",
    "    gdf['pred'] = y_pred_agg\n",
    "    models_grouped[label] = gdf\n",
    "\n",
    "# Plot mean annual and mean winter PMB for all models together\n",
    "plot_mean_pmb_multi_fixed_lines(models_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_obs_grid(models_grouped,\n",
    "                          ncols=3,\n",
    "                          point_size=45,\n",
    "                          region_name='CH->NOR'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    period_colors = {'annual': '#e31a1c', 'winter': '#1f78b4'}\n",
    "    labels = list(models_grouped.keys())\n",
    "    n_models = len(labels)\n",
    "    ncols_eff = min(ncols, n_models)\n",
    "    nrows = int(np.ceil(n_models / ncols_eff))\n",
    "\n",
    "    # Global axis limits\n",
    "    all_targets = np.concatenate(\n",
    "        [models_grouped[l].target.values for l in labels])\n",
    "    all_preds = np.concatenate([models_grouped[l].pred.values for l in labels])\n",
    "    gmin = np.nanmin([all_targets.min(), all_preds.min()])\n",
    "    gmax = np.nanmax([all_targets.max(), all_preds.max()])\n",
    "    pad = 0.05 * (gmax - gmin)\n",
    "    gmin -= pad\n",
    "    gmax += pad\n",
    "\n",
    "    fig, axes = plt.subplots(nrows,\n",
    "                             ncols_eff,\n",
    "                             figsize=(4.4 * ncols_eff, 4.6 * nrows),\n",
    "                             squeeze=False)\n",
    "    metrics_all = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        r, c = divmod(i, ncols_eff)\n",
    "        ax = axes[r][c]\n",
    "        df_plot = models_grouped[label]\n",
    "        df_plot = df_plot[df_plot.PERIOD.isin(['annual', 'winter'])].copy()\n",
    "\n",
    "        # Metrics per period\n",
    "        panel_metrics = {}\n",
    "        for period in ['annual', 'winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                y_t = sub.target.values\n",
    "                y_p = sub.pred.values\n",
    "                rmse = np.sqrt(np.mean((y_p - y_t)**2))\n",
    "                rho = np.corrcoef(y_t, y_p)[0, 1] if len(sub) > 1 else np.nan\n",
    "                panel_metrics[period] = (rmse, rho)\n",
    "\n",
    "        # Combined\n",
    "        y_t_all = df_plot.target.values\n",
    "        y_p_all = df_plot.pred.values\n",
    "        rmse_all = np.sqrt(np.mean((y_p_all - y_t_all)**2))\n",
    "        rho_all = np.corrcoef(y_t_all,\n",
    "                              y_p_all)[0, 1] if len(df_plot) > 1 else np.nan\n",
    "        panel_metrics['combined'] = (rmse_all, rho_all)\n",
    "        metrics_all[label] = panel_metrics\n",
    "\n",
    "        # Scatter (no per‑subplot point handles; handled globally)\n",
    "        for period in ['annual', 'winter']:\n",
    "            sub = df_plot[df_plot.PERIOD == period]\n",
    "            if len(sub):\n",
    "                ax.scatter(sub.target,\n",
    "                           sub.pred,\n",
    "                           s=point_size,\n",
    "                           alpha=0.65,\n",
    "                           edgecolor='none',\n",
    "                           color=period_colors[period])\n",
    "\n",
    "        # 1:1 line + handle for subplot legend\n",
    "        ax.plot([gmin, gmax], [gmin, gmax], 'k--', linewidth=1, alpha=0.55)\n",
    "\n",
    "        # Metric legend entries\n",
    "        def metric_handle(name, key):\n",
    "            rmse, rho = panel_metrics[key]\n",
    "            return Line2D([], [],\n",
    "                          linestyle='',\n",
    "                          label=f\"{name}: RMSE {rmse:.2f} m w.e., ρ:{rho:.2f}\",\n",
    "                          color='none')\n",
    "\n",
    "        metric_handles = [\n",
    "            metric_handle(\"Combined\", 'combined'),\n",
    "            *([metric_handle(\"Annual\", 'annual')]\n",
    "              if 'annual' in panel_metrics else []),\n",
    "            *([metric_handle(\"Winter\", 'winter')]\n",
    "              if 'winter' in panel_metrics else []),\n",
    "        ]\n",
    "\n",
    "        ax.set_xlim(gmin, gmax)\n",
    "        ax.set_ylim(gmin, gmax)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        ax.set_xlabel('Observed PMB', fontsize=9)\n",
    "        ax.set_ylabel('Predicted PMB', fontsize=9)\n",
    "\n",
    "        # Subplot legend: only 1:1 + metrics\n",
    "        ax.legend(handles=metric_handles,\n",
    "                  fontsize=7,\n",
    "                  loc='upper left',\n",
    "                  frameon=True,\n",
    "                  handlelength=1.2,\n",
    "                  borderpad=0.5)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(n_models, nrows * ncols_eff):\n",
    "        r, c = divmod(j, ncols_eff)\n",
    "        axes[r][c].axis('off')\n",
    "\n",
    "    # Global legend for point types (annual / winter)\n",
    "    global_point_handles = [\n",
    "        Line2D([0], [0],\n",
    "               marker='o',\n",
    "               linestyle='',\n",
    "               color=period_colors['annual'],\n",
    "               label='Annual'),\n",
    "        Line2D([0], [0],\n",
    "               marker='o',\n",
    "               linestyle='',\n",
    "               color=period_colors['winter'],\n",
    "               label='Winter')\n",
    "    ]\n",
    "    fig.legend(handles=global_point_handles,\n",
    "               loc='upper center',\n",
    "               ncol=2,\n",
    "               frameon=True,\n",
    "               fontsize=9,\n",
    "               bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "    return metrics_all\n",
    "\n",
    "\n",
    "pred_grid_metrics = plot_pred_vs_obs_grid(models_grouped,\n",
    "                                          ncols=3,\n",
    "                                          region_name='CH->NOR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
