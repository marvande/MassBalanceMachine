{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "import xarray as xr\n",
    "\n",
    "from regions.Norway_mb.scripts.config_NOR import *\n",
    "from regions.Norway_mb.scripts.dataset import get_stakes_data_NOR\n",
    "from regions.Norway_mb.scripts.utils import *\n",
    "\n",
    "from regions.Switzerland.scripts.dataset import process_or_load_data, get_CV_splits\n",
    "from regions.Switzerland.scripts.plotting import plot_predictions_summary, plot_individual_glacier_pred, plot_history_lstm, get_cmap_hex,plot_tsne_overlap, plot_feature_kde_overlap, pred_vs_truth_density, alpha_labels\n",
    "from regions.Switzerland.scripts.dataset import get_stakes_data, build_combined_LSTM_dataset, inspect_LSTM_sample, prepare_monthly_dfs_with_padding\n",
    "from regions.Switzerland.scripts.models import compute_seasonal_scores\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Regional Transfer Learning (Switzerland → Norway)\n",
    "\n",
    "This approach uses the Swiss dataset to try and model Norway glaciers.\n",
    "\n",
    "## Create Combined Swiss and Norway Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_NOR = get_stakes_data_NOR(cfg)\n",
    "data_CH = get_stakes_data(cfg)\n",
    "\n",
    "# Adjust dfs to match\n",
    "data_CH = data_CH.drop(\n",
    "    columns=['aspect_sgi', 'slope_sgi', 'topo_sgi', 'asvf', 'opns'],\n",
    "    errors='ignore')\n",
    "data_CH['GLACIER_ZONE'] = ''\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "\n",
    "print('Number NOR glaciers:', data_NOR['GLACIER'].nunique())\n",
    "print('NOR glaciers:', data_NOR['GLACIER'].unique())\n",
    "print('Number CH glaciers:', data_CH['GLACIER'].nunique())\n",
    "print('CH glaciers:', data_CH['GLACIER'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PERIOD column just in case\n",
    "data_NOR[\"PERIOD\"] = data_NOR[\"PERIOD\"].str.strip().str.lower()\n",
    "data_CH[\"PERIOD\"] = data_CH[\"PERIOD\"].str.strip().str.lower()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5), sharey=True)\n",
    "\n",
    "for ax, period in zip(axes, [\"annual\", \"winter\"]):\n",
    "    mb_nor = data_NOR.loc[data_NOR.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "    mb_ch = data_CH.loc[data_CH.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "\n",
    "    # Common bins for fair comparison\n",
    "    all_vals = np.concatenate([mb_nor, mb_ch])\n",
    "    bins = np.linspace(all_vals.min(), all_vals.max(), 21)\n",
    "\n",
    "    ax.hist(mb_nor, bins=bins, alpha=0.6, label=\"Norway\")\n",
    "    ax.hist(mb_ch, bins=bins, alpha=0.6, label=\"Switzerland\")\n",
    "\n",
    "    ax.axvline(mb_nor.mean(), linestyle=\"--\")\n",
    "    ax.axvline(mb_ch.mean(), linestyle=\"--\")\n",
    "\n",
    "    ax.set_title(f\"{period.capitalize()} Mass Balance\")\n",
    "    ax.set_xlabel(\"Mass balance [m w.e.]\")\n",
    "    ax.legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Number of measurements\")\n",
    "\n",
    "plt.suptitle(\"Seasonal Point Mass Balance Distribution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning NOR datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'csv_path':\n",
    "    os.path.join(cfg.dataPath, path_PMB_GLACIOCLIM_csv),\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_NOR_Alps.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_NOR_Alps.nc\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 50% FINE-TUNING SPLIT (NOR)\n",
    "# ---------------------------\n",
    "\n",
    "finetune_glaciers_50pct = [\n",
    "    'Engabreen', 'Storglombreen N', 'Moesevassbrea', 'Blaaisen', 'Blabreen',\n",
    "    'Harbardsbreen', 'Graasubreen', 'Svelgjabreen', 'Aalfotbreen',\n",
    "    'Rundvassbreen', 'Juvfonne', 'Storsteinsfjellbreen', 'Hansebreen',\n",
    "    'Vesledalsbreen', 'Vetlefjordbreen', 'Blomstoelskardsbreen',\n",
    "    'Vestre Memurubreen', 'Austre Memurubreen'\n",
    "]\n",
    "\n",
    "all_nor_glaciers = list(data_NOR['GLACIER'].unique())\n",
    "holdout_glaciers_50pct = [\n",
    "    g for g in all_nor_glaciers if g not in finetune_glaciers_50pct\n",
    "]\n",
    "\n",
    "data_NOR_ft_50pct = data_NOR[data_NOR['GLACIER'].isin(finetune_glaciers_50pct)].copy()\n",
    "data_NOR_holdout_50pct = data_NOR[~data_NOR['GLACIER'].isin(finetune_glaciers_50pct)].copy()\n",
    "\n",
    "print(f\"50% fine-tuning glaciers ({len(finetune_glaciers_50pct)}): {finetune_glaciers_50pct}\")\n",
    "print(f\"Hold-out glaciers ({len(holdout_glaciers_50pct)}): {holdout_glaciers_50pct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_NOR_50pct = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_NOR,\n",
    "    region_name=\"NOR\",\n",
    "    region_id=8,\n",
    "    paths=paths,\n",
    "    test_glaciers=holdout_glaciers_50pct,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    "    output_file_monthly='NOR_50pct_ft_dataset_monthly.csv',\n",
    "    output_file_monthly_aug='NOR_50pct_ft_dataset_monthly_Aug.csv'\n",
    ")\n",
    "\n",
    "df_ft_NOR_50pct = res_NOR_50pct[\"df_train\"]\n",
    "df_holdout_NOR_50pct = res_NOR_50pct[\"df_test\"]\n",
    "df_ft_NOR_50pct_Aug = res_NOR_50pct[\"df_train_aug\"]\n",
    "df_holdout_NOR_50pct_Aug = res_NOR_50pct[\"df_test_aug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "ds_ft_NOR_50pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_ft_NOR_50pct,\n",
    "    df_full=df_ft_NOR_50pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_NOR_50pct['months_head_pad'],\n",
    "    months_tail_pad=res_NOR_50pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True\n",
    ")\n",
    "\n",
    "ds_holdout_NOR_50pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_holdout_NOR_50pct,\n",
    "    df_full=df_holdout_NOR_50pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_NOR_50pct['months_head_pad'],\n",
    "    months_tail_pad=res_NOR_50pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True\n",
    ")\n",
    "\n",
    "assert set(df_ft_NOR_50pct.GLACIER.unique()) == set(finetune_glaciers_50pct)\n",
    "assert set(df_holdout_NOR_50pct.GLACIER.unique()).isdisjoint(set(finetune_glaciers_50pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5% FINE-TUNING SPLIT (NOR)\n",
    "# ---------------------------\n",
    "\n",
    "finetune_glaciers_5pct = [\n",
    "    'Tunsbergdalsbreen', 'Austre Memurubreen', 'Svartisheibreen',\n",
    "    'Bondhusbrea', 'Harbardsbreen', 'Moesevassbrea', 'Graasubreen'\n",
    "]\n",
    "\n",
    "# All remaining glaciers = holdout set\n",
    "all_nor_glaciers = list(data_NOR['GLACIER'].unique())\n",
    "holdout_glaciers_5pct = [\n",
    "    g for g in all_nor_glaciers if g not in finetune_glaciers_5pct\n",
    "]\n",
    "\n",
    "data_NOR_ft_5pct = data_NOR[data_NOR['GLACIER'].isin(finetune_glaciers_5pct)].copy()\n",
    "data_NOR_holdout_5pct = data_NOR[~data_NOR['GLACIER'].isin(finetune_glaciers_5pct)].copy()\n",
    "\n",
    "print(f\"5% fine-tuning glaciers ({len(finetune_glaciers_5pct)}): {finetune_glaciers_5pct}\")\n",
    "print(f\"Hold-out glaciers ({len(holdout_glaciers_5pct)}): {holdout_glaciers_5pct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_NOR_5pct = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_NOR,\n",
    "    region_name=\"NOR\",\n",
    "    region_id=8,\n",
    "    paths=paths,\n",
    "    test_glaciers=holdout_glaciers_5pct,   # holdout = test set\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    "    output_file_monthly='NOR_5pct_ft_dataset_monthly.csv',\n",
    "    output_file_monthly_aug='NOR_5pct_ft_dataset_monthly_Aug.csv'\n",
    ")\n",
    "\n",
    "df_ft_NOR_5pct = res_NOR_5pct[\"df_train\"]\n",
    "df_holdout_NOR_5pct = res_NOR_5pct[\"df_test\"]\n",
    "df_ft_NOR_5pct_Aug = res_NOR_5pct[\"df_train_aug\"]\n",
    "df_holdout_NOR_5pct_Aug = res_NOR_5pct[\"df_test_aug\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "ds_ft_NOR_5pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_ft_NOR_5pct,\n",
    "    df_full=df_ft_NOR_5pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_NOR_5pct['months_head_pad'],\n",
    "    months_tail_pad=res_NOR_5pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True\n",
    ")\n",
    "\n",
    "ds_holdout_NOR_5pct = build_combined_LSTM_dataset(\n",
    "    df_loss=df_holdout_NOR_5pct,\n",
    "    df_full=df_holdout_NOR_5pct_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_NOR_5pct['months_head_pad'],\n",
    "    months_tail_pad=res_NOR_5pct['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True\n",
    ")\n",
    "\n",
    "assert set(df_ft_NOR_5pct.GLACIER.unique()) == set(finetune_glaciers_5pct)\n",
    "assert set(df_holdout_NOR_5pct.GLACIER.unique()).isdisjoint(set(finetune_glaciers_5pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In sample CH dataset (used for the pretrained model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_CH = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_CH,\n",
    "    region_name=\"CH\",\n",
    "    region_id=11,\n",
    "    paths=paths,\n",
    "    test_glaciers=[],\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    "    add_pcsr=False,\n",
    "    output_file_monthly='CH_wgms_dataset_monthly_LSTM_IS.csv',\n",
    "    output_file_monthly_aug='CH_wgms_dataset_monthly_LSTM_Aug_IS.csv')\n",
    "\n",
    "df_train = res_CH[\"df_train\"]\n",
    "df_train_Aug = res_CH[\"df_train_aug\"]\n",
    "\n",
    "# Check that train set contains all glaciers\n",
    "existing_glaciers = set(df_train.GLACIER.unique())\n",
    "print('Number of glaciers in train data:', len(existing_glaciers))\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "ds_train_CH = build_combined_LSTM_dataset(\n",
    "    df_loss=df_train,  # hydrological-year POINT_BALANCE\n",
    "    df_full=df_train_Aug,  # August-anchored monthly sequences\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_CH['months_head_pad'],\n",
    "    months_tail_pad=res_CH['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_CH, val_idx_CH = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_CH), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CH model (w/o pcsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"Fm\": 8,\n",
    "    \"Fs\": 3,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"static_layers\": 1,\n",
    "    \"static_hidden\": 128,\n",
    "    \"static_dropout\": 0.3,\n",
    "    \"lr\": 0.0005,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"two_heads\": False,\n",
    "    \"head_dropout\": 0.0,\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_CH_model_{current_date}_IS_norm_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl_CH, val_dl_CH = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model_CH.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl_CH,\n",
    "        val_dl=val_dl_CH,\n",
    "        epochs=150,\n",
    "        lr=best_params['lr'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Load and evaluate on test\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model_CH.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained CH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"Fm\": 8,\n",
    "    \"Fs\": 3,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"static_layers\": 1,\n",
    "    \"static_hidden\": 128,\n",
    "    \"static_dropout\": 0.3,\n",
    "    \"lr\": 0.0005,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"two_heads\": False,\n",
    "    \"head_dropout\": 0.0,\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl, val_dl = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# DATALOADERS for NOR 5% fine-tune split\n",
    "# ---------------------------------------\n",
    "\n",
    "# pristine clone (fine-tune set)\n",
    "ds_ft_NOR_5pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_ft_NOR_5pct\n",
    ")\n",
    "\n",
    "# split indices on NOR 5%-ft\n",
    "train_idx_NOR_5pct, val_idx_NOR_5pct = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_ft_NOR_5pct_copy), val_ratio=0.2, seed=cfg.seed\n",
    ")\n",
    "\n",
    "# IMPORTANT: copy CH scalers -> NOR 5%-ft, then transform in-place\n",
    "ds_ft_NOR_5pct_copy.set_scalers_from(ds_train_CH_copy)\n",
    "ds_ft_NOR_5pct_copy.transform_inplace()\n",
    "\n",
    "# create loaders WITHOUT fitting scalers\n",
    "ft_train_dl_NOR_5pct, ft_val_dl_NOR_5pct = ds_ft_NOR_5pct_copy.make_loaders(\n",
    "    train_idx=train_idx_NOR_5pct,\n",
    "    val_idx=val_idx_NOR_5pct,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=False,  # <-- key!\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True\n",
    ")\n",
    "\n",
    "# holdout loader (NOR 5% split)\n",
    "ds_holdout_NOR_5pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_holdout_NOR_5pct\n",
    ")\n",
    "\n",
    "holdout_dl_NOR_5pct = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_holdout_NOR_5pct_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Safe” fine-tune for small NOR-ft set (freeze LSTM, train only static+head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Safe” fine-tune for small NOR-ft set (freeze LSTM, train only static+head):\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft.load_state_dict(state)\n",
    "\n",
    "# 1) freeze recurrent encoder\n",
    "for name, p in model_CH_ft.named_parameters():\n",
    "    if name.startswith(\"lstm.\"):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# 2) new optimizer on trainable params only (small LR)\n",
    "opt = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_CH_ft.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# 3) fine-tune\n",
    "history, best_val, best_state = model_CH_ft.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_5pct,\n",
    "    val_dl=ft_val_dl_NOR_5pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=8,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR.pt\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Full” fine-tune (unfreeze everything, very small LR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_2 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_2.load_state_dict(state)\n",
    "\n",
    "# unfreeze everything\n",
    "for p in model_CH_ft_2.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    model_CH_ft_2.parameters(),\n",
    "    lr=1e-5,  # smaller because we’re updating the LSTM too\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_2.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_5pct,\n",
    "    val_dl=ft_val_dl_NOR_5pct,\n",
    "    epochs=80,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_full.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practice: two-stage fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_3 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_3.load_state_dict(state)\n",
    "\n",
    "# Stage 1: freeze LSTM, tune heads\n",
    "for name, p in model_CH_ft_3.named_parameters():\n",
    "    p.requires_grad = not name.startswith(\"lstm.\")\n",
    "\n",
    "opt1 = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                model_CH_ft_3.parameters()),\n",
    "                         lr=2e-4,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "model_CH_ft_3.train_loop(device,\n",
    "                         ft_train_dl_NOR_5pct,\n",
    "                         ft_val_dl_NOR_5pct,\n",
    "                         epochs=20,\n",
    "                         optimizer=opt1,\n",
    "                         loss_fn=loss_fn,\n",
    "                         es_patience=5,\n",
    "                         save_best_path=\"models/tmp_stage1.pt\")\n",
    "\n",
    "# Stage 2: unfreeze all, very small LR\n",
    "for p in model_CH_ft_3.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt2 = torch.optim.AdamW(model_CH_ft_3.parameters(),\n",
    "                         lr=1e-5,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_3.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_5pct,\n",
    "    val_dl=ft_val_dl_NOR_5pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt2,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_2stage.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare fine-tuning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_scores(model_, holdout_dl, ds_holdout_copy):\n",
    "    test_metrics, df_preds = model_.evaluate_with_preds(device, holdout_dl, ds_holdout_copy)\n",
    "    scores_annual, scores_winter = compute_seasonal_scores(\n",
    "        df_preds, target_col=\"target\", pred_col=\"pred\"\n",
    "    )\n",
    "    return test_metrics, df_preds, scores_annual, scores_winter\n",
    "\n",
    "\n",
    "def add_metrics_box(ax, scores_annual, scores_winter, title=None):\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=18)\n",
    "\n",
    "    legend_txt = \"\\n\".join([\n",
    "        r\"$\\mathrm{RMSE_a}=%.2f$, $\\mathrm{RMSE_w}=%.2f$\" %\n",
    "        (scores_annual[\"rmse\"], scores_winter[\"rmse\"]),\n",
    "        r\"$\\mathrm{R^2_a}=%.2f$, $\\mathrm{R^2_w}=%.2f$\" %\n",
    "        (scores_annual[\"R2\"], scores_winter[\"R2\"]),\n",
    "        r\"$\\mathrm{Bias_a}=%.2f$, $\\mathrm{Bias_w}=%.2f$\" %\n",
    "        (scores_annual[\"Bias\"], scores_winter[\"Bias\"]),\n",
    "    ])\n",
    "    ax.text(\n",
    "        0.02, 0.98, legend_txt,\n",
    "        transform=ax.transAxes, va=\"top\",\n",
    "        fontsize=14,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6),\n",
    "    )\n",
    "    \n",
    "methods = [\n",
    "    (\"No fine-tune (CH→NOR)\", model_CH),\n",
    "    (\"Heads-only FT (freeze LSTM)\", model_CH_ft),\n",
    "    (\"Full FT (unfreeze all)\", model_CH_ft_2),\n",
    "    (\"Two-stage FT\", model_CH_ft_3),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, m in methods:\n",
    "    test_metrics, df_preds, s_a, s_w = eval_and_scores(m, holdout_dl_NOR_5pct, ds_holdout_NOR_5pct_copy)\n",
    "    results.append((name, df_preds, s_a, s_w))\n",
    "    print(name, \"| RMSE annual:\", test_metrics[\"RMSE_annual\"], \"| RMSE winter:\", test_metrics[\"RMSE_winter\"])\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (name, df_preds, s_a, s_w) in zip(axes, results):\n",
    "    pred_vs_truth_density(\n",
    "        ax,\n",
    "        df_preds,\n",
    "        s_a,\n",
    "        add_legend=False,\n",
    "        palette=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "        ax_xlim=(-14, 8),\n",
    "        ax_ylim=(-14, 8),\n",
    "    )\n",
    "    add_metrics_box(ax, s_a, s_w, title=name)\n",
    "\n",
    "fig.supxlabel(\"Observed PMB [m w.e.]\", fontsize=20)\n",
    "fig.supylabel(\"Modeled PMB [m w.e.]\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# DATALOADERS for NOR 50% fine-tune split\n",
    "# ---------------------------------------\n",
    "\n",
    "# pristine clone (fine-tune set)\n",
    "ds_ft_NOR_50pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_ft_NOR_50pct\n",
    ")\n",
    "\n",
    "# split indices on NOR 50%-ft\n",
    "train_idx_NOR_50pct, val_idx_NOR_50pct = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_ft_NOR_50pct_copy), val_ratio=0.2, seed=cfg.seed\n",
    ")\n",
    "\n",
    "# IMPORTANT: copy CH scalers -> NOR 50%-ft, then transform in-place\n",
    "ds_ft_NOR_50pct_copy.set_scalers_from(ds_train_CH_copy)\n",
    "ds_ft_NOR_50pct_copy.transform_inplace()\n",
    "\n",
    "# create loaders WITHOUT fitting scalers\n",
    "ft_train_dl_NOR_50pct, ft_val_dl_NOR_50pct = ds_ft_NOR_50pct_copy.make_loaders(\n",
    "    train_idx=train_idx_NOR_50pct,\n",
    "    val_idx=val_idx_NOR_50pct,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=False,  # <-- key!\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True\n",
    ")\n",
    "\n",
    "# holdout loader (NOR 50% split)\n",
    "ds_holdout_NOR_50pct_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_holdout_NOR_50pct\n",
    ")\n",
    "\n",
    "holdout_dl_NOR_50pct = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_holdout_NOR_50pct_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Safe” fine-tune for small NOR-ft set (freeze LSTM, train only static+head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Safe” fine-tune for small NOR-ft set (freeze LSTM, train only static+head):\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft.load_state_dict(state)\n",
    "\n",
    "# 1) freeze recurrent encoder\n",
    "for name, p in model_CH_ft.named_parameters():\n",
    "    if name.startswith(\"lstm.\"):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# 2) new optimizer on trainable params only (small LR)\n",
    "opt = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_CH_ft.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# 3) fine-tune\n",
    "history, best_val, best_state = model_CH_ft.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_50pct,\n",
    "    val_dl=ft_val_dl_NOR_50pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=8,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_50pct.pt\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Full” fine-tune (unfreeze everything, very small LR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_2 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_2.load_state_dict(state)\n",
    "\n",
    "# unfreeze everything\n",
    "for p in model_CH_ft_2.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    model_CH_ft_2.parameters(),\n",
    "    lr=1e-5,  # smaller because we’re updating the LSTM too\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_2.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_50pct,\n",
    "    val_dl=ft_val_dl_NOR_50pct,\n",
    "    epochs=80,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_full_50pct.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practice: two-stage fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_3 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_3.load_state_dict(state)\n",
    "\n",
    "# Stage 1: freeze LSTM, tune heads\n",
    "for name, p in model_CH_ft_3.named_parameters():\n",
    "    p.requires_grad = not name.startswith(\"lstm.\")\n",
    "\n",
    "opt1 = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                model_CH_ft_3.parameters()),\n",
    "                         lr=2e-4,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "model_CH_ft_3.train_loop(device,\n",
    "                         ft_train_dl_NOR_50pct,\n",
    "                         ft_val_dl_NOR_50pct,\n",
    "                         epochs=20,\n",
    "                         optimizer=opt1,\n",
    "                         loss_fn=loss_fn,\n",
    "                         es_patience=5,\n",
    "                         save_best_path=\"models/tmp_stage1.pt\")\n",
    "\n",
    "# Stage 2: unfreeze all, very small LR\n",
    "for p in model_CH_ft_3.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt2 = torch.optim.AdamW(model_CH_ft_3.parameters(),\n",
    "                         lr=1e-5,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_3.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl_NOR_50pct,\n",
    "    val_dl=ft_val_dl_NOR_50pct,\n",
    "    epochs=60,\n",
    "    optimizer=opt2,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_2stage_50pct.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare fine-tuning methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    (\"No fine-tune (CH→NOR)\", model_CH),\n",
    "    (\"Heads-only FT (freeze LSTM)\", model_CH_ft),\n",
    "    (\"Full FT (unfreeze all)\", model_CH_ft_2),\n",
    "    (\"Two-stage FT\", model_CH_ft_3),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, m in methods:\n",
    "    test_metrics, df_preds, s_a, s_w = eval_and_scores(m, holdout_dl_NOR_50pct, ds_holdout_NOR_50pct_copy)\n",
    "    results.append((name, df_preds, s_a, s_w))\n",
    "    print(name, \"| RMSE annual:\", test_metrics[\"RMSE_annual\"], \"| RMSE winter:\", test_metrics[\"RMSE_winter\"])\n",
    "    \n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (name, df_preds, s_a, s_w) in zip(axes, results):\n",
    "    pred_vs_truth_density(\n",
    "        ax,\n",
    "        df_preds,\n",
    "        s_a,\n",
    "        add_legend=False,\n",
    "        palette=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "        ax_xlim=(-14, 8),\n",
    "        ax_ylim=(-14, 8),\n",
    "    )\n",
    "    add_metrics_box(ax, s_a, s_w, title=name)\n",
    "\n",
    "fig.supxlabel(\"Observed PMB [m w.e.]\", fontsize=20)\n",
    "fig.supylabel(\"Modeled PMB [m w.e.]\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual glaciers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average areas per glaciers\n",
    "gl_per_el = data_NOR[data_NOR.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "shapefile_path = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                              \"08_rgi60_Scandinavia.shp\")\n",
    "gl_area = get_gl_area_NOR(data_NOR, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, df_preds, s_a, s_w = eval_and_scores(model_CH_ft_3, holdout_dl_NOR_50pct, ds_holdout_NOR_50pct_copy)\n",
    "\n",
    "df_preds['gl_elv'] = df_preds['GLACIER'].map(gl_per_el)\n",
    "test_gl_per_el = gl_per_el[holdout_glaciers_50pct].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(5, 3, figsize=(35, 30), sharex=True)\n",
    "\n",
    "subplot_labels = alpha_labels(len(holdout_glaciers_50pct))\n",
    "\n",
    "axs = plot_individual_glacier_pred(df_preds,\n",
    "                                   axs=axs,\n",
    "                                   subplot_labels=subplot_labels,\n",
    "                                   color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "                                   color_winter=mbm.plots.COLOR_WINTER,\n",
    "                                   custom_order=test_gl_per_el,\n",
    "                                   gl_area=gl_area, \n",
    "                                   ax_xlim=(-14, 8),\n",
    "                                      ax_ylim=(-14, 8))\n",
    "\n",
    "fig.supxlabel('Observed PMB [m w.e.]', fontsize=20, y=0.06)\n",
    "fig.supylabel('Modeled PMB [m w.e.]', fontsize=20, x=0.09)\n",
    "legend_scatter_annual = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=mbm.plots.COLOR_ANNUAL,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Annual')\n",
    "\n",
    "legend_scatter_winter = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=mbm.plots.COLOR_WINTER,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Winter')\n",
    "\n",
    "handles = [legend_scatter_annual, legend_scatter_winter]\n",
    "fig.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 0.05),\n",
    "           ncol=4,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
