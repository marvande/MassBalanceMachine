{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import massbalancemachine as mbm\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch \n",
    "from matplotlib.lines import Line2D\n",
    "import xarray as xr\n",
    "\n",
    "from regions.Norway_mb.scripts.config_NOR import *\n",
    "from regions.Norway_mb.scripts.dataset import get_stakes_data_NOR\n",
    "from regions.Norway_mb.scripts.utils import *\n",
    "\n",
    "from regions.Switzerland.scripts.dataset import process_or_load_data, get_CV_splits\n",
    "from regions.Switzerland.scripts.plotting import plot_predictions_summary, plot_individual_glacier_pred, plot_history_lstm, get_cmap_hex,plot_tsne_overlap, plot_feature_kde_overlap\n",
    "from regions.Switzerland.scripts.dataset import get_stakes_data, build_combined_LSTM_dataset, inspect_LSTM_sample, prepare_monthly_dfs_with_padding\n",
    "from regions.Switzerland.scripts.models import compute_seasonal_scores\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig()\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Regional Transfer Learning (Switzerland → Norway)\n",
    "\n",
    "This approach uses the Swiss dataset to try and model Norway glaciers.\n",
    "\n",
    "### Create Combined Swiss and Norway Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in\n",
    "data_NOR = get_stakes_data_NOR(cfg)\n",
    "data_CH = get_stakes_data(cfg)\n",
    "\n",
    "# Adjust dfs to match\n",
    "data_CH = data_CH.drop(\n",
    "    columns=['aspect_sgi', 'slope_sgi', 'topo_sgi', 'asvf', 'opns'],\n",
    "    errors='ignore')\n",
    "data_CH['GLACIER_ZONE'] = ''\n",
    "data_CH['DATA_MODIFICATION'] = ''\n",
    "\n",
    "print('Number NOR glaciers:', data_NOR['GLACIER'].nunique())\n",
    "print('NOR glaciers:', data_NOR['GLACIER'].unique())\n",
    "print('Number CH glaciers:', data_CH['GLACIER'].nunique())\n",
    "print('CH glaciers:', data_CH['GLACIER'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PERIOD column just in case\n",
    "data_NOR[\"PERIOD\"] = data_NOR[\"PERIOD\"].str.strip().str.lower()\n",
    "data_CH[\"PERIOD\"] = data_CH[\"PERIOD\"].str.strip().str.lower()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5), sharey=True)\n",
    "\n",
    "for ax, period in zip(axes, [\"annual\", \"winter\"]):\n",
    "    mb_nor = data_NOR.loc[data_NOR.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "    mb_ch = data_CH.loc[data_CH.PERIOD == period, \"POINT_BALANCE\"].dropna()\n",
    "\n",
    "    # Common bins for fair comparison\n",
    "    all_vals = np.concatenate([mb_nor, mb_ch])\n",
    "    bins = np.linspace(all_vals.min(), all_vals.max(), 21)\n",
    "\n",
    "    ax.hist(mb_nor, bins=bins, alpha=0.6, label=\"Norway\")\n",
    "    ax.hist(mb_ch, bins=bins, alpha=0.6, label=\"Switzerland\")\n",
    "\n",
    "    ax.axvline(mb_nor.mean(), linestyle=\"--\")\n",
    "    ax.axvline(mb_ch.mean(), linestyle=\"--\")\n",
    "\n",
    "    ax.set_title(f\"{period.capitalize()} Mass Balance\")\n",
    "    ax.set_xlabel(\"Mass balance [m w.e.]\")\n",
    "    ax.legend()\n",
    "\n",
    "axes[0].set_ylabel(\"Number of measurements\")\n",
    "\n",
    "plt.suptitle(\"Seasonal Point Mass Balance Distribution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning NOR datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHLY_COLS = [\n",
    "    't2m',\n",
    "    'tp',\n",
    "    'slhf',\n",
    "    'sshf',\n",
    "    'ssrd',\n",
    "    'fal',\n",
    "    'str',\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "]\n",
    "STATIC_COLS = ['aspect', 'slope', 'svf']\n",
    "\n",
    "feature_columns = MONTHLY_COLS + STATIC_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50%\n",
    "\n",
    "finetune_glaciers = [\n",
    "    'Engabreen', 'Storglombreen N', 'Moesevassbrea', 'Blaaisen', 'Blabreen',\n",
    "    'Harbardsbreen', 'Graasubreen', 'Svelgjabreen', 'Aalfotbreen',\n",
    "    'Rundvassbreen', 'Juvfonne', 'Storsteinsfjellbreen', 'Hansebreen',\n",
    "    'Vesledalsbreen', 'Vetlefjordbreen', 'Blomstoelskardsbreen',\n",
    "    'Vestre Memurubreen', 'Austre Memurubreen'\n",
    "]\n",
    "\n",
    "# Test glaciers (all remaining Norway glaciers)\n",
    "all_france_glaciers = list(data_NOR['GLACIER'].unique())\n",
    "holdout_glaciers = [\n",
    "    g for g in all_france_glaciers if g not in finetune_glaciers\n",
    "]\n",
    "\n",
    "data_NOR_ft = data_NOR[data_NOR['GLACIER'].isin(finetune_glaciers)].copy()\n",
    "data_NOR_holdout = data_NOR[~data_NOR['GLACIER'].isin(finetune_glaciers)].copy(\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(finetune_glaciers)}): {finetune_glaciers}\")\n",
    "print(f\"Hold-out glaciers ({len(holdout_glaciers)}): {holdout_glaciers}\")\n",
    "\n",
    "paths = {\n",
    "    'csv_path':\n",
    "    os.path.join(cfg.dataPath, path_PMB_GLACIOCLIM_csv),\n",
    "    'era5_climate_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_monthly_averaged_data_NOR_Alps.nc\"),\n",
    "    'geopotential_data':\n",
    "    os.path.join(cfg.dataPath, path_ERA5_raw,\n",
    "                 \"era5_geopotential_pressure_NOR_Alps.nc\")\n",
    "}\n",
    "\n",
    "res_ft = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_NOR,\n",
    "    region_name=\"NOR\",\n",
    "    region_id=8,\n",
    "    paths=paths,\n",
    "    test_glaciers=holdout_glaciers,\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=True,\n",
    "    output_file_monthly='NOR_ft_wgms_dataset_monthly.csv',\n",
    "    output_file_monthly_aug='NOR_ft_wgms_dataset_monthly_Aug.csv')\n",
    "\n",
    "df_ft_NOR = res_ft[\"df_train\"]\n",
    "df_holdout_NOR = res_ft[\"df_test\"]\n",
    "df_ft_NOR_Aug = res_ft[\"df_train_aug\"]\n",
    "df_holdout_NOR_Aug = res_ft[\"df_test_aug\"]\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "\n",
    "ds_ft_NOR = build_combined_LSTM_dataset(\n",
    "    df_loss=df_ft_NOR,\n",
    "    df_full=df_ft_NOR_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_ft['months_head_pad'],\n",
    "    months_tail_pad=res_ft['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "ds_holdout_NOR = build_combined_LSTM_dataset(\n",
    "    df_loss=df_holdout_NOR,\n",
    "    df_full=df_holdout_NOR_Aug,\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_ft['months_head_pad'],\n",
    "    months_tail_pad=res_ft['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "# train_idx_NOR, val_idx_NOR = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "#     len(ds_ft_NOR), val_ratio=0.2, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(df_ft_NOR.GLACIER.unique()) == set(finetune_glaciers)\n",
    "assert set(df_holdout_NOR.GLACIER.unique()).isdisjoint(set(finetune_glaciers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In sample CH dataset (used for the pretrained model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_CH = prepare_monthly_dfs_with_padding(\n",
    "    cfg=cfg,\n",
    "    df_region=data_CH,\n",
    "    region_name=\"CH\",\n",
    "    region_id=11,\n",
    "    paths=paths,\n",
    "    test_glaciers=[],\n",
    "    vois_climate=VOIS_CLIMATE,\n",
    "    vois_topographical=VOIS_TOPOGRAPHICAL,\n",
    "    run_flag=False,\n",
    "    add_pcsr=False,\n",
    "    output_file_monthly='CH_wgms_dataset_monthly_LSTM_IS.csv',\n",
    "    output_file_monthly_aug='CH_wgms_dataset_monthly_LSTM_Aug_IS.csv')\n",
    "\n",
    "df_train = res_CH[\"df_train\"]\n",
    "df_train_Aug = res_CH[\"df_train_aug\"]\n",
    "\n",
    "# Check that train set contains all glaciers\n",
    "existing_glaciers = set(df_train.GLACIER.unique())\n",
    "print('Number of glaciers in train data:', len(existing_glaciers))\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "ds_train_CH = build_combined_LSTM_dataset(\n",
    "    df_loss=df_train,  # hydrological-year POINT_BALANCE\n",
    "    df_full=df_train_Aug,  # August-anchored monthly sequences\n",
    "    monthly_cols=MONTHLY_COLS,\n",
    "    static_cols=STATIC_COLS,\n",
    "    months_head_pad=res_CH['months_head_pad'],\n",
    "    months_tail_pad=res_CH['months_tail_pad'],\n",
    "    normalize_target=True,\n",
    "    expect_target=True)\n",
    "\n",
    "train_idx_CH, val_idx_CH = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_train_CH), val_ratio=0.2, seed=cfg.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CH model (w/o pcsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"Fm\": 8,\n",
    "    \"Fs\": 3,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"static_layers\": 1,\n",
    "    \"static_hidden\": 128,\n",
    "    \"static_dropout\": 0.3,\n",
    "    \"lr\": 0.0005,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"two_heads\": False,\n",
    "    \"head_dropout\": 0.0,\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model_filename = f\"models/lstm_CH_model_{current_date}_IS_norm_y_past.pt\"\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl_CH, val_dl_CH = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "TRAIN = False\n",
    "if TRAIN:\n",
    "    if os.path.exists(model_filename): os.remove(model_filename)\n",
    "\n",
    "    history, best_val, best_state = model_CH.train_loop(\n",
    "        device=device,\n",
    "        train_dl=train_dl_CH,\n",
    "        val_dl=val_dl_CH,\n",
    "        epochs=150,\n",
    "        lr=best_params['lr'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        clip_val=1,\n",
    "        # scheduler\n",
    "        sched_factor=0.5,\n",
    "        sched_patience=6,\n",
    "        sched_threshold=0.01,\n",
    "        sched_threshold_mode=\"rel\",\n",
    "        sched_cooldown=1,\n",
    "        sched_min_lr=1e-6,\n",
    "        # early stopping\n",
    "        es_patience=15,\n",
    "        es_min_delta=1e-4,\n",
    "        # logging\n",
    "        log_every=5,\n",
    "        verbose=True,\n",
    "        # checkpoint\n",
    "        save_best_path=model_filename,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    plot_history_lstm(history)\n",
    "\n",
    "ds_test_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "test_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_test_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed)\n",
    "\n",
    "# Load and evaluate on test\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)\n",
    "test_metrics, test_df_preds = model_CH.evaluate_with_preds(\n",
    "    device, test_dl, ds_test_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(test_df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=test_df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained CH model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"Fm\": 8,\n",
    "    \"Fs\": 3,\n",
    "    \"hidden_size\": 96,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"static_layers\": 1,\n",
    "    \"static_hidden\": 128,\n",
    "    \"static_dropout\": 0.3,\n",
    "    \"lr\": 0.0005,\n",
    "    \"weight_decay\": 1e-05,\n",
    "    \"loss_name\": \"neutral\",\n",
    "    \"two_heads\": False,\n",
    "    \"head_dropout\": 0.0,\n",
    "    \"loss_spec\": None,\n",
    "}\n",
    "\n",
    "# --- loaders (fit scalers on TRAIN, apply to whole ds_train) ---\n",
    "ds_train_CH_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_train_CH)\n",
    "\n",
    "train_dl, val_dl = ds_train_CH_copy.make_loaders(\n",
    "    train_idx=train_idx_CH,\n",
    "    val_idx=val_idx_CH,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=\n",
    "    True,  # fit scalers on TRAIN and transform Xm/Xs/y in-place\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # use weighted sampler for training\n",
    ")\n",
    "\n",
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH = mbm.models.LSTM_MB.build_model_from_params(cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation and training NOR set:\n",
    "# pristine clone\n",
    "ds_ft_NOR_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_ft_NOR)\n",
    "\n",
    "# split indices on NOR-ft\n",
    "train_idx_NOR, val_idx_NOR = mbm.data_processing.MBSequenceDataset.split_indices(\n",
    "    len(ds_ft_NOR_copy), val_ratio=0.2, seed=cfg.seed)\n",
    "\n",
    "# IMPORTANT: copy CH scalers -> NOR, then transform NOR in-place\n",
    "ds_ft_NOR_copy.set_scalers_from(ds_train_CH_copy)\n",
    "ds_ft_NOR_copy.transform_inplace()\n",
    "\n",
    "# now create loaders WITHOUT fitting scalers\n",
    "ft_train_dl, ft_val_dl = ds_ft_NOR_copy.make_loaders(\n",
    "    train_idx=train_idx_NOR,\n",
    "    val_idx=val_idx_NOR,\n",
    "    batch_size_train=64,\n",
    "    batch_size_val=128,\n",
    "    seed=cfg.seed,\n",
    "    fit_and_transform=False,  # <-- key!\n",
    "    shuffle_train=True,\n",
    "    use_weighted_sampler=True  # optional\n",
    ")\n",
    "\n",
    "# holdout loader:\n",
    "ds_holdout_NOR_copy = mbm.data_processing.MBSequenceDataset._clone_untransformed_dataset(\n",
    "    ds_holdout_NOR)\n",
    "holdout_dl = mbm.data_processing.MBSequenceDataset.make_test_loader(\n",
    "    ds_holdout_NOR_copy, ds_train_CH_copy, batch_size=128, seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics, df_preds = model_CH.evaluate_with_preds(device, holdout_dl,\n",
    "                                                      ds_holdout_NOR_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Safe” fine-tune for small NOR-ft set (freeze LSTM, train only static+head):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft.load_state_dict(state)\n",
    "\n",
    "# 1) freeze recurrent encoder\n",
    "for name, p in model_CH_ft.named_parameters():\n",
    "    if name.startswith(\"lstm.\"):\n",
    "        p.requires_grad = False\n",
    "\n",
    "# 2) new optimizer on trainable params only (small LR)\n",
    "opt = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model_CH_ft.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# 3) fine-tune\n",
    "history, best_val, best_state = model_CH_ft.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl,\n",
    "    val_dl=ft_val_dl,\n",
    "    epochs=60,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=8,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR.pt\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "test_metrics, df_preds = model_CH_ft.evaluate_with_preds(\n",
    "    device, holdout_dl, ds_holdout_NOR_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Full” fine-tune (unfreeze everything, very small LR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_2 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_2.load_state_dict(state)\n",
    "\n",
    "# unfreeze everything\n",
    "for p in model_CH_ft_2.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    model_CH_ft_2.parameters(),\n",
    "    lr=1e-5,  # smaller because we’re updating the LSTM too\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_2.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl,\n",
    "    val_dl=ft_val_dl,\n",
    "    epochs=80,\n",
    "    optimizer=opt,\n",
    "    clip_val=1.0,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_full.pt\",\n",
    ")\n",
    "\n",
    "test_metrics, df_preds = model_CH_ft_2.evaluate_with_preds(\n",
    "    device, holdout_dl, ds_holdout_NOR_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practice: two-stage fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build model, resolve loss, train, reload best ---\n",
    "model_CH_ft_3 = mbm.models.LSTM_MB.build_model_from_params(\n",
    "    cfg, best_params, device)\n",
    "loss_fn = mbm.models.LSTM_MB.resolve_loss_fn(best_params)\n",
    "\n",
    "# Load\n",
    "model_filename = f\"models/lstm_CH_model_2026-02-09_IS_norm_y_past.pt\"\n",
    "state = torch.load(model_filename, map_location=device)\n",
    "model_CH_ft_3.load_state_dict(state)\n",
    "\n",
    "# Stage 1: freeze LSTM, tune heads\n",
    "for name, p in model_CH_ft_3.named_parameters():\n",
    "    p.requires_grad = not name.startswith(\"lstm.\")\n",
    "\n",
    "opt1 = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                model_CH_ft_3.parameters()),\n",
    "                         lr=2e-4,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "model_CH_ft_3.train_loop(device,\n",
    "                         ft_train_dl,\n",
    "                         ft_val_dl,\n",
    "                         epochs=20,\n",
    "                         optimizer=opt1,\n",
    "                         loss_fn=loss_fn,\n",
    "                         es_patience=5,\n",
    "                         save_best_path=\"models/tmp_stage1.pt\")\n",
    "\n",
    "# Stage 2: unfreeze all, very small LR\n",
    "for p in model_CH_ft_3.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt2 = torch.optim.AdamW(model_CH_ft_3.parameters(),\n",
    "                         lr=1e-5,\n",
    "                         weight_decay=best_params[\"weight_decay\"])\n",
    "\n",
    "history, best_val, best_state = model_CH_ft_3.train_loop(\n",
    "    device=device,\n",
    "    train_dl=ft_train_dl,\n",
    "    val_dl=ft_val_dl,\n",
    "    epochs=60,\n",
    "    optimizer=opt2,\n",
    "    loss_fn=loss_fn,\n",
    "    es_patience=10,\n",
    "    save_best_path=\"models/lstm_finetuned_CH_to_NOR_2stage.pt\",\n",
    ")\n",
    "\n",
    "test_metrics, df_preds = model_CH_ft_3.evaluate_with_preds(\n",
    "    device, holdout_dl, ds_holdout_NOR_copy)\n",
    "test_rmse_a, test_rmse_w = test_metrics['RMSE_annual'], test_metrics[\n",
    "    'RMSE_winter']\n",
    "\n",
    "print('Test RMSE annual: {:.3f} | winter: {:.3f}'.format(\n",
    "    test_rmse_a, test_rmse_w))\n",
    "\n",
    "scores_annual, scores_winter = compute_seasonal_scores(df_preds,\n",
    "                                                       target_col='target',\n",
    "                                                       pred_col='pred')\n",
    "\n",
    "print(\"Annual scores:\", scores_annual)\n",
    "print(\"Winter scores:\", scores_winter)\n",
    "\n",
    "fig = plot_predictions_summary(\n",
    "    grouped_ids=df_preds,\n",
    "    scores_annual=scores_annual,\n",
    "    scores_winter=scores_winter,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_per_el = data_NOR[data_NOR.PERIOD == 'annual'].groupby(\n",
    "    ['GLACIER'])['POINT_ELEVATION'].mean()\n",
    "gl_per_el = gl_per_el.sort_values(ascending=False)\n",
    "test_gl_per_el = gl_per_el[holdout_glaciers].sort_values().index\n",
    "\n",
    "shapefile_path = os.path.join(cfg.dataPath, \"RGI_v6/RGI_08_Scandinavia\",\n",
    "                              \"08_rgi60_Scandinavia.shp\")\n",
    "\n",
    "gl_area = get_gl_area_NOR(data_NOR, shapefile_path)\n",
    "\n",
    "df_preds['gl_elv'] = df_preds['GLACIER'].map(gl_per_el)\n",
    "test_gl_per_el = gl_per_el[holdout_glaciers].sort_values().index\n",
    "\n",
    "fig, axs = plt.subplots(5, 3, figsize=(30, 30), sharex=True)\n",
    "\n",
    "subplot_labels = [\n",
    "    '(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)', '(i)'\n",
    "]\n",
    "\n",
    "axs = plot_individual_glacier_pred(\n",
    "    df_preds,\n",
    "    axs=axs,\n",
    "    subplot_labels=subplot_labels,\n",
    "    color_annual=mbm.plots.COLOR_ANNUAL,\n",
    "    color_winter=mbm.plots.COLOR_WINTER,\n",
    "    custom_order=test_gl_per_el,\n",
    "    gl_area=gl_area,\n",
    "    ax_xlim=(-14, 6),\n",
    "    ax_ylim=(-14, 6),\n",
    ")\n",
    "\n",
    "fig.supxlabel('Observed PMB [m w.e.]', fontsize=20, y=0.06)\n",
    "fig.supylabel('Modeled PMB [m w.e.]', fontsize=20, x=0.09)\n",
    "# two distinct handles\n",
    "legend_scatter_annual = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=mbm.plots.COLOR_ANNUAL,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Annual')\n",
    "\n",
    "legend_scatter_winter = Line2D([0], [0],\n",
    "                               marker='o',\n",
    "                               linestyle='None',\n",
    "                               linewidth=0,\n",
    "                               markersize=10,\n",
    "                               markerfacecolor=mbm.plots.COLOR_WINTER,\n",
    "                               markeredgecolor='k',\n",
    "                               markeredgewidth=0.8,\n",
    "                               label='Winter')\n",
    "\n",
    "# if you already have other handles (e.g., bands/means), append these:\n",
    "# handles = existing_handles + [legend_scatter_annual, legend_scatter_winter]\n",
    "handles = [legend_scatter_annual, legend_scatter_winter]\n",
    "\n",
    "# You can let matplotlib use the labels from the handles; no need to pass `labels=...`\n",
    "fig.legend(handles=handles,\n",
    "           loc='upper center',\n",
    "           bbox_to_anchor=(0.5, 0.05),\n",
    "           ncol=4,\n",
    "           fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nigardsbreen = data_NOR[data_NOR.GLACIER == 'Nigardsbreen']\n",
    "nigardsbreen_w = nigardsbreen[nigardsbreen.PERIOD == 'winter']\n",
    "\n",
    "df_w = df_preds[(df_preds[\"GLACIER\"] == \"Nigardsbreen\")\n",
    "                & (df_preds[\"PERIOD\"] == \"winter\")].copy()\n",
    "\n",
    "df_w = df_w.sort_values(\"YEAR\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sc = plt.scatter(df_w[\"target\"],\n",
    "                 df_w[\"pred\"],\n",
    "                 c=df_w[\"YEAR\"],\n",
    "                 cmap=\"viridis\",\n",
    "                 s=80)\n",
    "\n",
    "plt.colorbar(sc, label=\"Year\")\n",
    "plt.xlabel(\"Observed Winter MB\")\n",
    "plt.ylabel(\"Predicted Winter MB\")\n",
    "plt.title(\"Nigardsbreen – Winter MB (Observed vs Predicted)\")\n",
    "plt.axline((0, 0), slope=1, linestyle=\"--\", color=\"gray\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- your selection logic (as you already have it) ---\n",
    "ids_weird_points = df_w[(df_w.target < 0.5) & (df_w.target > -0.5)].ID.values\n",
    "point_ids_weird_points = df_holdout_NOR[df_holdout_NOR.ID.isin(\n",
    "    ids_weird_points)].POINT_ID.unique()\n",
    "df_outliers = data_NOR_holdout[data_NOR_holdout.POINT_ID.isin(\n",
    "    point_ids_weird_points)]\n",
    "\n",
    "# coordinates to plot\n",
    "coords = df_outliers[['POINT_LAT', 'POINT_LON', 'POINT_ID']].drop_duplicates()\n",
    "\n",
    "# --- open RGI zarr and plot raster ---\n",
    "path_RGI = cfg.dataPath + 'RGI_v6/RGI_08_Scandinavia/xr_masked_grids/'\n",
    "rgi_nigardsbreen = df_outliers.RGIId.unique()[0]\n",
    "\n",
    "ds = xr.open_zarr(path_RGI + f\"{rgi_nigardsbreen}.zarr\")\n",
    "\n",
    "# choose what you want underlay: masked_aspect (as you did) or masked_dem if available\n",
    "da = ds[\"masked_aspect\"]  # or ds[\"masked_dem\"] if that's your DEM variable\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# xarray plot on the axis (important: pass ax=...)\n",
    "da.plot(ax=ax, add_colorbar=True)\n",
    "\n",
    "# --- overlay points (lon/lat) ---\n",
    "ax.scatter(coords[\"POINT_LON\"].values,\n",
    "           coords[\"POINT_LAT\"].values,\n",
    "           s=60,\n",
    "           marker=\"o\",\n",
    "           facecolors=\"none\",\n",
    "           edgecolors=\"red\",\n",
    "           linewidths=1.8,\n",
    "           zorder=10,\n",
    "           label=f\"Weird points (n={len(coords)})\")\n",
    "\n",
    "# Optional: annotate point IDs (can get cluttered)\n",
    "for _, r in coords.iterrows():\n",
    "    ax.text(r[\"POINT_LON\"],\n",
    "            r[\"POINT_LAT\"],\n",
    "            str(r[\"POINT_ID\"]),\n",
    "            fontsize=8,\n",
    "            ha=\"left\",\n",
    "            va=\"bottom\",\n",
    "            zorder=11)\n",
    "\n",
    "ax.set_title(f\"{rgi_nigardsbreen} — Outlier points over {da.name}\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_points_df = df_w[(df_w.target < 0.5) & (df_w.target > -0.5)]\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sc = plt.scatter(weird_points_df[\"target\"],\n",
    "                 weird_points_df[\"pred\"],\n",
    "                 c=weird_points_df[\"YEAR\"],\n",
    "                 cmap=\"viridis\",\n",
    "                 s=80)\n",
    "\n",
    "plt.colorbar(sc, label=\"Year\")\n",
    "plt.xlabel(\"Observed Winter MB\")\n",
    "plt.ylabel(\"Predicted Winter MB\")\n",
    "plt.title(\"Nigardsbreen – Winter MB (Observed vs Predicted)\")\n",
    "plt.axline((0, 0), slope=1, linestyle=\"--\", color=\"gray\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
