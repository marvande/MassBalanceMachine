{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modelling for Glacier Mass Balance Prediction\n",
    "\n",
    "This notebook implements **neural network models** for predicting glacier point mass balance measurements using meteorological and topographical features. The notebook supports two main modelling approaches:\n",
    "\n",
    "##  **Modelling Strategies**\n",
    "\n",
    "### 1. **Regional Modelling (Norway Only)**\n",
    "- Train and test exclusively on Norwegian glacier data\n",
    "- Uses glacier-based train/test splits to ensure spatial generalization\n",
    "\n",
    "### 2. **Cross-Regional Modelling (Switzerland → Norway)**\n",
    "- Combines data from Swiss glaciers and Norway glaciers\n",
    "- Can be used as either a cross-regional model, training exclusively on Swiss data and testing on Norwegian data\n",
    "- or as a baseline for transfer learning models, training on Swiss data and a subset of Norwegian data and testing on remaining Norwegian data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  **Prerequisites**\n",
    "- Norwegian glacier dataset from `../1.1. Norway-prepro.ipynb`\n",
    "- Swiss glacier dataset from `regions/Switzerland/1.1. GLAMOS-prepro.ipynb`\n",
    "- ERA5 climate data for both regions from `../1.2. ERA5Land-prepro.ipynb`\n",
    "---\n",
    "\n",
    "### Feature Definitions\n",
    "\n",
    "**Climate Features (ERA5 Reanalysis):**\n",
    "- `t2m`: 2-meter temperature\n",
    "- `tp`: Total precipitation\n",
    "- `slhf`/`sshf`: Surface heat fluxes\n",
    "- `ssrd`: Surface solar radiation downwards\n",
    "- `fal`: Albedo\n",
    "- `str`: Surface net thermal radiation\n",
    "- `u10`/`v10`: Wind components\n",
    "\n",
    "**Topographical Features (OGGM):**\n",
    "- `aspect`/`slope`: Terrain geometry\n",
    "- `hugonnet_dhdt`: Ice thickness changes\n",
    "- `consensus_ice_thickness`: Ice depth\n",
    "- `millan_v`: Ice surface velocity\n",
    "- `elevation_difference`: Measurement elevation − ERA5-Land grid cell elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.norway_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_NOR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Norway/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Regional Modelling (Norway Only)\n",
    "\n",
    "This approach trains neural networks exclusively on Norwegian glacier data.\n",
    "\n",
    "## Data Loading & Initial Processing\n",
    "\n",
    "### Create Norwegian Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wgms = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'Nor_dataset_all_oggm_with_millanv.csv')\n",
    "# Drop Nan entries in millan_v of Norway dataset\n",
    "data_wgms = data_wgms.dropna(subset=data_wgms.columns.drop('DATA_MODIFICATION'))\n",
    "\n",
    "print('Number of glaciers:', len(data_wgms['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_wgms[data_wgms.PERIOD == 'annual']) + len(data_wgms[data_wgms.PERIOD == 'winter']) + len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "\n",
    "data_wgms.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transformation to Monthly Format\n",
    "\n",
    "Transform point mass balance data to monthly resolution and integrate with ERA5 climate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wgms_test = data_wgms.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_NOR_CH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_NOR_CH.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_wgms_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file='NOR_dataset_monthly_full_with_millanv.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "display(data_monthly.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splitting Strategy\n",
    "\n",
    "**Spatial Generalization Approach:** Select test set based on glaciers, remaining glaciers will be the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = ['Cainhavarre', 'Rundvassbreen', 'Svartisheibreen', 'Trollbergdalsbreen', 'Hansebreen', 'Tunsbergdalsbreen', 'Austdalsbreen', 'Hellstugubreen', 'Austre Memurubreen', 'Bondhusbrea', 'Svelgjabreen', 'Moesevassbrea', 'Blomstoelskardsbreen']\n",
    "\n",
    "\"\"\"\n",
    "# 50% \n",
    "test_glaciers = ['Cainhavarre',\n",
    " 'Svartisheibreen',\n",
    " 'Hoegtuvbreen',\n",
    " 'Trollbergdalsbreen',\n",
    " 'Ruklebreen',\n",
    " 'Graafjellsbrea',\n",
    " 'Breidablikkbrea',\n",
    " 'Bondhusbrea',\n",
    " 'Tunsbergdalsbreen',\n",
    " 'Hellstugubreen',\n",
    " 'Vesledalsbreen',\n",
    " 'Nigardsbreen',\n",
    " 'Rembesdalskaaka']\n",
    "\"\"\"             \n",
    "\n",
    "\"\"\"  \n",
    "# 5-10% 7 glaciers\n",
    "test_glaciers = ['Rundvassbreen', 'Engabreen', 'Aalfotbreen', 'Hansebreen', 'Nigardsbreen',\n",
    "                'Austdalsbreen', 'Juvfonne', 'Hellstugubreen', 'Rembesdalskaaka',\n",
    "                'Svelgjabreen', 'Blomstoelskardsbreen', 'Storsteinsfjellbreen',\n",
    "                'Trollbergdalsbreen', 'Vetlefjordbreen', 'Storglombreen N', 'Graafjellsbrea',\n",
    "                'Breidablikkbrea', 'Blaaisen', 'Blabreen', 'Ruklebreen', 'Cainhavarre',\n",
    "                'Vesledalsbreen', 'Vestre Memurubreen', 'Hoegtuvbreen']\n",
    "\"\"\"  \n",
    "\n",
    "# 63 lat split\n",
    "#test_glaciers = ['Rundvassbreen', 'Engabreen', 'Storsteinsfjellbreen', 'Svartisheibreen', 'Trollbergdalsbreen', 'Storglombreen N', 'Blaaisen', 'Cainhavarre', 'Hoegtuvbreen']\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = [i for i in existing_glaciers if i not in test_glaciers]\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split Strategy (80/20)\n",
    "\n",
    "**Standard Approach:** Random 80/20 split across all available training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Cross-Regional Transfer Learning (Switzerland → Norway)\n",
    "\n",
    "This approach uses the rich Swiss dataset to try and modell Norwegian glaciers. Either purely cross-regional (no Norwegian data seen by the model during training), or as a baseline for transfer learning by including a subset of Norwegian data into training.\n",
    "\n",
    "\n",
    "### Create Combined Swiss and Norwegian Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "data_NOR = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'Nor_dataset_all_oggm_with_millanv.csv')\n",
    "\n",
    "# Drop Nan entries in millan_v of Norway dataset\n",
    "data_NOR = data_NOR.dropna(subset=data_NOR.columns.drop('DATA_MODIFICATION'))\n",
    "display(data_NOR)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "display(data_NOR.columns)\n",
    "\n",
    "data_CH = data_CH.drop(['aspect_sgi', 'slope_sgi', 'topo_sgi'], axis=1)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "# Merge CH with NOR\n",
    "data_NOR_CH = pd.concat([data_NOR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(data_NOR_CH.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transformation to Monthly Format\n",
    "\n",
    "Transform point mass balance data to monthly resolution and integrate with ERA5 climate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH_NOR_test = data_NOR_CH.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_NOR_CH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_NOR_CH.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_CH_NOR_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'CH_NOR_wgms_dataset_monthly_full_with_millanv_v2.csv')\n",
    "data_monthly_CH_NOR = dataloader_gl.data\n",
    "\n",
    "display(data_monthly_CH_NOR.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splitting Strategy\n",
    "\n",
    "**Spatial Generalization Approach:** Select test set based on glaciers, remaining glaciers will be the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_glaciers = list(data_NOR['GLACIER'].unique())\n",
    "\n",
    "\"\"\"\n",
    "# 50%\n",
    "test_glaciers = ['Cainhavarre',\n",
    " 'Svartisheibreen',\n",
    " 'Hoegtuvbreen',\n",
    " 'Trollbergdalsbreen',\n",
    " 'Ruklebreen',\n",
    " 'Graafjellsbrea',\n",
    " 'Breidablikkbrea',\n",
    " 'Bondhusbrea',\n",
    " 'Tunsbergdalsbreen',\n",
    " 'Hellstugubreen',\n",
    " 'Vesledalsbreen',\n",
    " 'Nigardsbreen',\n",
    " 'Rembesdalskaaka']\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# 5-10% 7 glaciers\n",
    "test_glaciers = ['Rundvassbreen', 'Engabreen', 'Aalfotbreen', 'Hansebreen', 'Nigardsbreen',\n",
    "                'Austdalsbreen', 'Juvfonne', 'Hellstugubreen', 'Rembesdalskaaka',\n",
    "                'Svelgjabreen', 'Blomstoelskardsbreen', 'Storsteinsfjellbreen',\n",
    "                'Trollbergdalsbreen', 'Vetlefjordbreen', 'Storglombreen N', 'Graafjellsbrea',\n",
    "                'Breidablikkbrea', 'Blaaisen', 'Blabreen', 'Ruklebreen', 'Cainhavarre',\n",
    "                'Vesledalsbreen', 'Vestre Memurubreen', 'Hoegtuvbreen']\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# north/south 63 lat split\n",
    "test_glaciers = ['Rundvassbreen', 'Engabreen', 'Storsteinsfjellbreen', 'Svartisheibreen',\n",
    "                'Trollbergdalsbreen', 'Storglombreen N', 'Blaaisen', 'Cainhavarre', 'Hoegtuvbreen']\n",
    "\"\"\"\n",
    "\n",
    "# Ensure all test glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_glaciers = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_glaciers:\n",
    "    print(\n",
    "        f\"Warning: The following test glaciers are not in the dataset: {missing_glaciers}\"\n",
    "    )\n",
    "\n",
    "# Define training glaciers\n",
    "train_glaciers = list(data_CH['GLACIER'].unique())\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('Size of test data:', len(data_test))\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(\n",
    "    train_glaciers)]\n",
    "print('Size of train data:', len(data_train))\n",
    "if len(data_train) == 0:\n",
    "    print(\"Warning: No training data available!\")\n",
    "else:\n",
    "    test_perc = (len(data_test) / len(data_train)) * 100\n",
    "    print('Percentage of test size: {:.2f}%'.format(test_perc))\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split Strategy (80/20)\n",
    "\n",
    "**Standard Approach:** Random 80/20 split across all available training data (Swiss + Norwegian training glaciers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "print(\"Train indices (first 10):\", train_indices[:10])\n",
    "print(\"Val indices (first 10):\", val_indices[:10])\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split - Target Domain Focus (80/20)\n",
    "\n",
    "**Domain-Aware Approach:** This split will only build the validation set from Norwegian data\n",
    "\n",
    "- **Training Set:** All Swiss data + 80% of Norwegian training glaciers\n",
    "- **Validation Set:** 20% of Norwegian training glaciers only\n",
    "\n",
    "This ensures that the same validation set is used as in fine-tuning and layer freezing in the `../3.2.4 Train-ML-model-NN_progressive_transfer` Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pool = CH + Norway subset\n",
    "data_train = train_set['df_X'].copy()\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "# Norway train_glaciers\n",
    "norway_train_glacier = [\n",
    "    g for g in data_NOR['GLACIER'].unique()\n",
    "    if g not in test_glaciers\n",
    "]\n",
    "display('train glaciers from target domain: ', norway_train_glacier)\n",
    "\n",
    "# Find Norway subset within this pool\n",
    "norway_mask = data_train['GLACIER'].isin(norway_train_glacier)\n",
    "data_norway = data_train.loc[norway_mask]\n",
    "\n",
    "# Split only the Norway subset\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_norway)\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "norway_train_idx = list(train_itr)\n",
    "norway_val_idx = list(val_itr)\n",
    "\n",
    "# Training set = CH + Norway train portion\n",
    "df_X_train = pd.concat([\n",
    "    data_train.loc[~norway_mask],                           # all CH glaciers\n",
    "    data_norway.iloc[norway_train_idx]                    # Norway train glaciers\n",
    "])\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Validation set = Norway val portion only\n",
    "df_X_val = data_norway.iloc[norway_val_idx]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Configuration & Training\n",
    "\n",
    "\n",
    "Implementing a **Multilayer perceptron deep neural network** designed specifically for glacier mass balance prediction\n",
    "\n",
    "\n",
    "The following cell is optional and initializes a period_indicator variable, which allows the NN to keep track if the current point is from a winter or annual measurement.\n",
    "\n",
    "Uncomment `#+ ['PERIOD_INDICATOR']` in the \"Define features\" cell if you want to use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_period_indicator(df):\n",
    "    \"\"\"Create numerical PERIOD_INDICATOR feature\"\"\"\n",
    "    df = df.copy()\n",
    "    df['PERIOD_INDICATOR'] = df['PERIOD'].map({'annual': 0, 'winter': 1})\n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "df_X_train = create_period_indicator(df_X_train)\n",
    "df_X_val = create_period_indicator(df_X_val)\n",
    "test_set['df_X'] = create_period_indicator(test_set['df_X'])\n",
    "\n",
    "print(\"PERIOD_INDICATOR created:\")\n",
    "print(\"Annual (0):\", (df_X_train['PERIOD_INDICATOR'] == 0).sum())\n",
    "print(\"Winter (1):\", (df_X_train['PERIOD_INDICATOR'] == 1).sum())\n",
    "print(\"Original PERIOD column preserved:\", df_X_train['PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate) #  + ['PERIOD_INDICATOR']\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current Norway feature order\n",
    "print(\"Current Norway feature order:\")\n",
    "for i, feature in enumerate(feature_columns):\n",
    "    print(f\"{i}: {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization & Hyperparameters\n",
    "\n",
    "**NN Configuration:**\n",
    "- **Learning Rate:** 0.001 with ReduceLROnPlateau scheduling\n",
    "- **Batch Size:** 128 samples per gradient update  \n",
    "- **Optimization:** Adam optimizer with L2 weight decay (1e-5)\n",
    "- **Architecture:** [128, 128, 64, 32] hidden layers with 20% dropout\n",
    "- **Regularization:** Batch normalization + early stopping (patience=15)\n",
    "\n",
    "**Callbacks for Robust Training:**\n",
    "- **Early Stopping:** Prevents overfitting by monitoring validation loss\n",
    "- **Learning Rate Scheduler:** Reduces LR when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Checkpointing\n",
    "\n",
    "Set `TRAIN = True` to train new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    custom_nn.fit(dataset.X, dataset.y)\n",
    "    # The dataset provided in fit is not used as the datasets are overwritten in the provided train_split function\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"nn_model_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # After Training: Best weights are already loaded\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "    \n",
    "    # save params dic\n",
    "    params_filename = f\"nn_params_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump(args, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"nn_model_2025-09-30.pt\"  # Replace with actual date if needed\n",
    "\n",
    "loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation & Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save individual grouped_ids as CSV\n",
    "split_name = \"cross_regional_all_CH_NOR_periodindicator\"\n",
    "csv_filename = f\"grouped_ids_{split_name.replace('%', 'pct').replace('-', '_')}.csv\"\n",
    "grouped_ids.to_csv(f\"results/{csv_filename}\", index=False)\n",
    "\n",
    "split_name = \"cross_regional_all_CH_NOR_periodindicator\" \n",
    "csv_filename = f\"results/grouped_ids_{split_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined_NN_additional(grouped_ids, region_name='CH Train NOR Test', nticks=8, min_val=-12.9, max_val=6.510000000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
