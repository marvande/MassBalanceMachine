{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain-Adversarial Neural Network (DANN) for Glacier Mass Balance Prediction\n",
    "\n",
    "This notebook implements **Domain-Adversarial Neural Networks (DANN)** for predicting glacier point mass balance measurements using meteorological and topographical features. DANN enables cross-regional transfer learning by learning domain-invariant features while maintaining prediction accuracy.\n",
    "\n",
    "## **Modelling Strategy: Cross-Regional Transfer Learning**\n",
    "\n",
    "### **Domain-Adversarial Transfer Learning (Switzerland → Norway)**\n",
    "- Combines data from Swiss glaciers (source domain) and Norwegian glaciers (target domain)\n",
    "- Uses gradient reversal layer to learn domain-invariant features\n",
    "- Enables knowledge transfer from data-rich Swiss glaciers to Norwegian glaciers\n",
    "- Balances regression loss (mass balance prediction) with domain classification loss\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "- Norwegian glacier dataset from `../1.1. Norway-prepro.ipynb`\n",
    "- Swiss glacier dataset from `regions/Switzerland/1.1. GLAMOS-prepro.ipynb`\n",
    "- ERA5 climate data for both regions from `../1.2. ERA5Land-prepro.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Definitions\n",
    "\n",
    "**Climate Features (ERA5 Reanalysis):**\n",
    "- `t2m`: 2-meter temperature\n",
    "- `tp`: Total precipitation\n",
    "- `slhf`/`sshf`: Surface heat fluxes\n",
    "- `ssrd`: Surface solar radiation downwards\n",
    "- `fal`: Albedo\n",
    "- `str`: Surface net thermal radiation\n",
    "- `u10`/`v10`: Wind components\n",
    "\n",
    "**Topographical Features (OGGM):**\n",
    "- `aspect`/`slope`: Terrain geometry\n",
    "- `hugonnet_dhdt`: Ice thickness changes\n",
    "- `consensus_ice_thickness`: Ice depth\n",
    "- `millan_v`: Ice surface velocity\n",
    "- `elevation_difference`: Measurement elevation − ERA5-Land grid cell elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle \n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.norway_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_NOR import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.NorwayConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Norway/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Combined Swiss and Norway Glacier Dataset\n",
    "\n",
    "Start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH = pd.read_csv('/home/mburlet/scratch/data/DATA_MB/CH_wgms_dataset_all_04_06_oggm.csv')\n",
    "data_NOR = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'Nor_dataset_all_oggm_with_millanv.csv')\n",
    "\n",
    "# Drop Nan entries in millan_v of Norway dataset\n",
    "data_NOR = data_NOR.dropna(subset=data_NOR.columns.drop('DATA_MODIFICATION'))\n",
    "display(data_NOR)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "display(data_NOR.columns)\n",
    "\n",
    "data_CH = data_CH.drop(['aspect_sgi', 'slope_sgi', 'topo_sgi'], axis=1)\n",
    "\n",
    "display(data_CH.columns)\n",
    "\n",
    "# Merge CH with NOR\n",
    "data_NOR_CH = pd.concat([data_NOR, data_CH], axis=0).reset_index(drop=True)\n",
    "\n",
    "display(data_NOR_CH.head(2))\n",
    "\n",
    "display(len(data_NOR_CH['GLACIER'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transformation to Monthly Format\n",
    "\n",
    "Transform point mass balance data to monthly resolution, integrate with ERA5 climate variables, and assign domain labels for adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CH_NOR_test = data_NOR_CH.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_NOR_CH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_NOR_CH.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_CH_NOR_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'CH_NOR_wgms_dataset_monthly_full_with_millanv_v2.csv')\n",
    "data_monthly_CH_NOR = dataloader_gl.data\n",
    "\n",
    "# Add DOMAIN column back using RGI ID patterns\n",
    "def assign_domain(rgi_id):\n",
    "    if rgi_id.startswith('RGI60-11'):\n",
    "        return 0  # Switzerland\n",
    "    elif rgi_id.startswith('RGI60-08'):\n",
    "        return 1  # Norway\n",
    "    else:\n",
    "        return -1  # Unknown\n",
    "\n",
    "data_monthly_CH_NOR['DOMAIN'] = data_monthly_CH_NOR['RGIId'].apply(assign_domain)\n",
    "\n",
    "# Verify domain assignment\n",
    "print(\"Domain distribution after assignment:\")\n",
    "print(data_monthly_CH_NOR['DOMAIN'].value_counts())\n",
    "print(f\"Unknown domains (-1): {(data_monthly_CH_NOR['DOMAIN'] == -1).sum()}\")\n",
    "\n",
    "# Update the dataloader with the new data including DOMAIN\n",
    "dataloader_gl.data = data_monthly_CH_NOR\n",
    "\n",
    "display(data_monthly_CH_NOR.head(2))\n",
    "\n",
    "# Ensure DOMAIN is part of metadata (so it travels with AggregatedDataset)\n",
    "if 'DOMAIN' not in cfg.metaData:\n",
    "    cfg.metaData = cfg.metaData + ['DOMAIN']\n",
    "print('Metadata columns now:', cfg.metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Splitting\n",
    "\n",
    "**Domain-Aware Spatial Generalization:** Select training set from Swiss glaciers plus a subset of  Norwegian glaciers, with remaining Norwegian glaciers being the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 glaciers 50%\n",
    "#train_glaciers_NOR_subset = ['Engabreen', 'Nigardsbreen', 'Aalfotbreen', 'Graasubreen']\n",
    "\"\"\"\n",
    "# 50% Kmeans\n",
    "train_glaciers_NOR_subset = ['Engabreen',\n",
    " 'Storglombreen N',\n",
    " 'Moesevassbrea',\n",
    " 'Blaaisen',\n",
    " 'Blabreen',\n",
    " 'Harbardsbreen',\n",
    " 'Graasubreen',\n",
    " 'Svelgjabreen',\n",
    " 'Aalfotbreen',\n",
    " 'Rundvassbreen',\n",
    " 'Juvfonne',\n",
    " 'Storsteinsfjellbreen',\n",
    " 'Hansebreen',\n",
    " 'Vesledalsbreen',\n",
    " 'Vetlefjordbreen',\n",
    " 'Blomstoelskardsbreen',\n",
    " 'Vestre Memurubreen',\n",
    " 'Austre Memurubreen']\n",
    "\"\"\"\n",
    "# 7 glaciers 5-10%\n",
    "train_glaciers_NOR_subset = ['Tunsbergdalsbreen','Austre Memurubreen','Svartisheibreen','Bondhusbrea','Harbardsbreen','Moesevassbrea','Graasubreen']\n",
    "\n",
    "\"\"\"\n",
    "# 63 lat split\n",
    "train_glaciers_NOR_subset = ['Aalfotbreen', 'Austdalsbreen', 'Austre Memurubreen', 'Blabreen', 'Blomstoelskardsbreen',\n",
    "                    'Bondhusbrea', 'Breidablikkbrea', 'Graafjellsbrea', 'Graasubreen', 'Hansebreen', 'Harbardsbreen',\n",
    "                    'Hellstugubreen', 'Juvfonne', 'Moesevassbrea', 'Nigardsbreen', 'Rembesdalskaaka', 'Ruklebreen',\n",
    "                    'Svelgjabreen', 'Tunsbergdalsbreen', 'Vesledalsbreen', 'Vestre Memurubreen', 'Vetlefjordbreen']\n",
    "\"\"\"\n",
    "\n",
    "# Define training glaciers: All CH + subset of NOR for DANN\n",
    "train_glaciers_CH = list(data_CH['GLACIER'].unique())\n",
    "train_glaciers = train_glaciers_CH + train_glaciers_NOR_subset\n",
    "\n",
    "# Define test glaciers: Remaining NOR glaciers (not in training subset)\n",
    "test_glaciers = [g for g in data_NOR['GLACIER'].unique() if g not in train_glaciers_NOR_subset]\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_train = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_train:\n",
    "    print(f\"Warning: The following training glaciers are not in the dataset: {missing_train}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: The following test glaciers are not in the dataset: {missing_test}\")\n",
    "\n",
    "# Get training and test data\n",
    "data_train = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(train_glaciers)]\n",
    "display(data_train)\n",
    "data_test = dataloader_gl.data[dataloader_gl.data.GLACIER.isin(test_glaciers)]\n",
    "\n",
    "# Statistics prints\n",
    "print('DANN Training Setup:')\n",
    "print(f'  Swiss training glaciers: {len(train_glaciers_CH)}')\n",
    "print(f'  Norwegian training subset: {len(train_glaciers_NOR_subset)} - {train_glaciers_NOR_subset}')\n",
    "print(f'  Total training glaciers: {len(train_glaciers)}')\n",
    "print(f'  Norwegian test glaciers: {len(test_glaciers)}')\n",
    "print(f'Size of train data: {len(data_train)}')\n",
    "print(f'Size of test data: {len(data_test)}')\n",
    "\n",
    "# Check domain distribution in training set\n",
    "print(f'Training domain distribution:')\n",
    "print(data_train['DOMAIN'].value_counts())\n",
    "\n",
    "# CV Splits for DANN\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split Strategy (80/20)\n",
    "\n",
    "**Standard Approach:** Random 80/20 split across all available training data (Swiss + Norwegian training glaciers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split for DANN:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and validation dataset\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "domain_train = df_X_train['DOMAIN'].values  # Extract domain labels\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "domain_val = df_X_val['DOMAIN'].values  # Extract domain labels\n",
    "\n",
    "print(\"Train indices (first 10):\", train_indices[:10])\n",
    "print(\"Val indices (first 10):\", val_indices[:10])\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)\n",
    "print(\"Train domain distribution:\", pd.Series(domain_train).value_counts())\n",
    "print(\"Val domain distribution:\", pd.Series(domain_val).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation Split - Target Domain Focus (80/20)\n",
    "\n",
    "**Domain-Aware Approach:** This split will only build the validation set from Norwegian data\n",
    "\n",
    "- **Training Set:** All Swiss data + 80% of Norwegian training glaciers\n",
    "- **Validation Set:** 20% of Norwegian training glaciers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training pool = CH + Norway subset\n",
    "data_train = train_set['df_X'].copy()\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "# Norway train_glaciers\n",
    "norway_train_glacier = [\n",
    "    g for g in data_NOR['GLACIER'].unique()\n",
    "    if g not in test_glaciers\n",
    "]\n",
    "display('train glaciers from target domain: ', norway_train_glacier)\n",
    "\n",
    "# Find Norway subset within this pool\n",
    "norway_mask = data_train['GLACIER'].isin(norway_train_glacier)\n",
    "data_norway = data_train.loc[norway_mask]\n",
    "\n",
    "# Split only the Norway subset\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_norway)\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "norway_train_idx = list(train_itr)\n",
    "norway_val_idx = list(val_itr)\n",
    "\n",
    "# Training set = CH + Norway train portion\n",
    "df_X_train = pd.concat([\n",
    "    data_train.loc[~norway_mask],                           # all CH glaciers\n",
    "    data_norway.iloc[norway_train_idx]                    # Norway train glaciers\n",
    "])\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Validation set = Norway val portion only\n",
    "df_X_val = data_norway.iloc[norway_val_idx]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-Adversarial Neural Network Configuration & Training\n",
    "\n",
    "Implementing a **Domain-Adversarial Neural Network (DANN)** \n",
    "\n",
    "The architecture combines:\n",
    "\n",
    "1. **Feature Extractor:** Shared multilayer perceptron extracting domain-invariant representations\n",
    "2. **Regression Head:** Predicts mass balance from extracted features  \n",
    "3. **Domain Classifier:** Distinguishes between Swiss (0) and Norway (1) domains (including gradient reversal layer)\n",
    "\n",
    "The gradient reversal layer enables adversarial training where the feature extractor learns to fool the domain classifier while maintaining prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DANN Architecture Implementation\n",
    "\n",
    "Implementation of Domain-Adversarial Neural Network components:\n",
    "- **Gradient Reversal Layer:** Multiplies gradients by -λ during backpropagation\n",
    "- **DANN Network:** Feature extractor with dual heads for regression and domain classification  \n",
    "- **Custom Loss:** Combines regression loss with adversarial domain loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANN components: Gradient Reversal, network, regressor wrapper, and dataset bindings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skorch.utils import to_tensor\n",
    "import massbalancemachine as mbm\n",
    "from pathlib import Path\n",
    "\n",
    "# Gradient Reversal Layer\n",
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class GradReverse(nn.Module):\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "class DANNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extractor identical to FlexibleNetwork's trunk, with two heads:\n",
    "    - Regressor head for SMB (label)\n",
    "    - Domain classifier head (binary)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout=0.2, use_batchnorm=False, domain_hidden=64, grl_lambda=1.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        if not isinstance(dropout, list):\n",
    "            dropout = [dropout] * len(hidden_layers)\n",
    "        for hidden_dim, drop_rate in zip(hidden_layers, dropout):\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(drop_rate))\n",
    "            current_dim = hidden_dim\n",
    "        # trunk outputs the last hidden representation\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        # heads\n",
    "        self.regressor = nn.Linear(current_dim, 1)\n",
    "        self.grl = GradReverse(lambda_=grl_lambda)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(current_dim, domain_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout[-1] if isinstance(dropout, list) else dropout),\n",
    "            nn.Linear(domain_hidden, 1),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.feature_extractor(x)\n",
    "        y_pred = self.regressor(h)\n",
    "        d_logits = self.domain_classifier(self.grl(h))\n",
    "        return y_pred, d_logits\n",
    "\n",
    "# Dataset that yields domain labels padded per ID to match monthly padding\n",
    "class DomainTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, aggregated_dataset):\n",
    "        self.base = aggregated_dataset\n",
    "        self.meta_has_domain = 'DOMAIN' in self.base.metadataColumns\n",
    "        if not self.meta_has_domain:\n",
    "            # fallback: try to read from features\n",
    "            assert 'DOMAIN' in self.base.cfg.featureColumns, \"DOMAIN must be in metadata or featureColumns\"\n",
    "            self.domain_feat_idx = self.base.cfg.featureColumns.index('DOMAIN')\n",
    "        else:\n",
    "            self.domain_idx = self.base.metadataColumns.index('DOMAIN')\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, index):\n",
    "        ind = self.base._getInd(index)\n",
    "        if self.meta_has_domain:\n",
    "            dval = self.base.metadata[ind[0]][self.domain_idx]\n",
    "        else:\n",
    "            dval = self.base.features[ind[0], self.domain_feat_idx]\n",
    "        dpad = np.empty(self.base.maxConcatNb, dtype=np.float32)\n",
    "        dpad.fill(np.nan)\n",
    "        dpad[:len(ind)] = dval\n",
    "        return dpad\n",
    "\n",
    "# Binding that returns (X, (y, d)) so y_true in get_loss can contain both\n",
    "class CombinedTargetBinding(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_slice, y_slice, d_dataset):\n",
    "        self.X = X_slice\n",
    "        self.y = y_slice\n",
    "        self.d = d_dataset\n",
    "        assert len(self.X) == len(self.y) == len(self.d)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], (self.y[idx], self.d[idx])\n",
    "\n",
    "# Skorch regressor that adds domain-adversarial loss on top of the SMB loss\n",
    "class CustomDANNRegressor(mbm.models.CustomNeuralNetRegressor):\n",
    "    def __init__(self, cfg, *args, dan_lambda=0.1, **kwargs):\n",
    "        super().__init__(cfg, *args, **kwargs)\n",
    "        self.dan_lambda = dan_lambda\n",
    "        self._last_domain_logits = None\n",
    "\n",
    "    def infer(self, x, **fit_params):\n",
    "        x = to_tensor(x, device=self.device)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        x, indNonNan = self._unpack_inp(x)\n",
    "        if self.modelDtype is not None:\n",
    "            x = x.type(self.modelDtype)\n",
    "        outputs = self.module_(x, **fit_params)\n",
    "        if isinstance(outputs, tuple):\n",
    "            y_monthly, d_monthly = outputs\n",
    "            y_packed = self._pack_out(y_monthly, indNonNan)\n",
    "            d_packed = self._pack_out(d_monthly, indNonNan)\n",
    "            self._last_domain_logits = d_packed\n",
    "            return y_packed\n",
    "        else:\n",
    "            return self._pack_out(outputs, indNonNan)\n",
    "\n",
    "    def get_loss(self, y_pred, y_true, X=None, training=False):\n",
    "        # y_true can be (label_pad, domain_pad) from CombinedTargetBinding\n",
    "        if isinstance(y_true, (tuple, list)) and len(y_true) == 2:\n",
    "            y_true_labels, y_true_domain = y_true\n",
    "        else:\n",
    "            y_true_labels, y_true_domain = y_true, None\n",
    "\n",
    "        # Label loss (same as base implementation)\n",
    "        loss = 0.0\n",
    "        cnt = 0\n",
    "        for yi_pred, yi_true in zip(y_pred, y_true_labels):\n",
    "            valid = ~torch.isnan(yi_pred)\n",
    "            if valid.any():\n",
    "                pred_sum = yi_pred[valid].sum()\n",
    "                true_mean = yi_true[valid].mean()\n",
    "                loss = loss + (pred_sum - true_mean) ** 2\n",
    "                cnt += 1\n",
    "        label_loss = loss / max(cnt, 1)\n",
    "\n",
    "        \"\"\"\"\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits\n",
    "            mask = ~torch.isnan(y_true_domain)\n",
    "            if mask.any():\n",
    "                domain_loss = F.binary_cross_entropy_with_logits(d_logits[mask], y_true_domain[mask].float())\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean, otherwise IDs with longer months have higher domain loss\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # shape: [batch, max_months] (or [batch, max_months, 1])\n",
    "            per_id_losses = []\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                # squeeze trailing dim if present\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                # mask valid months\n",
    "                mask = ~torch.isnan(d_true_row) # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    per_id_losses.append(loss_i)\n",
    "\n",
    "            if len(per_id_losses) > 0:\n",
    "                domain_loss = torch.stack(per_id_losses).mean()  # mean over IDs\n",
    "        \"\"\"\n",
    "        # Domain loss a bit more complicated due to Per-ID mean and then batch mean for each domain\n",
    "        # otherwise IDs with longer months have higher domain loss and CH domain loss with more data is exaggerated\n",
    "        domain_loss = torch.tensor(0.0, device=y_pred.device)\n",
    "        if training and (y_true_domain is not None) and (self._last_domain_logits is not None):\n",
    "            d_logits = self._last_domain_logits  # [batch_ids, max_months(,1)]\n",
    "            per_id_losses_ch, per_id_losses_nor = [], []\n",
    "\n",
    "            for d_log_row, d_true_row in zip(d_logits, y_true_domain):\n",
    "                if d_log_row.ndim > 1:\n",
    "                    d_log_row = d_log_row.squeeze(-1)\n",
    "\n",
    "                mask = ~torch.isnan(d_true_row)  # Select valid months per ID, NaN padding months are False\n",
    "                if mask.any():\n",
    "                    loss_i = F.binary_cross_entropy_with_logits(\n",
    "                        d_log_row[mask],\n",
    "                        d_true_row[mask].float(),\n",
    "                        reduction='mean',  # mean over valid months for this ID\n",
    "                    )\n",
    "                    # domain for this ID (same across valid months)\n",
    "                    dom_i = int(d_true_row[mask][0].item())  # 0=CH, 1=NOR\n",
    "                    (per_id_losses_ch if dom_i == 0 else per_id_losses_nor).append(loss_i)\n",
    "\n",
    "            parts = []\n",
    "            if len(per_id_losses_ch) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_ch).mean())\n",
    "            if len(per_id_losses_nor) > 0:\n",
    "                parts.append(torch.stack(per_id_losses_nor).mean())\n",
    "            if len(parts) > 0:\n",
    "                domain_loss = torch.stack(parts).mean()\n",
    "\n",
    "        return label_loss + self.dan_lambda * domain_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(cfg, fname: str, *args, **kwargs):\n",
    "        \"\"\"Loads a pre-trained DANN model from a file.\"\"\"\n",
    "        model = CustomDANNRegressor(cfg, *args, **kwargs)\n",
    "        model.initialize()\n",
    "        models_dir = Path(\"./models\")\n",
    "        model.load_params(f_params=models_dir / fname)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization & Hyperparameters\n",
    "\n",
    "**DANN Configuration:**\n",
    "- **Learning Rate:** 0.001 with ReduceLROnPlateau scheduling\n",
    "- **Batch Size:** 128 samples per gradient update\n",
    "- **Optimization:** Adam optimizer with L2 weight decay (1e-5)\n",
    "- **Architecture:** [128, 128, 64, 32] hidden layers with 20% dropout\n",
    "- **Domain Adversarial Loss Weight (λ):** 0.05 for balanced training\n",
    "- **Domain Classifier:** 64 neurons with gradient reversal\n",
    "\n",
    "**Callbacks for Robust Training:**\n",
    "- **Early Stopping:** Prevents overfitting by monitoring validation loss (patience=15)\n",
    "- **Learning Rate Scheduler:** Reduces LR when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=10,\n",
    "    threshold=1e-4,  # Optional: stop only when improvement is very small\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None  # Initialized hereafter\n",
    "\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "\n",
    "# param_init = {'device': 'cuda:0'}\n",
    "param_init = {'device': 'cpu'}  # Use CPU for training\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "    'module__domain_hidden': 64,\n",
    "    'module__grl_lambda': 1.0,\n",
    "}\n",
    "\n",
    "# Use DANN network\n",
    "args = {\n",
    "    'module': DANNNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'module__domain_hidden': params['module__domain_hidden'],\n",
    "    'module__grl_lambda': params['module__grl_lambda'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# DANN loss weight\n",
    "DAN_LAMBDA = 0.1\n",
    "custom_nn = CustomDANNRegressor(cfg, dan_lambda=DAN_LAMBDA, **args, **param_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for DANN Training\n",
    "\n",
    "Create aggregated datasets with domain labels and configure domain-balanced sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the aggregated datasets\n",
    "agg_train = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                  features=features,\n",
    "                                                  metadata=metadata,\n",
    "                                                  targets=y_train)\n",
    "agg_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features_val,\n",
    "                                                metadata=metadata_val,\n",
    "                                                targets=y_val)\n",
    "\n",
    "# Build domain targets per ID\n",
    "domain_train_ds = DomainTargetDataset(agg_train)\n",
    "domain_val_ds = DomainTargetDataset(agg_val)\n",
    "\n",
    "# Slice features/labels\n",
    "X_train_slice = SliceDataset(agg_train, idx=0)\n",
    "y_train_slice = SliceDataset(agg_train, idx=1)\n",
    "X_val_slice = SliceDataset(agg_val, idx=0)\n",
    "y_val_slice = SliceDataset(agg_val, idx=1)\n",
    "\n",
    "# Bind (X, (y, d)) so CustomDANNRegressor receives both labels and domains\n",
    "dataset = CombinedTargetBinding(X_train_slice, y_train_slice, domain_train_ds)\n",
    "dataset_val = CombinedTargetBinding(X_val_slice, y_val_slice, domain_val_ds)\n",
    "\n",
    "# For info\n",
    "print(\"train:\", len(dataset))\n",
    "print(\"validation:\", len(dataset_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-Balanced Sampling Configuration\n",
    "\n",
    "Implement WeightedRandomSampler to ensure balanced representation of Swiss and French glaciers during training, preventing domain imbalance from affecting adversarial learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-balanced sampling (~50/50 CH:ICE) using WeightedRandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Map each training aggregated item (ID) to its DOMAIN\n",
    "# 1) get the ID per aggregated sample in the training SliceDataset\n",
    "train_ids = X_train_slice.dataset.indexToId(list(range(len(X_train_slice))))\n",
    "\n",
    "# 2) build an ID -> DOMAIN dict from the training dataframe\n",
    "id_to_domain = (\n",
    "    df_X_train_subset.groupby('ID')['DOMAIN'].first().to_dict()\n",
    ")\n",
    "\n",
    "# 3) lookup domains per training aggregated item\n",
    "sample_domains = np.array([id_to_domain.get(i, -1) for i in train_ids], dtype=int)\n",
    "\n",
    "# Safety: ensure only CH(0) and ICE(1) are present; any other values get zero weight\n",
    "n_ch = int((sample_domains == 0).sum())\n",
    "n_ice = int((sample_domains == 1).sum())\n",
    "if (n_ch + n_ice) == 0:\n",
    "    raise ValueError(\"No CH/ICE samples found in training set for sampler.\")\n",
    "\n",
    "# Compute weights so the expected sampling is 50% CH and 50% ICE per epoch\n",
    "w = np.zeros_like(sample_domains, dtype=float)\n",
    "if n_ch > 0:\n",
    "    w[sample_domains == 0] = 0.5 / n_ch\n",
    "if n_ice > 0:\n",
    "    w[sample_domains == 1] = 0.5 / n_ice\n",
    "# Any unknown domains remain with weight 0 (won't be sampled)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.as_tensor(w, dtype=torch.double),\n",
    "    num_samples=len(w),   # one epoch worth of samples, drawn with replacement\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "# Sanity Check\n",
    "# Single simulated epoch\n",
    "sim_idx = list(iter(WeightedRandomSampler(torch.as_tensor(w, dtype=torch.double),\n",
    "                                         num_samples=len(w),\n",
    "                                         replacement=True)))\n",
    "sim_ratio = (sample_domains[sim_idx] == 1).mean()  # ICE share\n",
    "print(f\"Simulated epoch: ICE share={sim_ratio:.3f}, unique IDs drawn={len(set(sim_idx))}/{len(w)}\")\n",
    "\n",
    "for t in range(3):\n",
    "    idx_t = list(iter(WeightedRandomSampler(torch.as_tensor(w, dtype=torch.double),\n",
    "                                           num_samples=len(w),\n",
    "                                           replacement=True)))\n",
    "    print(f\" trial {t+1}: ICE={(sample_domains[idx_t] == 1).mean():.3f}, unique={len(set(idx_t))}\")\n",
    "\n",
    "\n",
    "custom_nn = custom_nn.set_params(\n",
    "    iterator_train__sampler=sampler,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__batch_sampler=None,\n",
    ")\n",
    "\n",
    "print(f\"Sampler configured: CH IDs={n_ch}, ICE IDs={n_ice}, total train items={len(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Checkpointing\n",
    "\n",
    "Set `TRAIN = True` to train new DANN model with adversarial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "if TRAIN:\n",
    "    custom_nn.seed_all()\n",
    "\n",
    "    print(\"Training the DANN model...\")\n",
    "    print('Model parameters:')\n",
    "    for key, value in args.items():\n",
    "        if key == 'module':\n",
    "            print(f\"{key}: DANNNetwork\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(f\"dan_lambda: {DAN_LAMBDA}\")\n",
    "    custom_nn.fit(dataset, None)\n",
    "\n",
    "\n",
    "    # Generate filename with current date\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    model_filename = f\"dann_model_{current_date}\"\n",
    "\n",
    "    plot_training_history(custom_nn, skip_first_n=5)\n",
    "\n",
    "    # Save the model\n",
    "    custom_nn.save_model(model_filename)\n",
    "    \n",
    "    # save params dic\n",
    "    params_filename = f\"dann_params_{current_date}.pkl\"\n",
    "\n",
    "    with open(f\"models/{params_filename}\", \"wb\") as f:\n",
    "        pickle.dump({**args, 'dan_lambda': DAN_LAMBDA}, f)\n",
    "\n",
    "else:\n",
    "    # Load model and set to CPU\n",
    "    model_filename = \"dann_model_2025-08-19_2nd_try_4_glaciers_50%_lamba0_05_mean_domainloss_even_domainloss_only_NOR_val.pt\"  # Replace if needed\n",
    "    loaded_model = CustomDANNRegressor.load_model(\n",
    "        cfg,\n",
    "        model_filename,\n",
    "        **{\n",
    "            **args,\n",
    "            **param_init\n",
    "        },\n",
    "    )\n",
    "    loaded_model = loaded_model.set_params(device='cpu')\n",
    "    loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously trained DANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and set to CPU\n",
    "model_filename = \"dann_model_2025-09-12_5-10%_lambda0_1_mean_domainloss_even_domainloss_only_NOR_val_50_50_epochsplit.pt\"  # Replace if needed\n",
    "loaded_model = CustomDANNRegressor.load_model(\n",
    "    cfg,\n",
    "    model_filename,\n",
    "    **{\n",
    "        **args,\n",
    "        **param_init\n",
    "    },\n",
    ")\n",
    "loaded_model = loaded_model.set_params(device='cpu')\n",
    "loaded_model = loaded_model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation & Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "# IMPORTANT: ensure 'DOMAIN' is NOT part of feature_columns (keep it in cfg.metaData)\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictionsCombined_NN_additional(grouped_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
