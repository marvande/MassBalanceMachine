{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "# from scripts.helpers import *\n",
    "from scripts.norway_preprocess import *\n",
    "from scripts.config_NOR import *\n",
    "\n",
    "from regions.Switzerland.scripts.oggm import initialize_oggm_glacier_directories, export_oggm_grids\n",
    "from regions.Switzerland.scripts.glamos import merge_pmb_with_oggm_data, rename_stakes_by_elevation, check_point_ids_contain_glacier, remove_close_points, check_multiple_rgi_ids\n",
    "\n",
    "from regions.French_Alps.scripts.glacioclim_preprocess import add_svf_from_rgi_zarr, plot_missing_svf_for_all_glaciers, add_svf_nearest_valid\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.NorwayConfig()\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing:\n",
    "Load stakes, fill missing start dates, split into winter and annual and transform to WGMS format. Dataset acquired from https://doi.org/10.58059/sjse-6w92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes = pd.read_csv(\n",
    "    os.path.join(cfg.dataPath, path_PMB_WGMS_raw,\n",
    "                 'glaciological_point_mass_balance_Norway.csv'))\n",
    "df_stakes = df_stakes.rename(columns={'rgiid': 'RGIId'})\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "df_stakes['DATA_MODIFICATION'] = ''\n",
    "\n",
    "# FROM_DATE is missing in some glaciers despite having pmb measurements, fill with start of hydr. year\n",
    "df_stakes = fill_missing_dates(df_stakes)\n",
    "\n",
    "# Split into winter and annual measurements\n",
    "df_stakes = split_stake_measurements(df_stakes)\n",
    "\n",
    "# Transform to WGMS format\n",
    "df_stakes = df_stakes.rename(\n",
    "    columns={\n",
    "        'lat': 'POINT_LAT',\n",
    "        'lon': 'POINT_LON',\n",
    "        'altitude': 'POINT_ELEVATION',\n",
    "        'breid': 'GLACIER',\n",
    "    })\n",
    "# Only keep relevant columns in df\n",
    "df_stakes = df_stakes[[\n",
    "    'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION', 'FROM_DATE', 'TO_DATE',\n",
    "    'POINT_BALANCE', 'PERIOD', 'RGIId', 'YEAR', 'GLACIER', 'DATA_MODIFICATION',\n",
    "    'approx_loc', 'approx_altitude'\n",
    "]]\n",
    "\n",
    "# Convert datetime to yyyymmdd\n",
    "df_stakes['FROM_DATE'] = pd.to_datetime(\n",
    "    df_stakes['FROM_DATE'], format='%d.%m.%Y').dt.strftime('%Y%m%d')\n",
    "df_stakes['TO_DATE'] = pd.to_datetime(df_stakes['TO_DATE'],\n",
    "                                      format='%d.%m.%Y').dt.strftime('%Y%m%d')\n",
    "\n",
    "df_stakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add glacier names from RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"08\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "export_oggm_grids(cfg, gdirs, rgi_region=\"08\")\n",
    "\n",
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "df_stakes['GLACIER'] = df_stakes['RGIId'].map(rgi_to_name_dict)\n",
    "\n",
    "# RGI60-08.02966 has no glacier name in the RGI map so directly give it name Blåbreen\n",
    "df_stakes.loc[df_stakes['GLACIER'].isna(), 'GLACIER'] = 'Blabreen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unique POINT_ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new POINT_ID column\n",
    "df_stakes['POINT_ID'] = (df_stakes['GLACIER'] + '_' +\n",
    "                         df_stakes['YEAR'].astype(str) + '_' +\n",
    "                         df_stakes['PERIOD'].astype(str) + '_' +\n",
    "                         df_stakes['POINT_LAT'].astype(str) + '_' +\n",
    "                         df_stakes['POINT_LON'].astype(str) + '_' +\n",
    "                         df_stakes['approx_loc'].astype(str) + '_' +\n",
    "                         df_stakes['approx_altitude'].astype(str) + '_' +\n",
    "                         df_stakes.index.astype(str))\n",
    "\n",
    "# Drop columns that are not needed anymore\n",
    "df_stakes = df_stakes.drop(columns=['approx_loc', 'approx_altitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix wrong date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(df_stakes)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First fix is to switch all the months that have been wrongfully recorded as 01 instead of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function corrects the dates where 01 (Jan) has been entered as the month instead of 10 (Oct)\n",
    "df_stakes_dates_fix = fix_january_to_october_dates(df_stakes,\n",
    "                                                   annual_inconsistent,\n",
    "                                                   winter_inconsistent)\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_dates_fix)\n",
    "\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second fix is some by hand and the rest are wrong years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix outliers that don't have common explanation by hand\n",
    "# May instead of september\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Svartisheibreen_1994_annual_66.55012_13.72724_N_N_883',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "        '19940915', 'Changed TO_DATE month from May to September'\n",
    "    ]\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Svartisheibreen_1994_annual_66.54826_13.73128_N_N_884',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "        '19940915', 'Changed TO_DATE month from May to September'\n",
    "    ]\n",
    "# TO_DATE annual wrong year\n",
    "df_stakes_dates_fix.loc[df_stakes_dates_fix['POINT_ID'] ==\n",
    "                        'Aalfotbreen_1974_annual_61.74236_5.64623_N_N_1386',\n",
    "                        ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "                            '19740920',\n",
    "                            'Changed TO_DATE year from 1975 to 1974'\n",
    "                        ]\n",
    "df_stakes_dates_fix.loc[df_stakes_dates_fix['POINT_ID'] ==\n",
    "                        'Aalfotbreen_1971_annual_61.75213_5.63165_N_N_1493',\n",
    "                        ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "                            '19711124',\n",
    "                            'Changed TO_DATE year from 1970 to 1971'\n",
    "                        ]\n",
    "df_stakes_dates_fix.loc[df_stakes_dates_fix['POINT_ID'] ==\n",
    "                        'Graafjellsbrea_2009_annual_60.06923_6.38925_N_N_3545',\n",
    "                        ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "                            '20091013',\n",
    "                            'Changed TO_DATE year from 2019 to 2009'\n",
    "                        ]\n",
    "df_stakes_dates_fix.loc[df_stakes_dates_fix['POINT_ID'] ==\n",
    "                        'Bondhusbrea_1981_annual_60.03108_6.31014_N_N_3738',\n",
    "                        ['TO_DATE', 'DATA_MODIFICATION']] = [\n",
    "                            '19810827',\n",
    "                            'Changed TO_DATE year fomr 1980 to 1981'\n",
    "                        ]\n",
    "# TO_DATE winter wrong year\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Langfjordjoekulen_2019_winter_70.12528_21.71827_N_N_4019',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = [\n",
    "        '20200526', 'Changed TO_DATE year fomr 2019 to 2020', '2020',\n",
    "        'Langfjordjoekulen_2020_winter_70.12528_21.71827_N_N_4019'\n",
    "    ]\n",
    "\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Blaaisen_1966_winter_68.33479_17.85005_N_N_4155',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = [\n",
    "        '19670520', 'Changed TO_DATE year fomr 1966 to 1967', '1967',\n",
    "        'Blaaisen_1967_winter_68.33479_17.85005_N_N_4155'\n",
    "    ]\n",
    "\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Nigardsbreen_1963_winter_61.71461_7.11601_N_N_5802',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = [\n",
    "        '19640507', 'Changed TO_DATE year fomr 1963 to 1964', '1964',\n",
    "        'Nigardsbreen_1964_winter_61.71461_7.11601_N_N_5802'\n",
    "    ]\n",
    "\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Vesledalsbreen_1967_winter_61.84804_7.25335_N_N_6694',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = [\n",
    "        '19680418', 'Changed TO_DATE year fomr 1967 to 1968', '1968',\n",
    "        'Vesledalsbreen_1968_winter_61.84804_7.25335_N_N_6694'\n",
    "    ]\n",
    "\n",
    "df_stakes_dates_fix.loc[\n",
    "    df_stakes_dates_fix['POINT_ID'] ==\n",
    "    'Hellstugubreen_2010_winter_61.57329_8.44438_N_N_6935',\n",
    "    ['TO_DATE', 'DATA_MODIFICATION', 'YEAR', 'POINT_ID']] = [\n",
    "        '20110505', 'Changed TO_DATE year fomr 2010 to 2011', '2011',\n",
    "        'Hellstugubreen_2011_winter_61.57329_8.44438_N_N_6935'\n",
    "    ]\n",
    "\n",
    "# These stakes have nonsensical periods, remove them out of df and index list\n",
    "stakes_to_remove = [\n",
    "    'Austdalsbreen_2017_annual_61.81113_7.36766_Y_N_3038',\n",
    "    'Austdalsbreen_2017_annual_61.80888_7.38239_Y_N_3065',\n",
    "    'Aalfotbreen_1967_winter_61.74294_5.6365_N_N_5379',\n",
    "    'Hansebreen_2012_winter_61.74307_5.66278_N_N_5625',\n",
    "    'Austdalsbreen_2017_winter_61.81113_7.36766_Y_N_6792',\n",
    "    'Austdalsbreen_2017_winter_61.80888_7.38239_Y_N_6819'\n",
    "]\n",
    "df_stakes_dates_fix = df_stakes_dates_fix[~df_stakes_dates_fix['POINT_ID'].\n",
    "                                          isin(stakes_to_remove)]\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_dates_fix)\n",
    "\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining inconsistencies are all wrong FROM_DATE year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_indices = list(annual_inconsistent.index) + list(\n",
    "    winter_inconsistent.index)\n",
    "\n",
    "# For each remaining inconsistent record, change the year in FROM_DATE to the previous year\n",
    "for idx in remaining_indices:\n",
    "    # Get year from the YEAR column\n",
    "    year = int(df_stakes_dates_fix.loc[idx, 'YEAR']) - 1\n",
    "\n",
    "    # Extract month and day part from current FROM_DATE (keeping positions 4-8 which contain MMDD)\n",
    "    month_day = df_stakes_dates_fix.loc[idx, 'FROM_DATE'][4:8]\n",
    "\n",
    "    # Create new FROM_DATE by combining YEAR with the extracted month_day\n",
    "    df_stakes_dates_fix.loc[idx, 'FROM_DATE'] = f\"{year}{month_day}\"\n",
    "    df_stakes_dates_fix.loc[\n",
    "        idx,\n",
    "        'DATA_MODIFICATION'] = \"Changed faulty year in FROM_DATE to previous year of TO_DATE\"\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_dates_fix)\n",
    "\n",
    "# Display the inconsistent records\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent[[\n",
    "        'GLACIER', 'FROM_DATE', 'TO_DATE', 'MONTH_DIFF', 'PERIOD', 'YEAR',\n",
    "        'RGIId', 'POINT_ID'\n",
    "    ]])\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge close stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stakes_merged = remove_close_points(df_stakes_dates_fix)\n",
    "df_stakes_merged = pd.DataFrame()\n",
    "for gl in tqdm(df_stakes_dates_fix.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_stakes_dates_fix[df_stakes_dates_fix.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_stakes_merged = pd.concat([df_stakes_merged, df_gl_cleaned])\n",
    "df_stakes_merged.drop(['x', 'y'], axis=1, inplace=True)\n",
    "df_stakes_merged.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add OGGM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rgis = df_stakes_merged['RGIId'].unique()\n",
    "df_pmb_topo = merge_pmb_with_oggm_data(df_pmb=df_stakes_merged,\n",
    "                                       gdirs=gdirs,\n",
    "                                       rgi_region=\"08\",\n",
    "                                       rgi_version=\"62\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'Langfjordjoekulen'\n",
    "# stakes\n",
    "df_pmb_topo_1 = df_pmb_topo.copy()\n",
    "df_pmb_topo_1 = df_pmb_topo_1[(df_pmb_topo_1['GLACIER'] == glacierName)]\n",
    "RGIId = df_pmb_topo_1['RGIId'].unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(\n",
    "    f'{cfg.dataPath + \"OGGM/rgi_region_08/xr_grids/\"}/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_pmb_topo_1[\"POINT_LON\"].values, df_pmb_topo_1[\"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_pmb_topo_1['x'] = x_stake\n",
    "df_pmb_topo_1['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ds_oggm.glacier_mask.plot(cmap='binary')\n",
    "sns.scatterplot(df_pmb_topo_1,\n",
    "                x='x',\n",
    "                y='y',\n",
    "                hue='within_glacier_shape',\n",
    "                palette=['r', 'b'])\n",
    "plt.title(f'Stakes on {glacierName} (OGGM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep glaciers within RGIId shape and drop rows with NaN values anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial size ---\n",
    "n_start = len(df_pmb_topo)\n",
    "\n",
    "# Restrict to within glacier shape\n",
    "mask_within = df_pmb_topo['within_glacier_shape'] == True\n",
    "n_drop_shape = (~mask_within).sum()\n",
    "df_pmb_topo = df_pmb_topo.loc[mask_within].copy()\n",
    "df_pmb_topo = df_pmb_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "print(f\"Dropped {n_drop_shape} points outside glacier shape\")\n",
    "\n",
    "# Drop rows with NaN in consensus_ice_thickness\n",
    "mask_nan_thick = df_pmb_topo['consensus_ice_thickness'].isna()\n",
    "n_drop_thick = mask_nan_thick.sum()\n",
    "df_pmb_topo = df_pmb_topo.loc[~mask_nan_thick].copy()\n",
    "\n",
    "print(f\"Dropped {n_drop_thick} points with NaN consensus_ice_thickness\")\n",
    "\n",
    "# --- Final counts ---\n",
    "print(f\"Total dropped: {n_start - len(df_pmb_topo)}\")\n",
    "print('Number of winter and annual samples:', len(df_pmb_topo))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_topo[df_pmb_topo.PERIOD == 'winter']))\n",
    "\n",
    "# Unique glaciers, sorted\n",
    "glacier_list = sorted(df_pmb_topo.GLACIER.unique())\n",
    "print(f\"Number of glaciers: {len(glacier_list)}\")\n",
    "print(f\"Glaciers: {glacier_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_topo.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "    ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_topo.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for wrong elevation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checked, df_bad = flag_elevation_mismatch(df_pmb_topo, threshold=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Skyview factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one svf file\n",
    "rgi_id = df_pmb_topo.loc[0].RGIId\n",
    "\n",
    "rgi_gl = \"RGI60-08.00010\"\n",
    "\n",
    "# read ds with svf\n",
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              'RGI_v6/RGI_08_Scandinavia/xr_masked_grids/')\n",
    "\n",
    "xr.open_zarr(path_masked_xr + f'{rgi_gl}.zarr').svf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              \"RGI_v6/RGI_08_Scandinavia/xr_masked_grids\")\n",
    "\n",
    "df_pmb_topo_svf = add_svf_from_rgi_zarr(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    ")\n",
    "df_missing = df_pmb_topo_svf[df_pmb_topo_svf[\"svf\"].isna()].copy()\n",
    "print(\"Missing SVF points:\", len(df_missing))\n",
    "print(\"Glaciers affected:\", sorted(df_missing[\"RGIId\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_topo_svf_new = add_svf_nearest_valid(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    "    max_radius=30,  # ~30 grid cells search; adjust if needed\n",
    ")\n",
    "\n",
    "print(\"Missing SVF points after nearest-valid fill:\",\n",
    "      df_pmb_topo_svf_new[\"svf\"].isna().sum())\n",
    "\n",
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf_new,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give new stake IDS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_new_ids = rename_stakes_by_elevation(df_pmb_topo_svf_new)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_new_ids)\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_new_ids))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_new_ids['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_clean = df_pmb_new_ids.copy()\n",
    "\n",
    "# Ensure YYYYMMDD format\n",
    "df_pmb_clean[\"FROM_DATE\"] = df_pmb_clean[\"FROM_DATE\"].astype(str).str.zfill(8)\n",
    "df_pmb_clean[\"TO_DATE\"] = df_pmb_clean[\"TO_DATE\"].astype(str).str.zfill(8)\n",
    "\n",
    "# Extract months\n",
    "df_pmb_clean[\"MONTH_START\"] = df_pmb_clean[\"FROM_DATE\"].str[4:6]\n",
    "df_pmb_clean[\"MONTH_END\"] = df_pmb_clean[\"TO_DATE\"].str[4:6]\n",
    "\n",
    "\n",
    "def print_months(df, label):\n",
    "    winter = df[df.PERIOD == \"winter\"]\n",
    "    annual = df[df.PERIOD == \"annual\"]\n",
    "\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"Winter measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(winter[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(winter[\"MONTH_END\"].unique()))\n",
    "\n",
    "    print(\"\\nAnnual measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(annual[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(annual[\"MONTH_END\"].unique()))\n",
    "\n",
    "\n",
    "# --- Before filtering ---\n",
    "print_months(df_pmb_clean, \"Before filtering\")\n",
    "\n",
    "# --- Remove unwanted months in start/end (July + December) ---\n",
    "bad_months = {\"07\", \"12\"}\n",
    "mask_bad_months = (df_pmb_clean[\"MONTH_START\"].isin(bad_months)\n",
    "                   | df_pmb_clean[\"MONTH_END\"].isin(bad_months))\n",
    "n_removed = mask_bad_months.sum()\n",
    "\n",
    "df_pmb_clean = df_pmb_clean.loc[~mask_bad_months].copy()\n",
    "print(\n",
    "    f\"\\nRemoved {n_removed} rows with MONTH_START or MONTH_END in {sorted(bad_months)}.\"\n",
    ")\n",
    "\n",
    "# --- Correct mislabeled winter MB ---\n",
    "# If MONTH_END == 06 and MB is negative, it should be annual (not winter)\n",
    "mask_fix = ((df_pmb_clean[\"PERIOD\"].str.strip().str.lower() == \"winter\") &\n",
    "            (df_pmb_clean[\"MONTH_END\"] == \"06\") &\n",
    "            (df_pmb_clean[\"POINT_BALANCE\"] < 0))\n",
    "print(\"Rows to relabel winter -> annual:\", int(mask_fix.sum()))\n",
    "\n",
    "df_pmb_clean.loc[mask_fix, \"PERIOD\"] = \"annual\"\n",
    "\n",
    "# --- After filtering + relabeling ---\n",
    "print_months(df_pmb_clean, \"After filtering + relabeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv:\n",
    "df_pmb_clean.to_csv(os.path.join(cfg.dataPath, path_PMB_WGMS_csv,\n",
    "                                 'NOR_wgms_dataset_all.csv'),\n",
    "                    index=False)\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_clean['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "def plot_stakes_folium(\n",
    "    df_pmb_clean,\n",
    "    glacier_col=\"GLACIER\",\n",
    "    lat_col=None,\n",
    "    lon_col=None,\n",
    "    elev_col=None,\n",
    "    id_col=None,\n",
    "    center=None,\n",
    "    zoom_start=10,\n",
    "    color_map=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive Folium map of stake points grouped by glacier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infer column names if not provided\n",
    "    if lat_col is None:\n",
    "        lat_col = \"lat\" if \"lat\" in df_pmb_clean.columns else \"POINT_LAT\"\n",
    "    if lon_col is None:\n",
    "        lon_col = \"lon\" if \"lon\" in df_pmb_clean.columns else \"POINT_LON\"\n",
    "    if elev_col is None:\n",
    "        elev_col = \"altitude\" if \"altitude\" in df_pmb_clean.columns else \"POINT_ELEVATION\"\n",
    "    if id_col is None:\n",
    "        id_col = \"stake_number\" if \"stake_number\" in df_pmb_clean.columns else \"POINT_ID\"\n",
    "\n",
    "    # Compute center if not provided\n",
    "    if center is None:\n",
    "        center_lat = float(df_pmb_clean[lat_col].median())\n",
    "        center_lon = float(df_pmb_clean[lon_col].median())\n",
    "    else:\n",
    "        center_lat, center_lon = center\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start)\n",
    "\n",
    "    # Default colors (cycled) if user doesn't give explicit mapping\n",
    "    default_colors = [\n",
    "        \"red\", \"blue\", \"green\", \"purple\", \"orange\", \"darkred\", \"cadetblue\",\n",
    "        \"darkgreen\", \"darkpurple\", \"pink\", \"gray\", \"black\"\n",
    "    ]\n",
    "\n",
    "    glaciers = sorted(df_pmb_clean[glacier_col].dropna().unique())\n",
    "\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            g: default_colors[i % len(default_colors)]\n",
    "            for i, g in enumerate(glaciers)\n",
    "        }\n",
    "    else:\n",
    "        # fill missing glaciers with default cycling\n",
    "        for i, g in enumerate(glaciers):\n",
    "            color_map.setdefault(g, default_colors[i % len(default_colors)])\n",
    "\n",
    "    # Add markers for each glacier\n",
    "    for glacier_name, df_g in df_pmb_clean.groupby(glacier_col):\n",
    "        if pd.isna(glacier_name):\n",
    "            continue\n",
    "\n",
    "        fg = folium.FeatureGroup(name=str(glacier_name))\n",
    "        color = color_map[str(glacier_name)]\n",
    "\n",
    "        for _, row in df_g.iterrows():\n",
    "            stake_id = row.get(id_col, \"NA\")\n",
    "            altitude = row.get(elev_col, \"NA\")\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=[row[lat_col], row[lon_col]],\n",
    "                radius=5,\n",
    "                color=color,\n",
    "                fill=True,\n",
    "                fill_color=color,\n",
    "                fill_opacity=0.9,\n",
    "                popup=f\"{glacier_name} - Stake {stake_id}: {altitude} m\",\n",
    "            ).add_to(fg)\n",
    "\n",
    "        fg.add_to(m)\n",
    "\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # Legend (auto-generated)\n",
    "    legend_rows = \"\\n\".join(\n",
    "        f'<p><span style=\"color: {color_map[g]};\">●</span> {g}</p>'\n",
    "        for g in glaciers)\n",
    "\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"\n",
    "        position: fixed; bottom: 50px; left: 50px; z-index: 1000;\n",
    "        background-color: white; padding: 10px; border-radius: 5px;\n",
    "        border: 1px solid #999;\n",
    "    \">\n",
    "        <p><strong>Glaciers</strong></p>\n",
    "        {legend_rows}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "m = plot_stakes_folium(df_pmb_clean, color_map=None)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
