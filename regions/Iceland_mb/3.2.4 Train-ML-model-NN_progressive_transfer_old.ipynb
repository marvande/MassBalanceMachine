{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "import xarray as xr\n",
    "import massbalancemachine as mbm\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from skorch.helper import SliceDataset\n",
    "from datetime import datetime\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.iceland_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_ICE import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.IcelandConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Iceland/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Plot styles:\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "vois_climate = [\n",
    "    't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10'\n",
    "]\n",
    "\n",
    "vois_topographical = [\n",
    "    \"aspect\", # OGGM\n",
    "    \"slope\", # OGGM\n",
    "    \"hugonnet_dhdt\",  # OGGM\n",
    "    \"consensus_ice_thickness\",  # OGGM\n",
    "    \"millan_v\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(cfg.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read GL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wgms = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'ICE_dataset_all_oggm_with_hugonnetdhdt.csv')\n",
    "\n",
    "# Drop Nan entries in hugonnetdhdt of Iceland dataset\n",
    "data_wgms = data_wgms.dropna(subset=data_wgms.columns.drop('DATA_MODIFICATION'))\n",
    "\n",
    "print('Number of glaciers:', len(data_wgms['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_wgms[data_wgms.PERIOD == 'annual']) + len(data_wgms[data_wgms.PERIOD == 'winter']) + len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "\n",
    "data_wgms.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Progressive Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ICE_test = data_wgms.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_ICECH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_ICECH.nc'\n",
    "}\n",
    "\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_ICE_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'ICE_dataset_monthly_full_with_hugonnetdhdt.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "display(data_monthly.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_monthly['GLACIER'].value_counts())\n",
    "display(data_monthly.shape)\n",
    "\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_monthly[data_monthly['GLACIER']=='RGI60-06.00478'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4 glaciers 50% train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFER LEARNING SETUP 50%\n",
    "# Fine-tuning glaciers (4 Iceland glaciers to adapt Swiss model)\n",
    "train_glaciers = ['Bruarjoekull', 'Skeidararjoekull', 'Koeldukvislarjoekull', 'Slettjoekull West', 'RGI60-06.00238', 'Hagafellsjoekull West']\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split east/west"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get glacier latitudes\n",
    "glacier_lat = data_wgms.groupby('GLACIER')['POINT_LON'].first()\n",
    "\n",
    "# Use the median latitude as the split\n",
    "lon_threshold = -18.25\n",
    "\n",
    "east_glaciers = glacier_lat[glacier_lat >= lon_threshold].index.tolist()\n",
    "west_glaciers = glacier_lat[glacier_lat < lon_threshold].index.tolist()\n",
    "\n",
    "print(f\"East glaciers ({len(east_glaciers)}): {east_glaciers}\")\n",
    "print(f\"West glaciers ({len(west_glaciers)}): {west_glaciers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# North/south at median, train is south set 50%\n",
    "train_glaciers = east_glaciers\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5-10% train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFER LEARNING SETUP 5-10%\n",
    "# Fine-tuning glaciers\n",
    "train_glaciers = ['Mulajoekull' ,'Slettjoekull West', 'Hagafellsjoekull East (Langjoekull S Dome)', 'Tungnaarjoekull', 'RGI60-06.00478']\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "## CV Splits\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_set)\n",
    "display(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train/val split on random 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valing dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Get val set\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train/val split on specific glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glacier-wise train/val split: validate on Engabreen, train on the other 3\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "val_glacier = ['Engabreen']\n",
    "train_glaciers = [g for g in train_glaciers if g not in val_glacier]\n",
    "\n",
    "df_X_train = data_train[data_train['GLACIER'].isin(train_glaciers)].copy()\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "df_X_val = data_train[data_train['GLACIER'].isin(val_glacier)].copy()\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_period_indicator(df):\n",
    "    \"\"\"Create numerical PERIOD_INDICATOR feature\"\"\"\n",
    "    df = df.copy()\n",
    "    df['PERIOD_INDICATOR'] = df['PERIOD'].map({'annual': 0, 'winter': 1})\n",
    "    return df\n",
    "\n",
    "# Apply to all datasets\n",
    "df_X_train = create_period_indicator(df_X_train)\n",
    "df_X_val = create_period_indicator(df_X_val)\n",
    "test_set['df_X'] = create_period_indicator(test_set['df_X'])\n",
    "\n",
    "print(\"PERIOD_INDICATOR created:\")\n",
    "print(\"Annual (0):\", (df_X_train['PERIOD_INDICATOR'] == 0).sum())\n",
    "print(\"Winter (1):\", (df_X_train['PERIOD_INDICATOR'] == 1).sum())\n",
    "print(\"Original PERIOD column preserved:\", df_X_train['PERIOD'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "feature_columns = features_topo + list(vois_climate)# + ['PERIOD_INDICATOR']\n",
    "\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=15,\n",
    "    threshold=1e-4,\n",
    ")\n",
    "\n",
    "lr_scheduler_cb = LRScheduler(policy=ReduceLROnPlateau,\n",
    "                              monitor='valid_loss',\n",
    "                              mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              threshold=0.01,\n",
    "                              threshold_mode='rel',\n",
    "                              verbose=True)\n",
    "\n",
    "dataset = dataset_val = None\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    return dataset, dataset_val\n",
    "\n",
    "param_init = {'device': 'cpu'}\n",
    "nInp = len(feature_columns)\n",
    "\n",
    "checkpoint_cb = Checkpoint(\n",
    "    monitor='valid_loss_best',\n",
    "    f_params='best_model.pt',\n",
    "    f_optimizer=None,     # do not save optimizer state\n",
    "    f_history=None,       # do not save training history\n",
    "    f_criterion=None,     # do not save criterion state\n",
    "    load_best=True,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "params = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'optimizer__weight_decay': 1e-05,\n",
    "    'module__hidden_layers': [128, 128, 64, 32],\n",
    "    'module__dropout': 0.2,\n",
    "    'module__use_batchnorm': True,\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'module': FlexibleNetwork,\n",
    "    'nbFeatures': nInp,\n",
    "    'module__input_dim': nInp,\n",
    "    'module__dropout': params['module__dropout'],\n",
    "    'module__hidden_layers': params['module__hidden_layers'],\n",
    "    'train_split': my_train_split,\n",
    "    'batch_size': params['batch_size'],\n",
    "    'verbose': 1,\n",
    "    'iterator_train__shuffle': True,\n",
    "    'lr': params['lr'],\n",
    "    'max_epochs': 200,\n",
    "    'optimizer': params['optimizer'],\n",
    "    'optimizer__weight_decay': params['optimizer__weight_decay'],\n",
    "    'module__use_batchnorm': params['module__use_batchnorm'],\n",
    "    'callbacks': [\n",
    "        ('early_stop', early_stop),\n",
    "        ('lr_scheduler', lr_scheduler_cb),\n",
    "    ]\n",
    "}\n",
    "\n",
    "custom_nn = mbm.models.CustomNeuralNetRegressor(cfg, **args, **param_init)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features, metadata = custom_nn._create_features_metadata(df_X_train_subset)\n",
    "\n",
    "features_val, metadata_val = custom_nn._create_features_metadata(\n",
    "    df_X_val_subset)\n",
    "\n",
    "# Define the dataset for the NN\n",
    "dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                features=features,\n",
    "                                                metadata=metadata,\n",
    "                                                targets=y_train)\n",
    "dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                  SliceDataset(dataset, idx=1))\n",
    "print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "\n",
    "dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features_val,\n",
    "                                                    metadata=metadata_val,\n",
    "                                                    targets=y_val)\n",
    "dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "    SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\"\"\"\n",
    "# Don't create datasets here, create them after loading the Swiss model\n",
    "features = features_val = None\n",
    "metadata = metadata_val = None\n",
    "dataset = dataset_val = None\n",
    "print(\"Datasets will be created after loading Swiss model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fine tuning and freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "\n",
    "class SaveBestAtEpochs(Callback):\n",
    "    def __init__(self, epochs, prefix=\"nn_model_best_epoch\"):\n",
    "        self.epochs = set(epochs)\n",
    "        self.prefix = prefix\n",
    "        self.best_score = float('inf')\n",
    "        self.best_state = None\n",
    "\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        epoch = net.history[-1]['epoch']\n",
    "        valid_loss = net.history[-1]['valid_loss']\n",
    "        if valid_loss < self.best_score:\n",
    "            self.best_score = valid_loss\n",
    "            self.best_state = {k: v.cpu().clone() for k, v in net.module_.state_dict().items()}\n",
    "        if epoch in self.epochs and self.best_state is not None:\n",
    "            filename = f\"{self.prefix}_{epoch}.pt\"\n",
    "            torch.save(self.best_state, filename)\n",
    "            print(f\"Best model up to epoch {epoch} saved as {filename}\")\n",
    "\n",
    "save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\n",
    "\n",
    "TRAIN = True  # Set to True to actually train\n",
    "if TRAIN:\n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True, # Important!!! this tells skorch to not re-initialize the weights etc.\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "            ('save_best_at_epochs', save_best_epochs_cb)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using the loaded Swiss model\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 2.5: Freeze layers\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        # Freeze layers\n",
    "        if name not in [#'model.0.weight', 'model.0.bias',\n",
    "                        'model.1.weight', 'model.1.bias',\n",
    "                        #'model.4.weight', 'model.4.bias',\n",
    "                        'model.5.weight', 'model.5.bias',\n",
    "                        #'model.8.weight', 'model.8.bias',\n",
    "                        'model.9.weight', 'model.9.bias',\n",
    "                        #'model.12.weight', 'model.12.bias',\n",
    "                        'model.13.weight', 'model.13.bias',\n",
    "                        #'model.16.weight', 'model.16.bias'\n",
    "                        ]:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # STEP 3: Update for fine-tuning\n",
    "    print(\"Updating model for fine-tuning...\")\n",
    "    loaded_model = loaded_model.set_params(\n",
    "        lr=0.0005,\n",
    "        max_epochs=100,\n",
    "    )\n",
    "    \n",
    "    # STEP 4: Fine-tune\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    loaded_model.fit(features, y_train)\n",
    "    \n",
    "    # STEP 5: Save\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"✓ Fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Progressively unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True  # Set to True to actually train\n",
    "if TRAIN:\n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True, # Important!!! this tells skorch not re-initialize the weights etc.\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using the loaded Swiss model\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(\n",
    "        SliceDataset(dataset_val, idx=0), SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 2.5: Freeze layers\n",
    "    \n",
    "    # Helper to freeze/unfreeze layers\n",
    "    def set_requires_grad(layer_names, requires_grad=True):\n",
    "        for name, param in loaded_model.module_.named_parameters():\n",
    "            if name in layer_names:\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    # List of layer groups to progressively unfreeze\n",
    "    layer_groups = [\n",
    "        #(\n",
    "            #[\n",
    "                #'model.1.weight', 'model.1.bias',\n",
    "                #'model.5.weight', 'model.5.bias',\n",
    "                #'model.9.weight', 'model.9.bias',\n",
    "                #'model.13.weight', 'model.13.bias'\n",
    "            #],200,  0.1\n",
    "        #),\n",
    "        \n",
    "        (['model.16.weight', 'model.16.bias'], 30, 0.01),\n",
    "        (['model.12.weight', 'model.12.bias'], 20, 0.005),\n",
    "        (['model.8.weight', 'model.8.bias'], 10, 0.001)\n",
    "    ]\n",
    "\n",
    "    # Freeze all layers first\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Progressive unfreezing loop with custom learning rates\n",
    "    for layers, epochs, lr in layer_groups:\n",
    "        set_requires_grad(layers, True)\n",
    "        print(f\"Fine-tuning layers: {layers} for {epochs} epochs with lr={lr}...\")\n",
    "        loaded_model = loaded_model.set_params(\n",
    "            lr=lr,\n",
    "            max_epochs=epochs,\n",
    "            callbacks=[\n",
    "                ('early_stop', early_stop),\n",
    "                ('lr_scheduler', lr_scheduler_cb),\n",
    "                ('checkpoint', checkpoint_cb),\n",
    "                ]\n",
    "            )\n",
    "        loaded_model.fit(features, y_train)\n",
    "\n",
    "        val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "        print(\"Validation score:\", val_score)\n",
    "    \n",
    "    # STEP 3: Save\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"✓ Fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "        loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "display(scores_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 15, 20, 30, 50, 100]\n",
    "model_prefix = \"nn_model_best_epoch\"\n",
    "\n",
    "for epoch in epochs:\n",
    "    model_name = f\"{model_prefix}_{epoch}.pt\"\n",
    "    if not os.path.exists(model_name):\n",
    "        continue\n",
    "\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor(\n",
    "        cfg,\n",
    "        **swiss_args,\n",
    "        **param_init\n",
    "    )\n",
    "    loaded_model = loaded_model.set_params(device='cpu')\n",
    "    loaded_model = loaded_model.to('cpu')\n",
    "\n",
    "    loaded_model.initialize()\n",
    "    state_dict = torch.load(model_name, map_location='cpu')\n",
    "    loaded_model.module_.load_state_dict(state_dict)\n",
    "\n",
    "    # 4. Evaluate\n",
    "    grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "        loaded_model, df_X_test_subset, test_set['y'], cfg, mbm)\n",
    "    print(f\"Scores for epoch {epoch}:\")\n",
    "    display(scores_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "print(\"Validation score (higher is better):\", val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in loaded_model.module_.named_parameters():\n",
    "    print(name)\n",
    "print(loaded_model.module_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in loaded_model.module_.named_parameters():\n",
    "    print(name, param.data.cpu().numpy().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features and metadata\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(\n",
    "    df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU if they are torch tensors\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "\n",
    "# Ensure targets are also on CPU\n",
    "targets_test = test_set['y']\n",
    "if hasattr(targets_test, 'cpu'):\n",
    "    targets_test = targets_test.cpu()\n",
    "\n",
    "# Create the dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),\n",
    "    SliceDataset(dataset_test, idx=1)\n",
    "]\n",
    "\n",
    "# Make predictions aggr to meas ID\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate scores\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "# Aggregate predictions\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "data = {\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "}\n",
    "grouped_ids = pd.DataFrame(data)\n",
    "\n",
    "# Add period\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "# Add glacier name\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "# Add YEAR\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPredictions_NN(grouped_ids)\n",
    "predVSTruth_all(grouped_ids, mae, rmse, title='NN on test')\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
