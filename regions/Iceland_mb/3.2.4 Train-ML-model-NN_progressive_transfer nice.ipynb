{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Glacier Mass Balance Prediction\n",
    "\n",
    "This notebook demonstrates **transfer learning** between glacier mass balance models trained on different regions. Specifically, we:\n",
    "\n",
    "1. **Load a pre-trained neural network** trained on Swiss glacier data\n",
    "2. **Fine-tune it on Icelandic glacier data** using various strategies\n",
    "3. **Evaluate performance** on unseen Icelandic glaciers\n",
    "\n",
    "## Key Features\n",
    "- **Progressive layer unfreezing** for gradual adaptation\n",
    "- **Multiple train/test split strategies** (50%, North/South, 5-10%)\n",
    "- **Comprehensive evaluation metrics** and visualizations\n",
    "- **Model checkpointing** at specific epochs for analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Pre-trained model on all Swiss data from ../regions/Switzerland/3.2.2 Train-ML-model-NN.ipynb e.g. `nn_model_2025-07-14_CH_flexible.pt`\n",
    "- Icelandic glacier dataset from ../regions/Iceland_mb/1.1. Iceland-prepro.ipynb\n",
    "- ERA5 climate data of Iceland from ../regions/Iceland_mb/1.2. ERA5Land-prepro.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root of repo to import MBM\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../'))\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "#import pickle # for displaying saved model parameters etc.\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmcrameri import cm\n",
    "\n",
    "# Scientific computing\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Skorch (scikit-learn compatible PyTorch)\n",
    "from skorch.helper import SliceDataset\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler, Checkpoint\n",
    "\n",
    "# MassBalanceMachine (custom package)\n",
    "import massbalancemachine as mbm\n",
    "\n",
    "# Local helper modules\n",
    "\n",
    "from scripts.helpers import *\n",
    "from scripts.iceland_preprocess import *\n",
    "from scripts.plots import *\n",
    "from scripts.config_ICE import *\n",
    "from scripts.nn_helpers import *\n",
    "from scripts.xgb_helpers import *\n",
    "from scripts.NN_networks import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "cfg = mbm.IcelandConfig(dataPath='/home/mburlet/scratch/data/DATA_MB/WGMS/Iceland/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "seed_all(cfg.seed)\n",
    "free_up_cuda()\n",
    "\n",
    "# Configure plotting style\n",
    "path_style_sheet = 'scripts/example.mplstyle'\n",
    "plt.style.use(path_style_sheet)\n",
    "\n",
    "# Define climate features from ERA5 reanalysis\n",
    "vois_climate = [\n",
    "    't2m',     # 2-meter temperature\n",
    "    'tp',      # Total precipitation  \n",
    "    'slhf',    # Surface latent heat flux\n",
    "    'sshf',    # Surface sensible heat flux\n",
    "    'ssrd',    # Surface solar radiation downwards\n",
    "    'fal',     # Forecast albedo\n",
    "    'str',     # Surface thermal radiation\n",
    "    'u10',     # 10-meter U wind component\n",
    "    'v10'      # 10-meter V wind component\n",
    "]\n",
    "\n",
    "# Define topographical features from OGGM\n",
    "vois_topographical = [\n",
    "    \"aspect\",                    # Terrain aspect (OGGM)\n",
    "    \"slope\",                     # Terrain slope (OGGM)\n",
    "    \"hugonnet_dhdt\",            # Ice thickness change (OGGM)\n",
    "    \"consensus_ice_thickness\",   # Ice thickness consensus (OGGM)\n",
    "    \"millan_v\",                 # Ice velocity (OGGM)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility across runs\n",
    "seed_all(cfg.seed)\n",
    "\n",
    "# Check for CUDA availability and configure accordingly\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    free_up_cuda()\n",
    "\n",
    "    # # Try to limit CPU usage of random search\n",
    "    # torch.set_num_threads(2)  # or 1\n",
    "    # os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    # os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "else:\n",
    "    print(\"CUDA is NOT available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "### Create Icelandic Glacier Dataset\n",
    "We start with point mass balance measurements and transform them to monthly format with ERA5 climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of glaciers: 47\n",
      "Number of winter, summer and annual samples: 5930\n",
      "Number of annual samples: 2883\n",
      "Number of winter samples: 3047\n",
      "Number of summer samples: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['POINT_ELEVATION', 'POINT_LAT', 'POINT_LON', 'DATA_MODIFICATION',\n",
       "       'FROM_DATE', 'TO_DATE', 'POINT_BALANCE', 'PERIOD', 'YEAR', 'RGIId',\n",
       "       'aspect', 'slope', 'topo', 'hugonnet_dhdt', 'consensus_ice_thickness',\n",
       "       'millan_v', 'GLACIER', 'POINT_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Icleandic glacier dataset with topographical features\n",
    "data_wgms = pd.read_csv(cfg.dataPath + path_PMB_WGMS_csv + 'ICE_dataset_all_oggm_with_hugonnetdhdt.csv')\n",
    "\n",
    "# Remove entries with missing hugonnet_dhdt data\n",
    "data_wgms = data_wgms.dropna(subset=data_wgms.columns.drop('DATA_MODIFICATION'))\n",
    "\n",
    "print('Number of glaciers:', len(data_wgms['GLACIER'].unique()))\n",
    "print('Number of winter, summer and annual samples:', len(data_wgms[data_wgms.PERIOD == 'annual']) + len(data_wgms[data_wgms.PERIOD == 'winter']) + len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "print('Number of annual samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'winter']))\n",
    "print('Number of summer samples:',\n",
    "      len(data_wgms[data_wgms.PERIOD == 'summer']))\n",
    "\n",
    "data_wgms.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 15:46:41,819 - INFO - Loaded preprocessed data.\n",
      "2025-08-23 15:46:41,820 - INFO - Number of monthly rows: 56209\n",
      "2025-08-23 15:46:41,828 - INFO - Number of annual samples: 34915\n",
      "2025-08-23 15:46:41,835 - INFO - Number of winter samples: 21294\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>POINT_LON</th>\n",
       "      <th>POINT_LAT</th>\n",
       "      <th>POINT_BALANCE</th>\n",
       "      <th>ALTITUDE_CLIMATE</th>\n",
       "      <th>ELEVATION_DIFFERENCE</th>\n",
       "      <th>POINT_ELEVATION</th>\n",
       "      <th>RGIId</th>\n",
       "      <th>POINT_ID</th>\n",
       "      <th>ID</th>\n",
       "      <th>...</th>\n",
       "      <th>millan_v</th>\n",
       "      <th>t2m</th>\n",
       "      <th>tp</th>\n",
       "      <th>slhf</th>\n",
       "      <th>sshf</th>\n",
       "      <th>ssrd</th>\n",
       "      <th>fal</th>\n",
       "      <th>str</th>\n",
       "      <th>u10</th>\n",
       "      <th>v10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989.0</td>\n",
       "      <td>-18.595688</td>\n",
       "      <td>64.790063</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1094.738918</td>\n",
       "      <td>-45.138918</td>\n",
       "      <td>1049.6</td>\n",
       "      <td>RGI60-06.00234</td>\n",
       "      <td>Thjorsarjoekull (Hofsjoekull E)_1989.0_annual_...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.446609</td>\n",
       "      <td>-4.787933</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>198886.0</td>\n",
       "      <td>590722.0</td>\n",
       "      <td>3990646.0</td>\n",
       "      <td>0.84564</td>\n",
       "      <td>-1779436.0</td>\n",
       "      <td>-0.590102</td>\n",
       "      <td>0.603381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989.0</td>\n",
       "      <td>-18.595688</td>\n",
       "      <td>64.790063</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1094.738918</td>\n",
       "      <td>-45.138918</td>\n",
       "      <td>1049.6</td>\n",
       "      <td>RGI60-06.00234</td>\n",
       "      <td>Thjorsarjoekull (Hofsjoekull E)_1989.0_annual_...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.446609</td>\n",
       "      <td>-5.262054</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>306376.0</td>\n",
       "      <td>576896.0</td>\n",
       "      <td>883468.0</td>\n",
       "      <td>0.84690</td>\n",
       "      <td>-1458576.0</td>\n",
       "      <td>0.129348</td>\n",
       "      <td>2.214950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     YEAR  POINT_LON  POINT_LAT  POINT_BALANCE  ALTITUDE_CLIMATE  \\\n",
       "0  1989.0 -18.595688  64.790063           0.45       1094.738918   \n",
       "1  1989.0 -18.595688  64.790063           0.45       1094.738918   \n",
       "\n",
       "   ELEVATION_DIFFERENCE  POINT_ELEVATION           RGIId  \\\n",
       "0            -45.138918           1049.6  RGI60-06.00234   \n",
       "1            -45.138918           1049.6  RGI60-06.00234   \n",
       "\n",
       "                                            POINT_ID  ID  ...   millan_v  \\\n",
       "0  Thjorsarjoekull (Hofsjoekull E)_1989.0_annual_...   0  ...  25.446609   \n",
       "1  Thjorsarjoekull (Hofsjoekull E)_1989.0_annual_...   0  ...  25.446609   \n",
       "\n",
       "        t2m        tp      slhf      sshf       ssrd      fal        str  \\\n",
       "0 -4.787933  0.003747  198886.0  590722.0  3990646.0  0.84564 -1779436.0   \n",
       "1 -5.262054  0.003766  306376.0  576896.0   883468.0  0.84690 -1458576.0   \n",
       "\n",
       "        u10       v10  \n",
       "0 -0.590102  0.603381  \n",
       "1  0.129348  2.214950  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_ICE_test = data_wgms.copy()\n",
    "\n",
    "# Transform data to monthly format (run or load data):\n",
    "paths = {\n",
    "    'csv_path': cfg.dataPath + path_PMB_WGMS_csv,\n",
    "    'era5_climate_data': cfg.dataPath + path_ERA5_raw + 'era5_monthly_averaged_data_ICECH.nc',\n",
    "    'geopotential_data': cfg.dataPath + path_ERA5_raw + 'era5_geopotential_pressure_ICECH.nc'\n",
    "}\n",
    "\n",
    "# Transform point measurements to monthly format and merge with ERA5 climate data\n",
    "# Set RUN=True to reprocess data, False to load existing preprocessed file\n",
    "RUN = False\n",
    "dataloader_gl = process_or_load_data(run_flag=RUN,\n",
    "                                     df=data_ICE_test,\n",
    "                                     paths=paths,\n",
    "                                     cfg=cfg,\n",
    "                                     vois_climate=vois_climate,\n",
    "                                     vois_topographical=vois_topographical,\n",
    "                                     output_file= 'ICE_dataset_monthly_full_with_hugonnetdhdt.csv')\n",
    "data_monthly = dataloader_gl.data\n",
    "\n",
    "display(data_monthly.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLACIER\n",
       "RGI60-06.00238                                8230\n",
       "Bruarjoekull                                  6814\n",
       "Skeidararjoekull                              5008\n",
       "Thjorsarjoekull (Hofsjoekull E)               3962\n",
       "Blagnipujoekull (Hofsjoekull SW)              3317\n",
       "Hagafellsjoekull West                         3207\n",
       "Breidamerkurjoekull                           2953\n",
       "Koeldukvislarjoekull                          2897\n",
       "Langjoekull Ice Cap                           2647\n",
       "Dyngjujoekull                                 2305\n",
       "Eyjabakkajoekull                              2110\n",
       "Tungnaarjoekull                               1430\n",
       "Hagafellsjoekull East (Langjoekull S Dome)    1050\n",
       "RGI60-06.00292                                1001\n",
       "RGI60-06.00302                                 973\n",
       "RGI60-06.00478                                 957\n",
       "RGI60-06.00294                                 946\n",
       "RGI60-06.00311                                 857\n",
       "RGI60-06.00465                                 741\n",
       "Mulajoekull                                    669\n",
       "RGI60-06.00466                                 609\n",
       "RGI60-06.00480                                 553\n",
       "RGI60-06.00305                                 473\n",
       "RGI60-06.00340                                 430\n",
       "RGI60-06.00232                                 401\n",
       "Slettjoekull West                              287\n",
       "RGI60-06.00328                                 260\n",
       "RGI60-06.00303                                 228\n",
       "RGI60-06.00228                                 205\n",
       "RGI60-06.00476                                 122\n",
       "Oeldufellsjoekull                               99\n",
       "RGI60-06.00411                                  77\n",
       "RGI60-06.00413                                  77\n",
       "RGI60-06.00409                                  59\n",
       "RGI60-06.00349                                  42\n",
       "RGI60-06.00359                                  40\n",
       "RGI60-06.00320                                  30\n",
       "RGI60-06.00422                                  29\n",
       "RGI60-06.00342                                  21\n",
       "RGI60-06.00301                                  21\n",
       "RGI60-06.00350                                  20\n",
       "RGI60-06.00425                                   9\n",
       "RGI60-06.00479                                   9\n",
       "RGI60-06.00474                                   9\n",
       "RGI60-06.00445                                   9\n",
       "RGI60-06.00306                                   8\n",
       "RGI60-06.00296                                   8\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(56209, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_monthly['GLACIER'].value_counts())\n",
    "display(data_monthly.shape)\n",
    "\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split Strategies\n",
    "\n",
    "Implement three different strategies for splitting the Norway data to test various transfer learning scenarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: 50% Random Split (4 glaciers for fine-tuning)\n",
    "**Data**: About 50 % of the available data is used as fine-tuning set, consisting of 6 glaciers, which have been chosen from to be representative of the 4 available ice caps.\n",
    "\n",
    "**Use case**: Balanced representation with good data availability for transfer learning adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning glaciers (6): ['Bruarjoekull', 'Skeidararjoekull', 'Koeldukvislarjoekull', 'Slettjoekull West', 'RGI60-06.00238', 'Hagafellsjoekull West']\n",
      "Test glaciers (41): ['Thjorsarjoekull (Hofsjoekull E)', 'Breidamerkurjoekull', 'Dyngjujoekull', 'RGI60-06.00328', 'Tungnaarjoekull', 'Eyjabakkajoekull', 'RGI60-06.00303', 'Langjoekull Ice Cap', 'Oeldufellsjoekull', 'RGI60-06.00466', 'RGI60-06.00411', 'RGI60-06.00302', 'RGI60-06.00359', 'RGI60-06.00340', 'Blagnipujoekull (Hofsjoekull SW)', 'RGI60-06.00320', 'Hagafellsjoekull East (Langjoekull S Dome)', 'RGI60-06.00342', 'RGI60-06.00480', 'RGI60-06.00465', 'RGI60-06.00294', 'RGI60-06.00292', 'RGI60-06.00232', 'RGI60-06.00478', 'Mulajoekull', 'RGI60-06.00301', 'RGI60-06.00413', 'RGI60-06.00311', 'RGI60-06.00350', 'RGI60-06.00476', 'RGI60-06.00228', 'RGI60-06.00409', 'RGI60-06.00349', 'RGI60-06.00422', 'RGI60-06.00305', 'RGI60-06.00425', 'RGI60-06.00306', 'RGI60-06.00479', 'RGI60-06.00296', 'RGI60-06.00445', 'RGI60-06.00474']\n",
      "Train glaciers: (6) ['Hagafellsjoekull West' 'Koeldukvislarjoekull' 'Skeidararjoekull'\n",
      " 'Slettjoekull West' 'Bruarjoekull' 'RGI60-06.00238']\n",
      "Test glaciers: (41) ['Thjorsarjoekull (Hofsjoekull E)' 'Breidamerkurjoekull' 'Dyngjujoekull'\n",
      " 'RGI60-06.00328' 'Tungnaarjoekull' 'Eyjabakkajoekull' 'RGI60-06.00303'\n",
      " 'Langjoekull Ice Cap' 'Oeldufellsjoekull' 'RGI60-06.00466'\n",
      " 'RGI60-06.00411' 'RGI60-06.00302' 'RGI60-06.00359' 'RGI60-06.00340'\n",
      " 'Blagnipujoekull (Hofsjoekull SW)' 'RGI60-06.00320'\n",
      " 'Hagafellsjoekull East (Langjoekull S Dome)' 'RGI60-06.00342'\n",
      " 'RGI60-06.00480' 'RGI60-06.00465' 'RGI60-06.00294' 'RGI60-06.00292'\n",
      " 'RGI60-06.00232' 'RGI60-06.00478' 'Mulajoekull' 'RGI60-06.00301'\n",
      " 'RGI60-06.00413' 'RGI60-06.00311' 'RGI60-06.00350' 'RGI60-06.00476'\n",
      " 'RGI60-06.00228' 'RGI60-06.00409' 'RGI60-06.00349' 'RGI60-06.00422'\n",
      " 'RGI60-06.00305' 'RGI60-06.00425' 'RGI60-06.00306' 'RGI60-06.00479'\n",
      " 'RGI60-06.00296' 'RGI60-06.00445' 'RGI60-06.00474']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'length train set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "26443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'length test set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29766"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRANSFER LEARNING SETUP 50%\n",
    "# Fine-tuning glaciers\n",
    "train_glaciers = ['Bruarjoekull', 'Skeidararjoekull', 'Koeldukvislarjoekull', 'Slettjoekull West', 'RGI60-06.00238', 'Hagafellsjoekull West']\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: East/West Geographic Split\n",
    "\n",
    "**Data:** Western glaciers are used as fine-tuning set, eastern as test set. Split is approx. 50% of data.\n",
    "\n",
    "**Use case**: Test geographic generalization across longitudinal gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "East glaciers (20): ['Breidamerkurjoekull', 'Bruarjoekull', 'Dyngjujoekull', 'Eyjabakkajoekull', 'Koeldukvislarjoekull', 'RGI60-06.00409', 'RGI60-06.00411', 'RGI60-06.00413', 'RGI60-06.00422', 'RGI60-06.00425', 'RGI60-06.00445', 'RGI60-06.00465', 'RGI60-06.00466', 'RGI60-06.00474', 'RGI60-06.00476', 'RGI60-06.00478', 'RGI60-06.00479', 'RGI60-06.00480', 'Skeidararjoekull', 'Tungnaarjoekull']\n",
      "West glaciers (27): ['Blagnipujoekull (Hofsjoekull SW)', 'Hagafellsjoekull East (Langjoekull S Dome)', 'Hagafellsjoekull West', 'Langjoekull Ice Cap', 'Mulajoekull', 'Oeldufellsjoekull', 'RGI60-06.00228', 'RGI60-06.00232', 'RGI60-06.00238', 'RGI60-06.00292', 'RGI60-06.00294', 'RGI60-06.00296', 'RGI60-06.00301', 'RGI60-06.00302', 'RGI60-06.00303', 'RGI60-06.00305', 'RGI60-06.00306', 'RGI60-06.00311', 'RGI60-06.00320', 'RGI60-06.00328', 'RGI60-06.00340', 'RGI60-06.00342', 'RGI60-06.00349', 'RGI60-06.00350', 'RGI60-06.00359', 'Slettjoekull West', 'Thjorsarjoekull (Hofsjoekull E)']\n"
     ]
    }
   ],
   "source": [
    "# Get glacier latitudes\n",
    "glacier_lat = data_wgms.groupby('GLACIER')['POINT_LON'].first()\n",
    "\n",
    "# Use the median latitude as the split\n",
    "lon_threshold = -18.25\n",
    "\n",
    "east_glaciers = glacier_lat[glacier_lat >= lon_threshold].index.tolist()\n",
    "west_glaciers = glacier_lat[glacier_lat < lon_threshold].index.tolist()\n",
    "\n",
    "print(f\"East glaciers ({len(east_glaciers)}): {east_glaciers}\")\n",
    "print(f\"West glaciers ({len(west_glaciers)}): {west_glaciers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning glaciers (20): ['Breidamerkurjoekull', 'Bruarjoekull', 'Dyngjujoekull', 'Eyjabakkajoekull', 'Koeldukvislarjoekull', 'RGI60-06.00409', 'RGI60-06.00411', 'RGI60-06.00413', 'RGI60-06.00422', 'RGI60-06.00425', 'RGI60-06.00445', 'RGI60-06.00465', 'RGI60-06.00466', 'RGI60-06.00474', 'RGI60-06.00476', 'RGI60-06.00478', 'RGI60-06.00479', 'RGI60-06.00480', 'Skeidararjoekull', 'Tungnaarjoekull']\n",
      "Test glaciers (27): ['Thjorsarjoekull (Hofsjoekull E)', 'RGI60-06.00328', 'Hagafellsjoekull West', 'RGI60-06.00303', 'Langjoekull Ice Cap', 'Oeldufellsjoekull', 'Slettjoekull West', 'RGI60-06.00302', 'RGI60-06.00359', 'RGI60-06.00340', 'Blagnipujoekull (Hofsjoekull SW)', 'RGI60-06.00238', 'RGI60-06.00320', 'Hagafellsjoekull East (Langjoekull S Dome)', 'RGI60-06.00342', 'RGI60-06.00294', 'RGI60-06.00292', 'RGI60-06.00232', 'Mulajoekull', 'RGI60-06.00301', 'RGI60-06.00311', 'RGI60-06.00350', 'RGI60-06.00228', 'RGI60-06.00349', 'RGI60-06.00305', 'RGI60-06.00306', 'RGI60-06.00296']\n",
      "Train glaciers: (20) ['Breidamerkurjoekull' 'Dyngjujoekull' 'Tungnaarjoekull'\n",
      " 'Eyjabakkajoekull' 'Koeldukvislarjoekull' 'Skeidararjoekull'\n",
      " 'RGI60-06.00466' 'RGI60-06.00411' 'Bruarjoekull' 'RGI60-06.00480'\n",
      " 'RGI60-06.00465' 'RGI60-06.00478' 'RGI60-06.00413' 'RGI60-06.00476'\n",
      " 'RGI60-06.00409' 'RGI60-06.00422' 'RGI60-06.00425' 'RGI60-06.00479'\n",
      " 'RGI60-06.00445' 'RGI60-06.00474']\n",
      "Test glaciers: (27) ['Thjorsarjoekull (Hofsjoekull E)' 'RGI60-06.00328'\n",
      " 'Hagafellsjoekull West' 'RGI60-06.00303' 'Langjoekull Ice Cap'\n",
      " 'Oeldufellsjoekull' 'Slettjoekull West' 'RGI60-06.00302' 'RGI60-06.00359'\n",
      " 'RGI60-06.00340' 'Blagnipujoekull (Hofsjoekull SW)' 'RGI60-06.00238'\n",
      " 'RGI60-06.00320' 'Hagafellsjoekull East (Langjoekull S Dome)'\n",
      " 'RGI60-06.00342' 'RGI60-06.00294' 'RGI60-06.00292' 'RGI60-06.00232'\n",
      " 'Mulajoekull' 'RGI60-06.00301' 'RGI60-06.00311' 'RGI60-06.00350'\n",
      " 'RGI60-06.00228' 'RGI60-06.00349' 'RGI60-06.00305' 'RGI60-06.00306'\n",
      " 'RGI60-06.00296']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'length train set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "26777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'length test set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29432"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_glaciers = east_glaciers\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Limited Data Split (5-10% for fine-tuning)\n",
    "\n",
    "**Data**: About 5-10 % of the available data is used as fine-tuning set, consisting of 5 glaciers, which have been chosen from to be representative of the 4 available ice caps.\n",
    "\n",
    "**Use case**: Test performance with minimal fine-tuning data (~500 measurements) to simulate data-scarce scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning glaciers (5): ['Mulajoekull', 'Slettjoekull West', 'Hagafellsjoekull East (Langjoekull S Dome)', 'Tungnaarjoekull', 'RGI60-06.00478']\n",
      "Test glaciers (42): ['Thjorsarjoekull (Hofsjoekull E)', 'Breidamerkurjoekull', 'Dyngjujoekull', 'RGI60-06.00328', 'Hagafellsjoekull West', 'Eyjabakkajoekull', 'RGI60-06.00303', 'Langjoekull Ice Cap', 'Koeldukvislarjoekull', 'Oeldufellsjoekull', 'Skeidararjoekull', 'RGI60-06.00466', 'RGI60-06.00411', 'RGI60-06.00302', 'RGI60-06.00359', 'RGI60-06.00340', 'Bruarjoekull', 'Blagnipujoekull (Hofsjoekull SW)', 'RGI60-06.00238', 'RGI60-06.00320', 'RGI60-06.00342', 'RGI60-06.00480', 'RGI60-06.00465', 'RGI60-06.00294', 'RGI60-06.00292', 'RGI60-06.00232', 'RGI60-06.00301', 'RGI60-06.00413', 'RGI60-06.00311', 'RGI60-06.00350', 'RGI60-06.00476', 'RGI60-06.00228', 'RGI60-06.00409', 'RGI60-06.00349', 'RGI60-06.00422', 'RGI60-06.00305', 'RGI60-06.00425', 'RGI60-06.00306', 'RGI60-06.00479', 'RGI60-06.00296', 'RGI60-06.00445', 'RGI60-06.00474']\n",
      "Train glaciers: (5) ['Tungnaarjoekull' 'Slettjoekull West'\n",
      " 'Hagafellsjoekull East (Langjoekull S Dome)' 'RGI60-06.00478'\n",
      " 'Mulajoekull']\n",
      "Test glaciers: (42) ['Thjorsarjoekull (Hofsjoekull E)' 'Breidamerkurjoekull' 'Dyngjujoekull'\n",
      " 'RGI60-06.00328' 'Hagafellsjoekull West' 'Eyjabakkajoekull'\n",
      " 'RGI60-06.00303' 'Langjoekull Ice Cap' 'Koeldukvislarjoekull'\n",
      " 'Oeldufellsjoekull' 'Skeidararjoekull' 'RGI60-06.00466' 'RGI60-06.00411'\n",
      " 'RGI60-06.00302' 'RGI60-06.00359' 'RGI60-06.00340' 'Bruarjoekull'\n",
      " 'Blagnipujoekull (Hofsjoekull SW)' 'RGI60-06.00238' 'RGI60-06.00320'\n",
      " 'RGI60-06.00342' 'RGI60-06.00480' 'RGI60-06.00465' 'RGI60-06.00294'\n",
      " 'RGI60-06.00292' 'RGI60-06.00232' 'RGI60-06.00301' 'RGI60-06.00413'\n",
      " 'RGI60-06.00311' 'RGI60-06.00350' 'RGI60-06.00476' 'RGI60-06.00228'\n",
      " 'RGI60-06.00409' 'RGI60-06.00349' 'RGI60-06.00422' 'RGI60-06.00305'\n",
      " 'RGI60-06.00425' 'RGI60-06.00306' 'RGI60-06.00479' 'RGI60-06.00296'\n",
      " 'RGI60-06.00445' 'RGI60-06.00474']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'length train set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4393"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'length test set'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "51816"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRANSFER LEARNING SETUP: 5-10% Limited Data Strategy\n",
    "\n",
    "train_glaciers = ['Mulajoekull' ,'Slettjoekull West', 'Hagafellsjoekull East (Langjoekull S Dome)', 'Tungnaarjoekull', 'RGI60-06.00478']\n",
    "\n",
    "# Test glaciers (all remaining Iceland glaciers)\n",
    "all_iceland_glaciers = list(data_wgms['GLACIER'].unique())\n",
    "test_glaciers = [g for g in all_iceland_glaciers if g not in train_glaciers]\n",
    "\n",
    "print(f\"Fine-tuning glaciers ({len(train_glaciers)}): {train_glaciers}\")\n",
    "print(f\"Test glaciers ({len(test_glaciers)}): {test_glaciers}\")\n",
    "\n",
    "# Ensure all glaciers exist in the dataset\n",
    "existing_glaciers = set(dataloader_gl.data.GLACIER.unique())\n",
    "missing_fine_tune = [g for g in train_glaciers if g not in existing_glaciers]\n",
    "missing_test = [g for g in test_glaciers if g not in existing_glaciers]\n",
    "\n",
    "if missing_fine_tune:\n",
    "    print(f\"Warning: Fine-tuning glaciers not in dataset: {missing_fine_tune}\")\n",
    "if missing_test:\n",
    "    print(f\"Warning: Test glaciers not in dataset: {missing_test}\")\n",
    "\n",
    "\n",
    "# Use helper function from XGBoost to create test/train set. CV splits are not used here.\n",
    "splits, test_set, train_set = get_CV_splits(dataloader_gl,\n",
    "                                            test_split_on='GLACIER',\n",
    "                                            test_splits=test_glaciers,\n",
    "                                            random_state=cfg.seed)\n",
    "    \n",
    "print('Train glaciers: ({}) {}'.format(len(train_set['splits_vals']),\n",
    "                                      train_set['splits_vals']))\n",
    "print('Test glaciers: ({}) {}'.format(len(test_set['splits_vals']),\n",
    "                                      test_set['splits_vals']))\n",
    "\n",
    "display('length train set', len(train_set['df_X']))\n",
    "display('length test set', len(test_set['df_X']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split Options\n",
    "\n",
    "Use the same Option as was used for the Swiss model.\n",
    "\n",
    "### Option 1: Random 80/20 Split\n",
    "**Recommended for**: General model validation with balanced representation across all fine-tuning glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data glacier distribution: GLACIER\n",
      "Tungnaarjoekull                               1110\n",
      "Hagafellsjoekull East (Langjoekull S Dome)     843\n",
      "RGI60-06.00478                                 767\n",
      "Mulajoekull                                    572\n",
      "Slettjoekull West                              221\n",
      "Name: count, dtype: int64\n",
      "Val data glacier distribution: GLACIER\n",
      "Tungnaarjoekull                               320\n",
      "Hagafellsjoekull East (Langjoekull S Dome)    207\n",
      "RGI60-06.00478                                190\n",
      "Mulajoekull                                    97\n",
      "Slettjoekull West                              66\n",
      "Name: count, dtype: int64\n",
      "Train data shape: (3513, 29)\n",
      "Val data shape: (880, 29)\n"
     ]
    }
   ],
   "source": [
    "# Validation and train split:\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "dataloader = mbm.dataloader.DataLoader(cfg, data=data_train)\n",
    "\n",
    "# Create random train/validation split\n",
    "train_itr, val_itr = dataloader.set_train_test_split(test_size=0.2)\n",
    "\n",
    "# Get all indices of the training and valdating dataset at once from the iterators. Once called, the iterators are empty.\n",
    "train_indices, val_indices = list(train_itr), list(val_itr)\n",
    "\n",
    "# Create training subset\n",
    "df_X_train = data_train.iloc[train_indices]\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Create validation subset\n",
    "df_X_val = data_train.iloc[val_indices]\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Glacier-wise Train/Val Split\n",
    "**Recommended for**: Testing glacier-level generalization by validating on a completely unseen glacier during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data glacier distribution: GLACIER\n",
      "Graafjellsbrea        2365\n",
      "Breidablikkbrea       1593\n",
      "Austre Memurubreen    1580\n",
      "Rembesdalskaaka       1561\n",
      "Tunsbergdalsbreen     1517\n",
      "Name: count, dtype: int64\n",
      "Val data glacier distribution: Series([], Name: count, dtype: int64)\n",
      "Train data shape: (13698, 29)\n",
      "Val data shape: (0, 29)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_train = train_set['df_X']\n",
    "data_train['y'] = train_set['y']\n",
    "\n",
    "val_glacier = ['Engabreen']\n",
    "train_glaciers = [g for g in train_glaciers if g not in val_glacier]\n",
    "\n",
    "# Create training subset (excluding validation glacier)\n",
    "df_X_train = data_train[data_train['GLACIER'].isin(train_glaciers)].copy()\n",
    "y_train = df_X_train['POINT_BALANCE'].values\n",
    "\n",
    "# Create validation subset (only validation glacier)\n",
    "df_X_val = data_train[data_train['GLACIER'].isin(val_glacier)].copy()\n",
    "y_val = df_X_val['POINT_BALANCE'].values\n",
    "\n",
    "print(\"Train data glacier distribution:\", df_X_train['GLACIER'].value_counts().head())\n",
    "print(\"Val data glacier distribution:\", df_X_val['GLACIER'].value_counts().head())\n",
    "print(\"Train data shape:\", df_X_train.shape)\n",
    "print(\"Val data shape:\", df_X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Configuration\n",
    "\n",
    "### Feature Engineering and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset: (3513, 28)\n",
      "Shape of validation dataset: (880, 28)\n",
      "Shape of testing dataset: (51816, 28)\n",
      "Running with features: ['ELEVATION_DIFFERENCE', 'aspect', 'slope', 'hugonnet_dhdt', 'consensus_ice_thickness', 'millan_v', 't2m', 'tp', 'slhf', 'sshf', 'ssrd', 'fal', 'str', 'u10', 'v10']\n",
      "Feature-target alignment verified\n"
     ]
    }
   ],
   "source": [
    "# Define complete feature set for model training\n",
    "features_topo = [\n",
    "    'ELEVATION_DIFFERENCE',\n",
    "] + list(vois_topographical)\n",
    "\n",
    "# Combine topographical and climate features\n",
    "feature_columns = features_topo + list(vois_climate)\n",
    "\n",
    "# Set features in config\n",
    "cfg.setFeatures(feature_columns)\n",
    "\n",
    "# Include all necessary columns (features + metadata)\n",
    "all_columns = feature_columns + cfg.fieldsNotFeatures\n",
    "\n",
    "# Because CH has some extra columns, we need to cut those\n",
    "df_X_train_subset = df_X_train[all_columns]\n",
    "df_X_val_subset = df_X_val[all_columns]\n",
    "df_X_test_subset = test_set['df_X'][all_columns]\n",
    "\n",
    "print('Shape of training dataset:', df_X_train_subset.shape)\n",
    "print('Shape of validation dataset:', df_X_val_subset.shape)\n",
    "print('Shape of testing dataset:', df_X_test_subset.shape)\n",
    "print('Running with features:', feature_columns)\n",
    "\n",
    "# Sanity check: ensure targets match features\n",
    "assert all(train_set['df_X'].POINT_BALANCE == train_set['y']), \"Target mismatch detected!\"\n",
    "print('Feature-target alignment verified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Callbacks and Training Configuration\n",
    "Set up training callbacks and configuration for optimal performance and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks and configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='valid_loss',    # Monitor validation loss\n",
    "    patience=15,             # Stop after 15 epochs without improvement\n",
    "    threshold=1e-4,          # Minimum change threshold\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for adaptive training\n",
    "lr_scheduler_cb = LRScheduler(\n",
    "    policy=ReduceLROnPlateau,\n",
    "    monitor='valid_loss',\n",
    "    mode='min',\n",
    "    factor=0.5,              # Reduce LR by half\n",
    "    patience=5,              # Wait 5 epochs before reducing\n",
    "    threshold=0.01,\n",
    "    threshold_mode='rel',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Global variables for dataset management\n",
    "dataset = dataset_val = None\n",
    "\n",
    "def my_train_split(ds, y=None, **fit_params):\n",
    "    \"\"\"Custom train/validation split function for skorch.\"\"\"\n",
    "    return dataset, dataset_val\n",
    "\n",
    "# Model configuration parameters\n",
    "param_init = {'device': 'cpu'}\n",
    "nInp = len(feature_columns)  # Number of input features\n",
    "\n",
    "# Model checkpointing to save best model during training\n",
    "checkpoint_cb = Checkpoint(\n",
    "    monitor='valid_loss_best',\n",
    "    f_params='best_model.pt',\n",
    "    f_optimizer=None,        # Don't save optimizer state\n",
    "    f_history=None,          # Don't save training history\n",
    "    f_criterion=None,        # Don't save criterion state\n",
    "    load_best=True,          # Load best model after training\n",
    ")\n",
    "\n",
    "# Custom callback to save models at specific epochs for analysis\n",
    "save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\n",
    "\n",
    "print('Callbacks and configuration ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "Datasets will be created in the training loop after loading the pre-trained Swiss model to ensure compatible preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creation deferred until Swiss model is loaded...\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset variables as None\n",
    "features = features_val = None\n",
    "metadata = metadata_val = None\n",
    "dataset = dataset_val = None\n",
    "\n",
    "print(\"Dataset creation deferred until Swiss model is loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Execution\n",
    "\n",
    "### Loading Pre-trained Swiss Model and Fine-tuning on Norwegian Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Standard Fine-tuning with Selective Layer Freezing\n",
    "\n",
    "After loading the model, all layers will be frozen by default, to unfreeze a layer you have to include it in \"if name not in [...]\" in Step 3.\n",
    "\n",
    " The SaveBestAtEpochs callback automatically saves the current best model at epochs [10, 15, 20, 30, 50, 100], which can then be evaluated in the Epoch-wise model evalution section. Comment out the callback if you don't want this feature. If you do and you continuously want to retrain models at different learning rates, you have to reexecute the \"save_best_epochs_cb = SaveBestAtEpochs([10, 15, 20, 30, 50, 100])\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Swiss model...\n",
      "✓ Swiss model loaded successfully!\n",
      "Creating datasets with Swiss model...\n",
      "train: (369,) (369,)\n",
      "validation: (93,) (93,)\n",
      "Updating model for fine-tuning...\n",
      "Starting fine-tuning...\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1        \u001b[36m3.2615\u001b[0m        \u001b[32m4.2779\u001b[0m     +  0.1000  0.1726\n",
      "      2        \u001b[36m2.1061\u001b[0m        4.6298        0.1000  0.1606\n",
      "      3        \u001b[36m1.6769\u001b[0m        \u001b[32m1.3100\u001b[0m     +  0.1000  0.1619\n",
      "      4        \u001b[36m1.2013\u001b[0m        \u001b[32m1.1354\u001b[0m     +  0.1000  0.1603\n",
      "      5        \u001b[36m1.1473\u001b[0m        1.2126        0.1000  0.1599\n",
      "      6        \u001b[36m0.7417\u001b[0m        1.6998        0.1000  0.1770\n",
      "      7        0.7966        1.5850        0.1000  0.1800\n",
      "      8        0.7583        1.3808        0.1000  0.1598\n",
      "      9        0.9263        1.2052        0.1000  0.1605\n",
      "     10        \u001b[36m0.6867\u001b[0m        \u001b[32m1.0459\u001b[0m     +  0.1000  0.1606\n",
      "     11        0.9300        1.0474        0.1000  0.1609\n",
      "     12        0.8220        \u001b[32m0.8767\u001b[0m     +  0.1000  0.2105\n",
      "     13        \u001b[36m0.6734\u001b[0m        0.9415        0.1000  0.1609\n",
      "     14        0.7192        0.9046        0.1000  0.1764\n",
      "     15        \u001b[36m0.6682\u001b[0m        \u001b[32m0.8694\u001b[0m     +  0.1000  0.1635\n",
      "     16        0.8811        \u001b[32m0.8394\u001b[0m     +  0.1000  0.1611\n",
      "     17        \u001b[36m0.6401\u001b[0m        \u001b[32m0.7284\u001b[0m     +  0.1000  0.1606\n",
      "     18        0.7588        \u001b[32m0.6993\u001b[0m     +  0.1000  0.1646\n",
      "     19        0.6558        0.7052        0.1000  0.1612\n",
      "     20        0.6799        0.7508        0.1000  0.1638\n",
      "     21        \u001b[36m0.5797\u001b[0m        \u001b[32m0.6850\u001b[0m     +  0.1000  0.1613\n",
      "     22        0.9017        \u001b[32m0.6619\u001b[0m     +  0.1000  0.1604\n",
      "     23        0.7302        0.7408        0.1000  0.1601\n",
      "     24        0.6815        1.0430        0.1000  0.1620\n",
      "     25        0.6860        0.7441        0.1000  0.1603\n",
      "     26        0.7709        0.6642        0.1000  0.1614\n",
      "     27        \u001b[36m0.5715\u001b[0m        0.7776        0.1000  0.1722\n",
      "     28        0.6574        0.9322        0.1000  0.1587\n",
      "     29        0.7296        0.8415        0.0500  0.1639\n",
      "     30        \u001b[36m0.5514\u001b[0m        \u001b[32m0.6461\u001b[0m     +  0.0500  0.1683\n",
      "     31        0.6593        \u001b[32m0.6310\u001b[0m     +  0.0500  0.1633\n",
      "     32        0.6482        0.6481        0.0500  0.1610\n",
      "     33        \u001b[36m0.5438\u001b[0m        0.8104        0.0500  0.1608\n",
      "     34        0.6449        0.9243        0.0500  0.1610\n",
      "     35        0.6336        0.7958        0.0500  0.1597\n",
      "     36        0.6255        0.6868        0.0500  0.1605\n",
      "     37        0.5476        0.6705        0.0500  0.1587\n",
      "     38        \u001b[36m0.5319\u001b[0m        0.6675        0.0250  0.1593\n",
      "     39        0.7480        0.6703        0.0250  0.1593\n",
      "     40        0.6470        0.7001        0.0250  0.1604\n",
      "     41        0.6295        0.7228        0.0250  0.1589\n",
      "     42        0.6234        0.7473        0.0250  0.1637\n",
      "     43        0.6146        0.6867        0.0250  0.1641\n",
      "     44        0.5504        0.6746        0.0125  0.1605\n",
      "     45        \u001b[36m0.5241\u001b[0m        0.6622        0.0125  0.1596\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n",
      "✓ Fine-tuned model saved as: nn_model_finetuned_2025-08-23\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True  # Set to True to actually train\n",
    "\n",
    "if TRAIN:\n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    # Define Swiss model architecture and parameters\n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True,             # CRITICAL: preserve pretrained weights\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "            #('save_best_at_epochs', save_best_epochs_cb)  # Save models at specific epochs\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using the loaded Swiss model\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset_val, idx=0), \n",
    "                                                          SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 3: Apply selective layer freezing\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        if name not in [#'model.0.weight', 'model.0.bias',\n",
    "                        'model.1.weight', 'model.1.bias',\n",
    "                        #'model.4.weight', 'model.4.bias',\n",
    "                        'model.5.weight', 'model.5.bias',\n",
    "                        #'model.8.weight', 'model.8.bias',\n",
    "                        'model.9.weight', 'model.9.bias',\n",
    "                        #'model.12.weight', 'model.12.bias',\n",
    "                        'model.13.weight', 'model.13.bias',\n",
    "                        #'model.16.weight', 'model.16.bias'\n",
    "                        ]:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # STEP 4: Configure for fine-tuning\n",
    "    print(\"Updating model for fine-tuning...\")\n",
    "    loaded_model = loaded_model.set_params(\n",
    "        lr=0.1,\n",
    "        max_epochs=200,\n",
    "    )\n",
    "    \n",
    "    # STEP 5: Execute fine-tuning\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    loaded_model.fit(features, y_train)\n",
    "    \n",
    "    # STEP 6: Save fine-tuned model\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_finetuned_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"✓ Fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"Training skipped (TRAIN=False)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Progressive Layer Unfreezing\n",
    "This advanced approach gradually unfreezes layers during training for more controlled adaptation to the Icelandic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True  # Set to True to actually train\n",
    "\n",
    "if TRAIN:\n",
    "    \n",
    "    # STEP 1: Load the pre-trained Swiss model FIRST\n",
    "    print(\"Loading pre-trained Swiss model...\")\n",
    "    model_filename = \"nn_model_2025-07-14_CH_flexible.pt\"\n",
    "    \n",
    "    # Define Swiss model architecture\n",
    "    swiss_args = {\n",
    "        'module': FlexibleNetwork,\n",
    "        'nbFeatures': nInp,\n",
    "        'module__input_dim': nInp,\n",
    "        'module__dropout': 0.2,\n",
    "        'module__hidden_layers': [128, 128, 64, 32],\n",
    "        'module__use_batchnorm': True,\n",
    "        'warm_start': True,             # CRITICAL: preserve pretrained weights\n",
    "        'train_split': my_train_split,\n",
    "        'batch_size': 128,\n",
    "        'verbose': 1,\n",
    "        'iterator_train__shuffle': True,\n",
    "        'lr': 0.001,\n",
    "        'max_epochs': 200,\n",
    "        'optimizer': torch.optim.Adam,\n",
    "        'optimizer__weight_decay': 1e-05,\n",
    "        'callbacks': [\n",
    "            ('early_stop', early_stop),\n",
    "            ('lr_scheduler', lr_scheduler_cb),\n",
    "            ('checkpoint', checkpoint_cb),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    loaded_model = mbm.models.CustomNeuralNetRegressor.load_model(\n",
    "        cfg, model_filename, **{**swiss_args, **param_init}\n",
    "    )\n",
    "\n",
    "    print(\"✓ Swiss model loaded successfully!\")\n",
    "    \n",
    "    # STEP 2: Create datasets using Swiss model preprocessing\n",
    "    print(\"Creating datasets with Swiss model...\")\n",
    "    features, metadata = loaded_model._create_features_metadata(df_X_train_subset)\n",
    "    features_val, metadata_val = loaded_model._create_features_metadata(df_X_val_subset)\n",
    "    \n",
    "    # Create global datasets\n",
    "    dataset = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                    features=features,\n",
    "                                                    metadata=metadata,\n",
    "                                                    targets=y_train)\n",
    "    dataset = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset, idx=0),\n",
    "                                                      SliceDataset(dataset, idx=1))\n",
    "    \n",
    "    dataset_val = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                        features=features_val,\n",
    "                                                        metadata=metadata_val,\n",
    "                                                        targets=y_val)\n",
    "    dataset_val = mbm.data_processing.SliceDatasetBinding(SliceDataset(dataset_val, idx=0), \n",
    "                                                          SliceDataset(dataset_val, idx=1))\n",
    "    \n",
    "    print(\"train:\", dataset.X.shape, dataset.y.shape)\n",
    "    print(\"validation:\", dataset_val.X.shape, dataset_val.y.shape)\n",
    "\n",
    "\n",
    "    # STEP 3: Define progressive unfreezing strategy\n",
    "    # Helper to freeze/unfreeze layers\n",
    "    def set_requires_grad(layer_names, requires_grad=True):\n",
    "        for name, param in loaded_model.module_.named_parameters():\n",
    "            if name in layer_names:\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    # List of layer groups to progressively unfreeze\n",
    "    layer_groups = [\n",
    "        #(\n",
    "            #[\n",
    "                #'model.1.weight', 'model.1.bias',\n",
    "                #'model.5.weight', 'model.5.bias',\n",
    "                #'model.9.weight', 'model.9.bias',\n",
    "                #'model.13.weight', 'model.13.bias'\n",
    "            #],200,  0.1\n",
    "        #),\n",
    "        \n",
    "        (['model.16.weight', 'model.16.bias'], 30, 0.01),\n",
    "        (['model.12.weight', 'model.12.bias'], 20, 0.005),\n",
    "        (['model.8.weight', 'model.8.bias'], 10, 0.001)\n",
    "    ]\n",
    "\n",
    "    # Start with all layers frozen\n",
    "    print(\"Freezing all layers initially...\")\n",
    "    for name, param in loaded_model.module_.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Progressive unfreezing loop\n",
    "    for i, (layers, epochs, lr) in enumerate(layer_groups, 1):\n",
    "        print(f\"Stage {i}: Unfreezing {len(layers)//2} layer(s) for {epochs} epochs (lr={lr})...\")\n",
    "        \n",
    "        # Unfreeze current layer group\n",
    "        set_requires_grad(layers, True)\n",
    "        \n",
    "        # Update model parameters\n",
    "        loaded_model = loaded_model.set_params(lr=lr, max_epochs=epochs)\n",
    "        \n",
    "        # Train current stage\n",
    "        loaded_model.fit(features, y_train)\n",
    "        \n",
    "        # Evaluate current stage\n",
    "        val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "        print(f\"   Stage {i} validation score: {val_score:.4f}\")\n",
    "    \n",
    "    # STEP 4: Save progressively fine-tuned model\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    finetuned_model_filename = f\"nn_model_progressive_{current_date}\"\n",
    "    loaded_model.save_model(finetuned_model_filename)\n",
    "    print(f\"Progressively fine-tuned model saved as: {finetuned_model_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"Progressive training skipped (TRAIN=False)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Performance Evaluation\n",
    "Get immediate performance metrics on the test set using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model performance...\n",
      "Test Set Performance Metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.4359645216393573,\n",
       " 'mse': 1.4359645253969087,\n",
       " 'rmse': 1.1983173725674299,\n",
       " 'mae': 0.8011468722030236,\n",
       " 'pearson': 0.8954499622882138}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score (for reference): -0.6310\n",
      "\n",
      "Performance by glacier:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLACIER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00480</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.4406</td>\n",
       "      <td>0.3501</td>\n",
       "      <td>0.2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00479</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4541</td>\n",
       "      <td>0.4541</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00411</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>0.3276</td>\n",
       "      <td>0.9217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00465</th>\n",
       "      <td>79.0</td>\n",
       "      <td>0.6888</td>\n",
       "      <td>0.5314</td>\n",
       "      <td>-0.1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Langjoekull Ice Cap</th>\n",
       "      <td>285.0</td>\n",
       "      <td>0.6899</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.8570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thjorsarjoekull (Hofsjoekull E)</th>\n",
       "      <td>419.0</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.5325</td>\n",
       "      <td>0.9143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00302</th>\n",
       "      <td>105.0</td>\n",
       "      <td>0.7533</td>\n",
       "      <td>0.6045</td>\n",
       "      <td>0.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blagnipujoekull (Hofsjoekull SW)</th>\n",
       "      <td>348.0</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.6016</td>\n",
       "      <td>0.8606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skeidararjoekull</th>\n",
       "      <td>514.0</td>\n",
       "      <td>0.7922</td>\n",
       "      <td>0.6524</td>\n",
       "      <td>0.2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00228</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>0.6883</td>\n",
       "      <td>-0.7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00409</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.7334</td>\n",
       "      <td>-1.9989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Koeldukvislarjoekull</th>\n",
       "      <td>309.0</td>\n",
       "      <td>0.8660</td>\n",
       "      <td>0.6613</td>\n",
       "      <td>0.8109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00292</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.6399</td>\n",
       "      <td>0.7987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dyngjujoekull</th>\n",
       "      <td>240.0</td>\n",
       "      <td>0.8881</td>\n",
       "      <td>0.7193</td>\n",
       "      <td>0.7622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00232</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.8940</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>0.8130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00294</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.7316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00342</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0103</td>\n",
       "      <td>0.8418</td>\n",
       "      <td>0.6924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hagafellsjoekull West</th>\n",
       "      <td>346.0</td>\n",
       "      <td>1.0267</td>\n",
       "      <td>0.7676</td>\n",
       "      <td>0.8450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eyjabakkajoekull</th>\n",
       "      <td>222.0</td>\n",
       "      <td>1.0742</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.8059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00296</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0947</td>\n",
       "      <td>1.0947</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00305</th>\n",
       "      <td>51.0</td>\n",
       "      <td>1.1616</td>\n",
       "      <td>1.0288</td>\n",
       "      <td>-0.9043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breidamerkurjoekull</th>\n",
       "      <td>315.0</td>\n",
       "      <td>1.2555</td>\n",
       "      <td>0.9316</td>\n",
       "      <td>0.9035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00311</th>\n",
       "      <td>92.0</td>\n",
       "      <td>1.2982</td>\n",
       "      <td>1.0029</td>\n",
       "      <td>0.7268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00306</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3689</td>\n",
       "      <td>1.3689</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00359</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3956</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>0.1871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00425</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4421</td>\n",
       "      <td>1.4421</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00238</th>\n",
       "      <td>862.0</td>\n",
       "      <td>1.4681</td>\n",
       "      <td>1.0658</td>\n",
       "      <td>0.3609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00320</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.5822</td>\n",
       "      <td>1.5275</td>\n",
       "      <td>0.5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bruarjoekull</th>\n",
       "      <td>718.0</td>\n",
       "      <td>1.6909</td>\n",
       "      <td>0.8819</td>\n",
       "      <td>0.5414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00474</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6953</td>\n",
       "      <td>1.6953</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00413</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.7043</td>\n",
       "      <td>1.2279</td>\n",
       "      <td>0.3024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00476</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.7244</td>\n",
       "      <td>1.5669</td>\n",
       "      <td>-3.8397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00445</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7853</td>\n",
       "      <td>1.7853</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00301</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9292</td>\n",
       "      <td>1.4382</td>\n",
       "      <td>-10.8676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00466</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.9363</td>\n",
       "      <td>1.1765</td>\n",
       "      <td>0.7496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00422</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.1330</td>\n",
       "      <td>1.9748</td>\n",
       "      <td>0.1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00303</th>\n",
       "      <td>24.0</td>\n",
       "      <td>2.2268</td>\n",
       "      <td>1.6368</td>\n",
       "      <td>0.4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00349</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.2286</td>\n",
       "      <td>2.1371</td>\n",
       "      <td>-0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00340</th>\n",
       "      <td>44.0</td>\n",
       "      <td>2.2998</td>\n",
       "      <td>1.6690</td>\n",
       "      <td>0.8563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00328</th>\n",
       "      <td>26.0</td>\n",
       "      <td>2.4176</td>\n",
       "      <td>2.1608</td>\n",
       "      <td>-1.8755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oeldufellsjoekull</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.4843</td>\n",
       "      <td>2.4295</td>\n",
       "      <td>0.1693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGI60-06.00350</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.6062</td>\n",
       "      <td>2.2469</td>\n",
       "      <td>-10.4557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  n_samples    rmse     mae       r2\n",
       "GLACIER                                                             \n",
       "RGI60-06.00480                         57.0  0.4406  0.3501   0.2169\n",
       "RGI60-06.00479                          1.0  0.4541  0.4541     -inf\n",
       "RGI60-06.00411                          8.0  0.4956  0.3276   0.9217\n",
       "RGI60-06.00465                         79.0  0.6888  0.5314  -0.1804\n",
       "Langjoekull Ice Cap                   285.0  0.6899  0.5176   0.8570\n",
       "Thjorsarjoekull (Hofsjoekull E)       419.0  0.7080  0.5325   0.9143\n",
       "RGI60-06.00302                        105.0  0.7533  0.6045   0.1264\n",
       "Blagnipujoekull (Hofsjoekull SW)      348.0  0.7742  0.6016   0.8606\n",
       "Skeidararjoekull                      514.0  0.7922  0.6524   0.2645\n",
       "RGI60-06.00228                         21.0  0.8200  0.6883  -0.7400\n",
       "RGI60-06.00409                          6.0  0.8345  0.7334  -1.9989\n",
       "Koeldukvislarjoekull                  309.0  0.8660  0.6613   0.8109\n",
       "RGI60-06.00292                        109.0  0.8734  0.6399   0.7987\n",
       "Dyngjujoekull                         240.0  0.8881  0.7193   0.7622\n",
       "RGI60-06.00232                         43.0  0.8940  0.6557   0.8130\n",
       "RGI60-06.00294                        103.0  0.9938  0.7409   0.7316\n",
       "RGI60-06.00342                          2.0  1.0103  0.8418   0.6924\n",
       "Hagafellsjoekull West                 346.0  1.0267  0.7676   0.8450\n",
       "Eyjabakkajoekull                      222.0  1.0742  0.7698   0.8059\n",
       "RGI60-06.00296                          1.0  1.0947  1.0947     -inf\n",
       "RGI60-06.00305                         51.0  1.1616  1.0288  -0.9043\n",
       "Breidamerkurjoekull                   315.0  1.2555  0.9316   0.9035\n",
       "RGI60-06.00311                         92.0  1.2982  1.0029   0.7268\n",
       "RGI60-06.00306                          1.0  1.3689  1.3689     -inf\n",
       "RGI60-06.00359                          4.0  1.3956  1.2214   0.1871\n",
       "RGI60-06.00425                          1.0  1.4421  1.4421     -inf\n",
       "RGI60-06.00238                        862.0  1.4681  1.0658   0.3609\n",
       "RGI60-06.00320                          3.0  1.5822  1.5275   0.5928\n",
       "Bruarjoekull                          718.0  1.6909  0.8819   0.5414\n",
       "RGI60-06.00474                          1.0  1.6953  1.6953     -inf\n",
       "RGI60-06.00413                          8.0  1.7043  1.2279   0.3024\n",
       "RGI60-06.00476                         13.0  1.7244  1.5669  -3.8397\n",
       "RGI60-06.00445                          1.0  1.7853  1.7853     -inf\n",
       "RGI60-06.00301                          2.0  1.9292  1.4382 -10.8676\n",
       "RGI60-06.00466                         65.0  1.9363  1.1765   0.7496\n",
       "RGI60-06.00422                          3.0  2.1330  1.9748   0.1008\n",
       "RGI60-06.00303                         24.0  2.2268  1.6368   0.4914\n",
       "RGI60-06.00349                          4.0  2.2286  2.1371  -0.0366\n",
       "RGI60-06.00340                         44.0  2.2998  1.6690   0.8563\n",
       "RGI60-06.00328                         26.0  2.4176  2.1608  -1.8755\n",
       "Oeldufellsjoekull                      10.0  2.4843  2.4295   0.1693\n",
       "RGI60-06.00350                          2.0  2.6062  2.2469 -10.4557"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick comprehensive evaluation of the fine-tuned model\n",
    "print(\"Evaluating fine-tuned model performance...\")\n",
    "\n",
    "grouped_ids, scores_NN, ids_NN, y_pred_NN = evaluate_model_and_group_predictions(\n",
    "    loaded_model, df_X_test_subset, test_set['y'], cfg, mbm\n",
    ")\n",
    "\n",
    "print(\"Test Set Performance Metrics:\")\n",
    "display(scores_NN)\n",
    "\n",
    "# Validation score for confirmation that model with the best val_loss is used\n",
    "val_score = loaded_model.score(dataset_val.X, dataset_val.y)\n",
    "print(f\"Validation score (for reference): {val_score:.4f}\")\n",
    "\n",
    "# Calculate additional performance metrics by glacier\n",
    "print(\"\\nPerformance by glacier:\")\n",
    "glacier_performance = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'n_samples': len(x),\n",
    "        'rmse': np.sqrt(np.mean((x['target'] - x['pred'])**2)),\n",
    "        'mae': np.mean(np.abs(x['target'] - x['pred'])),\n",
    "        'r2': 1 - np.sum((x['target'] - x['pred'])**2) / np.sum((x['target'] - x['target'].mean())**2)\n",
    "    })\n",
    ").round(4)\n",
    "\n",
    "display(glacier_performance.sort_values('rmse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch-wise Model Evaluation\n",
    "Evaluate models saved at different training epochs to understand training dynamics and optimal stopping points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models saved at different training epochs...\n",
      "Evaluating model at epoch 10...\n",
      "Epoch 10 performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.4062722418195912,\n",
       " 'mse': 1.406272245011415,\n",
       " 'rmse': 1.1858635018464034,\n",
       " 'mae': 0.8299792948443621,\n",
       " 'pearson': 0.9080360718256053}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Evaluating model at epoch 15...\n",
      "Epoch 15 performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.4122355176131365,\n",
       " 'mse': 1.4122355174869037,\n",
       " 'rmse': 1.1883751585618507,\n",
       " 'mae': 0.8056200029053975,\n",
       " 'pearson': 0.9117571720726377}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Evaluating model at epoch 20...\n",
      "Epoch 20 performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.3612312674412101,\n",
       " 'mse': 1.3612312668899431,\n",
       " 'rmse': 1.1667181608640294,\n",
       " 'mae': 0.7989613910792752,\n",
       " 'pearson': 0.9129781928397409}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Evaluating model at epoch 30...\n",
      "Epoch 30 performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.2567828676256818,\n",
       " 'mse': 1.2567828863550845,\n",
       " 'rmse': 1.1210632838315082,\n",
       " 'mae': 0.7633953157995983,\n",
       " 'pearson': 0.9196548044204494}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Evaluating model at epoch 50...\n",
      "Epoch 50 performance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': -1.2293091799955342,\n",
       " 'mse': 1.2293091718595768,\n",
       " 'rmse': 1.10874215751886,\n",
       " 'mae': 0.7577432498879246,\n",
       " 'pearson': 0.9215459395388584}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Model for epoch 100 not found, skipping...\n",
      "Epoch-wise evaluation completed!\n",
      "\n",
      "Performance comparison across epochs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>pearson</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.4063</td>\n",
       "      <td>1.4063</td>\n",
       "      <td>1.1859</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.4122</td>\n",
       "      <td>1.4122</td>\n",
       "      <td>1.1884</td>\n",
       "      <td>0.8056</td>\n",
       "      <td>0.9118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.3612</td>\n",
       "      <td>1.3612</td>\n",
       "      <td>1.1667</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>0.9130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1.2568</td>\n",
       "      <td>1.2568</td>\n",
       "      <td>1.1211</td>\n",
       "      <td>0.7634</td>\n",
       "      <td>0.9197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-1.2293</td>\n",
       "      <td>1.2293</td>\n",
       "      <td>1.1087</td>\n",
       "      <td>0.7577</td>\n",
       "      <td>0.9215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score     mse    rmse     mae  pearson\n",
       "Epoch                                         \n",
       "10    -1.4063  1.4063  1.1859  0.8300   0.9080\n",
       "15    -1.4122  1.4122  1.1884  0.8056   0.9118\n",
       "20    -1.3612  1.3612  1.1667  0.7990   0.9130\n",
       "30    -1.2568  1.2568  1.1211  0.7634   0.9197\n",
       "50    -1.2293  1.2293  1.1087  0.7577   0.9215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate models saved at different epochs to analyze training dynamics\n",
    "print(\"Evaluating models saved at different training epochs...\")\n",
    "\n",
    "epochs_to_evaluate = [10, 15, 20, 30, 50, 100]\n",
    "model_prefix = \"nn_model_best_epoch\"\n",
    "\n",
    "epoch_results = {}\n",
    "\n",
    "for epoch in epochs_to_evaluate:\n",
    "    model_name = f\"{model_prefix}_{epoch}.pt\"\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(model_name):\n",
    "        print(f\"Model for epoch {epoch} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating model at epoch {epoch}...\")\n",
    "    \n",
    "    # Load model with same architecture as Swiss model\n",
    "    epoch_model = mbm.models.CustomNeuralNetRegressor(\n",
    "        cfg, **swiss_args, **param_init\n",
    "    )\n",
    "    epoch_model = epoch_model.set_params(device='cpu').to('cpu')\n",
    "    epoch_model.initialize()\n",
    "    \n",
    "    # Load saved weights\n",
    "    state_dict = torch.load(model_name, map_location='cpu')\n",
    "    epoch_model.module_.load_state_dict(state_dict)\n",
    "\n",
    "    # Evaluate the model\n",
    "    grouped_ids_epoch, scores_NN_epoch, ids_NN_epoch, y_pred_NN_epoch = evaluate_model_and_group_predictions(\n",
    "        epoch_model, df_X_test_subset, test_set['y'], cfg, mbm\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    epoch_results[epoch] = scores_NN_epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch} performance:\")\n",
    "    display(scores_NN_epoch)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Epoch-wise evaluation completed!\")\n",
    "\n",
    "# Compare performance across epochs\n",
    "if epoch_results:\n",
    "    print(\"\\nPerformance comparison across epochs:\")\n",
    "    comparison_df = pd.DataFrame(epoch_results).T\n",
    "    comparison_df.index.name = 'Epoch'\n",
    "    display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehensive Visualization and Analysis\n",
    "Generate detailed visualizations to understand model performance across different glaciers and time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score (higher is better): -0.33718592127632885\n"
     ]
    }
   ],
   "source": [
    "# Prepare comprehensive visualization data\n",
    "print(\"Preparing data for comprehensive visualizations...\")\n",
    "\n",
    "# Create features and metadata for final evaluation\n",
    "features_test, metadata_test = loaded_model._create_features_metadata(df_X_test_subset)\n",
    "\n",
    "# Ensure all tensors are on CPU for visualization\n",
    "if hasattr(features_test, 'cpu'):\n",
    "    features_test = features_test.cpu()\n",
    "if hasattr(test_set['y'], 'cpu'):\n",
    "    targets_test = test_set['y'].cpu()\n",
    "else:\n",
    "    targets_test = test_set['y']\n",
    "\n",
    "# Create final test dataset\n",
    "dataset_test = mbm.data_processing.AggregatedDataset(cfg,\n",
    "                                                     features=features_test,\n",
    "                                                     metadata=metadata_test,\n",
    "                                                     targets=targets_test)\n",
    "\n",
    "dataset_test = [\n",
    "    SliceDataset(dataset_test, idx=0),  # Features\n",
    "    SliceDataset(dataset_test, idx=1)   # Targets\n",
    "]\n",
    "\n",
    "# Generate final predictions\n",
    "print(\"Generating final predictions for visualization...\")\n",
    "y_pred = loaded_model.predict(dataset_test[0])\n",
    "y_pred_agg = loaded_model.aggrPredict(dataset_test[0])\n",
    "\n",
    "# Prepare evaluation metrics\n",
    "batchIndex = np.arange(len(y_pred_agg))\n",
    "y_true = np.array([e for e in dataset_test[1][batchIndex]])\n",
    "\n",
    "# Calculate comprehensive performance metrics\n",
    "score = loaded_model.score(dataset_test[0], dataset_test[1])\n",
    "mse, rmse, mae, pearson = loaded_model.evalMetrics(y_pred, y_true)\n",
    "\n",
    "print(f\"Final Model Performance Summary:\")\n",
    "print(f\"   R² Score: {score:.4f}\")\n",
    "print(f\"   RMSE: {rmse:.4f} mm w.e.\")\n",
    "print(f\"   MAE: {mae:.4f} mm w.e.\")\n",
    "print(f\"   Pearson r: {pearson:.4f}\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "id = dataset_test[0].dataset.indexToId(batchIndex)\n",
    "grouped_ids = pd.DataFrame({\n",
    "    'target': [e[0] for e in dataset_test[1]],\n",
    "    'ID': id,\n",
    "    'pred': y_pred_agg\n",
    "})\n",
    "\n",
    "# Add comprehensive metadata\n",
    "periods_per_ids = df_X_test_subset.groupby('ID')['PERIOD'].first()\n",
    "grouped_ids = grouped_ids.merge(periods_per_ids, on='ID')\n",
    "\n",
    "glacier_per_ids = df_X_test_subset.groupby('ID')['GLACIER'].first()\n",
    "grouped_ids = grouped_ids.merge(glacier_per_ids, on='ID')\n",
    "\n",
    "years_per_ids = df_X_test_subset.groupby('ID')['YEAR'].first()\n",
    "grouped_ids = grouped_ids.merge(years_per_ids, on='ID')\n",
    "\n",
    "print(\"Visualization data prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive publication-ready visualizations\n",
    "print(\"Creating comprehensive visualizations...\")\n",
    "\n",
    "# 1. Time series predictions by glacier\n",
    "print(\"   Generating time series predictions by glacier...\")\n",
    "PlotPredictions_NN(grouped_ids)\n",
    "\n",
    "# 2. Predicted vs. observed scatter plot with performance metrics\n",
    "print(\"   Creating prediction vs. truth scatter plot...\")\n",
    "predVSTruth_all(grouped_ids, mae, rmse, title='Transfer Learning: Swiss→Norwegian Glaciers')\n",
    "\n",
    "# 3. Individual glacier performance analysis\n",
    "print(\"   Generating individual glacier performance analysis...\")\n",
    "PlotIndividualGlacierPredVsTruth(grouped_ids, base_figsize=(20, 15))\n",
    "\n",
    "print(\"All visualizations generated successfully!\")\n",
    "\n",
    "# Summary statistics by glacier for detailed analysis\n",
    "print(\"\\nDetailed Performance Summary by Glacier:\")\n",
    "glacier_stats = grouped_ids.groupby('GLACIER').agg({\n",
    "    'target': ['count', 'mean', 'std'],\n",
    "    'pred': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "# Calculate RMSE and MAE per glacier\n",
    "glacier_rmse = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: np.sqrt(np.mean((x['target'] - x['pred'])**2))\n",
    ").round(4)\n",
    "\n",
    "glacier_mae = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: np.mean(np.abs(x['target'] - x['pred']))\n",
    ").round(4)\n",
    "\n",
    "glacier_r2 = grouped_ids.groupby('GLACIER').apply(\n",
    "    lambda x: 1 - np.sum((x['target'] - x['pred'])**2) / np.sum((x['target'] - x['target'].mean())**2)\n",
    ").round(4)\n",
    "\n",
    "# Combine all metrics\n",
    "performance_summary = pd.DataFrame({\n",
    "    'N_samples': glacier_stats[('target', 'count')],\n",
    "    'RMSE': glacier_rmse,\n",
    "    'MAE': glacier_mae,\n",
    "    'R²': glacier_r2,\n",
    "    'Target_mean': glacier_stats[('target', 'mean')],\n",
    "    'Target_std': glacier_stats[('target', 'std')]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "print(\"Performance by glacier (sorted by RMSE):\")\n",
    "display(performance_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
