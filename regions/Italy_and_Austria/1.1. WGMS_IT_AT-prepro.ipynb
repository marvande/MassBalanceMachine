{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from oggm import utils\n",
    "from tqdm import tqdm\n",
    "from cmcrameri import cm\n",
    "\n",
    "# from scripts.helpers import *\n",
    "# from scripts.italy_austria_preprocess import *\n",
    "# from scripts.config_IT_AT import *\n",
    "\n",
    "from regions.Switzerland.scripts.oggm import initialize_oggm_glacier_directories, export_oggm_grids\n",
    "from regions.Switzerland.scripts.glamos import merge_pmb_with_oggm_data, rename_stakes_by_elevation, check_point_ids_contain_glacier, remove_close_points, check_multiple_rgi_ids\n",
    "\n",
    "from regions.French_Alps.scripts.glacioclim_preprocess import add_svf_from_rgi_zarr, plot_missing_svf_for_all_glaciers, add_svf_nearest_valid\n",
    "from scripts.italy_austria_preprocess import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.ItalyAustriaConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stakes into 1 df\n",
    "\n",
    "The data has been acquired directly from WGMS's Fluctuations of Glaciers (FoG) Database. Version  10.5904/wgms-fog-2025-02b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_PMB_WGMS_raw = \"WGMS/raw/\"\n",
    "\n",
    "df_stakes = pd.read_csv(cfg.dataPath + path_PMB_WGMS_raw +\n",
    "                        'mass_balance_point.csv')\n",
    "df_it_at_RGIId = pd.read_csv(cfg.dataPath + path_PMB_WGMS_raw + 'glacier.csv')\n",
    "\n",
    "# Filter df_stakes to include only rows where country is AT or IT\n",
    "df_stakes = df_stakes[df_stakes['country'].isin(['AT',\n",
    "                                                 'IT'])].reset_index(drop=True)\n",
    "\n",
    "# Create a mapping dictionary from id to rgi60_ids\n",
    "id_to_rgi_map = dict(zip(df_it_at_RGIId['id'], df_it_at_RGIId['rgi60_ids']))\n",
    "\n",
    "# Add the RGIId column to the filtered DataFrame using glacier_id instead of id\n",
    "df_stakes['RGIId'] = df_stakes['glacier_id'].map(id_to_rgi_map)\n",
    "\n",
    "# Display glacier names with NaN RGIId\n",
    "display(f\"Number of rows with NaN RGIId: {df_stakes['RGIId'].isna().sum()}\")\n",
    "display(df_stakes[df_stakes['RGIId'].isna()]['glacier_name'].unique())\n",
    "\n",
    "# Only Careser glacier has NaN RGIIds as only RGIId_50 are listed in the csv file.\n",
    "\n",
    "## find RGIId_60 for Careser glaciers\n",
    "glacier_outline = gpd.read_file(\n",
    "    cfg.dataPath + \"RGI_v6/RGI_11_CentralEurope/11_rgi60_CentralEurope.shp\")\n",
    "\n",
    "# Search by name\n",
    "careser_glacier = glacier_outline[\n",
    "    glacier_outline['Name'].notna()\n",
    "    & glacier_outline['Name'].str.contains('CARESER', case=False)]\n",
    "display(careser_glacier[['RGIId', 'Name']])\n",
    "\n",
    "# RGIId_60 and 50 are the same: RGI50-11.01834 and RGI60-11.01834, add to df\n",
    "for glacier_id in df_stakes[df_stakes['RGIId'].isna()]['glacier_id'].unique():\n",
    "    df_stakes.loc[df_stakes['glacier_id'] == glacier_id,\n",
    "                  'RGIId'] = 'RGI60-11.01834'\n",
    "\n",
    "display(f\"Number of rows with NaN RGIId: {df_stakes['RGIId'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stakes['remarks'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns\n",
    "df_stakes_renamed = df_stakes.rename(\n",
    "    columns={\n",
    "        'point_id': 'POINT_ID',\n",
    "        'latitude': 'POINT_LAT',\n",
    "        'longitude': 'POINT_LON',\n",
    "        'elevation': 'POINT_ELEVATION',\n",
    "        'begin_date': 'FROM_DATE',\n",
    "        'end_date': 'TO_DATE',\n",
    "        'balance': 'POINT_BALANCE',\n",
    "        'glacier_name': 'GLACIER',\n",
    "        'year': 'YEAR',\n",
    "        'country': 'COUNTRY',\n",
    "        'balance_code': 'PERIOD'\n",
    "    })\n",
    "\n",
    "# Create new POINT_ID column\n",
    "df_stakes_renamed['POINT_ID'] = (df_stakes_renamed['GLACIER'] + '_' +\n",
    "                                 df_stakes_renamed['YEAR'].astype(str) + '_' +\n",
    "                                 df_stakes['id'].astype(str) + '_' +\n",
    "                                 df_stakes_renamed['COUNTRY'])\n",
    "# Only keep relevant columns in df\n",
    "df_stakes_renamed = df_stakes_renamed[[\n",
    "    'POINT_ID', 'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION', 'FROM_DATE',\n",
    "    'TO_DATE', 'POINT_BALANCE', 'GLACIER', 'PERIOD', 'RGIId', 'YEAR',\n",
    "    'begin_date_unc', 'end_date_unc'\n",
    "]]\n",
    "\n",
    "# Remove rows with NaN values in POINT_LAT, POINT_LON, and POINT_ELEVATION\n",
    "df_stakes_renamed = df_stakes_renamed.dropna(\n",
    "    subset=['POINT_LAT', 'POINT_LON', 'POINT_ELEVATION'])\n",
    "\n",
    "# change date format to YYYYMMDD\n",
    "df_stakes_renamed['FROM_DATE'] = df_stakes_renamed['FROM_DATE'].astype(\n",
    "    str).str.replace('-', '')\n",
    "df_stakes_renamed['TO_DATE'] = df_stakes_renamed['TO_DATE'].astype(\n",
    "    str).str.replace('-', '')\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "df_stakes_renamed['DATA_MODIFICATION'] = ''\n",
    "\n",
    "display(df_stakes_renamed.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any entry anywhere is NaN\n",
    "display(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])\n",
    "\n",
    "# One stake has a wrong elevation of 296 instead of 2960\n",
    "display(df_stakes_renamed[df_stakes_renamed['POINT_ID'] ==\n",
    "                          'VERNAGT F._2013_15124_AT'])\n",
    "df_stakes_renamed.loc[df_stakes_renamed['POINT_ID'] ==\n",
    "                      'VERNAGT F._2013_15124_AT', 'POINT_ELEVATION'] = 2960\n",
    "df_stakes_renamed.loc[\n",
    "    df_stakes_renamed['POINT_ID'] == 'VERNAGT F._2013_15124_AT',\n",
    "    'DATA_MODIFICATION'] = 'Elevation corrected from 296 to 2960 m'\n",
    "display(df_stakes_renamed[df_stakes_renamed['POINT_ID'] ==\n",
    "                          'VERNAGT F._2013_15124_AT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some stakes have the year 1012 instead of 2012 etc. find all these stakes\n",
    "problematic_dates = []\n",
    "for i, date in enumerate(df_stakes_renamed['FROM_DATE']):\n",
    "    try:\n",
    "        pd.to_datetime(str(date), format=\"%Y%m%d\")\n",
    "    except:\n",
    "        problematic_dates.append((i, date, 'FROM_DATE'))\n",
    "\n",
    "for i, date in enumerate(df_stakes_renamed['TO_DATE']):\n",
    "    try:\n",
    "        pd.to_datetime(str(date), format=\"%Y%m%d\")\n",
    "    except:\n",
    "        problematic_dates.append((i, date, 'TO_DATE'))\n",
    "\n",
    "print(f\"Found {len(problematic_dates)} problematic date entries\")\n",
    "if problematic_dates:\n",
    "    print(problematic_dates)\n",
    "\n",
    "# All stakes from same glacier MALAVALLE and date 10120508. Correct the date\n",
    "df_stakes_renamed.loc[\n",
    "    df_stakes_renamed['FROM_DATE'] == '10120508',\n",
    "    'DATA_MODIFICATION'] = 'Date corrected from 10120508 to 20120508'\n",
    "df_stakes_renamed['FROM_DATE'] = df_stakes_renamed['FROM_DATE'].replace(\n",
    "    '10120508', '20120508')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the original dataset Glaciers \"OE. WURTEN K.\", \"VERNAGT F.\" and \"GRAND ETRET\" have multiple measurements with date_unc 182 or 182.5\n",
    "## These dates are always entered as start of july, correct them to 30.04 and 01.10\n",
    "\n",
    "display(df_stakes_renamed[(df_stakes_renamed['begin_date_unc'] >= 182)\n",
    "                          | (df_stakes_renamed['end_date_unc'] >= 182)])\n",
    "\n",
    "# Update the DATA_MODIFICATION column for these rows\n",
    "uncertain_date_mask = (df_stakes_renamed['begin_date_unc']\n",
    "                       >= 182) | (df_stakes_renamed['end_date_unc'] >= 182)\n",
    "df_stakes_renamed.loc[\n",
    "    uncertain_date_mask,\n",
    "    'DATA_MODIFICATION'] = \"Dates corrected due to high uncertainty (~= 182 days)\"\n",
    "\n",
    "# Update dates\n",
    "df_stakes_renamed = fix_uncertain_dates(df_stakes_renamed)\n",
    "\n",
    "display(df_stakes_renamed[(df_stakes_renamed['begin_date_unc'] >= 182)\n",
    "                          | (df_stakes_renamed['end_date_unc'] >= 182)])\n",
    "\n",
    "# Remove _unc columns, were only needed for fixing uncertain dates\n",
    "df_stakes_renamed = df_stakes_renamed[[\n",
    "    'POINT_ID', 'POINT_LAT', 'POINT_LON', 'POINT_ELEVATION', 'FROM_DATE',\n",
    "    'TO_DATE', 'POINT_BALANCE', 'GLACIER', 'PERIOD', 'RGIId', 'YEAR',\n",
    "    'DATA_MODIFICATION'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_renamed)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(annual_inconsistent)\n",
    "display(winter_inconsistent)\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "## 2 Cases of inconsistent periods:\n",
    "# 1. HALLSTAETTER G._2024_63282_AT has MONTH_DIFF of 1, unclear whether this is a date error or an actual measurement (since pmb is also lower than other stakes in that year, just remove it)\n",
    "df_stakes_renamed = df_stakes_renamed.loc[df_stakes_renamed['POINT_ID'] !=\n",
    "                                          'HALLSTAETTER G._2024_63282_AT']\n",
    "\n",
    "# 2. GRAND ETRET in Year 2008 goes from 1999 to 2008, assuming this is a date error and changing year to 2007\n",
    "mask = (df_stakes_renamed['GLACIER']\n",
    "        == 'GRAND ETRET') & (df_stakes_renamed['YEAR'] == 2008)\n",
    "df_stakes_renamed.loc[\n",
    "    mask, 'DATA_MODIFICATION'] = 'FROM_DATE year corrected from 1999 to 2007'\n",
    "df_stakes_renamed.loc[mask, 'FROM_DATE'] = df_stakes_renamed.loc[\n",
    "    mask, 'FROM_DATE'].str.replace('1999', '2007')\n",
    "\n",
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any entry anywhere is NaN\n",
    "display(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add OGGM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"11\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "export_oggm_grids(cfg, gdirs, rgi_region=\"11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rgis = df_stakes_renamed['RGIId'].unique()\n",
    "\n",
    "df_stakes_topo = merge_pmb_with_oggm_data(\n",
    "    df_pmb=df_stakes_renamed,\n",
    "    gdirs=gdirs,\n",
    "    rgi_region=\"11\",  # Central Europe\n",
    "    rgi_version=\"62\")\n",
    "\n",
    "# Restrict to within glacier shape\n",
    "df_stakes_topo = df_stakes_topo[df_stakes_topo['within_glacier_shape'] == True]\n",
    "df_stakes_topo = df_stakes_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "# Display rows that have any NaN values\n",
    "display(df_stakes_topo[df_stakes_topo[\"aspect\"].isna()])\n",
    "display(df_stakes_topo[df_stakes_topo[\"slope\"].isna()])\n",
    "display(df_stakes_topo[df_stakes_topo[\"topo\"].isna()])\n",
    "\n",
    "# Drop 3 rows where consensus_ice_thickness is NaN\n",
    "#df_stakes_topo_dropped = df_stakes_topo.dropna(subset=['consensus_ice_thickness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge closes stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_pmb_topo = pd.DataFrame()\n",
    "for gl in tqdm(df_stakes_topo.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_stakes_topo[df_stakes_topo.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_topo = pd.concat([df_pmb_topo, df_gl_cleaned])\n",
    "df_pmb_topo.drop(['x', 'y'], axis=1, inplace=True)\n",
    "df_pmb_topo.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add skyview factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              \"RGI_v6/RGI_11_CentralEurope/xr_masked_grids\")\n",
    "\n",
    "df_pmb_topo_svf = add_svf_from_rgi_zarr(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    ")\n",
    "df_missing = df_pmb_topo_svf[df_pmb_topo_svf[\"svf\"].isna()].copy()\n",
    "print(\"Missing SVF points:\", len(df_missing))\n",
    "print(\"Glaciers affected:\", sorted(df_missing[\"RGIId\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_topo_svf_new = add_svf_nearest_valid(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    "    max_radius=30,  # ~30 grid cells search; adjust if needed\n",
    ")\n",
    "\n",
    "print(\"Missing SVF points after nearest-valid fill:\",\n",
    "      df_pmb_topo_svf_new[\"svf\"].isna().sum())\n",
    "\n",
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf_new,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give new stake ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_new_ids = rename_stakes_by_elevation(df_pmb_topo_svf)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_new_ids)\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_new_ids))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_new_ids['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_clean = df_pmb_new_ids.copy()\n",
    "\n",
    "# Ensure YYYYMMDD format\n",
    "df_pmb_clean[\"FROM_DATE\"] = df_pmb_clean[\"FROM_DATE\"].astype(str).str.zfill(8)\n",
    "df_pmb_clean[\"TO_DATE\"] = df_pmb_clean[\"TO_DATE\"].astype(str).str.zfill(8)\n",
    "\n",
    "# Extract months\n",
    "df_pmb_clean[\"MONTH_START\"] = df_pmb_clean[\"FROM_DATE\"].str[4:6]\n",
    "df_pmb_clean[\"MONTH_END\"] = df_pmb_clean[\"TO_DATE\"].str[4:6]\n",
    "\n",
    "\n",
    "def print_months(df, label):\n",
    "    winter = df[df.PERIOD == \"winter\"]\n",
    "    annual = df[df.PERIOD == \"annual\"]\n",
    "\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"Winter measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(winter[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(winter[\"MONTH_END\"].unique()))\n",
    "\n",
    "    print(\"\\nAnnual measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(annual[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(annual[\"MONTH_END\"].unique()))\n",
    "\n",
    "\n",
    "# --- Before filtering ---\n",
    "print_months(df_pmb_clean, \"Before filtering\")\n",
    "\n",
    "# -----------------------\n",
    "# Filtering masks (define + count BEFORE filtering)\n",
    "# -----------------------\n",
    "\n",
    "mask_winter_end_07 = (\n",
    "    (df_pmb_clean[\"PERIOD\"].astype(str).str.strip().str.lower() == \"winter\") &\n",
    "    (df_pmb_clean[\"MONTH_END\"] == \"07\"))\n",
    "\n",
    "# counts (on original df)\n",
    "n_total_removed = int(mask_winter_end_07.sum())\n",
    "n_winter_end_07 = n_total_removed\n",
    "\n",
    "# Apply removal\n",
    "df_pmb_clean = df_pmb_clean.loc[~mask_winter_end_07].copy()\n",
    "\n",
    "# --- Correct mislabeled winter MB ---\n",
    "mask_fix = (\n",
    "    (df_pmb_clean[\"PERIOD\"].astype(str).str.strip().str.lower() == \"winter\") &\n",
    "    (df_pmb_clean[\"MONTH_END\"] == \"06\") & (df_pmb_clean[\"POINT_BALANCE\"] < 0))\n",
    "n_relabel = int(mask_fix.sum())\n",
    "df_pmb_clean.loc[mask_fix, \"PERIOD\"] = \"annual\"\n",
    "\n",
    "print(f\"\\nRemoved {n_total_removed} rows total.\\n\"\n",
    "      f\"  - winter-end-07 rows removed: {n_winter_end_07}\\n\"\n",
    "      f\"Relabeled winter -> annual: {n_relabel}\")\n",
    "\n",
    "print_months(df_pmb_clean, \"After filtering + relabeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv:\n",
    "df_pmb_clean.to_csv(os.path.join(cfg.dataPath, \"WGMS/Italy_Austria/csv\",\n",
    "                                 'IT_AT_wgms_dataset_all.csv'),\n",
    "                    index=False)\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_clean['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "def plot_stakes_folium(\n",
    "    df_pmb_clean,\n",
    "    glacier_col=\"GLACIER\",\n",
    "    lat_col=None,\n",
    "    lon_col=None,\n",
    "    elev_col=None,\n",
    "    id_col=None,\n",
    "    center=None,\n",
    "    zoom_start=10,\n",
    "    color_map=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive Folium map of stake points grouped by glacier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infer column names if not provided\n",
    "    if lat_col is None:\n",
    "        lat_col = \"lat\" if \"lat\" in df_pmb_clean.columns else \"POINT_LAT\"\n",
    "    if lon_col is None:\n",
    "        lon_col = \"lon\" if \"lon\" in df_pmb_clean.columns else \"POINT_LON\"\n",
    "    if elev_col is None:\n",
    "        elev_col = \"altitude\" if \"altitude\" in df_pmb_clean.columns else \"POINT_ELEVATION\"\n",
    "    if id_col is None:\n",
    "        id_col = \"stake_number\" if \"stake_number\" in df_pmb_clean.columns else \"POINT_ID\"\n",
    "\n",
    "    # Compute center if not provided\n",
    "    if center is None:\n",
    "        center_lat = float(df_pmb_clean[lat_col].median())\n",
    "        center_lon = float(df_pmb_clean[lon_col].median())\n",
    "    else:\n",
    "        center_lat, center_lon = center\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start)\n",
    "\n",
    "    # Default colors (cycled) if user doesn't give explicit mapping\n",
    "    default_colors = [\n",
    "        \"red\", \"blue\", \"green\", \"purple\", \"orange\", \"darkred\", \"cadetblue\",\n",
    "        \"darkgreen\", \"darkpurple\", \"pink\", \"gray\", \"black\"\n",
    "    ]\n",
    "\n",
    "    glaciers = sorted(df_pmb_clean[glacier_col].dropna().unique())\n",
    "\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            g: default_colors[i % len(default_colors)]\n",
    "            for i, g in enumerate(glaciers)\n",
    "        }\n",
    "    else:\n",
    "        # fill missing glaciers with default cycling\n",
    "        for i, g in enumerate(glaciers):\n",
    "            color_map.setdefault(g, default_colors[i % len(default_colors)])\n",
    "\n",
    "    # Add markers for each glacier\n",
    "    for glacier_name, df_g in df_pmb_clean.groupby(glacier_col):\n",
    "        if pd.isna(glacier_name):\n",
    "            continue\n",
    "\n",
    "        fg = folium.FeatureGroup(name=str(glacier_name))\n",
    "        color = color_map[str(glacier_name)]\n",
    "\n",
    "        for _, row in df_g.iterrows():\n",
    "            stake_id = row.get(id_col, \"NA\")\n",
    "            altitude = row.get(elev_col, \"NA\")\n",
    "\n",
    "            folium.CircleMarker(\n",
    "                location=[row[lat_col], row[lon_col]],\n",
    "                radius=5,\n",
    "                color=color,\n",
    "                fill=True,\n",
    "                fill_color=color,\n",
    "                fill_opacity=0.9,\n",
    "                popup=f\"{glacier_name} - Stake {stake_id}: {altitude} m\",\n",
    "            ).add_to(fg)\n",
    "\n",
    "        fg.add_to(m)\n",
    "\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # Legend (auto-generated)\n",
    "    legend_rows = \"\\n\".join(\n",
    "        f'<p><span style=\"color: {color_map[g]};\">‚óè</span> {g}</p>'\n",
    "        for g in glaciers)\n",
    "\n",
    "    legend_html = f\"\"\"\n",
    "    <div style=\"\n",
    "        position: fixed; bottom: 50px; left: 50px; z-index: 1000;\n",
    "        background-color: white; padding: 10px; border-radius: 5px;\n",
    "        border: 1px solid #999;\n",
    "    \">\n",
    "        <p><strong>Glaciers</strong></p>\n",
    "        {legend_rows}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "m = plot_stakes_folium(df_pmb_clean, color_map=None)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of winter and annual samples:', len(df_pmb_clean))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_clean[df_pmb_clean.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_clean[df_pmb_clean.PERIOD == 'winter']))\n",
    "\n",
    "# Number of measurements per year:\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 15))\n",
    "ax = axs.flatten()[0]\n",
    "df_pmb_clean.groupby(['YEAR', 'PERIOD']).size().unstack().plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=[mbm.plots.COLOR_ANNUAL, mbm.plots.COLOR_WINTER],\n",
    "    ax=ax)\n",
    "ax.set_title('Number of measurements per year for all glaciers')\n",
    "\n",
    "ax = axs.flatten()[1]\n",
    "num_gl = df_pmb_clean.groupby(['GLACIER']).size().sort_values()\n",
    "num_gl.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of total measurements per glacier since 1951')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
