{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../../')) # Add root of repo to import MBM\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import massbalancemachine as mbm\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "import glob\n",
    "from cmcrameri import cm\n",
    "from oggm import utils\n",
    "\n",
    "from regions.Iceland.scripts.iceland_preprocess import *\n",
    "from regions.Iceland.scripts.config_ICE import *\n",
    "\n",
    "from regions.Switzerland.scripts.oggm import initialize_oggm_glacier_directories, export_oggm_grids\n",
    "from regions.Switzerland.scripts.glamos import merge_pmb_with_oggm_data, rename_stakes_by_elevation, check_point_ids_contain_glacier, remove_close_points, check_multiple_rgi_ids\n",
    "\n",
    "from regions.French_Alps.scripts.glacioclim_preprocess import add_svf_from_rgi_zarr, plot_missing_svf_for_all_glaciers, add_svf_nearest_valid\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cfg = mbm.IcelandConfig()\n",
    "\n",
    "# Module logger\n",
    "log = logging.getLogger('.'.join(__name__.split('.')[:-1]))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mbm.utils.seed_all(cfg.seed)\n",
    "mbm.utils.free_up_cuda()\n",
    "mbm.plots.use_mbm_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all stake csv files into 1 df\n",
    "The data used in this code comes from the data scraping done in the 1.0 Iceland-data-acquisition notebook in June 2025, only winter and annual measurements are used. Code might have to be adjusted if new data is added to https://joklavefsja.vedur.is/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(cfg.dataPath + path_PMB_WGMS_raw, \"*.csv\"))\n",
    "\n",
    "# Initialize empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV file into a dataframe and append to list\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print info\n",
    "print(\n",
    "    f\"Combined {len(all_files)} CSV files into one dataframe with {len(combined_df)} rows\"\n",
    ")\n",
    "\n",
    "# Add data modification column to keep track of mannual changes\n",
    "combined_df['DATA_MODIFICATION'] = ''\n",
    "\n",
    "display(combined_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into annual and winter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_split = split_stake_measurements(combined_df)\n",
    "\n",
    "# Convert date columns to string in 'YYYYMMDD' format\n",
    "df_stakes_split['TO_DATE'] = pd.to_datetime(\n",
    "    df_stakes_split['TO_DATE']).dt.strftime('%Y%m%d')\n",
    "df_stakes_split['FROM_DATE'] = pd.to_datetime(\n",
    "    df_stakes_split['FROM_DATE']).dt.strftime('%Y%m%d')\n",
    "\n",
    "display(df_stakes_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Fixes\n",
    "\n",
    "Fix NaN dates by adding hydrological year dates. (It would be nicer if this code also checked if there was a previous year of the same stake with a date and then takes that date instead of hydr. year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_stakes_split[df_stakes_split['FROM_DATE'].isna()])\n",
    "display(df_stakes_split[df_stakes_split['TO_DATE'].isna()])\n",
    "display(df_stakes_split[df_stakes_split['YEAR'].isna()])\n",
    "\n",
    "# Change NaN year values to the year of the TO_DATE\n",
    "df_stakes_split.loc[df_stakes_split['YEAR'].isna(),\n",
    "                    'YEAR'] = df_stakes_split.loc[\n",
    "                        df_stakes_split['YEAR'].isna(),\n",
    "                        'TO_DATE'].astype(str).str[:4].astype(float)\n",
    "\n",
    "# Data modification column update\n",
    "date_nan_mask = df_stakes_split['FROM_DATE'].isna(\n",
    ") | df_stakes_split['TO_DATE'].isna()\n",
    "df_stakes_split.loc[\n",
    "    date_nan_mask,\n",
    "    'DATA_MODIFICATION'] = 'Dates filled in according to hydrological year'\n",
    "# Set FROM_DATE from NaN to 01 Oct of previous year\n",
    "df_stakes_split.loc[df_stakes_split['FROM_DATE'].isna(), 'FROM_DATE'] = (\n",
    "    (df_stakes_split.loc[df_stakes_split['FROM_DATE'].isna(),\n",
    "                         'YEAR'].astype(int) - 1).astype(str) + '1001')\n",
    "# Set TO_DATE from NaN to 30 Sept of the year (as only annual rows have NaN, no need for period distinction)\n",
    "df_stakes_split.loc[df_stakes_split['TO_DATE'].isna(), 'TO_DATE'] = (\n",
    "    df_stakes_split.loc[df_stakes_split['TO_DATE'].isna(),\n",
    "                        'YEAR'].astype(int).astype(str) + '0930')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for problematic date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_inconsistent, winter_inconsistent = check_period_consistency(\n",
    "    df_stakes_split)\n",
    "\n",
    "# Display the inconsistent records\n",
    "if len(annual_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent annual periods:\")\n",
    "    display(annual_inconsistent)\n",
    "\n",
    "if len(winter_inconsistent) > 0:\n",
    "    print(\"\\nInconsistent winter periods:\")\n",
    "    display(winter_inconsistent)\n",
    "\n",
    "# Only index 5084 is unreasonabl (-2), probably wrong FROM_DATE year, change to year - 1\n",
    "df_stakes_split.loc[df_stakes_split['stake'] == 'GL10a',\n",
    "                    'FROM_DATE'] = '19960825'\n",
    "df_stakes_split.loc[\n",
    "    df_stakes_split['stake'] == 'GL10a',\n",
    "    'DATA_MODIFICATION'] = 'FROM_DATE year corrected from 1997 to 1996'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Columns and general data cleaning, we can skip the close stake removal, as seen form the leaflet map online, the stakes are spaced out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stakes_renamed = df_stakes_split.rename(\n",
    "    columns={\n",
    "        'lat': 'POINT_LAT',\n",
    "        'lon': 'POINT_LON',\n",
    "        'elevation': 'POINT_ELEVATION',\n",
    "        'stake': 'ID',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN check\n",
    "display(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])\n",
    "\n",
    "# Remove all rows with any NaN values\n",
    "df_stakes_renamed = df_stakes_renamed.dropna()\n",
    "\n",
    "# Confirm removal - this should show 0 rows if all NaNs were removed\n",
    "print(\n",
    "    f\"Rows with NaN values after removal: {len(df_stakes_renamed[df_stakes_renamed.isna().any(axis=1)])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OGGM glacier directories\n",
    "gdirs, rgidf = initialize_oggm_glacier_directories(\n",
    "    cfg,\n",
    "    rgi_region=\"06\",\n",
    "    rgi_version=\"62\",\n",
    "    base_url=\n",
    "    \"https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/2025.6/elev_bands_w_data/\",\n",
    "    log_level='WARNING',\n",
    "    task_list=None,\n",
    ")\n",
    "\n",
    "export_oggm_grids(cfg, gdirs, rgi_region=\"06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glacier outlines\n",
    "rgi_file = utils.get_rgi_region_file(region=\"06\", version=\"62\")\n",
    "glacier_outline = gpd.read_file(rgi_file)\n",
    "\n",
    "# Add RGI IDs through intersection\n",
    "df_stakes_renamed_rgiid = mbm.data_processing.utils.get_rgi(\n",
    "    data=df_stakes_renamed, glacier_outlines=glacier_outline)\n",
    "print('Number of measurements without RGI:',\n",
    "      len(df_stakes_renamed_rgiid[df_stakes_renamed_rgiid['RGIId'].isna()]))\n",
    "\n",
    "# Remove (nine) stakes without RGIId, as they wont have OGGM data anyways\n",
    "df_stakes_renamed_rgiid = df_stakes_renamed_rgiid.dropna(subset=['RGIId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add OGGM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rgis = df_stakes_renamed_rgiid['RGIId'].unique()\n",
    "\n",
    "## Around 10% of all the measurements have no hugonnet_dhdt data, so I removed the entire variable from merge_pmb_with_oggm_data()\n",
    "df_stakes_topo = merge_pmb_with_oggm_data(\n",
    "    df_pmb=df_stakes_renamed_rgiid,\n",
    "    gdirs=gdirs,\n",
    "    rgi_region=\"06\",  #06 iceland\n",
    "    rgi_version=\"62\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Glacier names from RGIId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping from RGIId to glacier name\n",
    "rgi_to_name_dict = dict(zip(rgidf.RGIId, rgidf.Name))\n",
    "df_stakes_topo['GLACIER'] = df_stakes_topo['RGIId'].map(rgi_to_name_dict)\n",
    "display(df_stakes_topo[df_stakes_topo['GLACIER'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple RGIIds have no associated glacier name, assign the 'RGIId' as the 'GLACIER' name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rgi_ids = df_stakes_topo.loc[df_stakes_topo['GLACIER'].isna(),\n",
    "                                     'RGIId'].unique()\n",
    "print(f\"Number of unique RGI IDs without names: {len(missing_rgi_ids)}\")\n",
    "print(\"RGI IDs without names:\", missing_rgi_ids)\n",
    "# Just assign RGIId to 'GLACIER' as name for the ones that are missing\n",
    "df_stakes_topo.loc[df_stakes_topo['GLACIER'].isna(),\n",
    "                   'GLACIER'] = df_stakes_topo.loc[\n",
    "                       df_stakes_topo['GLACIER'].isna(), 'RGIId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "glacierName = 'Thjorsarjoekull (Hofsjoekull E)'\n",
    "# stakes\n",
    "df_stakes_topo_1 = df_stakes_topo.copy()\n",
    "df_stakes_topo_1 = df_stakes_topo_1[(\n",
    "    df_stakes_topo_1['GLACIER'] == glacierName)]\n",
    "RGIId = df_stakes_topo_1['RGIId'].unique()[0]\n",
    "print(RGIId)\n",
    "# open OGGM xr for glacier\n",
    "# Get oggm data for that RGI grid\n",
    "ds_oggm = xr.open_dataset(f'{cfg.dataPath + path_OGGM_xrgrids}/{RGIId}.zarr')\n",
    "\n",
    "# Define the coordinate transformation\n",
    "transf = pyproj.Transformer.from_proj(\n",
    "    pyproj.CRS.from_user_input(\"EPSG:4326\"),  # Input CRS (WGS84)\n",
    "    pyproj.CRS.from_user_input(ds_oggm.pyproj_srs),  # Output CRS from dataset\n",
    "    always_xy=True)\n",
    "\n",
    "# Transform all coordinates in the group\n",
    "lon, lat = df_stakes_topo_1[\"POINT_LON\"].values, df_stakes_topo_1[\n",
    "    \"POINT_LAT\"].values\n",
    "x_stake, y_stake = transf.transform(lon, lat)\n",
    "df_stakes_topo_1['x'] = x_stake\n",
    "df_stakes_topo_1['y'] = y_stake\n",
    "\n",
    "# plot stakes\n",
    "plt.figure(figsize=(8, 6))\n",
    "ds_oggm.glacier_mask.plot(cmap='binary')\n",
    "sns.scatterplot(df_stakes_topo_1,\n",
    "                x='x',\n",
    "                y='y',\n",
    "                hue='within_glacier_shape',\n",
    "                palette=['r', 'b'])\n",
    "plt.title(f'Stakes on {glacierName} (OGGM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to within glacier shape\n",
    "df_stakes_topo = df_stakes_topo[df_stakes_topo['within_glacier_shape'] == True]\n",
    "df_stakes_topo = df_stakes_topo.drop(columns=['within_glacier_shape'])\n",
    "\n",
    "# Display rows that have any NaN values\n",
    "display(df_stakes_topo[df_stakes_topo.isna().any(axis=1)])\n",
    "\n",
    "# Drop 3 rows where consensus_ice_thickness is NaN\n",
    "#df_stakes_topo_dropped = df_stakes_topo.dropna(subset=['consensus_ice_thickness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new POINT_ID column\n",
    "df_stakes_topo['POINT_ID'] = (df_stakes_topo['GLACIER'] + '_' +\n",
    "                              df_stakes_topo['YEAR'].astype(str) + '_' +\n",
    "                              df_stakes_topo['PERIOD'].astype(str) + '_' +\n",
    "                              df_stakes_topo['ID'].astype(str))\n",
    "\n",
    "df_stakes_topo = df_stakes_topo.drop(columns=['ID'])\n",
    "\n",
    "display(df_stakes_topo.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "display(df_stakes_topo[df_stakes_topo.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge close stakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# df_pmb_topo = remove_close_points(df_stakes_topo)\n",
    "df_pmb_topo = pd.DataFrame()\n",
    "for gl in tqdm(df_stakes_topo.GLACIER.unique(), desc='Merging stakes'):\n",
    "    print(f'-- {gl.capitalize()}:')\n",
    "    df_gl = df_stakes_topo[df_stakes_topo.GLACIER == gl]\n",
    "    df_gl_cleaned = remove_close_points(df_gl)\n",
    "    df_pmb_topo = pd.concat([df_pmb_topo, df_gl_cleaned])\n",
    "df_pmb_topo.drop(['x', 'y'], axis=1, inplace=True)\n",
    "df_pmb_topo.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for wrong elevation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checked, df_bad = flag_elevation_mismatch(df_pmb_topo, threshold=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Skyview factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one svf file\n",
    "rgi_id = df_pmb_topo.loc[0].RGIId\n",
    "\n",
    "nigardsbreen_rgi = \"RGI60-06.00002\"\n",
    "\n",
    "# read ds with svf\n",
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              'RGI_v6/RGI_06_Iceland/xr_masked_grids/')\n",
    "\n",
    "xr.open_zarr(path_masked_xr + f'{nigardsbreen_rgi}.zarr').svf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_masked_xr = os.path.join(cfg.dataPath,\n",
    "                              \"RGI_v6/RGI_06_Iceland/xr_masked_grids\")\n",
    "\n",
    "df_pmb_topo_svf = add_svf_from_rgi_zarr(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    ")\n",
    "df_missing = df_pmb_topo_svf[df_pmb_topo_svf[\"svf\"].isna()].copy()\n",
    "print(\"Missing SVF points:\", len(df_missing))\n",
    "print(\"Glaciers affected:\", sorted(df_missing[\"RGIId\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_topo_svf_new = add_svf_nearest_valid(\n",
    "    df_pmb_topo,\n",
    "    path_masked_xr,\n",
    "    rgi_col=\"RGIId\",\n",
    "    lon_col=\"POINT_LON\",\n",
    "    lat_col=\"POINT_LAT\",\n",
    "    svf_var=\"svf\",\n",
    "    out_col=\"svf\",\n",
    "    max_radius=30,  # ~30 grid cells search; adjust if needed\n",
    ")\n",
    "\n",
    "print(\"Missing SVF points after nearest-valid fill:\",\n",
    "      df_pmb_topo_svf_new[\"svf\"].isna().sum())\n",
    "\n",
    "plot_missing_svf_for_all_glaciers(\n",
    "    df_with_svf=df_pmb_topo_svf_new,\n",
    "    path_masked_xr=path_masked_xr,\n",
    "    plot_valid_points=True,\n",
    "    save_dir=\n",
    "    None  # or e.g. os.path.join(cfg.dataPath, \"diagnostics/svf_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give new stake IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_new_ids = rename_stakes_by_elevation(df_pmb_topo_svf_new)\n",
    "\n",
    "# Check the condition\n",
    "check_point_ids_contain_glacier(df_pmb_new_ids)\n",
    "\n",
    "print('Number of winter and annual samples:', len(df_pmb_new_ids))\n",
    "print('Number of annual samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'annual']))\n",
    "print('Number of winter samples:',\n",
    "      len(df_pmb_new_ids[df_pmb_new_ids.PERIOD == 'winter']))\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_new_ids['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pmb_clean = df_pmb_new_ids.copy()\n",
    "\n",
    "# Ensure YYYYMMDD format\n",
    "df_pmb_clean[\"FROM_DATE\"] = df_pmb_clean[\"FROM_DATE\"].astype(str).str.zfill(8)\n",
    "df_pmb_clean[\"TO_DATE\"] = df_pmb_clean[\"TO_DATE\"].astype(str).str.zfill(8)\n",
    "\n",
    "# Extract months\n",
    "df_pmb_clean[\"MONTH_START\"] = df_pmb_clean[\"FROM_DATE\"].str[4:6]\n",
    "df_pmb_clean[\"MONTH_END\"] = df_pmb_clean[\"TO_DATE\"].str[4:6]\n",
    "\n",
    "def print_months(df, label):\n",
    "    winter = df[df.PERIOD == \"winter\"]\n",
    "    annual = df[df.PERIOD == \"annual\"]\n",
    "\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"Winter measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(winter[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(winter[\"MONTH_END\"].unique()))\n",
    "\n",
    "    print(\"\\nAnnual measurement months:\")\n",
    "    print(\"  Unique start months:\", sorted(annual[\"MONTH_START\"].unique()))\n",
    "    print(\"  Unique end months:  \", sorted(annual[\"MONTH_END\"].unique()))\n",
    "\n",
    "# --- Before filtering ---\n",
    "print_months(df_pmb_clean, \"Before filtering\")\n",
    "\n",
    "# -----------------------\n",
    "# Filtering masks (define + count BEFORE filtering)\n",
    "# -----------------------\n",
    "bad_months = {\"07\", \"12\", \"01\"}\n",
    "\n",
    "mask_bad_months_all = (\n",
    "    df_pmb_clean[\"MONTH_START\"].isin(bad_months) |\n",
    "    df_pmb_clean[\"MONTH_END\"].isin(bad_months)\n",
    ")\n",
    "\n",
    "mask_bad_winter_aug = (\n",
    "    (df_pmb_clean[\"PERIOD\"].astype(str).str.strip().str.lower() == \"winter\") &\n",
    "    (df_pmb_clean[\"MONTH_END\"] == \"08\")\n",
    ")\n",
    "\n",
    "mask_remove = mask_bad_months_all | mask_bad_winter_aug\n",
    "\n",
    "# counts (on original df)\n",
    "n_total_removed = int(mask_remove.sum())\n",
    "n_bad_months = int(mask_bad_months_all.sum())\n",
    "n_winter_aug = int(mask_bad_winter_aug.sum())\n",
    "n_overlap = int((mask_bad_months_all & mask_bad_winter_aug).sum())\n",
    "n_bad_months_only = int((mask_bad_months_all & ~mask_bad_winter_aug).sum())\n",
    "n_winter_aug_only = int((mask_bad_winter_aug & ~mask_bad_months_all).sum())\n",
    "\n",
    "# Apply removal\n",
    "df_pmb_clean = df_pmb_clean.loc[~mask_remove].copy()\n",
    "\n",
    "# --- Correct mislabeled winter MB ---\n",
    "mask_fix = (\n",
    "    (df_pmb_clean[\"PERIOD\"].astype(str).str.strip().str.lower() == \"winter\") &\n",
    "    (df_pmb_clean[\"MONTH_END\"] == \"06\") &\n",
    "    (df_pmb_clean[\"POINT_BALANCE\"] < 0)\n",
    ")\n",
    "n_relabel = int(mask_fix.sum())\n",
    "df_pmb_clean.loc[mask_fix, \"PERIOD\"] = \"annual\"\n",
    "\n",
    "print(\n",
    "    f\"\\nRemoved {n_total_removed} rows total.\\n\"\n",
    "    f\"  - bad-month rows removed: {n_bad_months}\\n\"\n",
    "    f\"  - winter-end-08 rows removed: {n_winter_aug}\\n\"\n",
    "    f\"  - overlap (counted in both above): {n_overlap}\\n\"\n",
    "    f\"  - bad-month only: {n_bad_months_only}\\n\"\n",
    "    f\"  - winter-end-08 only: {n_winter_aug_only}\\n\"\n",
    "    f\"Relabeled winter -> annual: {n_relabel}\"\n",
    ")\n",
    "\n",
    "print_months(df_pmb_clean, \"After filtering + relabeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv:\n",
    "df_pmb_clean.to_csv(os.path.join(cfg.dataPath, path_PMB_WGMS_csv,\n",
    "                                 'ICE_wgms_dataset_all.csv'),\n",
    "                    index=False)\n",
    "\n",
    "# Histogram of mass balance\n",
    "df_pmb_clean['POINT_BALANCE'].hist(bins=20)\n",
    "plt.xlabel('Mass balance [m w.e.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MassBalanceMachine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
